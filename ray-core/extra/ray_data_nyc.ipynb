{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45343450-2bc8-4afe-b704-94790c5e81e6",
   "metadata": {},
   "source": [
    "## Processing NYC taxi data using Ray Datasets¶\n",
    "\n",
    "The [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) is a popular tabular dataset. In this example, we demonstrate some basic data processing on this dataset using Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93f6eb-d804-471d-8cfa-38a6892a0edd",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "This tutorial will cover:\n",
    "\n",
    "* Reading Parquet data\n",
    "* Inspecting the metadata and first few rows of a large Ray Dataset\n",
    "* Calculating some common global and grouped statistics on the datase\n",
    "* Dropping columns and rows\n",
    "* Adding a derived column\n",
    "* Shuffling the dataset\n",
    "* Sharding the dataset and feeding it to parallel consumers (trainers)\n",
    "* Applying batch (offline) inference to the data\n",
    "\n",
    "Let's get started by importing some modules and starting our Ray\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c10a44-1423-4138-af2c-d2c5239d442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, random\n",
    "import ray\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a52bc0d-8688-4115-9f5b-b17113e0725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699ba6dd-e2be-4de8-9199-7e165caf2770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.13', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-15_07-53-19_846686_70344/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-15_07-53-19_846686_70344/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-06-15_07-53-19_846686_70344', 'metrics_export_port': 62450, 'gcs_address': '127.0.0.1:61005', 'address': '127.0.0.1:61005', 'node_id': '0b5d84b6c00e053ad8a9fda7ea8809ca78f83a693e8d2f3d342c2df9'})\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ctx = ray.init(logging_level=logging.ERROR)\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a98c37-9442-4921-ad6b-dc2efa2ef627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard url: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dashboard url: http://{ctx.address_info['webui_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f24c02-f6b1-471d-a1d2-92916d01bbf7",
   "metadata": {},
   "source": [
    "### Reading and Inspecting the Data\n",
    "\n",
    "Next, we read a few of the files from the dataset. This read is semi-lazy, where reading of the first file is eagerly executed, but reading of all other files is delayed until the underlying data is needed by downstream operations (e.g. consuming the data with `ds.take()`, or transforming the data with `ds.map_batches())`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f149bf3-9f97-4fb5-a632-2619050d5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read two Parquet files in parallel.\n",
    "ds = ray.data.read_parquet([\n",
    "    \"s3://ursa-labs-taxi-data/2009/01/data.parquet\",\n",
    "    \"s3://ursa-labs-taxi-data/2009/02/data.parquet\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68ddbc-fb38-41b6-ae37-f3e2ca22e1ac",
   "metadata": {},
   "source": [
    "We can easily inspect the schema of this dataset. For Parquet files, we don’t even have to read the actual data to get the schema; we can read it from the lightweight Parquet metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add06aff-f36a-4008-a577-88b35ca2f8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_id: string\n",
       "pickup_at: timestamp[us]\n",
       "dropoff_at: timestamp[us]\n",
       "passenger_count: int8\n",
       "trip_distance: float\n",
       "pickup_longitude: float\n",
       "pickup_latitude: float\n",
       "rate_code_id: null\n",
       "store_and_fwd_flag: string\n",
       "dropoff_longitude: float\n",
       "dropoff_latitude: float\n",
       "payment_type: string\n",
       "fare_amount: float\n",
       "extra: float\n",
       "mta_tax: float\n",
       "tip_amount: float\n",
       "tolls_amount: float\n",
       "total_amount: float\n",
       "-- schema metadata --\n",
       "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2527"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ab540-c6bb-4c8f-95f1-612136c16d86",
   "metadata": {},
   "source": [
    "Parquet even stores the number of rows per file in the Parquet metadata, so we can get the number of rows in `ds` without triggering a full data read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4c1310-e66a-473f-8256-ea6589ba0f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27472535"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a262a81-bcc2-4ed9-bab9-6580bac41d7e",
   "metadata": {},
   "source": [
    "We can get a nice, cheap summary of the `Dataset` by leveraging it’s informative repr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7cb0f7-a76c-43fb-9221-0079a300ddb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=2, num_rows=27472535, schema={vendor_id: string, pickup_at: timestamp[us], dropoff_at: timestamp[us], passenger_count: int8, trip_distance: float, pickup_longitude: float, pickup_latitude: float, rate_code_id: null, store_and_fwd_flag: string, dropoff_longitude: float, dropoff_latitude: float, payment_type: string, fare_amount: float, extra: float, mta_tax: float, tip_amount: float, tolls_amount: float, total_amount: float})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some metadata about the dataset.\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d75cc4-bf16-45fe-91ee-c17c112b05dc",
   "metadata": {},
   "source": [
    "We can also poke at the actual data, taking a peek at a single row. Since this is only returning a row from the first file, reading of the second file is not triggered yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae9341a2-5cb6-4672-bd2d-fe1220c58d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'vendor_id': 'VTS',\n",
       "           'pickup_at': datetime.datetime(2009, 1, 4, 2, 52),\n",
       "           'dropoff_at': datetime.datetime(2009, 1, 4, 3, 2),\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 2.630000114440918,\n",
       "           'pickup_longitude': -73.99195861816406,\n",
       "           'pickup_latitude': 40.72156524658203,\n",
       "           'rate_code_id': None,\n",
       "           'store_and_fwd_flag': None,\n",
       "           'dropoff_longitude': -73.99380493164062,\n",
       "           'dropoff_latitude': 40.6959228515625,\n",
       "           'payment_type': 'CASH',\n",
       "           'fare_amount': 8.899999618530273,\n",
       "           'extra': 0.5,\n",
       "           'mta_tax': None,\n",
       "           'tip_amount': 0.0,\n",
       "           'tolls_amount': 0.0,\n",
       "           'total_amount': 9.399999618530273})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8a17a-5bd4-45fc-aca5-b21ab475d4b2",
   "metadata": {},
   "source": [
    "To get a better sense of the data size, we can calculate the size in bytes of the full dataset. Note that for Parquet files, this size-in-bytes will be pulled from the Parquet metadata (not triggering a data read) and will therefore be the on-disk size of the data; this might be significantly smaller than the in-memory size!\n",
    "\n",
    "**Note**: Datasets will only read one file eagerly, which allows us to inspect a subset of the data without having to read the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eedbf6e-eb9c-43f2-ab20-e147305120a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897130464"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.size_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34e797-ef9b-4450-8021-3e9cc7a5efe7",
   "metadata": {},
   "source": [
    "In order to get the in-memory size, we can trigger full reading of the dataset and inspect the size in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44cbe795-5907-49b3-ad14-d972d36858cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2263031675"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.fully_executed().size_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3591ef0-50e1-4d69-9ebc-2354eee97731",
   "metadata": {},
   "source": [
    "### Data Exploration and Cleaning¶\n",
    "\n",
    "Let’s calculate some stats to get a better picture of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c5c29c-204d-4c92-9797-b178e5deeaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:27<00:00, 13.69s/it]\n",
      "Shuffle Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.09it/s]\n",
      "Shuffle Reduce: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 133.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArrowRow({'max(trip_distance)': 50.0,\n",
       "          'max(tip_amount)': 100.0,\n",
       "          'max(passenger_count)': 113})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the longest trip distance, largest tip amount, and most number of passengers?\n",
    "ds.max([\"trip_distance\", \"tip_amount\", \"passenger_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815f454-30f1-4666-90e4-07654a6eb8a1",
   "metadata": {},
   "source": [
    "Whoa, looking at the results above, there was a trip with **113** people in the taxi!? Let’s check out these kind of many-passenger records by filtering to just these records using our `ds.map_batches()` batch mapping API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51b4f3-1fc2-468d-818c-f10c4e269673",
   "metadata": {},
   "source": [
    "**Note**: Our filtering UDF receives a Pandas DataFrame, which is the default batch format for tabular data, and returns a Pandas DataFrame, which keeps the Dataset in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd249e5-a1d7-4b2f-a295-b239a12fc183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:33<00:00, 16.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'vendor_id': 'VTS',\n",
       "            'pickup_at': Timestamp('2009-01-22 11:47:00'),\n",
       "            'dropoff_at': Timestamp('2009-01-22 12:00:00'),\n",
       "            'passenger_count': 113,\n",
       "            'trip_distance': 0.0,\n",
       "            'pickup_longitude': 3555.912841796875,\n",
       "            'pickup_latitude': 935.5253295898438,\n",
       "            'rate_code_id': None,\n",
       "            'store_and_fwd_flag': None,\n",
       "            'dropoff_longitude': -74.01129913330078,\n",
       "            'dropoff_latitude': 1809.957763671875,\n",
       "            'payment_type': 'CASH',\n",
       "            'fare_amount': 13.300000190734863,\n",
       "            'extra': 0.0,\n",
       "            'mta_tax': nan,\n",
       "            'tip_amount': 0.0,\n",
       "            'tolls_amount': 0.0,\n",
       "            'total_amount': 13.300000190734863})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whoa, 113 passengers? I need to see this record and other ones with lots of passengers.\n",
    "ds.map_batches(lambda df: df[df[\"passenger_count\"] > 10]).take()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0adf786-3ba7-4011-98f6-c9d0345d8dbf",
   "metadata": {},
   "source": [
    "That seems weird, probably bad data, or at least data points that I’m not interested in. We should filter these out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bd7f1b7-ec9d-40d1-bb65-e7a7758ea077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:05<00:00, 32.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# Filter out all records with over 10 passengers.\n",
    "ds = ds.map_batches(lambda df: df[df[\"passenger_count\"] <= 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07c5da-e866-4698-abc0-4189e8db3f2a",
   "metadata": {},
   "source": [
    "We don’t have any use for the store_and_fwd_flag or mta_tax columns, so let’s drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30504e9e-e413-45e7-b260-72e2b61c4e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches:  50%|██████████████████████████████████████████████████████████████████████████▌                                                                          | 1/2 [00:24<00:00,  7.15it/s]\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 3894 MiB, 9 objects, write throughput 497 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "Map_Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:47<00:00, 24.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# Drop some columns.\n",
    "ds = ds.map_batches(lambda df: df.drop(columns=[\"store_and_fwd_flag\", \"mta_tax\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a1881-0542-46a7-a0b3-b11049ddc804",
   "metadata": {},
   "source": [
    "Let’s say we want to know how many trips there are for each passenger count. This can be done using `.groupby()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c574cbe8-0687-45af-b863-a7f564aee8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sort Sample:  50%|██████████████████████████████████████████████████████████████████████████▌                                                                          | 1/2 [00:03<00:00,  6.57it/s]\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 4760 MiB, 10 objects, write throughput 546 MiB/s.\n",
      "Sort Sample: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.64s/it]\n",
      "Shuffle Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [10:34<00:00, 317.41s/it]\n",
      "Shuffle Reduce: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 103.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'passenger_count': -127, 'count()': 2}),\n",
       " ArrowRow({'passenger_count': -48, 'count()': 45}),\n",
       " ArrowRow({'passenger_count': 0, 'count()': 794}),\n",
       " ArrowRow({'passenger_count': 1, 'count()': 18634337}),\n",
       " ArrowRow({'passenger_count': 2, 'count()': 4503747}),\n",
       " ArrowRow({'passenger_count': 3, 'count()': 1196381}),\n",
       " ArrowRow({'passenger_count': 4, 'count()': 559279}),\n",
       " ArrowRow({'passenger_count': 5, 'count()': 2452176}),\n",
       " ArrowRow({'passenger_count': 6, 'count()': 125773})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby(\"passenger_count\").count().take()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff518d-1698-491f-8925-bea34af06676",
   "metadata": {},
   "source": [
    "Again, it looks like there are some more nonsensical passenger counts, i.e., the negative ones. Let’s filter those out too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dd535f6-8571-41dc-b47a-485058b21c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:46<00:00, 23.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# Filter out all records with over 10 passengers.\n",
    "ds = ds.map_batches(lambda df: df[df[\"passenger_count\"] <= 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ef01f-002b-4618-b57c-19eb5ad7a7db",
   "metadata": {},
   "source": [
    "### Projection (selection) and Filter Pushdown\n",
    "\n",
    "Note that Ray Datasets’ Parquet reader supports projection (column selection) and row filter pushdown, where we can push the above column selection and the row-based filter to the Parquet read. If we specify column selection at Parquet read time, the unselected columns won’t even be read from disk!\n",
    "\n",
    "The row-based filter is specified via [Arrow’s dataset field expressions](https://arrow.apache.org/docs/6.0/python/generated/pyarrow.dataset.Expression.html#pyarrow.dataset.Expression). See the [feature guide for reading Parquet data](https://docs.ray.io/en/master/data/creating-datasets.html#dataset-supported-file-formats) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56526a47-c7e6-478e-b341-76b24ed04f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.21s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=2, num_rows=27471693, schema={passenger_count: int8, trip_distance: float})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only read the passenger_count and trip_distance columns.\n",
    "import pyarrow as pa\n",
    "\n",
    "filter_expr = (\n",
    "    (pa.dataset.field(\"passenger_count\") <= 10)\n",
    "    & (pa.dataset.field(\"passenger_count\") > 0)\n",
    ")\n",
    "\n",
    "pushdown_ds = ray.data.read_parquet(\n",
    "    [\n",
    "        \"s3://ursa-labs-taxi-data/2009/01/data.parquet\",\n",
    "        \"s3://ursa-labs-taxi-data/2009/02/data.parquet\",\n",
    "    ],\n",
    "    columns=[\"passenger_count\", \"trip_distance\"],\n",
    "    filter=filter_expr,\n",
    ")\n",
    "\n",
    "# Force full execution of both of the file reads.\n",
    "pushdown_ds = pushdown_ds.fully_executed()\n",
    "pushdown_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2a8d3dc-54e1-4b65-9753-5380d08ef391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the pushdown dataset. Deleting the Dataset object\n",
    "# will release the underlying memory in the cluster. This was only for ilustration of pushdown functionality.\n",
    "del pushdown_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca47d16-5bed-44de-b73d-a9dadfb25474",
   "metadata": {},
   "source": [
    "Do the passenger counts influences the typical trip distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9bb3130-b860-48dd-b7e3-5cbc0cb4bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sort Sample: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.19s/it]\n",
      "Shuffle Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [10:39<00:00, 319.95s/it]\n",
      "Shuffle Reduce: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 213.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'passenger_count': -127,\n",
       "           'mean(trip_distance)': 0.7800000160932541}),\n",
       " ArrowRow({'passenger_count': -48, 'mean(trip_distance)': 0.0}),\n",
       " ArrowRow({'passenger_count': 0, 'mean(trip_distance)': 1.772506297863926}),\n",
       " ArrowRow({'passenger_count': 1, 'mean(trip_distance)': 2.5442271984282017}),\n",
       " ArrowRow({'passenger_count': 2, 'mean(trip_distance)': 2.701997813992574}),\n",
       " ArrowRow({'passenger_count': 3, 'mean(trip_distance)': 2.624621515664268}),\n",
       " ArrowRow({'passenger_count': 4, 'mean(trip_distance)': 2.6351745332066048}),\n",
       " ArrowRow({'passenger_count': 5, 'mean(trip_distance)': 2.628660744359485}),\n",
       " ArrowRow({'passenger_count': 6, 'mean(trip_distance)': 2.5804354108726586})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean trip distance grouped by passenger count.\n",
    "ds.groupby(\"passenger_count\").mean(\"trip_distance\").take()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bad57-12cd-483b-8b35-7d456a7c64e5",
   "metadata": {},
   "source": [
    "### Ingesting into Model Trainers\n",
    "\n",
    "Now that we’ve learned more about our data and we have cleaned up our dataset a bit, we now look at how we can feed this dataset into some dummy model trainers.\n",
    "\n",
    "First, let’s do a full global random shuffle of the dataset to decorrelate these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb92abae-0e4c-415e-b23b-39412ab1156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:35<00:00, 17.85s/it]\n",
      "Shuffle Reduce:   0%|                                                                                                                                                          | 0/2 [00:00<?, ?it/s]\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 8666 MiB, 18 objects, write throughput 603 MiB/s.\n",
      "Shuffle Reduce: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:45<00:00, 22.50s/it]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.random_shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43736a-41c8-4b4a-ba60-2c51f4d25f36",
   "metadata": {},
   "source": [
    "#### Create a model trainer\n",
    "\n",
    "We define a dummy `Trainer` actor, where each trainer will consume a dataset shard in batches and simulate model training.\n",
    "\n",
    "**Note**: In a real training workflow, we would feed `ds` to Ray Train, which would do this sharding and creation of training actors for us, under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "671b0aea-cff0-43ed-9445-551f9f9894d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Actor(Trainer, 83f89ffc29aa242833502a3701000000),\n",
       " Actor(Trainer, aa836dd827123523b3ae361c01000000),\n",
       " Actor(Trainer, 8578d216040edde8b16643b401000000),\n",
       " Actor(Trainer, 5e6b4dae56df4760eaf0583401000000)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ray.remote\n",
    "class Trainer:\n",
    "    def __init__(self, rank: int):\n",
    "        pass\n",
    "\n",
    "    def train(self, shard: ray.data.Dataset) -> int:\n",
    "        for batch in shard.iter_batches(batch_size=256):\n",
    "            pass\n",
    "        return shard.count()\n",
    "\n",
    "trainers = [Trainer.remote(i) for i in range(4)]\n",
    "trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ddafe5-03c2-4f2f-aaf0-ea20d4047e07",
   "metadata": {},
   "source": [
    "Next, we split the dataset into `len(trainers)` shards, ensuring that the shards are of equal size, and providing the trainer actor handles to Ray Datasets as locality hints, so Datasets can try to colocate shard data with trainers in order to decrease data movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9da65e8-e774-433d-8097-4860bd759826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset(num_blocks=1, num_rows=6868133, schema={vendor_id: object, pickup_at: datetime64[ns], dropoff_at: datetime64[ns], passenger_count: int8, trip_distance: float32, pickup_longitude: float32, pickup_latitude: float32, rate_code_id: object, dropoff_longitude: float32, dropoff_latitude: float32, payment_type: object, fare_amount: float32, extra: float32, tip_amount: float32, tolls_amount: float32, total_amount: float32}),\n",
       " Dataset(num_blocks=1, num_rows=6868133, schema={vendor_id: object, pickup_at: datetime64[ns], dropoff_at: datetime64[ns], passenger_count: int8, trip_distance: float32, pickup_longitude: float32, pickup_latitude: float32, rate_code_id: object, dropoff_longitude: float32, dropoff_latitude: float32, payment_type: object, fare_amount: float32, extra: float32, tip_amount: float32, tolls_amount: float32, total_amount: float32}),\n",
       " Dataset(num_blocks=1, num_rows=6868133, schema={vendor_id: object, pickup_at: datetime64[ns], dropoff_at: datetime64[ns], passenger_count: int8, trip_distance: float32, pickup_longitude: float32, pickup_latitude: float32, rate_code_id: object, dropoff_longitude: float32, dropoff_latitude: float32, payment_type: object, fare_amount: float32, extra: float32, tip_amount: float32, tolls_amount: float32, total_amount: float32}),\n",
       " Dataset(num_blocks=1, num_rows=6868133, schema={vendor_id: object, pickup_at: datetime64[ns], dropoff_at: datetime64[ns], passenger_count: int8, trip_distance: float32, pickup_longitude: float32, pickup_latitude: float32, rate_code_id: object, dropoff_longitude: float32, dropoff_latitude: float32, payment_type: object, fare_amount: float32, extra: float32, tip_amount: float32, tolls_amount: float32, total_amount: float32})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shards = ds.split(n=len(trainers), equal=True, locality_hints=trainers)\n",
    "shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed0280-6166-4d6a-9938-cd31c6eaaadc",
   "metadata": {},
   "source": [
    "Finally, we simulate training, passing each shard to the corresponding trainer. The number of rows per shard is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "448e907c-34aa-412b-b086-c5772d1efb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6868133, 6868133, 6868133, 6868133]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get([w.train.remote(s) for w, s in zip(trainers, shards)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d3a6afd-b27c-451e-96b9-55b729639338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete trainer actor handle references, which should terminate the actors.\n",
    "del trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7691d1-28b6-49ed-aac1-36839de61571",
   "metadata": {},
   "source": [
    "#### Parallel Batch Inference\n",
    "\n",
    "After we’ve trained a model, we may want to perform batch (offline) inference on such a tabular dataset. With Ray Datasets, this is as easy as a `ds.map_batches()` call!\n",
    "\n",
    "First, we define a callable class that will cache the loading of the model in its constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63eca396-d798-400c-bcbb-056988434c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_model():\n",
    "    # A dummy model.This could be loaded from a model registry or checkpoint\n",
    "    def model(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\"score\": batch[\"passenger_count\"] % 2 == 0})\n",
    "    \n",
    "    return model\n",
    "\n",
    "class BatchInferModel:\n",
    "    def __init__(self):\n",
    "        self.model = load_model()\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbaff3-d4cc-4f22-8dc1-b3df4e6f58b9",
   "metadata": {},
   "source": [
    "`BatchInferModel`’s constructor will only be called once per actor worker when using the actor pool compute strategy in `ds.map_batches()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cdc43e9-b4f0-4b3a-9d95-268cd2ac466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress (8 actors 2 pending):  50%|███████████████████████████████████████████████████████████████▌                                                               | 1/2 [00:10<00:10, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +5h19m41s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress (8 actors 2 pending): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:20<00:00, 10.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'score': False}),\n",
       " PandasRow({'score': True}),\n",
       " PandasRow({'score': True}),\n",
       " PandasRow({'score': False}),\n",
       " PandasRow({'score': False})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.map_batches(BatchInferModel, batch_size=2048, compute=\"actors\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b1934-09ed-421d-9234-cc52d155c659",
   "metadata": {},
   "source": [
    "#### Auto scaling batch inferences \n",
    "\n",
    "We can also configure the autoscaling actor pool that this inference stage uses, setting upper and lower bounds on the actor pool size, and even tweak the batch prefetching vs. inference task queueing tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9137b2d6-3ff9-48df-a1a2-394b6c4e28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress (8 actors 0 pending): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:03<00:00, 61.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'score': False}),\n",
       " PandasRow({'score': True}),\n",
       " PandasRow({'score': True}),\n",
       " PandasRow({'score': False}),\n",
       " PandasRow({'score': False})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data import ActorPoolStrategy\n",
    "\n",
    "# The actor pool will have at least 2 workers and at most 8 workers.\n",
    "strategy = ActorPoolStrategy(min_size=2, max_size=8)\n",
    "\n",
    "ds.map_batches(\n",
    "    BatchInferModel,\n",
    "    batch_size=256,\n",
    "    #num_gpus=1,  # Uncomment this to run this on GPUs!\n",
    "    compute=strategy,\n",
    ").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8fe93-2f70-43d7-aaa2-212f916cd763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
