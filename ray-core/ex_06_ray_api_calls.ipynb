{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Ray API Calls\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "\n",
    "This lesson explores a few of the other API calls you might find useful, as well as options that can be used with the API calls we've already learned. Additionally, we will walk through some tips and tricks for first time users.\n",
    "\n",
    "> **Tip:** The [Ray Package Reference](https://docs.ray.io/en/latest/package-ref.html) in the [Ray Docs](https://docs.ray.io/en/latest/) is useful for exploring the API features we'll learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, time, sys, logging\n",
    "import numpy as np \n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.8.13', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-05-24_17-27-19_715650_95151/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-05-24_17-27-19_715650_95151/sockets/raylet', 'webui_url': '127.0.0.1:8266', 'session_dir': '/tmp/ray/session_2022-05-24_17-27-19_715650_95151', 'metrics_export_port': 64244, 'gcs_address': '127.0.0.1:63793', 'address': '127.0.0.1:63793', 'node_id': '0402eca80ad929f1431dae0365addb2dd8b672a2d5b68b2f2afcf162'})\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "context = ray.init(logging_level=logging.ERROR)\n",
    "pprint(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard URL is printed above. Use it on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard url: http://127.0.0.1:8266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dashboard url: http://{context.address_info['webui_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ray.init()\n",
    "\n",
    "When we used [`ray.init()`](https://ray.readthedocs.io/en/latest/package-ref.html#ray.init), we used it to start Ray on our local machine. When the optional `address=...` argument is specified, the driver connects to the corresponding Ray cluster.\n",
    "\n",
    "There are a lot of optional keyword arguments you can pass to `ray.init()`. Here are some of them. All options are described in the [documentation](https://ray.readthedocs.io/en/latest/package-ref.html#ray.init). \n",
    "\n",
    "| Name | Type | Example | Description |\n",
    "| :--- | :--- | :------ | :---------- |\n",
    "| `address` | `str` | `address='auto'` | The address of the Ray cluster to connect to. If this address is not provided, then this command will start Redis, a raylet, a plasma store, a plasma manager, and some workers. It will also kill these processes when Python exits. If the driver is running on a node in a Ray cluster, using `auto` as the value tells the driver to detect the the cluster, removing the need to specify a specific node address. |\n",
    "| `num_cpus` | `int` | `num_cpus=4` | Number of CPUs the user wishes to assign to each _raylet_. |\n",
    "| `num_gpus` | `int` | `num_gpus=1` | Number of GPUs the user wishes to assign to each _raylet_. |\n",
    "| `resources` | `dictionary` | `resources={'resource1': 4, 'resource2': 16}` | Maps the names of custom resources to the quantities of those resources available. |\n",
    "| `memory` | `int` | `memory=1000000000` | The amount of memory (in bytes) that is available for use by workers requesting memory resources. By default, this is automatically set based on the available system memory. |\n",
    "| `object_store_memory` | `int` | `object_store_memory=1000000000` | The amount of memory (in bytes) for the object store. By default, this is automatically set based on available system memory, subject to a 20GB cap. |\n",
    "| `log_to_driver` | `bool` | `log_to_driver=True` | If true, then the output from all of the worker processes on all nodes will be directed to the driver program. |\n",
    "| `local_mode` | `bool` | `local_mode=True` | If true, the code will be executed serially. This is useful for debugging. |\n",
    "| `ignore_reinit_error` | `bool` | `ignore_reinit_error=True` | If true, Ray suppresses errors from calling `ray.init()` a second time (as we've done in these notebooks). Ray won't be restarted. |\n",
    "| `include_webui` | `bool` | `include_webui=False` | Boolean flag indicating whether or not to start the web UI, which displays the status of the Ray cluster. By default, or if this argument is `None`, then the UI will be started if the relevant dependencies are present. |\n",
    "| `webui_host` | _address_ | `webui_host=1.2.3.4` | The host to bind the web UI server to. Can either be `localhost` (or `127.0.0.1`) or `0.0.0.0` (available from all interfaces). By default, this is set to `localhost` to prevent access from external machines. |\n",
    "| `configure_logging` | `bool` | `configure_logging=True` | If true (default), configuration of logging is allowed here. Otherwise, the user may want to configure it separately. |\n",
    "| `logging_level` | _Flag_ | `logging_level=logging.INFO` | The logging level, defaults to `logging.INFO`. Ignored unless \"configure_logging\" is true. |\n",
    "| `logging_format` | `str` | `logging_format='...'` | The logging format to use, defaults to a string containing a timestamp, filename, line number, and message. See the Ray source code `ray_constants.py` for details. Ignored unless \"configure_logging\" is true. |\n",
    "| `temp_dir` | `str` | `temp_dir=/tmp/myray` | If provided, specifies the root temporary directory for the Ray process. Defaults to an OS-specific conventional location, e.g., `/tmp/ray`. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the documentation for [ray.shutdown()](https://ray.readthedocs.io/en/latest/package-ref.html#ray.shutdown), which is needed in some contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ray.is_initialized()\n",
    "\n",
    "Is Ray [initialized](https://ray.readthedocs.io/en/latest/package-ref.html#ray.is_initialized)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.is_initialized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @ray.remote()\n",
    "\n",
    "We've used [@ray.remote](https://ray.readthedocs.io/en/latest/package-ref.html#ray.remote) a lot. You can pass arguments when using it. Here are some of them.\n",
    "\n",
    "| Name | Type | Example | Description |\n",
    "| :--- | :--- | :------ | :---------- |\n",
    "| `num_cpus` | `int` | `num_cpus=4` | The number of CPU cores to reserve for this task or for the lifetime of the actor. |\n",
    "| `num_gpus` | `int` | `num_gpus=1` | The number of GPU cores to reserve for this task or for the lifetime of the actor. |\n",
    "| `num_returns` | `int` | `num_returns=2` | (Only for tasks, not actors.) The number of object refs returned by the remote function invocation. |\n",
    "| `runtime_env` | `map` | `runtime_env = {\"working_dir\": \".\", \"pip\": [\"requests\"]}}` | The runtime environment to use for this job (see [Runtime environments](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments) for details. |\n",
    "| `max_calls` | `int` | `max_calls=5` | Only for *remote tasks*. This specifies the maximum of times that a given worker can execute the given remote function before it must exit (this can be used to address memory leaks in third-party libraries or to reclaim resources that cannot easily be released, e.g., GPU memory that was acquired by TensorFlow). By default this is infinite. |\n",
    "| `max_restarts` | `int` | `max_restarts=-1` | Only for *actors*. This specifies the maximum number of times that the actor should be restarted when it dies unexpectedly. The minimum valid value is 0 (default), which indicates that the actor doesn't need to be restarted. A value of -1 indicates that an actor should be restarted indefinitely. |\n",
    "| `max_task_retries` | `int` | `max_task_retries=-1` | Only for *actors*. How many times to retry an actor task if the task fails due to a system error, e.g., the actor has died. If set to -1, the system will retry the failed task until the task succeeds, or the actor has reached its max_restarts limit. If set to n > 0, the system will retry the failed task up to n times, after which the task will throw a `RayActorError` exception upon `ray.get`. Note that Python exceptions are not considered system errors and will not trigger retries. |\n",
    "| `max_retries` | `int` | `max_retries=-1` | Only for *remote functions*. This specifies the maximum number of times that the remote function should be rerun when the worker process executing it crashes unexpectedly. The minimum valid value is 0, the default is 4 (default), and a value of -1 indicates infinite retries. |\n",
    "\n",
    "Here's an example with and without `num_return_vals`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a, 1, 2.2)\n",
      "(a, 1, 2.2)\n"
     ]
    }
   ],
   "source": [
    "@ray.remote(num_returns=3)\n",
    "def tuple3(one, two, three):\n",
    "    return (one, two, three)\n",
    "\n",
    "x_ref, y_ref, z_ref = tuple3.remote(\"a\", 1, 2.2)\n",
    "x, y, z = ray.get([x_ref, y_ref, z_ref])\n",
    "print(f'({x}, {y}, {z})')\n",
    "\n",
    "@ray.remote\n",
    "def tuple3(one, two, three):\n",
    "    return (one, two, three)\n",
    "\n",
    "xyz_ref = tuple3.remote(\"a\", 1, 2.2)\n",
    "x, y, z = ray.get(xyz_ref)\n",
    "print(f'({x}, {y}, {z})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @ray.method()\n",
    "\n",
    "Related to `@ray.remote()`, [@ray.method()](https://ray.readthedocs.io/en/latest/package-ref.html#ray.method) allows you to specify the number of return values for a method in an actor, by passing the `num_returns` keyword argument. None of the other `@ray.remote()` keyword arguments are allowed. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a, 1, 2.2)\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "class Tupleator:\n",
    "    @ray.method(num_returns=3)\n",
    "    def tuple3(self, one, two, three):\n",
    "        return (one, two, three)\n",
    "    \n",
    "tupleator = Tupleator.remote()\n",
    "x_ref, y_ref, z_ref = tupleator.tuple3.remote(\"a\", 1, 2.2)\n",
    "x, y, z = ray.get([x_ref, y_ref, z_ref])\n",
    "print(f'({x}, {y}, {z})')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ray.put()\n",
    "\n",
    "We used [`ray.get`](https://ray.readthedocs.io/en/latest/package-ref.html#ray.gett) a lot to retrieve objects and we used actor methods to retrieve state from an actor. You can actually put objects into the object store explicitly with [`ray.put`](https://ray.readthedocs.io/en/latest/package-ref.html#ray.put), as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object returned: Hello World!\n"
     ]
    }
   ],
   "source": [
    "ref = ray.put(\"Hello World!\")\n",
    "print(f'Object returned: {ray.get(ref)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object returned: [[0.92193964 0.851933   0.35130997 ... 0.12112416 0.20866389 0.07780157]\n",
      " [0.67046852 0.61188893 0.8130983  ... 0.03276641 0.54650956 0.38075173]\n",
      " [0.36338666 0.63422341 0.53683446 ... 0.83471161 0.6846704  0.00754574]\n",
      " ...\n",
      " [0.30825078 0.08873442 0.29099552 ... 0.45384649 0.64807062 0.01454769]\n",
      " [0.24976259 0.60217567 0.40592398 ... 0.11249347 0.58073364 0.72220796]\n",
      " [0.8478374  0.5418938  0.07727733 ... 0.35070766 0.20725226 0.15816054]]\n"
     ]
    }
   ],
   "source": [
    "ref = ray.put(np.random.rand(2_000, 5_000))\n",
    "print(f'Object returned: {ray.get(ref)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an optional flag you can pass `weakref=True` (defaults to `False`). If true, Ray is allowed to evict the object while a reference to the returned ref still exists. This is useful if you are putting a lot of objects into the object store and many of them might not be needed in the future. It allows Ray to aggressively reclaim memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Information\n",
    "\n",
    "Many methods return information:\n",
    "\n",
    "| Method | Brief Description |\n",
    "| :----- | :---------------- |\n",
    "| [`ray.get_gpu_ids()`](https://ray.readthedocs.io/en/latest/package-ref.html#ray.get_gpu_ids) | GPUs |\n",
    "| [`ray.nodes()`](https://ray.readthedocs.io/en/latest/package-ref.html#ray.nodes) | Cluster nodes |\n",
    "| [`ray.cluster_resources()`](https://ray.readthedocs.io/en/latest/package-ref.html#ray.cluster_resources) | All the available resources, used or not |\n",
    "| [`ray.available_resources()`](https://ray.readthedocs.io/en/latest/package-ref.html#ray.available_resources) | Resources not in use |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ray.get_gpu_ids():          []\n",
      "ray.nodes():                [{'NodeID': '5df18d5eeef2d75e4ddca098078a662c0a831077794089ed7482faf2', 'Alive': True, 'NodeManagerAddress': '127.0.0.1', 'NodeManagerHostname': 'Juless-MacBook-Pro-16-inch-2019', 'NodeManagerPort': 59828, 'ObjectManagerPort': 59827, 'ObjectStoreSocketName': '/tmp/ray/session_2022-04-06_17-09-07_012306_14342/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2022-04-06_17-09-07_012306_14342/sockets/raylet', 'MetricsExportPort': 60893, 'alive': True, 'Resources': {'CPU': 12.0, 'object_store_memory': 2147483648.0, 'memory': 14545055335.0, 'node:127.0.0.1': 1.0}}]\n",
      "ray.cluster_resources():    {'node:127.0.0.1': 1.0, 'memory': 14545055335.0, 'CPU': 12.0, 'object_store_memory': 2147483648.0}\n",
      "ray.available_resources():  {'CPU': 12.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 2067483391.0, 'memory': 14545055335.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "ray.get_gpu_ids():          {ray.get_gpu_ids()}\n",
    "ray.nodes():                {ray.nodes()}\n",
    "ray.cluster_resources():    {ray.cluster_resources()}\n",
    "ray.available_resources():  {ray.available_resources()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used `ray.nodes()[0]['Resources']['CPU']` in the second lesson to determine the number of CPU cores on our machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ray.nodes()[0]['Resources']['CPU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Tricks for first-time users\n",
    "First time users can trip upon certain API calls usage patterns. This short tip & triks will insure you against unexpected results. Below is a brief exploration of a handful of API calls and its best practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 1: Delay ray.get()\n",
    "\n",
    "With Ray, all invocations of `.remote()` calls are aynchronous, meaning the operation is returned immediately with a promise/future object ID. This is key to achieving massive parallelism, as it allows a devloper to launnch many remote tasks, each returning a remote futre object ID. This object ID can be fetched with `ray.get`. Because `ray.get` is a blocking call, where and how often you use can affect the performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_some_work(x):\n",
    "    time.sleep(1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad usage\n",
    "We are using `ray.get` inside a loop and blocking on each call of `.remote()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 ms, sys: 20.8 ms, total: 43.7 ms\n",
      "Wall time: 4.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "results = [ray.get(do_some_work.remote(x)) for x in range(4)]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good usage\n",
    "We delay `ray.get` after all the tasks have been invoked and their references have been returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.61 ms, sys: 6.99 ms, total: 14.6 ms\n",
      "Wall time: 1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "results = ray.get([do_some_work.remote(x) for x in range(4)])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeway tip 1: \n",
    "Since `ray.get` is a blocking call, postpone its use only when you need object ID's value. If called eagerly, it can\n",
    "affect the performance of your desired parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 2: Avoid tiny remote tasks\n",
    "Ray APIs are general and simple to use. As a result, new comers natural intinct to parallelize all tasks, including tiny small ones, which can incur the overhead overtime. In short, if the Ray remote tasks are tiny and small, they may take longer to execute than their serial Python equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_task(x):\n",
    "    time.sleep(0.0001)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 392 ms, sys: 434 ms, total: 826 ms\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = [tiny_task(x) for x in range(100000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert this into Ray remote task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def remote_tiny_task(x):\n",
    "    time.sleep(0.0001)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 8.54 s, total: 38.7 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_ids = [remote_tiny_task.remote(x) for x in range(100000)]\n",
    "results = ray.get(result_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, not only Ray didn’t improve the execution time, but the Ray program is actually slower than the sequential program! What can we do to remedy it? What's going on?\n",
    "\n",
    "Well, the issue here is that every task invocation has a non-trivial overhead (e.g., scheduling, inter-process communication, updating the system state) and this overhead dominates the actual time it takes to execute the task.\n",
    "\n",
    "One way to mitigate is to make the remote tasks \"larger\" in order to amortize invocation overhead. This is achieved by aggregating tasks into bigger chunks of 1000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def mega_work(start, end):\n",
    "    return [tiny_task(x) for x in range(start, end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.9 ms, sys: 18.7 ms, total: 63.6 ms\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_ids = []\n",
    "[result_ids.append(mega_work.remote(x*1000, (x+1)*1000)) for x in range(100)]\n",
    "results = ray.get(result_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge difference in execution time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 3: Using ray.wait() with ray.get()\n",
    "\n",
    "As we noted above, an idiomatic way of using `ray.get()` is delay fetching the object until you need them. Another way is to use with `ray.wait()` and only fetch values that already are available. \n",
    "\n",
    "Let's look at a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "@ray.remote\n",
    "def make_array(n):\n",
    "    time.sleep(n/10.0)\n",
    "    return np.random.standard_normal(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a task that can add two NumPy arrays together. The arrays need to be the same size, but we'll ignore any checking for this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def add_arrays(a1, a2):\n",
    "    time.sleep(a1.size/10.0)\n",
    "    return np.add(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `ray.wait` and `ray.get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: []\n",
      "10: [-1.7781109   0.88573117  0.86475269 -4.57406466 -5.06717503 -2.29265262\n",
      "  2.86728976 -1.06289166  1.24273203  2.59255893]\n",
      "20: [-0.53915161 -0.34215306  2.24751532  2.20941352 -1.12683093 -1.27652722\n",
      "  0.87340067 -0.40808127 -1.38325214 -3.54937726  2.68672585  1.05317156\n",
      " -0.86190813 -2.7779964  -1.15920784 -0.06553598 -0.01776134  2.06111113\n",
      " -1.31447923  2.34664896]\n",
      "30: [ 2.86070977  1.54727811  0.27623417 -0.92417809 -4.9424149   3.51745492\n",
      "  1.92693489  2.55436313 -0.30649244 -2.15913155  0.46628628  2.94806825\n",
      "  1.96059157  2.93584918 -1.73202179 -0.1542359   1.08681397 -1.53762225\n",
      " -1.59247656 -0.76548422 -3.43715225 -1.23557788  3.51236954 -0.16132308\n",
      "  1.06301097 -1.12866998 -0.63942135 -1.12765888 -1.34416691 -3.07318492]\n",
      "40: [-1.60077828 -1.22828887  0.03049364 -1.18360244  3.99246954 -0.09282656\n",
      "  0.30693929  0.38435625 -7.96970459  2.32797177 -1.07742711  0.65223155\n",
      " -0.70776815 -0.76976502 -1.82593219 -1.10776212 -2.02381039 -0.23749677\n",
      "  2.05909044  1.54057844  0.92882461  2.70242934 -2.60292047 -2.21239759\n",
      " -1.44472026  3.27378877 -1.72084532  0.67442813  0.80795243 -1.87657919\n",
      "  2.51010026  0.1147937   1.92312501  2.69815765 -3.32524411 -1.43783347\n",
      "  4.2584812   1.84273694 -0.83177466  1.29427387]\n",
      "50: [ 4.15369839 -0.10216928 -3.60685348  0.41727837  0.28710965 -0.44209254\n",
      "  4.70319332  1.47670719 -1.5765868  -0.66781107  1.64569098  0.98878573\n",
      " -0.95399519 -2.80102391 -0.85347813 -3.50093224 -3.6230844   0.69432853\n",
      "  0.8897488  -1.44413281  1.24235572  1.72516417  1.78182794  0.02837645\n",
      " -2.44223632 -0.87880466  0.26493952 -1.12293056 -1.96130201  3.79966633\n",
      "  3.37151741 -2.27555937 -0.66581819  1.79767797  4.16334547  0.39877439\n",
      "  0.88542259 -1.60472822 -0.53386358  1.12903069 -0.72920723 -0.64208183\n",
      " -1.90125664  2.9272626   1.45444774 -0.40470815 -0.80265091  2.12948629\n",
      " -0.61001211 -1.3020379 ]\n",
      "CPU times: user 70 ms, sys: 56.1 ms, total: 126 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "array_refs = [make_array.remote(n*10) for n in range(6)]\n",
    "added_array_refs = [add_arrays.remote(ref, ref) for ref in array_refs]\n",
    "\n",
    "arrays = []\n",
    "waiting_refs = list(added_array_refs)  # Assign a working list to the full list of refs\n",
    "while len(waiting_refs) > 0:           # Loop until all tasks have completed\n",
    "    # Call ray.wait with:\n",
    "    #   1. the list of refs we're still waiting to complete,\n",
    "    #   2. tell it to return immediately as soon as one of them completes,\n",
    "    #   3. tell it wait up to 10 seconds before timing out.\n",
    "    ready_refs, remaining_refs = ray.wait(waiting_refs, num_returns=2, timeout=10.0)\n",
    "    new_arrays = ray.get(ready_refs)\n",
    "    arrays.extend(new_arrays)\n",
    "    for array in new_arrays:\n",
    "        print(f'{array.size}: {array}')\n",
    "    waiting_refs = remaining_refs  # Reset this list; don't include the completed refs in the list again!\n",
    "    \n",
    "# print(f\"\\nall arrays: {arrays}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework \n",
    "\n",
    "Read some more [tricks and tips](https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html) in the documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
