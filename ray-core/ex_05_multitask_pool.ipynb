{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Tour of Ray Core: Multiprocessing Pool\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Distributed multiprocessing.Pool*](https://docs.ray.io/en/latest/multiprocessing.html) makes it easy to scale existing Python applications that use [`multiprocessing.Pool`](https://docs.python.org/3/library/multiprocessing.html) by leveraging *Ray Actors*. Ray supports running distributed python programs with the **multiprocessing.Pool** API using Ray Actors, each running on a [workder node](https://docs.ray.io/en/latest/ray-core/actors.html#faq-actors-workers-and-resources), instead of local processes. This makes it easy to scale existing applications that use `multiprocessing.Pool` from a single node to a cluster.\n",
    "\n",
    "<img src=\"images/dist_multi_pool.png\" width=\"80%\" height=\"55%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import time\n",
    "import logging\n",
    "import ray\n",
    "from ray.util.multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing Pool example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a simple Python function with a slight delay added (to make it behave like a more complex calculation)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be some complicated and compute intensive task\n",
    "def func(x):\n",
    "    time.sleep(1.5)\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    for divisor in range(2, int(n ** 0.5) + 1):\n",
    "        if n % divisor == 0:\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the Ray's drop-in replacement for [multiprocessing pool](https://docs.ray.io/en/latest/multiprocessing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.12', ray_version='1.12.0', ray_commit='f18fc31c7562990955556899090f8e8656b48d2d', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-05-20_09-37-51_403506_16545/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-05-20_09-37-51_403506_16545/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-05-20_09-37-51_403506_16545', 'metrics_export_port': 63960, 'gcs_address': '127.0.0.1:63855', 'address': '127.0.0.1:63855', 'node_id': 'cb62157f82b601209b5790d701b56a3067dd1e298c216d6bf20bf7f9'})\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "context = ray.init(logging_level=logging.ERROR)\n",
    "pprint(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard url: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dashboard url: http://{context.address_info['webui_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a *Pool* using and distribute its tasks across a cluster (or across the available cores on a laptop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "CPU times: user 48.8 ms, sys: 28.3 ms, total: 77.2 ms\n",
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pool = Pool()\n",
    "\n",
    "for result in pool.map(func, range(10)):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed version has the trade-off of increased overhead, although now it can scale-out horizontally across a cluster. The benefits would be more pronounced with a more computationally expensive calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2000000\n",
    "lst = list(range(num))\n",
    "results = []\n",
    "pool = Pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All primes in 2000000 are 148935\n",
      "CPU times: user 7.41 s, sys: 147 ms, total: 7.56 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for result in pool.map(is_prime, lst):\n",
    "    results.append(result)\n",
    "print(f\"Total number of primes in {num} are {sum(results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a compute intensive class that does some matrix\n",
    "computation. Consider this could be a compute intenstive task\n",
    "doing massive tensor transformation or computation.\n",
    "\n",
    "**NOTE**: This will your excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(n):\n",
    "    # Simulate a long intensive task\n",
    "    #TODO\n",
    "    \n",
    "    # do some matrix computation \n",
    "    # and return results\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Ray remote task that launches `task()` across a pool of Actors on the cluster. It creates\n",
    "a pool of `Ray Actors`, each scheduled on a cluster worker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def launch_long_running_tasks(num_pool):\n",
    "    # Doing the work, collecting data, updating the database\n",
    "    # create an Actor pool of num_pool workers nodes\n",
    "    pool = Pool(num_pool)\n",
    "    results = []\n",
    "    # Iterate over 50 times in batches of 10\n",
    "    for result in pool.map(func, range(1, 50, 10)):\n",
    "        results.append(result)\n",
    "        \n",
    "    # Done so terminate pool\n",
    "    pool.terminate()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Actor like supervisor that launches all these remote tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class LaunchDistributedTasks:\n",
    "    def __init__(self, limit=5):\n",
    "        self._limit = limit\n",
    "\n",
    "    def launch(self):\n",
    "        # launch the remote task\n",
    "        return launch_long_running_tasks.remote(self._limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch our supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched remote jobs\n"
     ]
    }
   ],
   "source": [
    "hdl = LaunchDistributedTasks.remote(5)\n",
    "print(\"Launched remote jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " list of results :[1, 121, 441, 961, 1681]\n",
      " Total results: 5\n"
     ]
    }
   ],
   "source": [
    "values = ray.get(ray.get(hdl.launch.remote()))\n",
    "print(f\" list of results :{values}\")\n",
    "print(f\" Total results: {len(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercises\n",
    "\n",
    "1. Can you convert `task()` into a complicated function?\n",
    "2. Use `task()` in `pool.map(task,....)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Write a Python `multiprocessing.pool` version of `task()` and compare the timings with \n",
    "the Ray distributed `multiprocessing.pool`. \n",
    "2. Do you see a difference in timings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
