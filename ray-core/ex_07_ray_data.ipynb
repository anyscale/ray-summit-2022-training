{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89505bc4-70e5-4ed5-9b91-f75804ce40db",
   "metadata": {},
   "source": [
    "# Gentle introduction to Ray datasets APIs\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this introductory tutorial you will learn to:\n",
    " * create, transform, read, and save Ray datasets\n",
    " * use shards for parallel processing of large datasets\n",
    " * understand datapipelines and their merits\n",
    " * use `DatasetPipeline` for last-mile ML ingestion for distributed training\n",
    " * why use datasets and for what\n",
    "\n",
    "### Overview\n",
    "\n",
    "This is a brief and gentle introduction to Ray's native library `ray dataset`. As a native Ray library, built atop Ray, it allows you to exchange data among Ray tasks, actors, libraries, and applications. \n",
    "\n",
    "Ray Datasets, using distributed Apache Arrow, are designed to load and preprocess data for distributed ML training pipelines. Compared to other loading solutions, Datasets are more flexible (e.g., you can express higher-quality per-epoch global shuffles) and provides higher overall performance.\n",
    "\n",
    "Additionally, Ray datasets provides standard transformations like `map`, `filter`, and `partition`. Ray datasets is *not* a replacement for a full-fledged data processing library for EDA, ETL or a subsitute for Apache Spark or Dask or Pandas DataFrames. Its primary objective is the last-mile rudimentary distributed data preprocessing and data ingestion for ML training.\n",
    "\n",
    "Supporting myriad [file formats and data sources](https://docs.ray.io/en/latest/data/dataset.html#datasource-compatibility), you can read from and write to local FS and cloud storage. \n",
    "\n",
    "<img src=\"images/dataset.png\" width=\"70%\" height=\"35%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca802b-f3d5-4227-b43c-08fd14878021",
   "metadata": {},
   "source": [
    "### Ray Datasets\n",
    "\n",
    "A Ray dataset implements a distributed [Apache Arrow](https://arrow.apache.org/). A Dataset consists of a list of Ray object references to blocks. Each block holds a set of items in either an [Arrow table](https://arrow.apache.org/docs/python/data.html#tables) or a Python list (for Arrow incompatible objects).\n",
    "\n",
    "<img src=\"images/dataset-arch.png\" width=\"70%\" height=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273919e-5310-45ff-8ec7-7a1db65a4a93",
   "metadata": {},
   "source": [
    "### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048e1661-7411-4ff2-8752-61a564b3987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, random\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c79e3c7-feba-4d77-afc2-c29ae62a4736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.13', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-13_13-52-22_433238_66760/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-13_13-52-22_433238_66760/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-06-13_13-52-22_433238_66760', 'metrics_export_port': 63271, 'gcs_address': '127.0.0.1:65094', 'address': '127.0.0.1:65094', 'node_id': '1a56f25a72880e3d81d1a9f6671ac41e2ed901438109ed839be8ba3c'})\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ctx = ray.init(logging_level=logging.ERROR)\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0cf1de-71d7-478e-a13e-3c53ae0ab115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard url: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dashboard url: http://{ctx.address_info['webui_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdbd1d-7083-44b6-918d-de1312af52f2",
   "metadata": {},
   "source": [
    "Let's create a generic dataset of 100K integers and look at the schema and underlying datatype. The difference between `show` and `take` is that the former takes one item at time and prints it, while the latter iterates over row items from the dataset, appends to a list and returns it. Underneath, `ds.show()` calls `ds.take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2460a28-e0dd-4745-b52b-2a66c74c48e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray.data.range(100_000)\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c44168c-e450-4d87-817c-e3513fda6f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da5a1b99-4cd1-4908-8c19-43d2bc35c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da31060e-b56c-4e9b-98d3-e935257989e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79f73a-b7a5-42f6-bfab-e0db44c13866",
   "metadata": {},
   "source": [
    "Let's create a synthetic dataset of Arrow records (750K) with several columns and data associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b84110-bcc8-4ea3-bdf2-ff6031caf6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'ssn': None,\n",
       "  'name': None,\n",
       "  'amount': 1.5,\n",
       "  'interest': 0.2,\n",
       "  'state': 'WA',\n",
       "  'marital_status': 'divorced',\n",
       "  'property': 'house',\n",
       "  'dependents': 2,\n",
       "  'defaulted': 1,\n",
       "  'gender': 'F'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATES = [\"CA\", \"AZ\", \"OR\", \"WA\", \"TX\", \"UT\"]\n",
    "M_STATUS = [\"married\", \"single\", \"domestic\", \"divorced\", \"undeclared\"]\n",
    "GENDER = [\"F\", \"M\", \"U\"]\n",
    "HOME_OWNER = [\"condo\", \"house\", \"rental\"]\n",
    "\n",
    "items = [{\"id\": i,\n",
    "          \"ssn\": None,\n",
    "          \"name\": None,\n",
    "          \"amount\": i * 1.5, \n",
    "          \"interest\": random.randint(1,5) * .1,\n",
    "          \"state\": random.choice(STATES),\n",
    "          \"marital_status\": random.choice(M_STATUS),\n",
    "          \"property\": random.choice(HOME_OWNER),\n",
    "          \"dependents\": random.randint(1, 5),\n",
    "          \"defaulted\": random.randint(0,1),\n",
    "          \"gender\":random.choice(GENDER) } for i in range(1,850_001)]\n",
    "items[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ba080-9860-46d0-a7b0-0475a0886b73",
   "metadata": {},
   "source": [
    "### Creating a dataset from list of dictionary items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a5f028-aa5b-4e40-9d89-c2458eae5a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=200, num_rows=850000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds = ray.data.from_items(items)\n",
    "arrow_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a52903-b132-4c7a-9920-274d5620a3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82e6d3d5-104d-40c6-be29-92b513d8abd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'ssn': None, 'name': None, 'amount': 1.5, 'interest': 0.2, 'state': 'WA', 'marital_status': 'divorced', 'property': 'house', 'dependents': 2, 'defaulted': 1, 'gender': 'F'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "169d1e5c-4d22-4ec8-98e7-a98e8c977b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.schema of Dataset(num_blocks=200, num_rows=850000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf52fcd-041c-4008-93db-e86cd8f6fd1c",
   "metadata": {},
   "source": [
    "### Saving datasets as a parquet file\n",
    "Ray datasets support myriad data formats and public storage. Let's save this dataset as a parquet file and create `N` partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3dbf30b-79dc-41ee-8aae-204b0fa8cd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.22it/s]\n",
      "Write Progress: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.04it/s]\n"
     ]
    }
   ],
   "source": [
    "arrow_ds.repartition(5).write_parquet(\"data/interest.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ca6b26-4a08-44a7-8b9b-5a7582945384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 43904\n",
      "-rw-r--r--  1 jules  staff  2139612 May 26 16:08 23b1cf0e40f24eafa473e43ee2ae774e_000000.parquet\n",
      "-rw-r--r--  1 jules  staff  2121423 May 26 16:08 23b1cf0e40f24eafa473e43ee2ae774e_000001.parquet\n",
      "-rw-r--r--  1 jules  staff  2119621 May 26 16:08 23b1cf0e40f24eafa473e43ee2ae774e_000002.parquet\n",
      "-rw-r--r--  1 jules  staff  2119252 May 26 16:08 23b1cf0e40f24eafa473e43ee2ae774e_000003.parquet\n",
      "-rw-r--r--  1 jules  staff  2169571 May 26 16:08 23b1cf0e40f24eafa473e43ee2ae774e_000004.parquet\n",
      "-rw-r--r--  1 jules  staff  2344964 Jun 13 14:00 a8ee48d8bc24428695d0eac5b8f6fbaa_000000.parquet\n",
      "-rw-r--r--  1 jules  staff  2325837 Jun 13 14:00 a8ee48d8bc24428695d0eac5b8f6fbaa_000001.parquet\n",
      "-rw-r--r--  1 jules  staff  2323989 Jun 13 14:00 a8ee48d8bc24428695d0eac5b8f6fbaa_000002.parquet\n",
      "-rw-r--r--  1 jules  staff  2323692 Jun 13 14:00 a8ee48d8bc24428695d0eac5b8f6fbaa_000003.parquet\n",
      "-rw-r--r--  1 jules  staff  2473384 Jun 13 14:00 a8ee48d8bc24428695d0eac5b8f6fbaa_000004.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/interest.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180628b1-d64f-44ad-8cb0-961da2a5ca40",
   "metadata": {},
   "source": [
    "### Transforming data with simple methods\n",
    "\n",
    "Ray datasets support transformation in parallel using `map`. It uses Ray tasks to execute eagerly or synchronously. Among others [transformations](https://docs.ray.io/en/latest/data/package-ref.html#dataset-api), it supports`filter`, `flat_map`, `groupBy`etc.\n",
    "\n",
    "Let's try a using `.map()`, `.filter()` and `.groupBy` on our dataset.\n",
    "\n",
    "_Explain what's happening behind the scenes here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ae991-6723-4664-a17e-c9c6e36681b8",
   "metadata": {},
   "source": [
    "Execute a lambda function for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc9f6e9f-c100-4b6d-9647-340a42686b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:03<00:00, 63.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.75, 7.5, 11.25, 15.0, 18.75]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.map(lambda x: x['amount'] * 2.5).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988688d-e40a-43fc-835a-44e8224732aa",
   "metadata": {},
   "source": [
    "Filter by amount and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc9b8286-942c-4c9d-af44-44c4b62120d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:04<00:00, 46.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 6669, 'ssn': None, 'name': None, 'amount': 10003.5, 'interest': 0.30000000000000004, 'state': 'CA', 'marital_status': 'divorced', 'property': 'rental', 'dependents': 2, 'defaulted': 1, 'gender': 'U'},\n",
       " {'id': 6672, 'ssn': None, 'name': None, 'amount': 10008.0, 'interest': 0.4, 'state': 'CA', 'marital_status': 'single', 'property': 'house', 'dependents': 2, 'defaulted': 0, 'gender': 'U'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.filter(lambda x: x['amount'] > 10000.00 and x['state'] == 'CA').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce01dfc-317b-4e71-bf54-bf07aee2b0d8",
   "metadata": {},
   "source": [
    "Use `groupBy` state and compute the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cbee373-1488-4385-a9b7-075212e5de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sort Sample: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 160.70it/s]\n",
      "GroupBy Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:03<00:00, 57.27it/s]\n",
      "GroupBy Reduce: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 6359.10it/s]\n"
     ]
    }
   ],
   "source": [
    "results = arrow_ds.groupby(\"state\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36af0a8d-c785-42cc-9771-b0597fe2208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': 'AZ', 'count()': 141813}\n",
      "{'state': 'CA', 'count()': 141835}\n",
      "{'state': 'OR', 'count()': 141713}\n",
      "{'state': 'TX', 'count()': 141258}\n",
      "{'state': 'UT', 'count()': 141625}\n",
      "{'state': 'WA', 'count()': 141756}\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2696939-7e54-47a1-91fb-e8a30f61257d",
   "metadata": {},
   "source": [
    "Get the max of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c1843ad-74f0-40af-ab06-0e91154b178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupBy Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:04<00:00, 44.32it/s]\n",
      "GroupBy Reduce: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max(amount)': 1275000.0, 'max(interest)': 0.5, 'max(dependents)': 5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=arrow_ds.max([\"amount\", \"interest\", \"dependents\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2dd6ac-6aa0-44dc-91ef-96382725eef9",
   "metadata": {},
   "source": [
    "### Accessing datasets using batches or iterating by rows\n",
    "\n",
    "Datasets can be passed to Ray tasks or actors and read with `.iter_batches()` or `.iter_rows()`. This does not incur a copy, since the blocks of the Dataset are passed by reference as Ray objects. Splitting data as shards and passing to individual Ray Actors to process shards in a common Ray pattern used in distributed training with Ray actors.\n",
    "\n",
    "Let's examine how.\n",
    "\n",
    "*Questions for clearificaiton*:\n",
    " * _why is shard a list now instead of Dataset_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c83caaaa-6871-4d67-86a3-4c3c6f000c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class BatchWorker:\n",
    "    def __init__(self, rank):\n",
    "        self.rank = rank\n",
    "        self.processed= 0\n",
    "    \n",
    "    @ray.method(num_returns=2)\n",
    "    def process_shard_list(self, shard: ray.data.Dataset) -> tuple:\n",
    "        for batch in shard.iter_batches(batch_size=1024):\n",
    "            # do something with the batch such as feature\n",
    "            # processing, transformation, and \n",
    "            # save as a parquet files \n",
    "            self.processed = self.processed + len(batch)\n",
    "        # return items processed, worker id\n",
    "        return (self.processed, self.rank)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e4b2d-acf0-4f50-90ea-f0b43f9e66f1",
   "metadata": {},
   "source": [
    "#### Create batch workers as Ray actors\n",
    "Each actor will get a shard, list of rows, to work on. We split\n",
    "our dataset `arrow_ds` into five shards. Each `BatchWorker` gets a shard.\n",
    "`.split`() splits shards across these batch of workers by using the `locality_hints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7371d6f7-b46c-42f3-b820-dfe3278bed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard row: Dataset(num_blocks=40, num_rows=170000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})\n",
      "Number of shards:5\n",
      "Number of shard workers:5\n"
     ]
    }
   ],
   "source": [
    "batch_workers = [BatchWorker.remote(i) for i in range(1, 6)]\n",
    "\n",
    "shards = arrow_ds.split(len(batch_workers), locality_hints=batch_workers)\n",
    "\n",
    "print(f\"Shard row: {shards[0]}\")\n",
    "print(f\"Number of shards:{len(shards)}\")\n",
    "print(f\"Number of shard workers:{len(batch_workers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db79aff-6118-4be7-b642-ba06a5d4c939",
   "metadata": {},
   "source": [
    "### Launch `BatchWorker` actors\n",
    "\n",
    "Process each shard. Each `BatchWorker.process_shard_list()` returns a object RefID with a tuple as its value. What we get from this comprehension is a list objectRefs as tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a09dcec4-50e3-493c-9a4b-090019d41eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[ObjectRef(d55f44605ed60eca524127effe1a504898b7ee7b0100000001000000),\n",
       "   ObjectRef(d55f44605ed60eca524127effe1a504898b7ee7b0100000002000000)],\n",
       "  [ObjectRef(cb926fc120e2294b967e852eb6601c2c385f12610100000001000000),\n",
       "   ObjectRef(cb926fc120e2294b967e852eb6601c2c385f12610100000002000000)],\n",
       "  [ObjectRef(941c76826abe64fd85b38342e5fc14b78101c6c90100000001000000),\n",
       "   ObjectRef(941c76826abe64fd85b38342e5fc14b78101c6c90100000002000000)],\n",
       "  [ObjectRef(2301943ae99f0a54a6a043d58b6aec2f8c102c5c0100000001000000),\n",
       "   ObjectRef(2301943ae99f0a54a6a043d58b6aec2f8c102c5c0100000002000000)],\n",
       "  [ObjectRef(e4131e9dab52e0600a5446e09564682ded9398b30100000001000000),\n",
       "   ObjectRef(e4131e9dab52e0600a5446e09564682ded9398b30100000002000000)]],\n",
       " 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_refs = [w.process_shard_list.remote(s) for w, s in zip(batch_workers, shards)]\n",
    "object_refs, len(object_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704782b6-ff54-4b99-8fd1-3c51635e0e02",
   "metadata": {},
   "source": [
    "Fetch the values from the returned list of ObjectRefs, which is a tuple of (batch_size, worker_rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb23e87b-0b28-43cf-b180-40a3e7fd2974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[170000, 1], [170000, 2], [170000, 3], [170000, 4], [170000, 5]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [ray.get(ref) for ref in object_refs]\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5aae6-029a-4689-8c36-2f1ca59050ee",
   "metadata": {},
   "source": [
    "### Creating and using dataset pipelines\n",
    "\n",
    "What are dataset pipelines and how are they different from Ray datasets? \n",
    "\n",
    "Datasets perform transformation or operations eagerly or synchronously, whereas [DataPipelines](https://docs.ray.io/en/latest/data/package-ref.html#datasetpipeline-api) can execute in an overlaped pipeline executions. For example, if you had operations that require reading from file, transforming data, and then doing some minor feature engineering, these operations can be executed in a normal pipeline fashion. This allows for the overlapped execution of data input (e.g., reading files), computation (e.g. feature preprocessing), and training (e.g., distributed ML training).\n",
    "\n",
    "A `DatasetPipeline` can be constructed in two ways: either by pipelining the execution of an existing Dataset (via `Dataset.window`) or generating repeats of an existing Dataset (via `Dataset.repeat`). \n",
    "\n",
    "Let's have a go at it and see what we can do with our synthetic data from above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38965a05-d9ce-449f-9db0-84171c00303a",
   "metadata": {},
   "source": [
    "### Using Dataset.window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35cd62-b5f5-494c-bd56-b005644a5558",
   "metadata": {},
   "source": [
    "Create some functions or operations to be executed in a overlapped manner in the pipeline. These functions\n",
    "are simple to illustrate a point. But they can be complex for a particular use case.\n",
    "\n",
    " _Questions for clarification_:\n",
    " * _how can we send arguments to these pipeline functions?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2fbec08-e650-4a9c-a360-b724fdc1055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_row_value(row: ray.data.impl.arrow_block.ArrowRow) -> int:\n",
    "    return round(row / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe4a2303-b340-4205-99d6-03ca14a91e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_row_value(row: ray.data.impl.arrow_block.ArrowRow) -> int:\n",
    "    return row * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04dab574-38c0-4900-a36d-c6fe6a774dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulo_row_value(row: ray.data.impl.arrow_block.ArrowRow) -> int:\n",
    "    return row % random.randint(1, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a426771-8f76-4ab0-ad36-ceef903772b3",
   "metadata": {},
   "source": [
    "#### Create a window based pipeline\n",
    "With a each window of 50 blocks. \n",
    "\n",
    "_Questions for clarification_:\n",
    " * _why the number of stages is 2?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7aebd77d-88e2-459a-ad83-c3e9ed1a7672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=4, num_stages=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_pipe = ds.window(blocks_per_window=50)\n",
    "ds_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757d2fb-f36d-4519-b9de-ab7c02eec0f8",
   "metadata": {},
   "source": [
    "### Applying transforms to pipelines adds more pipeline stages.\n",
    "_Questions for clarification_:\n",
    " * _how can we send arguments to thse pipeline functions?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "868c3fc6-e796-4eb7-bbfa-959c41e5c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=4, num_stages=5)\n"
     ]
    }
   ],
   "source": [
    "ds_pipe = ds_pipe.map(divide_row_value)\n",
    "ds_pipe = ds_pipe.map(double_row_value)\n",
    "ds_pipe = ds_pipe.map(modulo_row_value)\n",
    "print(ds_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd6fcb-00f2-44d3-bbaf-55ef50d80c9e",
   "metadata": {},
   "source": [
    "#### Iterate our pipeline\n",
    "\n",
    " * _Questions for clearification_:\n",
    "     * _how is this executed_?\n",
    "     * _why are we iterating over rows_?\n",
    "     * _what is row comprised of? Blocks?_?\n",
    "     * _is the value of the row an already computed value_?\n",
    "     * _if the `num_stages=5`, why am I seeing only stage 0 and 1 in the output of stages?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cf43855-6977-4192-9c54-479f060b9f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                                          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[2m\u001b[36m(_map_block_nosplit pid=88622)\u001b[0m E0613 14:55:37.417788000 123145559744512 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=88621)\u001b[0m E0613 14:55:37.397263000 123145471377408 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=88619)\u001b[0m E0613 14:55:37.417704000 123145596600320 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\n",
      "Stage 0:  50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 2/4 [00:01<00:01,  1.21it/s]\u001b[A\u001b[2m\u001b[36m(_map_block_nosplit pid=88620)\u001b[0m E0613 14:55:37.521509000 123145399263232 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=88623)\u001b[0m E0613 14:55:37.521551000 123145442594816 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\n",
      "Stage 0:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 3/4 [00:03<00:01,  1.06s/it]\u001b[A\n",
      "Stage 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.34s/it]\u001b[A\n",
      "Stage 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.64s/it]\u001b[A\n",
      "Stage 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value: 985328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "print(f\"Total value: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302cec8-1121-4ba3-adf5-c48389ed18a5",
   "metadata": {},
   "source": [
    "Let's try a Datapipeline with our synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e210ee27-8f26-4f85-9b17-952136d3f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count or return based on the condition\n",
    "def count_ca(row: ray.data.impl.arrow_block.ArrowRow) -> int:\n",
    "    return 1 if row['state'] == \"CA\" and row[\"defaulted\"] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35c01edd-aa8c-4ed5-bfb3-e754e7712511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=4, num_stages=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds_pipe = arrow_ds.window(blocks_per_window=50)\n",
    "arrow_ds_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e74deb3e-567f-418f-b7b3-a30315bbadec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=4, num_stages=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds_pipe = arrow_ds_pipe.map(count_ca)\n",
    "arrow_ds_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a440f3c-109b-4e5f-aa87-4208302514eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                                          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 0:  50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 2/4 [00:01<00:01,  1.02it/s]\u001b[A\n",
      "Stage 0:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 3/4 [00:02<00:00,  1.04it/s]\u001b[A\n",
      "Stage 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.04it/s]\u001b[A\n",
      "Stage 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.34s/it]\u001b[A\n",
      "Stage 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows for CA state and defaulted loans rows: 70765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in arrow_ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "print(f\"Total rows for CA state and defaulted loans rows: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea86c2-0afd-46cf-b8d8-030cab817119",
   "metadata": {},
   "source": [
    "### Ingesting into Model Trainers\n",
    "Let's define a dummy trainer that takes our synthetic data and trains the model and returns loss for that trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fade88c1-b564-465a-af28-8476c6755587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    return random.uniform(0, 1)\n",
    "\n",
    "@ray.remote\n",
    "class Trainer:\n",
    "    def __init__(self, rank, model):\n",
    "        self.rank = rank\n",
    "        self.model = model\n",
    "        self.loss = 0.0\n",
    "        \n",
    "    def train(self, shard:ray.data.Dataset) -> float:\n",
    "        for batch in shard.iter_batches(batch_size=1024):\n",
    "            for epoch in range(1,21):\n",
    "                output = self.model(batch)\n",
    "                self.loss = output \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'epoch {epoch}, loss: {self.loss:.3f}')\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b69fda5-7f03-432f-9d02-2dcb9072965d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Actor(Trainer, 4db2622e1f9b65f2113c86d901000000),\n",
       " Actor(Trainer, 17b052035cf001bb4d68581d01000000),\n",
       " Actor(Trainer, 760c1cf11447dd72c8daefda01000000),\n",
       " Actor(Trainer, b3d8b2cf153d1e9922c1e93d01000000),\n",
       " Actor(Trainer, e2bf5cf7fcc09cf709b4eb3301000000)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainers = [Trainer.remote(i, model) for i in range(1, 6)]\n",
    "trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d69adf1b-7678-484d-ad3e-fb1d7ef804dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset(num_blocks=40, num_rows=170000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=40, num_rows=170000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=40, num_rows=170000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=40, num_rows=170000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=40, num_rows=170000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shards = arrow_ds.split(n=len(trainers), locality_hints=trainers)\n",
    "shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51e6234c-20fb-43f5-ab40-97f8ad1c1cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14435674286580868,\n",
       " 0.10737865179730799,\n",
       " 0.8368477658282624,\n",
       " 0.7149714607805753,\n",
       " 0.1402075824517085]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Trainer pid=89411)\u001b[0m epoch 20, loss: 0.144\n",
      "\u001b[2m\u001b[36m(Trainer pid=89414)\u001b[0m epoch 20, loss: 0.715\n",
      "\u001b[2m\u001b[36m(Trainer pid=89412)\u001b[0m epoch 20, loss: 0.107\n",
      "\u001b[2m\u001b[36m(Trainer pid=89413)\u001b[0m epoch 20, loss: 0.837\n",
      "\u001b[2m\u001b[36m(Trainer pid=89415)\u001b[0m epoch 20, loss: 0.140\n"
     ]
    }
   ],
   "source": [
    "object_refs = [t.train.remote(s) for t, s in zip(trainers, shards)]\n",
    "ray.get(object_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df42a9ca-54a9-4b93-8dc0-dcc18b5636a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751265c-7a31-4d17-87c7-1ca5a163b8b5",
   "metadata": {},
   "source": [
    "### Exercises\n",
    " 1. Write some transformers, filters, and aggregators with our systhetic data\n",
    " 2. Add additional pipleline stages with our synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e825ac0-f6d6-47f5-8d1b-413a000ee0bb",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Work through the NYC example tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
