{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe990923",
   "metadata": {},
   "source": [
    "# Gentle introduction to Ray datasets APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_08_ray_air_in_ten_minutes.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_06_ray_api_calls.ipynb) <br>\n",
    "\n",
    "### Overview\n",
    "\n",
    "As a native library and built atop Ray, Ray data allows you to exchange data among Ray tasks, actors, libraries, and applications. It also allows you to read/write training data from different file sources and storage, including csv, binary, parquet, text, etc., and supports myriad [file formats and data sources](https://docs.ray.io/en/latest/data/dataset.html#datasource-compatibility).\n",
    "<img src=\"images/dataset.png\" width=\"75%\" height=\"45%\">\n",
    "\n",
    "Using [Apache Arrow](https://arrow.apache.org/), Ray dataset is designed to load and preprocess data for distributed ML training pipelines. Datasets are flexible (e.g., you can express higher-quality per-epoch global shuffles) and provides higher overall performance. Additionally, datasets comes with standard and simple transformations like `map`, `filter`, and `partition`. \n",
    "\n",
    "**Note**: Ray datasets is not a replacement for a full-fledged data processing library for doing exploratory data analysis (EDA), extract, transform and load (ETL) or a subsitute for Apache Spark or Dask or Pandas DataFrames. Its primary objective is the last-mile rudimentary distributed data preprocessing and data ingestion for distributed ML training by providing simple primitive transformational functions.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this introductory tutorial you will learn:\n",
    " * what are Ray datasets, why use them, and when\n",
    " * create, transform, read, and save Ray datasets\n",
    " * use shards for parallel processing of large datasets\n",
    " * understand datapipelines and their merits\n",
    " * use `DatasetPipeline` for parallel computation \n",
    " * use datasets for last-mile ML ingestion for distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6abd1f",
   "metadata": {},
   "source": [
    "### Key concepts\n",
    "\n",
    "To work with Ray Datasets, you need to understand how `Datasets` and `Dataset Pipelines` work. That is, how datasets are stored internally and in what format. And what benefit does `Datapipelines` offer for faster processing and execution. A quick peek into each of these will shed some light into overall benefits of Ray Datasets.\n",
    "\n",
    "Let's start with the internal format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7d84d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ray Datasets\n",
    "\n",
    "A Ray dataset implements a distributed [Apache Arrow](https://arrow.apache.org/). As such, a Dataset consists of a list of Ray object references to blocks. Each block holds a set of items in either an [Arrow table](https://arrow.apache.org/docs/python/data.html#tables) (when created from or transformed to tabular or tensor data), a Pandas DataFrame (when created from or transformed to Pandas data), or a Python list (otherwise).\n",
    "\n",
    "<img src=\"images/dataset-arch.png\" width=\"70%\" height=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456d3a7",
   "metadata": {},
   "source": [
    "#### Dataset Pipelines\n",
    "Datasets execute their transformations synchronously or eagerly in blocking calls. However, it can be useful to overlap dataset computations with output. This can be done with a `DatasetPipeline`.\n",
    "\n",
    "A `DatasetPipeline` is a unified iterator over a (potentially infinite) sequence of Ray Datasets, each of which represents a window over the original data. Conceptually, it is similar to a `Spark DStream`, but manages execution over a bounded amount of source data instead of an unbounded stream. Ray computes each dataset window on-demand and stitches their output together into a single logical data iterator. `DatasetPipeline` implements most of the same transformation and output methods as Datasets (e.g., `map`, `filter`, `split`, `iter_rows`, `to_torch`, etc.).\n",
    "\n",
    "<img src=\"images/ray_dataset_pipeline.jpg\" width=\"75%\" height=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579c07b",
   "metadata": {},
   "source": [
    "### Datasets Execution Model\n",
    "In this section, we briefly discuss the execution model of Datasets, which may be useful for understanding and tuning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c6074",
   "metadata": {},
   "source": [
    "#### Reading Data\n",
    "Datasets uses Ray tasks, for parallelism, to read data from remote storage or source. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of parallel\n",
    "read tasks equal to the specified read parallelism (200 by default). One or more files will be assigned to each read task. Each read task reads its assigned files and produces one or more output blocks (Ray objects):\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/master/_images/dataset-read.svg\" height=\"25%\" width=\"50%\">\n",
    "\n",
    "In the common case, each read task produces a single output block. Read tasks may split the output into multiple blocks if the data exceeds the target max block size (2GiB by default). This automatic block splitting avoids out-of-memory errors when reading very large single files (e.g., a 100-gigabyte CSV file). All of the built-in datasources except for JSON currently support automatic block splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01de3f6",
   "metadata": {},
   "source": [
    "#### Deferred Read Task Execution\n",
    "\n",
    "When a Dataset is created using `ray.data.read_*`, only the first read task will be executed initially. This avoids blocking Dataset creation on the reading of all data files, enabling inspection functions like `ds.schema()` without incurring high read costs. `<ray.data.Dataset.schema>`() and `ds.show()` can be used right away. Executing further transformations on the Dataset will trigger execution of all read tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d75d13",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset Transforms\n",
    "\n",
    "Datasets use either Ray tasks or Ray actors to transform datasets (i.e., for `ds.map_batches()`, `ds.map()`, or `ds.flat_map()`). By default, tasks are used `(compute=\"tasks\")`. Actors can be specified with `compute=\"actors\"`, in which case an autoscaling pool of Ray actors will be used to apply transformations. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be re-used. Whichever compute strategy is used, each map task generally takes in one block and produces one or more output blocks. The output block splitting rule is the same as for file reads (blocks are split after hitting the target max block size of 2GiB):\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/master/_images/dataset-map.svg\" height=\"25%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0e08",
   "metadata": {},
   "source": [
    "#### Shuffling Data\n",
    "\n",
    "Certain operations like `ds.sort()` and `ds.groupby()` require data blocks to be partitioned by value. Datasets executes this in three phases. \n",
    "\n",
    "First, a wave of sampling tasks determines suitable partition boundaries based on a random sample of data. Second, map tasks divide each input block into a number of output blocks equal to the number of reduce tasks. Third, reduce tasks take assigned output blocks from each map task and combines them into one block. (Overall, this strategy generates O(n^2) intermediate objects where n is the number of input blocks.)\n",
    "\n",
    "You can also change the partitioning of a Dataset using `ds.random_shuffle()` or `ds.repartition()`. The former should be used if you want to randomize the order of elements in the dataset. The second should be used if you only want to equalize the size of the Dataset blocks (e.g., after a read or transformation that may skew the distribution of block sizes). Note that repartition has two modes, `shuffle=False`, which performs the minimal data movement needed to equalize block sizes, and `shuffle=True`, which performs a full (non-random) distributed shuffle:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/master/_images/dataset-shuffle.svg\" height=\"25%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fce60",
   "metadata": {},
   "source": [
    "#### Fault tolerance\n",
    "\n",
    "Datasets relies on task-based ü§π‚Äç‚ôÄÔ∏è [fault tolerance](https://docs.ray.io/en/latest/ray-core/tasks/fault-tolerance.html) in Ray core. Specifically, a `Dataset` will be automatically recovered by Ray in case of failures. This works through **lineage reconstruction**: a Dataset is a collection of Ray objects stored in shared memory, and if any of these objects are lost, then Ray will recreate them by re-executing the task(s) that created them.\n",
    "\n",
    "There are a few cases that are not currently supported: \n",
    "\n",
    " 1. If the original creator of the Dataset dies ‚ò†Ô∏è. This is because the creator stores the metadata for the objects that comprise the Dataset. \n",
    " 2. For a `DatasetPipeline.split()` üëØ‚Äç‚ôÇÔ∏è, we do not support recovery for a consumer failure. When there are multiple consumers, they must all read the split pipeline in lockstep. To recover from this case, the pipeline and all consumers must be restarted together. \n",
    " 3. The `compute=actors`üßë‚Äçüè≠ option for transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3626753",
   "metadata": {},
   "source": [
    "#### Execution and Memory Management\n",
    "\n",
    "See [Execution and Memory Management](https://docs.ray.io/en/master/data/memory-management.html#data-advanced) for more details about how Datasets manages memory and optimizations such as lazy vs eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48005fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os, random, warnings\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab95cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d52759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0rc0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.8.13', ray_version='2.0.0rc0', ray_commit='a0588094ec52b45a878f59e98258cd5e90f4ec36', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-08-05_14-33-16_113573_96947/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-08-05_14-33-16_113573_96947/sockets/raylet', 'webui_url': '127.0.0.1:8266', 'session_dir': '/tmp/ray/session_2022-08-05_14-33-16_113573_96947', 'metrics_export_port': 60162, 'gcs_address': '127.0.0.1:56677', 'address': '127.0.0.1:56677', 'dashboard_agent_listen_port': 52365, 'node_id': 'eb5a81007da1d056746432b0c7b692d3f3a96fd19db44ef2bea299ad'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a40fc",
   "metadata": {},
   "source": [
    "### Creating a simple Ray Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86adcdd",
   "metadata": {},
   "source": [
    "For quick illustration, let's create a generic dataset of 1K integers and look at the schema and underlying datatype. [Ray Data API Documentation](https://docs.ray.io/en/latest/data/package-ref.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74585645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray.data.range(1000)\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790b11f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13aa79-802f-4af1-9d4a-82c8eec49abd",
   "metadata": {},
   "source": [
    "The difference between `show` and `take` is that the former takes one item at time and prints it, while the latter iterates over row items from the dataset, appends to a list and returns it. Underneath, `ds.show()` calls `ds.take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa1bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52f3e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6b809",
   "metadata": {},
   "source": [
    "### Creating a large Ray Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9604bc8",
   "metadata": {},
   "source": [
    "Let's create a synthetic dataset, *Homeowners*, of Arrow records (800K) with several columns and data associated with it. \n",
    "\n",
    "To illustrate some simple transformational functions, we'll use this *Homeowners* generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b5a7c0-75d9-45fa-8915-83e3531dad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anonomized_name(num_letters=6):\n",
    "    import string\n",
    "    return (\"anonomized-\" + \"\".join(random.choices(string.ascii_uppercase + string.ascii_lowercase, k=num_letters)))\n",
    "\n",
    "def get_anonomized_ssn():\n",
    "    return (\"anonomized-\" + \"\".join([\"{}\".format(random.randint(0, 9)) for num in range(1, 10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de1fad4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'ssn': 'anonomized-224714442',\n",
       "  'name': 'anonomized-WzmKkn',\n",
       "  'amount': 15.0,\n",
       "  'interest': 0.4,\n",
       "  'state': 'NM',\n",
       "  'marital_status': 'single',\n",
       "  'property': 'house',\n",
       "  'dependents': 3,\n",
       "  'defaulted': 1,\n",
       "  'gender': 'M'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ROWS = 800001\n",
    "STATES = [\"CA\", \"AZ\", \"OR\", \"WA\", \"TX\", \"UT\", \"NV\", \"NM\"]\n",
    "M_STATUS = [\"married\", \"single\", \"domestic\", \"divorced\", \"undeclared\"]\n",
    "GENDER = [\"F\", \"M\", \"U\"]\n",
    "HOME_OWNER = [\"condo\", \"house\", \"rental\", \"cottage\"]\n",
    "\n",
    "items = [{\"id\": i,\n",
    "          \"ssn\": get_anonomized_ssn(),\n",
    "          \"name\": get_anonomized_name(),\n",
    "          \"amount\": i * 1.5 * 10, \n",
    "          \"interest\": random.randint(1,5) * .1,\n",
    "          \"state\": random.choice(STATES),\n",
    "          \"marital_status\": random.choice(M_STATUS),\n",
    "          \"property\": random.choice(HOME_OWNER),\n",
    "          \"dependents\": random.randint(1, 5),\n",
    "          \"defaulted\": random.randint(0,1),\n",
    "          \"gender\":random.choice(GENDER) } for i in range(1,NUM_ROWS)]\n",
    "items[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38232ec7",
   "metadata": {},
   "source": [
    "#### Creating a dataset from list of dictionary items\n",
    "\n",
    "Ray data can be created of a dictionary of items. \n",
    "\n",
    "*Question: how is num_blocks computed?*\n",
    "\n",
    "The default number of blocks is 200, just like in Spark, the default number of partitions created when reading data is 200. This can be [tuned as need](https://docs.ray.io/en/master/data/performance-tips.html#tuning-read-parallelism). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5968c4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=200, num_rows=800000, schema={id: int64, ssn: string, name: string, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds = ray.data.from_items(items)\n",
    "arrow_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a87ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845b3a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 1,\n",
       "           'ssn': 'anonomized-224714442',\n",
       "           'name': 'anonomized-WzmKkn',\n",
       "           'amount': 15.0,\n",
       "           'interest': 0.4,\n",
       "           'state': 'NM',\n",
       "           'marital_status': 'single',\n",
       "           'property': 'house',\n",
       "           'dependents': 3,\n",
       "           'defaulted': 1,\n",
       "           'gender': 'M'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f84ceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id: int64\n",
       "ssn: string\n",
       "name: string\n",
       "amount: double\n",
       "interest: double\n",
       "state: string\n",
       "marital_status: string\n",
       "property: string\n",
       "dependents: int64\n",
       "defaulted: int64\n",
       "gender: string"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b622e02",
   "metadata": {},
   "source": [
    "### Saving datasets and reading as a parquet files üóÉ\n",
    "Ray datasets support myriad data formats. Let's save this dataset as a parquet file and create `N` partitions, where N=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c934bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 27.72it/s]\n",
      "Write Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 24.56it/s]\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(\"data_homeowners/interest.parquet\")\n",
    "arrow_ds.repartition(5).write_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65ef04e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53848\n",
      "-rw-r--r--  1 jules  staff  5418758 Aug  5 14:33 bb3cec7f3a0643fd96d74a8e5d7e1b27_000000.parquet\n",
      "-rw-r--r--  1 jules  staff  5534534 Aug  5 14:33 bb3cec7f3a0643fd96d74a8e5d7e1b27_000001.parquet\n",
      "-rw-r--r--  1 jules  staff  5535043 Aug  5 14:33 bb3cec7f3a0643fd96d74a8e5d7e1b27_000002.parquet\n",
      "-rw-r--r--  1 jules  staff  5534382 Aug  5 14:33 bb3cec7f3a0643fd96d74a8e5d7e1b27_000003.parquet\n",
      "-rw-r--r--  1 jules  staff  5534811 Aug  5 14:33 bb3cec7f3a0643fd96d74a8e5d7e1b27_000004.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l data_homeowners/interest.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95d585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds = ray.data.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0028f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 1,\n",
       "           'ssn': 'anonomized-224714442',\n",
       "           'name': 'anonomized-WzmKkn',\n",
       "           'amount': 15.0,\n",
       "           'interest': 0.4,\n",
       "           'state': 'NM',\n",
       "           'marital_status': 'single',\n",
       "           'property': 'house',\n",
       "           'dependents': 3,\n",
       "           'defaulted': 1,\n",
       "           'gender': 'M'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef1509",
   "metadata": {},
   "source": [
    "### Transforming data with simple methods\n",
    "\n",
    "Ray datasets support transformation in parallel using `map`. It uses Ray tasks to execute eagerly or synchronously. Among others [transformations](https://docs.ray.io/en/latest/data/package-ref.html#dataset-api), it supports`filter`, `flat_map`, `groupBy`etc.\n",
    "\n",
    "Let's try a using `.map()`, `.filter()` and `.groupBy` on our dataset. \n",
    "\n",
    "The `map()` and `filter()` are row-based operations. This can be expensive for large datasets. However, you can use `map_batches(...)` with batch_size=4096 as default. This will create a task per block and each batch will be vectorized and executed in parallel. Ray tasks are created per block for a map operation. \n",
    "\n",
    "Let's try first with row-based transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7297bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 ms, sys: 17.1 ms, total: 45.7 ms\n",
      "Wall time: 4.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 667,\n",
       "           'ssn': 'anonomized-582442492',\n",
       "           'name': 'anonomized-dxmpMC',\n",
       "           'amount': 10005.0,\n",
       "           'interest': 0.5,\n",
       "           'state': 'OR',\n",
       "           'marital_status': 'single',\n",
       "           'property': 'cottage',\n",
       "           'dependents': 2,\n",
       "           'defaulted': 0,\n",
       "           'gender': 'U'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.filter(lambda x: x['amount'] > 10000).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b0897",
   "metadata": {},
   "source": [
    "Let's try a `.map_batches()`, which is vectorized. We should expect faster execution. \n",
    "\n",
    "*Question: Why the `.map()` returned `ArrowRow` and `.map_batches` returned `PandasRow`*?\n",
    "\n",
    "Because `map_batch(..., batch_format='native')` promotes it to Pandas DataFrame. You can\n",
    "promote it to [other formats](https://docs.ray.io/en/master/data/package-ref.html#ray.data.Dataset.map_batches) such as `pyarrow`. The default is `native`, which is pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8116c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.4 ms, sys: 28.5 ms, total: 111 ms\n",
      "Wall time: 469 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'id': 667,\n",
       "            'ssn': 'anonomized-582442492',\n",
       "            'name': 'anonomized-dxmpMC',\n",
       "            'amount': 10005.0,\n",
       "            'interest': 0.5,\n",
       "            'state': 'OR',\n",
       "            'marital_status': 'single',\n",
       "            'property': 'cottage',\n",
       "            'dependents': 2,\n",
       "            'defaulted': 0,\n",
       "            'gender': 'U'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.map_batches(lambda df: df[df[\"amount\"] > 10000]).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95926ece",
   "metadata": {},
   "source": [
    "You can see that `.map_batches()` is a lot faster than row based. So for large datasets use \n",
    "`.map_batches()`.\n",
    "\n",
    "Let's try a filter operation: both per row operation and per block as vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9164836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 ms, sys: 8.15 ms, total: 28.9 ms\n",
      "Wall time: 1.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 676,\n",
       "           'ssn': 'anonomized-066634188',\n",
       "           'name': 'anonomized-OUBbcc',\n",
       "           'amount': 10140.0,\n",
       "           'interest': 0.1,\n",
       "           'state': 'CA',\n",
       "           'marital_status': 'domestic',\n",
       "           'property': 'cottage',\n",
       "           'dependents': 4,\n",
       "           'defaulted': 0,\n",
       "           'gender': 'F'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.filter(lambda x: x[\"amount\"] > 10000.00 and x[\"state\"] == 'CA').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f98958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 32.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 ms, sys: 6.26 ms, total: 26.9 ms\n",
      "Wall time: 162 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'id': 676,\n",
       "            'ssn': 'anonomized-066634188',\n",
       "            'name': 'anonomized-OUBbcc',\n",
       "            'amount': 10140.0,\n",
       "            'interest': 0.1,\n",
       "            'state': 'CA',\n",
       "            'marital_status': 'domestic',\n",
       "            'property': 'cottage',\n",
       "            'dependents': 4,\n",
       "            'defaulted': 0,\n",
       "            'gender': 'F'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.map_batches(lambda df: df[(df[\"amount\"] > 10000) & (df[\"state\"] == \"CA\")]).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e754a08",
   "metadata": {},
   "source": [
    "Use `groupBy` state and compute the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7588807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 118.04it/s]\n",
      "Sort Sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 4831.03it/s]\n",
      "Shuffle Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  4.75it/s]\n",
      "Shuffle Reduce: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 1529.43it/s]\n"
     ]
    }
   ],
   "source": [
    "results = arrow_ds.groupby(\"state\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec43ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': 'AZ', 'count()': 100060}\n",
      "{'state': 'CA', 'count()': 99716}\n",
      "{'state': 'NM', 'count()': 100443}\n",
      "{'state': 'NV', 'count()': 100006}\n",
      "{'state': 'OR', 'count()': 99730}\n",
      "{'state': 'TX', 'count()': 100341}\n",
      "{'state': 'UT', 'count()': 99523}\n",
      "{'state': 'WA', 'count()': 100181}\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edfea7",
   "metadata": {},
   "source": [
    "Get the max of certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7491c77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 142.57it/s]\n",
      "Shuffle Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 771.95it/s]\n",
      "Shuffle Reduce: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 329.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArrowRow({'max(amount)': 12000000.0,\n",
       "          'max(interest)': 0.5,\n",
       "          'max(dependents)': 5})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = arrow_ds.max([\"amount\", \"interest\", \"dependents\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9ae36",
   "metadata": {},
   "source": [
    "### Accessing datasets using batches or iterating by rows\n",
    "\n",
    "Datasets can be passed to Ray tasks or actors and iterated over with `.iter_batches()` or `.iter_rows()`. (This does not incur a copy, since the blocks of the Dataset are passed by reference as Ray objects.) Splitting data as shards and passing to individual Ray Actors to process shards is a common Ray pattern used in distributed training with Ray actors.\n",
    "\n",
    "Let's examine how we can process a list of shards with a `BatchWorker` Actor  in a distributed fashion\n",
    "\n",
    "<img src=\"images/batch_worker.jpg\" width=\"60%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f8e93-f64a-4e2f-9261-74888e43418d",
   "metadata": {},
   "source": [
    "A Ray actor `BatchWorker` working through shards in a batch size of 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ccb43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class BatchWorker:\n",
    "    def __init__(self, rank):\n",
    "        self.rank = rank         # this could be rank of CPU/GPU or worker id\n",
    "        self.processed = 0       # how much was processed\n",
    "    \n",
    "    @ray.method(num_returns=2)   # we want to return a tuple\n",
    "    def process_shard_list(self, shard: ray.data.Dataset) -> tuple:\n",
    "        for batch in shard.iter_batches(batch_size=1024):\n",
    "            # here you could do something with the batch such as feature\n",
    "            # preprocessing, minor transformation and then\n",
    "            # save as a parquet file \n",
    "            self.processed = self.processed + len(batch)\n",
    "        # return items processed, worker id\n",
    "        return (self.processed, self.rank)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe723a",
   "metadata": {},
   "source": [
    "#### Create batch workers as Ray actors\n",
    "Each actor will get a shard (list of rows) to work on. We split\n",
    "our dataset `arrow_ds` into five shards. Each `BatchWorker` gets a shard.\n",
    "\n",
    "`.split`() splits shards across these batch of workers by using the `locality_hints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecefae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 103.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard row: Dataset(num_blocks=1, num_rows=160000, schema={id: int64, ssn: string, name: string, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})\n",
      "Number of shards:5\n",
      "Number of shard workers:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# comprehension to create five actors as BatchWorker\n",
    "batch_workers = [BatchWorker.remote(i) for i in range(1, 6)]\n",
    "\n",
    "# split into five shards, each one for an actor and co-locate the blocks of the dataset with each actor to maximize data locality.\n",
    "shards = arrow_ds.split(len(batch_workers), locality_hints=batch_workers)\n",
    "\n",
    "print(f\"Shard row: {shards[0]}\")\n",
    "print(f\"Number of shards:{len(shards)}\")\n",
    "print(f\"Number of shard workers:{len(batch_workers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305ab3b",
   "metadata": {},
   "source": [
    "### Launch `BatchWorker` actors\n",
    "\n",
    "Process each shard. Each `BatchWorker.process_shard_list()` returns a object RefID with a tuple as its value. What we get from this comprehension is a list objectRefs as tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3e8db4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[ObjectRef(c0ca37ae352c9211e393bbe044e12a0bf4ab23ea0100000001000000),\n",
       "   ObjectRef(c0ca37ae352c9211e393bbe044e12a0bf4ab23ea0100000002000000)],\n",
       "  [ObjectRef(798376f73f71e22c2b16b883d166c66d175583d50100000001000000),\n",
       "   ObjectRef(798376f73f71e22c2b16b883d166c66d175583d50100000002000000)],\n",
       "  [ObjectRef(d99fa8d5c923d7c8061aedb3c547c0de01f90c4f0100000001000000),\n",
       "   ObjectRef(d99fa8d5c923d7c8061aedb3c547c0de01f90c4f0100000002000000)],\n",
       "  [ObjectRef(aa5ec071964645d251ad01e14554d0dd071f84c20100000001000000),\n",
       "   ObjectRef(aa5ec071964645d251ad01e14554d0dd071f84c20100000002000000)],\n",
       "  [ObjectRef(e3a0a2b2c6761c994d49b58e603f479384edcd550100000001000000),\n",
       "   ObjectRef(e3a0a2b2c6761c994d49b58e603f479384edcd550100000002000000)]],\n",
       " 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_refs = [w.process_shard_list.remote(s) for w, s in zip(batch_workers, shards)]\n",
    "object_refs, len(object_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afaddf5",
   "metadata": {},
   "source": [
    "Fetch the values from the returned list of ObjectRefs, which is a tuple of (batch_size, worker_rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa6d80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[160000, 1], [160000, 2], [160000, 3], [160000, 4], [160000, 5]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [ray.get(ref) for ref in object_refs]\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea260db0",
   "metadata": {},
   "source": [
    "### Creating and using Ray dataset pipelines\n",
    "\n",
    "What are dataset pipelines and how they are different from Ray datasets? \n",
    "\n",
    "Datasets perform transformation or operations eagerly or synchronously, whereas [DataPipelines](https://docs.ray.io/en/latest/data/package-ref.html#datasetpipeline-api) can execute in an overlapped pipeline executions. For example, if you had operations that require reading from file, transforming data, and then doing some minor feature engineering, these operations can be executed in a normal pipeline fashion. This allows for the overlapped execution of data input (e.g., reading files), computation (e.g., feature preprocessing), and training (e.g., distributed ML training). \n",
    "\n",
    "<img src=\"images/pipeline_window.jpg\" width=\"60%\" height=\"30%\">\n",
    "\n",
    "A `DatasetPipeline` can be constructed in two ways: either by pipelining the execution of an existing Dataset (via `Dataset.window`) or generating repeats of an existing Dataset (via `Dataset.repeat`). \n",
    "\n",
    "Let's have a go at it and see what we can do with our simple and synthetic data from above using `Dataset.window`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e4aec",
   "metadata": {},
   "source": [
    "### Using Dataset.window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4270d8d",
   "metadata": {},
   "source": [
    "Create simple functions or operations to be executed in a overlapped manner in the pipeline. These functions are simple to illustrate a point. But they can be complex for a particular use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fde9105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_row_value(row, n) -> int:\n",
    "    return round(row / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d544e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_row_value(row, n) -> int:\n",
    "    return row * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25ae7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulo_row_value(row , n) -> int:\n",
    "    return row % random.randint(1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e447c5",
   "metadata": {},
   "source": [
    "#### Create a window based pipeline\n",
    "With each window of 50 blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0575078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=1, num_stages=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use our original simple dataset from above with 1K rows in integer\n",
    "ds_pipe = ds.window(blocks_per_window=50)\n",
    "ds_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cc58b",
   "metadata": {},
   "source": [
    "### Applying transforms to pipelines adds more pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b722702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=1, num_stages=5)\n"
     ]
    }
   ],
   "source": [
    "ds_pipe = ds_pipe.map(lambda row: divide_row_value(row, 2))\n",
    "ds_pipe = ds_pipe.map(lambda row: double_row_value(row, 3))\n",
    "ds_pipe = ds_pipe.map(lambda row: modulo_row_value(row, 4))\n",
    "print(ds_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f9992",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Iterate our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c0c2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                                                                                 | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                                                                                 | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.76it/s]\u001b[A\n",
      "Stage 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value of the results: 376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=96976)\u001b[0m E0805 14:49:08.635977000 12901707776 chttp2_transport.cc:1111]         Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "# print(f\"Results from each pipeline map function:{results}\")\n",
    "print(f\"Total value of the results: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e28847",
   "metadata": {},
   "source": [
    "Let's try a `Datapipeline` with our synthetic data *Homewowners*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "184f2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count or return based on the condition\n",
    "def count_state(row, state) -> int:\n",
    "    return 1 if row['state'] == state and row[\"defaulted\"] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5ce3775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=1, num_stages=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds_pipe = arrow_ds.window(blocks_per_window=50)\n",
    "arrow_ds_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5db9b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=1, num_stages=3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds_pipe = arrow_ds_pipe.map(lambda row: count_state(row, \"CA\"))\n",
    "arrow_ds_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc6376fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                                                                                 | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                                                                                 | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.81s/it]\u001b[A\n",
      "Stage 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows for CA state and defaulted loans rows: 49920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in arrow_ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "print(f\"Total rows for CA state and defaulted loans rows: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7071a",
   "metadata": {},
   "source": [
    "## Ingesting data into Model Trainers\n",
    "Let's define a toy `Trainer` actor that takes our synthetic data and trains the model and returns loss for that trainer. This is a common pattern in Ray for distributing data to Trainers in a Ray cluster.\n",
    "\n",
    "<img src=\"images/trainer_worker.jpg\" width=\"60%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e66729d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dummy model\n",
    "import time\n",
    "\n",
    "def model(input):\n",
    "    # do some training and compute the loss\n",
    "    # ....\n",
    "    # time.sleep(0.005)\n",
    "    return random.uniform(0, 1)\n",
    "\n",
    "@ray.remote\n",
    "class Trainer:\n",
    "    def __init__(self, rank, model):\n",
    "        self.rank = rank\n",
    "        self.model = model\n",
    "        self.loss = 0.0\n",
    "        \n",
    "    def train(self, shard:ray.data.Dataset) -> float:\n",
    "        for epoch in range(1,21):\n",
    "            for batch in shard.iter_batches(batch_size=1024):\n",
    "                output = self.model(batch)\n",
    "                self.loss = output \n",
    "            if epoch % 5 == 0:\n",
    "                print(f'rank: {self.rank} epoch: {epoch}, loss: {self.loss:.3f}')\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a4fb8",
   "metadata": {},
   "source": [
    "#### Create five trainers, each with a copy of the model and each training on its respective shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "632e4cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Actor(Trainer, 0803878d1bffc8d829db861901000000),\n",
       " Actor(Trainer, 5b452c42806d0ffa45947d1d01000000),\n",
       " Actor(Trainer, 755ac10d68681bf6053b1d9301000000),\n",
       " Actor(Trainer, ff8a9bb92701c94f53a2084a01000000),\n",
       " Actor(Trainer, 93a507a9d2547256ffadab0601000000)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainers = [Trainer.remote(i, model) for i in range(1, 6)]\n",
    "trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba3400",
   "metadata": {},
   "source": [
    "#### Split the shards across all trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78fccfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dataset(num_blocks=1, num_rows=160000, schema={id: int64, ssn: string, name: string, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=160000, schema={id: int64, ssn: string, name: string, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=160000, schema={id: int64, ssn: string, name: string, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=160000, schema={id: int64, ssn: string, name: string, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=160000, schema={id: int64, ssn: string, name: string, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shards = arrow_ds.split(n=len(trainers), locality_hints=trainers)\n",
    "shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e27a15",
   "metadata": {},
   "source": [
    "#### Launch our trainers in a distributed fashion\n",
    "\n",
    "This will run across the cluster. Check the dashbard to see five actors launched. On a cluster, they will be on five different nodes, whereas on a single node on  \n",
    "five different cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "276306c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Trainer pid=98914)\u001b[0m rank: 2 epoch: 5, loss: 0.418\n",
      "\u001b[2m\u001b[36m(Trainer pid=98915)\u001b[0m rank: 3 epoch: 5, loss: 0.199\n",
      "\u001b[2m\u001b[36m(Trainer pid=98917)\u001b[0m rank: 5 epoch: 5, loss: 0.730\n",
      "\u001b[2m\u001b[36m(Trainer pid=98916)\u001b[0m rank: 4 epoch: 5, loss: 0.687\n",
      "\u001b[2m\u001b[36m(Trainer pid=98913)\u001b[0m rank: 1 epoch: 5, loss: 0.785\n",
      "\u001b[2m\u001b[36m(Trainer pid=98914)\u001b[0m rank: 2 epoch: 10, loss: 0.478\n",
      "\u001b[2m\u001b[36m(Trainer pid=98915)\u001b[0m rank: 3 epoch: 10, loss: 0.896\n",
      "\u001b[2m\u001b[36m(Trainer pid=98917)\u001b[0m rank: 5 epoch: 10, loss: 0.626\n",
      "\u001b[2m\u001b[36m(Trainer pid=98916)\u001b[0m rank: 4 epoch: 10, loss: 0.400\n",
      "\u001b[2m\u001b[36m(Trainer pid=98913)\u001b[0m rank: 1 epoch: 10, loss: 0.039\n",
      "\u001b[2m\u001b[36m(Trainer pid=98914)\u001b[0m rank: 2 epoch: 15, loss: 0.401\n",
      "\u001b[2m\u001b[36m(Trainer pid=98915)\u001b[0m rank: 3 epoch: 15, loss: 0.452\n",
      "\u001b[2m\u001b[36m(Trainer pid=98917)\u001b[0m rank: 5 epoch: 15, loss: 0.170\n",
      "\u001b[2m\u001b[36m(Trainer pid=98916)\u001b[0m rank: 4 epoch: 15, loss: 0.075\n",
      "\u001b[2m\u001b[36m(Trainer pid=98913)\u001b[0m rank: 1 epoch: 15, loss: 0.637\n",
      "\u001b[2m\u001b[36m(Trainer pid=98914)\u001b[0m rank: 2 epoch: 20, loss: 0.971\n",
      "\u001b[2m\u001b[36m(Trainer pid=98915)\u001b[0m rank: 3 epoch: 20, loss: 0.866\n",
      "\u001b[2m\u001b[36m(Trainer pid=98917)\u001b[0m rank: 5 epoch: 20, loss: 0.760\n",
      "\u001b[2m\u001b[36m(Trainer pid=98916)\u001b[0m rank: 4 epoch: 20, loss: 0.171\n",
      "\u001b[2m\u001b[36m(Trainer pid=98913)\u001b[0m rank: 1 epoch: 20, loss: 0.426\n"
     ]
    }
   ],
   "source": [
    "object_refs = [t.train.remote(s) for t, s in zip(trainers, shards)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d113e8e0-ca87-4cd4-9c8e-0d08f56cd51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42582662056684895,\n",
       " 0.9712170637373934,\n",
       " 0.8663240922489218,\n",
       " 0.17089797719933408,\n",
       " 0.7597569545682595]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(object_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd4776d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2afff0",
   "metadata": {},
   "source": [
    "### Exercises\n",
    " 1. Write some simple transformers, filters, and aggregators with our synthetic data. For example:\n",
    "  * use [`.add_column()`](https://docs.ray.io/en/master/data/package-ref.html) to add an `age` column\n",
    "  * filter by gender == 'U'\n",
    "  * aggregate (or groupby `property`) and count each. \n",
    " 2. Add additional pipleline stages function `def count_tx(...)` with our synthetic data. For example, count all people in state of `TX`, `married` and `defaulted`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2d47a",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "So far we have covered the basics of Ray Datasets. There are advanced topics that you can now explore since you know the basics. Below is a list of tasks you will want to work through at home.\n",
    "\n",
    "1. Work through the [NYC example tutorial](extra/ray_data_nyc.ipynb). This explores how you use `.map_batches()` for filtering and map operations using vectorized UDFs\n",
    "2. Peruse the user guides for advanced examples in [data transformation](https://docs.ray.io/en/master/data/transforming-datasets.html#transforming-datasets) and [ML preprocessing](https://docs.ray.io/en/master/data/dataset-ml-preprocessing.html#datasets-ml-preprocessing)\n",
    "3. Read how to do large scale [ML ingest](https://docs.ray.io/en/master/data/examples/big_data_ingestion.html)\n",
    "4. Advanced [pipeline usage](https://docs.ray.io/en/latest/data/advanced-pipelines.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7f39f-087f-4d60-8300-e91c85c0374e",
   "metadata": {},
   "source": [
    "### Next Step\n",
    "\n",
    "In Ray 2.0 we are introducing Ray AI Runtime (AIR) beta, an end-to-end machine learning toolkit. \n",
    "Let's take a peek into all its components and how you can use it as our [final lesson](ex_08_ray_air_in_ten_minutes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cfca1-e601-4e9c-b2c2-f05e4af86b4d",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [Ray Data Documentation](https://docs.ray.io/en/latest/data/dataset.html)\n",
    "2. [Ray Data Webinar Talk](https://www.anyscale.com/events/2022/02/23/ray-datasets-scalable-data-preprocessing-for-distributed-ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a252c3-c756-4f2e-976b-f3056f6722cb",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_08_ray_air_in_ten_minutes.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_06_ray_api_calls.ipynb) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
