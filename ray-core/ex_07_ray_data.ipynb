{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe990923",
   "metadata": {},
   "source": [
    "# Gentle introduction to Ray datasets APIs\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this introductory tutorial you will learn to:\n",
    " * create, transform, read, and save Ray datasets\n",
    " * use shards for parallel processing of large datasets\n",
    " * understand datapipelines and their merits\n",
    " * use `DatasetPipeline` for parallel computation \n",
    " * use datasets for last-mile ML ingestion for distributed training\n",
    " * why use datasets and what for\n",
    "\n",
    "### Overview\n",
    "\n",
    "This is a brief and gentle introduction to Ray's native library `ray dataset`. As a native Ray library, built atop Ray, it allows you to exchange data among Ray tasks, actors, libraries, and applications. It also allows you to read/write training data from different file sources, include csv, parquet, text, etc.\n",
    "\n",
    "Ray Datasets, using distributed [Apache Arrow](https://arrow.apache.org/), are designed to load and preprocess data for distributed ML training pipelines. Compared to other loading solutions, Datasets are more flexible (e.g., you can express higher-quality per-epoch global shuffles) and provides higher overall performance.\n",
    "\n",
    "Additionally, Ray datasets provides standard and simple transformations like `map`, `filter`, and `partition`. Ray datasets is *not* a replacement for a full-fledged data processing library for doing exploratory data analysis (EDA), extract, transform and load (ETL) or a subsitute for Apache Spark or Dask or Pandas DataFrames. Its primary objective is the last-mile rudimentary distributed data preprocessing and data ingestion for ML training.\n",
    "\n",
    "Supporting myriad [file formats and data sources](https://docs.ray.io/en/latest/data/dataset.html#datasource-compatibility), you can read from and write to local FS and cloud storage. \n",
    "\n",
    "<img src=\"images/dataset.png\" width=\"70%\" height=\"35%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6abd1f",
   "metadata": {},
   "source": [
    "### Key concepts\n",
    "\n",
    "To work with Ray Datasets, you need to understand how Datasets and Dataset Pipelines work. That is, how datasets are stored internally and in what format. And what benefit does Datapipelines offer for faster processing and execution. A quick peek into each of these will shed some light into overall benefits of Ray Datasets.\n",
    "\n",
    "Let's start with the internal format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7d84d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ray Datasets\n",
    "\n",
    "A Ray dataset implements a distributed [Apache Arrow](https://arrow.apache.org/). As such, a Dataset consists of a list of Ray object references to blocks. Each block holds a set of items in either an [Arrow table](https://arrow.apache.org/docs/python/data.html#tables) or a Python list (for Arrow incompatible objects).\n",
    "\n",
    "<img src=\"images/dataset-arch.png\" width=\"70%\" height=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456d3a7",
   "metadata": {},
   "source": [
    "#### Dataset Pipelines\n",
    "Datasets execute their transformations synchronously in blocking calls. However, it can be useful to overlap dataset computations with output. This can be done with a `DatasetPipeline`.\n",
    "\n",
    "A `DatasetPipeline` is an unified iterator over a (potentially infinite) sequence of Ray Datasets, each of which represents a window over the original data. Conceptually, it is similar to a `Spark DStream`, but manages execution over a bounded amount of source data instead of an unbounded stream. Ray computes each dataset window on-demand and stitches their output together into a single logical data iterator. `DatasetPipeline` implements most of the same transformation and output methods as Datasets (e.g., `map`, `filter`, `split`, `iter_rows`, `to_torch`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579c07b",
   "metadata": {},
   "source": [
    "### Datasets Execution Model\n",
    "This section overviews the execution model of Datasets, which may be useful for understanding and tuning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c6074",
   "metadata": {},
   "source": [
    "#### Reading Data\n",
    "Datasets uses Ray tasks, for parallelism, to read data from remote storage or source. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of parallel\n",
    "read tasks equal to the specified read parallelism (200 by default). One or more files will be assigned to each read task. Each read task reads its assigned files and produces one or more output blocks (Ray objects):\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/master/_images/dataset-read.svg\" height=\"25%\" width=\"50%\">\n",
    "\n",
    "In the common case, each read task produces a single output block. Read tasks may split the output into multiple blocks if the data exceeds the target max block size (2GiB by default). This automatic block splitting avoids out-of-memory errors when reading very large single files (e.g., a 100-gigabyte CSV file). All of the built-in datasources except for JSON currently support automatic block splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01de3f6",
   "metadata": {},
   "source": [
    "#### Deferred Read Task Execution\n",
    "\n",
    "When a Dataset is created using `ray.data.read_*`, only the first read task will be executed initially. This avoids blocking Dataset creation on the reading of all data files, enabling inspection functions like `ds.schema()` without incurring high read costs. `<ray.data.Dataset.schema>`() and `ds.show()` can be used right away. Executing further transformations on the Dataset will trigger execution of all read tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d75d13",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset Transforms\n",
    "\n",
    "Datasets use either Ray tasks or Ray actors to transform datasets (i.e., for `ds.map_batches()`, `ds.map()`, or `ds.flat_map()`). By default, tasks are used `(compute=\"tasks\")`. Actors can be specified with `compute=\"actors\"`, in which case an autoscaling pool of Ray actors will be used to apply transformations. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be re-used. Whichever compute strategy is used, each map task generally takes in one block and produces one or more output blocks. The output block splitting rule is the same as for file reads (blocks are split after hitting the target max block size of 2GiB):\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/master/_images/dataset-map.svg\" height=\"25%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0e08",
   "metadata": {},
   "source": [
    "#### Shuffling Data\n",
    "\n",
    "Certain operations like `ds.sort()` and `ds.groupby()` require data blocks to be partitioned by value. Datasets executes this in three phases. First, a wave of sampling tasks determines suitable partition boundaries based on a random sample of data. Second, map tasks divide each input block into a number of output blocks equal to the number of reduce tasks. Third, reduce tasks take assigned output blocks from each map task and combines them into one block. Overall, this strategy generates O(n^2) intermediate objects where n is the number of input blocks.\n",
    "\n",
    "You can also change the partitioning of a Dataset using `ds.random_shuffle()` or `ds.repartition()`. The former should be used if you want to randomize the order of elements in the dataset. The second should be used if you only want to equalize the size of the Dataset blocks (e.g., after a read or transformation that may skew the distribution of block sizes). Note that repartition has two modes, `shuffle=False`, which performs the minimal data movement needed to equalize block sizes, and `shuffle=True`, which performs a full (non-random) distributed shuffle:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/master/_images/dataset-shuffle.svg\" height=\"25%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fce60",
   "metadata": {},
   "source": [
    "#### Fault tolerance\n",
    "\n",
    "Datasets relies on task-based [fault tolerance](https://docs.ray.io/en/latest/ray-core/tasks/fault-tolerance.html) in Ray core. Specifically, a `Dataset` will be automatically recovered by Ray in case of failures. This works through **lineage reconstruction**: a Dataset is a collection of Ray objects stored in shared memory, and if any of these objects are lost, then Ray will recreate them by re-executing the task(s) that created them.\n",
    "\n",
    "There are a few cases that are not currently supported: 1. If the original creator of the Dataset dies. This is because the creator stores the metadata for the objects that comprise the Dataset. 2. `For a DatasetPipeline.split()`, we do not support recovery for a consumer failure. When there are multiple consumers, they must all read the split pipeline in lockstep. To recover from this case, the pipeline and all consumers must be restarted together. 3. The `compute=actors` option for transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3626753",
   "metadata": {},
   "source": [
    "#### Execution and Memory Management\n",
    "\n",
    "See [Execution and Memory Management](https://docs.ray.io/en/master/data/memory-management.html#data-advanced) for more details about how Datasets manages memory and optimizations such as lazy vs eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48005fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os, random, warnings\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab95cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d52759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.13', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-07-11_22-08-32_423805_50280/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-07-11_22-08-32_423805_50280/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-07-11_22-08-32_423805_50280', 'metrics_export_port': 61166, 'gcs_address': '127.0.0.1:58235', 'address': '127.0.0.1:58235', 'node_id': '1a1dc4771d109d4b868c0d9f111d11f787a55803e95168d24d784259'})\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ctx = ray.init(logging_level=logging.ERROR)\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38a8fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard url: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dashboard url: http://{ctx.address_info['webui_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a40fc",
   "metadata": {},
   "source": [
    "### Creating a simple Ray Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86adcdd",
   "metadata": {},
   "source": [
    "Let's create a generic dataset of 100K integers and look at the schema and underlying datatype. The difference between `show` and `take` is that the former takes one item at time and prints it, while the latter iterates over row items from the dataset, appends to a list and returns it. Underneath, `ds.show()` calls `ds.take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74585645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray.data.range(100_000)\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "790b11f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa1bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52f3e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6b809",
   "metadata": {},
   "source": [
    "### Creating a large Ray Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9604bc8",
   "metadata": {},
   "source": [
    "Let's create a synthetic dataset, *Homeowners*, of Arrow records (750K) with several columns and data associated with it. \n",
    "\n",
    "To illustrate some simple transformational functions, we'll use this generated \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de1fad4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'ssn': None,\n",
       "  'name': None,\n",
       "  'amount': 1.5,\n",
       "  'interest': 0.30000000000000004,\n",
       "  'state': 'UT',\n",
       "  'marital_status': 'single',\n",
       "  'property': 'rental',\n",
       "  'dependents': 1,\n",
       "  'defaulted': 0,\n",
       "  'gender': 'M'},\n",
       " {'id': 2,\n",
       "  'ssn': None,\n",
       "  'name': None,\n",
       "  'amount': 3.0,\n",
       "  'interest': 0.1,\n",
       "  'state': 'CA',\n",
       "  'marital_status': 'undeclared',\n",
       "  'property': 'house',\n",
       "  'dependents': 3,\n",
       "  'defaulted': 1,\n",
       "  'gender': 'M'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATES = [\"CA\", \"AZ\", \"OR\", \"WA\", \"TX\", \"UT\"]\n",
    "M_STATUS = [\"married\", \"single\", \"domestic\", \"divorced\", \"undeclared\"]\n",
    "GENDER = [\"F\", \"M\", \"U\"]\n",
    "HOME_OWNER = [\"condo\", \"house\", \"rental\"]\n",
    "\n",
    "items = [{\"id\": i,\n",
    "          \"ssn\": None,\n",
    "          \"name\": None,\n",
    "          \"amount\": i * 1.5, \n",
    "          \"interest\": random.randint(1,5) * .1,\n",
    "          \"state\": random.choice(STATES),\n",
    "          \"marital_status\": random.choice(M_STATUS),\n",
    "          \"property\": random.choice(HOME_OWNER),\n",
    "          \"dependents\": random.randint(1, 5),\n",
    "          \"defaulted\": random.randint(0,1),\n",
    "          \"gender\":random.choice(GENDER) } for i in range(1,750_001)]\n",
    "items[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38232ec7",
   "metadata": {},
   "source": [
    "#### Creating a dataset from list of dictionary items\n",
    "\n",
    "Ray data can be created of a dictionary of items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5968c4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=200, num_rows=750000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds = ray.data.from_items(items)\n",
    "arrow_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a87ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845b3a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 1,\n",
       "           'ssn': None,\n",
       "           'name': None,\n",
       "           'amount': 1.5,\n",
       "           'interest': 0.30000000000000004,\n",
       "           'state': 'UT',\n",
       "           'marital_status': 'single',\n",
       "           'property': 'rental',\n",
       "           'dependents': 1,\n",
       "           'defaulted': 0,\n",
       "           'gender': 'M'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f84ceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id: int64\n",
       "ssn: null\n",
       "name: null\n",
       "amount: double\n",
       "interest: double\n",
       "state: string\n",
       "marital_status: string\n",
       "property: string\n",
       "dependents: int64\n",
       "defaulted: int64\n",
       "gender: string"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b622e02",
   "metadata": {},
   "source": [
    "### Saving datasets and reading as a parquet file\n",
    "Ray datasets support myriad data formats and public storage. Let's save this dataset as a parquet file and create `N` partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c934bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition:   0%|                                                                                                                      | 0/5 [00:00<?, ?it/s]\u001b[2m\u001b[36m(_execute_read_task pid=50773)\u001b[0m E0711 22:13:21.997592000 6215544832 chttp2_transport.cc:1111]          Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "Repartition: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.73it/s]\n",
      "Write Progress: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 28.99it/s]\n"
     ]
    }
   ],
   "source": [
    "arrow_ds.repartition(5).write_parquet(\"data_homeowners/interest.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65ef04e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20856\n",
      "-rw-r--r--  1 jules  staff  2139602 Jul 11 22:13 e985818c43a44346ab1b59fc6d90ba54_000000.parquet\n",
      "-rw-r--r--  1 jules  staff  2121397 Jul 11 22:13 e985818c43a44346ab1b59fc6d90ba54_000001.parquet\n",
      "-rw-r--r--  1 jules  staff  2119623 Jul 11 22:13 e985818c43a44346ab1b59fc6d90ba54_000002.parquet\n",
      "-rw-r--r--  1 jules  staff  2119273 Jul 11 22:13 e985818c43a44346ab1b59fc6d90ba54_000003.parquet\n",
      "-rw-r--r--  1 jules  staff  2169557 Jul 11 22:13 e985818c43a44346ab1b59fc6d90ba54_000004.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l data_homeowners/interest.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds = ray.data.read_parquet(\"data_homeowners/interest.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0028f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 1,\n",
       "           'ssn': None,\n",
       "           'name': None,\n",
       "           'amount': 1.5,\n",
       "           'interest': 0.30000000000000004,\n",
       "           'state': 'UT',\n",
       "           'marital_status': 'single',\n",
       "           'property': 'rental',\n",
       "           'dependents': 1,\n",
       "           'defaulted': 0,\n",
       "           'gender': 'M'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef1509",
   "metadata": {},
   "source": [
    "### Transforming data with simple methods\n",
    "\n",
    "Ray datasets support transformation in parallel using `map`. It uses Ray tasks to execute eagerly or synchronously. Among others [transformations](https://docs.ray.io/en/latest/data/package-ref.html#dataset-api), it supports`filter`, `flat_map`, `groupBy`etc.\n",
    "\n",
    "Let's try a using `.map()`, `.filter()` and `.groupBy` on our dataset. The `map()` and `filter()` are\n",
    "row-based operations. This can be expensive for large datasets. However you can use `map_batches(...)` with batch_size=4096 as default. This will create a task per block and each batch will be vectorized and executed in parallel. Ray tasks are created per block for a map operation. \n",
    "\n",
    "Let's try first with row-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7297bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.3 ms, sys: 20.8 ms, total: 72.2 ms\n",
      "Wall time: 4.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 6667,\n",
       "           'ssn': None,\n",
       "           'name': None,\n",
       "           'amount': 10000.5,\n",
       "           'interest': 0.2,\n",
       "           'state': 'TX',\n",
       "           'marital_status': 'divorced',\n",
       "           'property': 'condo',\n",
       "           'dependents': 5,\n",
       "           'defaulted': 1,\n",
       "           'gender': 'M'}),\n",
       " ArrowRow({'id': 6668,\n",
       "           'ssn': None,\n",
       "           'name': None,\n",
       "           'amount': 10002.0,\n",
       "           'interest': 0.2,\n",
       "           'state': 'AZ',\n",
       "           'marital_status': 'divorced',\n",
       "           'property': 'rental',\n",
       "           'dependents': 5,\n",
       "           'defaulted': 0,\n",
       "           'gender': 'F'}),\n",
       " ArrowRow({'id': 6669,\n",
       "           'ssn': None,\n",
       "           'name': None,\n",
       "           'amount': 10003.5,\n",
       "           'interest': 0.5,\n",
       "           'state': 'AZ',\n",
       "           'marital_status': 'single',\n",
       "           'property': 'condo',\n",
       "           'dependents': 4,\n",
       "           'defaulted': 0,\n",
       "           'gender': 'U'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.filter(lambda x: x['amount'] > 10000).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b0897",
   "metadata": {},
   "source": [
    "Let's try a `.map_batches()`, which is vectorized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8116c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.4 ms, sys: 13.8 ms, total: 53.2 ms\n",
      "Wall time: 329 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'id': 6667,\n",
       "            'ssn': None,\n",
       "            'name': None,\n",
       "            'amount': 10000.5,\n",
       "            'interest': 0.2,\n",
       "            'state': 'TX',\n",
       "            'marital_status': 'divorced',\n",
       "            'property': 'condo',\n",
       "            'dependents': 5,\n",
       "            'defaulted': 1,\n",
       "            'gender': 'M'}),\n",
       " PandasRow({'id': 6668,\n",
       "            'ssn': None,\n",
       "            'name': None,\n",
       "            'amount': 10002.0,\n",
       "            'interest': 0.2,\n",
       "            'state': 'AZ',\n",
       "            'marital_status': 'divorced',\n",
       "            'property': 'rental',\n",
       "            'dependents': 5,\n",
       "            'defaulted': 0,\n",
       "            'gender': 'F'}),\n",
       " PandasRow({'id': 6669,\n",
       "            'ssn': None,\n",
       "            'name': None,\n",
       "            'amount': 10003.5,\n",
       "            'interest': 0.5,\n",
       "            'state': 'AZ',\n",
       "            'marital_status': 'single',\n",
       "            'property': 'condo',\n",
       "            'dependents': 4,\n",
       "            'defaulted': 0,\n",
       "            'gender': 'U'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.map_batches(lambda df: df[df[\"amount\"] > 10000]).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95926ece",
   "metadata": {},
   "source": [
    "You can see that `.map_batches()` is a lot faster than row based. So for large datasets use \n",
    "`.map_batches()`\n",
    "\n",
    "Let's try a filter operation: both per row operation and per block as vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9164836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 ms, sys: 12.1 ms, total: 42.1 ms\n",
      "Wall time: 1.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'id': 6670,\n",
       "           'ssn': None,\n",
       "           'name': None,\n",
       "           'amount': 10005.0,\n",
       "           'interest': 0.5,\n",
       "           'state': 'CA',\n",
       "           'marital_status': 'single',\n",
       "           'property': 'rental',\n",
       "           'dependents': 4,\n",
       "           'defaulted': 1,\n",
       "           'gender': 'F'}),\n",
       " ArrowRow({'id': 6674,\n",
       "           'ssn': None,\n",
       "           'name': None,\n",
       "           'amount': 10011.0,\n",
       "           'interest': 0.4,\n",
       "           'state': 'CA',\n",
       "           'marital_status': 'married',\n",
       "           'property': 'rental',\n",
       "           'dependents': 5,\n",
       "           'defaulted': 1,\n",
       "           'gender': 'F'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.filter(lambda x: x['amount'] > 10000.00 and x['state'] == 'CA').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f98958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 36.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.9 ms, sys: 10.2 ms, total: 40.1 ms\n",
      "Wall time: 159 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'id': 2,\n",
       "            'ssn': None,\n",
       "            'name': None,\n",
       "            'amount': 3.0,\n",
       "            'interest': 0.1,\n",
       "            'state': 'CA',\n",
       "            'marital_status': 'undeclared',\n",
       "            'property': 'house',\n",
       "            'dependents': 3,\n",
       "            'defaulted': 1,\n",
       "            'gender': 'M'}),\n",
       " PandasRow({'id': 4,\n",
       "            'ssn': None,\n",
       "            'name': None,\n",
       "            'amount': 6.0,\n",
       "            'interest': 0.5,\n",
       "            'state': 'CA',\n",
       "            'marital_status': 'domestic',\n",
       "            'property': 'rental',\n",
       "            'dependents': 2,\n",
       "            'defaulted': 1,\n",
       "            'gender': 'F'}),\n",
       " PandasRow({'id': 8,\n",
       "            'ssn': None,\n",
       "            'name': None,\n",
       "            'amount': 12.0,\n",
       "            'interest': 0.30000000000000004,\n",
       "            'state': 'CA',\n",
       "            'marital_status': 'married',\n",
       "            'property': 'house',\n",
       "            'dependents': 2,\n",
       "            'defaulted': 0,\n",
       "            'gender': 'U'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arrow_ds.map_batches(lambda df: df[[df[\"amount\"] > 10000] and df[\"state\"] == \"CA\"]).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e754a08",
   "metadata": {},
   "source": [
    "Use `groupBy` state and compute the count\n",
    "\n",
    "Under the hood is distributed parallel group and vectorized not using UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7588807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 208.96it/s]\n",
      "Sort Sample: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1973.98it/s]\n",
      "Shuffle Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.19it/s]\n",
      "Shuffle Reduce: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1269.62it/s]\n"
     ]
    }
   ],
   "source": [
    "results = arrow_ds.groupby(\"state\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec43ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': 'AZ', 'count()': 125395}\n",
      "{'state': 'CA', 'count()': 124339}\n",
      "{'state': 'OR', 'count()': 124197}\n",
      "{'state': 'TX', 'count()': 125754}\n",
      "{'state': 'UT', 'count()': 125485}\n",
      "{'state': 'WA', 'count()': 124830}\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edfea7",
   "metadata": {},
   "source": [
    "Get the max of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7491c77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 216.87it/s]\n",
      "Shuffle Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 659.27it/s]\n",
      "Shuffle Reduce: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 345.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArrowRow({'max(amount)': 1125000.0,\n",
       "          'max(interest)': 0.5,\n",
       "          'max(dependents)': 5})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=arrow_ds.max([\"amount\", \"interest\", \"dependents\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9ae36",
   "metadata": {},
   "source": [
    "### Accessing datasets using batches or iterating by rows\n",
    "\n",
    "Datasets can be passed to Ray tasks or actors and read with `.iter_batches()` or `.iter_rows()`. This does not incur a copy, since the blocks of the Dataset are passed by reference as Ray objects. Splitting data as shards and passing to individual Ray Actors to process shards in a common Ray pattern used in distributed training with Ray actors.\n",
    "\n",
    "Let's examine how we can process a list of shards with a `BatchWorker` Actor  in a distributed fashion\n",
    "\n",
    "<img src=\"images/batch_worker.jpg\" width=\"80%\" height=\"35%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ccb43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class BatchWorker:\n",
    "    def __init__(self, rank):\n",
    "        self.rank = rank\n",
    "        self.processed = 0\n",
    "    \n",
    "    @ray.method(num_returns=2)\n",
    "    def process_shard_list(self, shard: ray.data.Dataset) -> tuple:\n",
    "        for batch in shard.iter_batches(batch_size=1024):\n",
    "            # here you could do something with the batch such as feature\n",
    "            # processing, transformation, and \n",
    "            # save as a parquet files \n",
    "            self.processed = self.processed + len(batch)\n",
    "        # return items processed, worker id\n",
    "        return (self.processed, self.rank)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe723a",
   "metadata": {},
   "source": [
    "#### Create batch workers as Ray actors\n",
    "Each actor will get a shard, list of rows, to work on. We split\n",
    "our dataset `arrow_ds` into five shards. Each `BatchWorker` gets a shard.\n",
    "`.split`() splits shards across these batch of workers by using the `locality_hints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecefae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 142.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard row: Dataset(num_blocks=1, num_rows=150000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})\n",
      "Number of shards:5\n",
      "Number of shard workers:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_workers = [BatchWorker.remote(i) for i in range(1, 6)]\n",
    "\n",
    "shards = arrow_ds.split(len(batch_workers), locality_hints=batch_workers)\n",
    "\n",
    "print(f\"Shard row: {shards[0]}\")\n",
    "print(f\"Number of shards:{len(shards)}\")\n",
    "print(f\"Number of shard workers:{len(batch_workers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305ab3b",
   "metadata": {},
   "source": [
    "### Launch `BatchWorker` actors\n",
    "\n",
    "Process each shard. Each `BatchWorker.process_shard_list()` returns a object RefID with a tuple as its value. What we get from this comprehension is a list objectRefs as tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3e8db4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[ObjectRef(5f70e045687d2f9af4ace96163e8c6fa53bbf3ae0100000001000000),\n",
       "   ObjectRef(5f70e045687d2f9af4ace96163e8c6fa53bbf3ae0100000002000000)],\n",
       "  [ObjectRef(a4dc031465f905f85008f36129600f8fb1f77eb10100000001000000),\n",
       "   ObjectRef(a4dc031465f905f85008f36129600f8fb1f77eb10100000002000000)],\n",
       "  [ObjectRef(9e7872a82e7456d9d43a11c5282a398555bdad870100000001000000),\n",
       "   ObjectRef(9e7872a82e7456d9d43a11c5282a398555bdad870100000002000000)],\n",
       "  [ObjectRef(cd25e647a728676b33635ef154b9167d239916360100000001000000),\n",
       "   ObjectRef(cd25e647a728676b33635ef154b9167d239916360100000002000000)],\n",
       "  [ObjectRef(57f023b5f2c83c933e245432417ea6404d8b57c30100000001000000),\n",
       "   ObjectRef(57f023b5f2c83c933e245432417ea6404d8b57c30100000002000000)]],\n",
       " 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_refs = [w.process_shard_list.remote(s) for w, s in zip(batch_workers, shards)]\n",
    "object_refs, len(object_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afaddf5",
   "metadata": {},
   "source": [
    "Fetch the values from the returned list of ObjectRefs, which is a tuple of (batch_size, worker_rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aa6d80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[150000, 1], [150000, 2], [150000, 3], [150000, 4], [150000, 5]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [ray.get(ref) for ref in object_refs]\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea260db0",
   "metadata": {},
   "source": [
    "### Creating and using Ray dataset pipelines\n",
    "\n",
    "What are dataset pipelines and how are they different from Ray datasets? \n",
    "\n",
    "Datasets perform transformation or operations eagerly or synchronously, whereas [DataPipelines](https://docs.ray.io/en/latest/data/package-ref.html#datasetpipeline-api) can execute in an overlapped pipeline executions. For example, if you had operations that require reading from file, transforming data, and then doing some minor feature engineering, these operations can be executed in a normal pipeline fashion. This allows for the overlapped execution of data input (e.g., reading files), computation (e.g. feature preprocessing), and training (e.g., distributed ML training).\n",
    "\n",
    "A `DatasetPipeline` can be constructed in two ways: either by pipelining the execution of an existing Dataset (via `Dataset.window`) or generating repeats of an existing Dataset (via `Dataset.repeat`). \n",
    "\n",
    "Let's have a go at it and see what we can do with our synthetic data from above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e4aec",
   "metadata": {},
   "source": [
    "### Using Dataset.window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4270d8d",
   "metadata": {},
   "source": [
    "Create some functions or operations to be executed in a overlapped manner in the pipeline. These functions\n",
    "are simple to illustrate a point. But they can be complex for a particular use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fde9105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_row_value(row: ray.data.impl.arrow_block.ArrowRow, n) -> int:\n",
    "    return round(row / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d544e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_row_value(row: ray.data.impl.arrow_block.ArrowRow, n) -> int:\n",
    "    return row * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25ae7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulo_row_value(row: ray.data.impl.arrow_block.ArrowRow, n) -> int:\n",
    "    return row % random.randint(1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e447c5",
   "metadata": {},
   "source": [
    "#### Create a window based pipeline\n",
    "With a each window of 50 blocks. \n",
    "\n",
    "_Questions for clarification_:\n",
    " * _why the number of stages is 2?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0575078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=4, num_stages=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_pipe = ds.window(blocks_per_window=50)\n",
    "ds_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cc58b",
   "metadata": {},
   "source": [
    "### Applying transforms to pipelines adds more pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b722702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=4, num_stages=5)\n"
     ]
    }
   ],
   "source": [
    "ds_pipe = ds_pipe.map(lambda row: divide_row_value(row, 2))\n",
    "ds_pipe = ds_pipe.map(lambda row: double_row_value(row, 3))\n",
    "ds_pipe = ds_pipe.map(lambda row: modulo_row_value(row, 4))\n",
    "print(ds_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f9992",
   "metadata": {},
   "source": [
    "#### Iterate our pipeline\n",
    "\n",
    " * _Questions for clearification_:\n",
    "     * _how is this executed_?\n",
    "     * _why are we iterating over rows_?\n",
    "     * _what is row comprised of? Blocks?_?\n",
    "     * _is the value of the row an already computed value_?\n",
    "     * _if the `num_stages=5`, why am I seeing only stage 0 and 1 in the output of stages?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c0c2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                                                          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                   | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                                                          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 0:  50%|█████████████████████████████████████████████████████████                                                         | 2/4 [00:00<00:00,  2.96it/s]\u001b[A\n",
      "Stage 0:  75%|█████████████████████████████████████████████████████████████████████████████████████▌                            | 3/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "Stage 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.25it/s]\u001b[A\n",
      "Stage 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value: 37036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "print(f\"Total value: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e28847",
   "metadata": {},
   "source": [
    "Let's try a Datapipeline with our synthetic data *Homewowners*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "184f2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count or return based on the condition\n",
    "def count_state(row: ray.data.impl.arrow_block.ArrowRow, state) -> int:\n",
    "    return 1 if row['state'] == \"CA\" and row[\"defaulted\"] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5ce3775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=1, num_stages=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds_pipe = arrow_ds.window(blocks_per_window=50)\n",
    "arrow_ds_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5db9b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=1, num_stages=3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_ds_pipe = arrow_ds_pipe.map(lambda row: count_state(row, \"CA\"))\n",
    "arrow_ds_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc6376fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                                                          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.71s/it]\u001b[A\n",
      "Stage 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows for CA state and defaulted loans rows: 62339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in arrow_ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "print(f\"Total rows for CA state and defaulted loans rows: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7071a",
   "metadata": {},
   "source": [
    "## Ingesting data into Model Trainers\n",
    "Let's define a toy `Trainer` actor that takes our synthetic data and trains the model and returns loss for that trainer. This is common pattern\n",
    "in Ray for distributing data to Trainers in a Ray cluster.\n",
    "\n",
    "<img src=\"images/trainer_worker.jpg\" width=\"75%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e66729d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    return random.uniform(0, 1)\n",
    "\n",
    "@ray.remote\n",
    "class Trainer:\n",
    "    def __init__(self, rank, model):\n",
    "        self.rank = rank\n",
    "        self.model = model\n",
    "        self.loss = 0.0\n",
    "        \n",
    "    def train(self, shard:ray.data.Dataset) -> float:\n",
    "        for batch in shard.iter_batches(batch_size=1024):\n",
    "            for epoch in range(1,21):\n",
    "                output = self.model(batch)\n",
    "                self.loss = output \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'epoch {epoch}, loss: {self.loss:.3f}')\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a4fb8",
   "metadata": {},
   "source": [
    "#### Create five trainers, each with a copy of the model and each training on its respective shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "632e4cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Actor(Trainer, e4fd45ef24f31d2a06522e6001000000),\n",
       " Actor(Trainer, e970e56818bb7578580f577501000000),\n",
       " Actor(Trainer, 43ee67fe2c6461d824cae15501000000),\n",
       " Actor(Trainer, 1fc03fbd81f81d1c1cbd1d6501000000),\n",
       " Actor(Trainer, 60917854ad08c0fd3556c6bd01000000)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=51739)\u001b[0m E0711 22:21:30.234691000 6212841472 chttp2_transport.cc:1111]          Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=51741)\u001b[0m E0711 22:21:30.402044000 6169604096 chttp2_transport.cc:1111]          Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n"
     ]
    }
   ],
   "source": [
    "trainers = [Trainer.remote(i, model) for i in range(1, 6)]\n",
    "trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba3400",
   "metadata": {},
   "source": [
    "#### Split the shards across all trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78fccfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dataset(num_blocks=1, num_rows=150000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=150000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=150000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=150000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string}),\n",
       " Dataset(num_blocks=1, num_rows=150000, schema={id: int64, ssn: null, name: null, amount: double, interest: double, state: string, marital_status: string, property: string, dependents: int64, defaulted: int64, gender: string})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shards = arrow_ds.split(n=len(trainers), locality_hints=trainers)\n",
    "shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e27a15",
   "metadata": {},
   "source": [
    "#### Launch our trainers in a distributed fashion\n",
    "\n",
    "This will run across the cluster. Check the dashbard to see five actors launched. On a cluster, they will on five different nodes, whereas on a single noded \n",
    "five different cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "276306c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03482623803499796,\n",
       " 0.2368608264989308,\n",
       " 0.9545626385599913,\n",
       " 0.5875607301935346,\n",
       " 0.14364284659846804]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Trainer pid=52095)\u001b[0m epoch 20, loss: 0.588\n",
      "\u001b[2m\u001b[36m(Trainer pid=52096)\u001b[0m epoch 20, loss: 0.144\n",
      "\u001b[2m\u001b[36m(Trainer pid=52092)\u001b[0m epoch 20, loss: 0.035\n",
      "\u001b[2m\u001b[36m(Trainer pid=52094)\u001b[0m epoch 20, loss: 0.955\n",
      "\u001b[2m\u001b[36m(Trainer pid=52093)\u001b[0m epoch 20, loss: 0.237\n"
     ]
    }
   ],
   "source": [
    "object_refs = [t.train.remote(s) for t, s in zip(trainers, shards)]\n",
    "ray.get(object_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd4776d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2afff0",
   "metadata": {},
   "source": [
    "### Exercises\n",
    " 1. Write some simple transformers, filters, and aggregators with our synthetic data. For example:\n",
    "  * use [`.add_column()`](https://docs.ray.io/en/master/data/package-ref.html) to add an `age` column\n",
    "  * filter by gender == 'U'\n",
    "  * aggregate (or groupby `property`) and count each. \n",
    " 2. Add additional pipleline stages function `def count_tx(...)` with our synthetic data. For example, count all people in state of `TX`, `married` and `defaulted`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2d47a",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Work through the [NYC example tutorial](extra/ray_data_nyc.ipynb). This explores how you use `.map_batches()` for filtering and map operations using vectorized UDFs\n",
    "2. Peruse the user guides for advanced examples in [data transformation](https://docs.ray.io/en/master/data/transforming-datasets.html#transforming-datasets) and [ML preprocessing](https://docs.ray.io/en/master/data/dataset-ml-preprocessing.html#datasets-ml-preprocessing)\n",
    "3. Read how to do large scale [ML ingest](https://docs.ray.io/en/master/data/examples/big_data_ingestion.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
