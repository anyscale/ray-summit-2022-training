{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89505bc4-70e5-4ed5-9b91-f75804ce40db",
   "metadata": {},
   "source": [
    "# Gentle introduction to Ray datasets APIs\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Overview\n",
    "\n",
    "This is a brief introduction to Ray's native library `ray dataset`. As a native Ray library, built atop Ray, it allows you to exchange data among Ray tasks, actors, libraries, and applications. Additionally, Ray datasets provides standard transformations like `map`, `filter`, and `partition`. Ray datasets is *not* a replacement for a full-fledged data processing library EDA, ETL or a subsitute for Apache Spark or Dask or Pandas DataFrames. It's primary objective is last-mile rudimentary data preprocessing and data ingestion for ML training.\n",
    "\n",
    "Supporting myriad [file formats and data sources](https://docs.ray.io/en/latest/data/dataset.html#datasource-compatibility), you can read from and write to local FS and cloud storage. \n",
    "\n",
    "<img src=\"images/dataset.png\" width=\"70%\" height=\"35%\">\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this introductory tutorial you will learn:\n",
    " * create, transform, read and save Ray datasets\n",
    " * use shards for parallel processing of large datasets\n",
    " * understand datapipelines and their merits\n",
    " * use DatasetPipeline for last-mile ML ingestion for your distributed trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca802b-f3d5-4227-b43c-08fd14878021",
   "metadata": {},
   "source": [
    "### Ray Datasets\n",
    "\n",
    "A Ray dataset implements a distributed [Apache Arrow](https://arrow.apache.org/). A Dataset consists of a list of Ray object references to blocks. Each block holds a set of items in either an [Arrow table](https://arrow.apache.org/docs/python/data.html#tables) or a Python list (for Arrow incompatible objects).\n",
    "\n",
    "<img src=\"images/dataset-arch.png\" width=\"70%\" height=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273919e-5310-45ff-8ec7-7a1db65a4a93",
   "metadata": {},
   "source": [
    "### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e1661-7411-4ff2-8752-61a564b3987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, random\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79e3c7-feba-4d77-afc2-c29ae62a4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ctx = ray.init(logging_level=logging.ERROR)\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cf1de-71d7-478e-a13e-3c53ae0ab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dashboard url: http://{ctx.address_info['webui_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdbd1d-7083-44b6-918d-de1312af52f2",
   "metadata": {},
   "source": [
    "Let's create a generic dataset of 50K integers and look at the schema and underlying datatype. The difference between `show` and `take` is that the former takes one item at time and prints it, while the latter iterates over rows items from the dataset, appends to a list and returns it. `ds.show()` calls `ds.take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2460a28-e0dd-4745-b52b-2a66c74c48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.range(100_000)\n",
    "ds.count(), ds.schema(), ds.show(5), ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79f73a-b7a5-42f6-bfab-e0db44c13866",
   "metadata": {},
   "source": [
    "Let's create a synthetic dataset of Arrow records with seven columns and data associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b84110-bcc8-4ea3-bdf2-ff6031caf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = [\"CA\", \"AZ\", \"OR\", \"WA\", \"TX\", \"UT\"]\n",
    "M_STATUS = [\"married\", \"single\", \"domestic\", \"divorced\", \"undeclared\"]\n",
    "GENDER = [\"F\", \"M\", \"U\"]\n",
    "\n",
    "items = [{\"id\": i, \n",
    "          \"amount\": i * 1.5, \n",
    "          \"interest\": random.randint(1,5) * .1,\n",
    "          \"state\": random.choice(STATES),\n",
    "          \"marital_status\": random.choice(M_STATUS),\n",
    "          \"defaulted\": random.randint(0,1),\n",
    "          \"gender\":random.choice(GENDER) } for i in range(1,250_001)]\n",
    "items[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5f028-aa5b-4e40-9d89-c2458eae5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds = ray.data.from_items(items)\n",
    "arrow_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a52903-b132-4c7a-9920-274d5620a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds.count(), arrow_ds.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d1e5c-4d22-4ec8-98e7-a98e8c977b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf52fcd-041c-4008-93db-e86cd8f6fd1c",
   "metadata": {},
   "source": [
    "### Saving datasets\n",
    "Ray datasets support myriad data formats and public storage. Let's save this dataset as a parquet file and create 4 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbf30b-79dc-41ee-8aae-204b0fa8cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds.repartition(5).write_parquet(\"data/interest.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca6b26-4a08-44a7-8b9b-5a7582945384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l data/interest.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180628b1-d64f-44ad-8cb0-961da2a5ca40",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "Ray datasets support transformation in parallel using `map`. It uses ray tasks to execute eagerly or synchronously. Among others [transformations](https://docs.ray.io/en/latest/data/package-ref.html#dataset-api), it supports`filter`, `flat_map`, `groupBy`etc.\n",
    "\n",
    "Let's try a using `.map()` and `.filter()` on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f6e9f-c100-4b6d-9647-340a42686b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds.map(lambda x: x['amount'] * 2.5).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b8286-942c-4c9d-af44-44c4b62120d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds.filter(lambda x: x['amount'] > 10000.00 and x['state'] == 'CA').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2dd6ac-6aa0-44dc-91ef-96382725eef9",
   "metadata": {},
   "source": [
    "### Accessing datasets\n",
    "\n",
    "Datasets can be passed to Ray tasks or actors and read with `.iter_batches()` or `.iter_rows()`. This does not incur a copy, since the blocks of the Dataset are passed by reference as Ray objects. Splitting data as shards and passing to individual Ray Actors to process shards in a common Ray pattern used in distributed training with Ray actors.\n",
    "\n",
    "Let's examine how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83caaaa-6871-4d67-86a3-4c3c6f000c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class BatchWorker:\n",
    "    def __init__(self, rank):\n",
    "        self.rank = rank\n",
    "        self.processed= 0\n",
    "    \n",
    "    @ray.method(num_returns=2)\n",
    "    def process_shard_list(self, shard) -> tuple:\n",
    "        for batch in shard.iter_batches(batch_size=1024):\n",
    "            # do something with the batch such as feature\n",
    "            # processing and transformation and \n",
    "            # save as a parquet files \n",
    "            self.processed = self.processed + len(batch)\n",
    "        # return items processed, worker id\n",
    "        return (self.processed, self.rank)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e4b2d-acf0-4f50-90ea-f0b43f9e66f1",
   "metadata": {},
   "source": [
    "#### Create batch workers as Ray actors\n",
    "Each actor will get a shard, list of rows, to work on. We split\n",
    "our dataset `arrow_ds` into five shards. Each `BatchWorker` gets a shard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371d6f7-b46c-42f3-b820-dfe3278bed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_workers = [BatchWorker.remote(i) for i in range(1, 6)]\n",
    "shards = arrow_ds.split(n=5, locality_hints=batch_workers)\n",
    "shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db79aff-6118-4be7-b642-ba06a5d4c939",
   "metadata": {},
   "source": [
    "Launch the `BatchWorker` actors to process each shard. Each `BatchWorker.process_shard_list()` returns a object RefID with a tuple as its value. What we get from this comprehension is a list objectRefs as tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09dcec4-50e3-493c-9a4b-090019d41eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_refs = [w.process_shard_list.remote(s) for w, s in zip(batch_workers, shards)]\n",
    "# object_refs, len(object_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704782b6-ff54-4b99-8fd1-3c51635e0e02",
   "metadata": {},
   "source": [
    "Fetch the values returned from the returne list of ObjectRefs, which is a tuple of (batch_size, worker_rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23e87b-0b28-43cf-b180-40a3e7fd2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [ray.get(ref) for ref in object_refs]\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5aae6-029a-4689-8c36-2f1ca59050ee",
   "metadata": {},
   "source": [
    "### Creating and using dataset pipelines\n",
    "\n",
    "What are dataset pipelines and how are they different from Ray datasets? Datasets perform transformation or operations eagerly or synchronously, whereas [DataPipelines](https://docs.ray.io/en/latest/data/package-ref.html#datasetpipeline-api) can execute in an overlap pipeline executions. For example, if you had operations that require reading from file, transforming data, and then doing some minor feature engineering, these operations can be executed in a normal pipeline fashion. This allows for the overlapped execution of data input (e.g., reading files), computation (e.g. feature preprocessing), and output (e.g., distributed ML training).\n",
    "\n",
    "A DatasetPipeline can be constructed in two ways: either by pipelining the execution of an existing Dataset (via `Dataset.window`), or generating repeats of an existing Dataset (via `Dataset.repeat`). As stated, there a couple of ways to create a pipeline in a staged manner from an existing Ray dataset.\n",
    "\n",
    "Let's have a go at it and see what we can do with our synthetic data from above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38965a05-d9ce-449f-9db0-84171c00303a",
   "metadata": {},
   "source": [
    "### Using Dataset.window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35cd62-b5f5-494c-bd56-b005644a5558",
   "metadata": {},
   "source": [
    "Create some functions or operations to be executed in a overlapped manner in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2fbec08-e650-4a9c-a360-b724fdc1055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_1(row: ray.data.impl.arrow_block.ArrowRow):\n",
    "    return row + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4a2303-b340-4205-99d6-03ca14a91e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_2(row: ray.data.impl.arrow_block.ArrowRow):\n",
    "    return row * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04dab574-38c0-4900-a36d-c6fe6a774dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_3(row: ray.data.impl.arrow_block.ArrowRow):\n",
    "    return row % 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a426771-8f76-4ab0-ad36-ceef903772b3",
   "metadata": {},
   "source": [
    "#### Create a window based pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aebd77d-88e2-459a-ad83-c3e9ed1a7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pipe = ds.window(blocks_per_window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "868c3fc6-e796-4eb7-bbfa-959c41e5c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=4, num_stages=5)\n"
     ]
    }
   ],
   "source": [
    "# Applying transforms to pipelines adds more pipeline stages.\n",
    "ds_pipe = ds_pipe.map(count_1)\n",
    "ds_pipe = ds_pipe.map(count_2)\n",
    "ds_pipe = ds_pipe.map(count_2)\n",
    "print(ds_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd6fcb-00f2-44d3-bbaf-55ef50d80c9e",
   "metadata": {},
   "source": [
    "#### Iterate our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cf43855-6977-4192-9c54-479f060b9f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                       | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                       | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 0:  50%|ââââââââââââââââââââââââââââââââââââââââ                                       | 2/4 [00:01<00:01,  1.51it/s]\u001b[A\n",
      "Stage 0:  75%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ                   | 3/4 [00:02<00:00,  1.08it/s]\u001b[A\n",
      "Stage 0: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 4/4 [00:03<00:00,  1.02s/it]\u001b[A\n",
      "Stage 1: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 4/4 [00:05<00:00,  1.29s/it]\u001b[A\n",
      "Stage 0: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 4/4 [00:05<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value: 20000200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "print(f\"Total value: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302cec8-1121-4ba3-adf5-c48389ed18a5",
   "metadata": {},
   "source": [
    "Let's try with our synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e210ee27-8f26-4f85-9b17-952136d3f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count or return based on the condition\n",
    "def count_ca(row: ray.data.impl.arrow_block.ArrowRow):\n",
    "    return 1 if row['state'] == \"CA\" and row[\"defaulted\"] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35c01edd-aa8c-4ed5-bfb3-e754e7712511",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds_pipe = arrow_ds.window(blocks_per_window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e74deb3e-567f-418f-b7b3-a30315bbadec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=4, num_stages=2)\n"
     ]
    }
   ],
   "source": [
    "arrow_ds_pipe = arrow_ds_pipe.map(count_ca)\n",
    "print(arrow_ds_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a440f3c-109b-4e5f-aa87-4208302514eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|                                                                                       | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|                                                                                       | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 0:  50%|ââââââââââââââââââââââââââââââââââââââââ                                       | 2/4 [00:01<00:01,  1.63it/s]\u001b[A\n",
      "Stage 0:  75%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ                   | 3/4 [00:01<00:00,  1.93it/s]\u001b[A\n",
      "Stage 0: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 4/4 [00:01<00:00,  2.21it/s]\u001b[A\n",
      "Stage 1: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 4/4 [00:02<00:00,  1.64it/s]\u001b[A\n",
      "Stage 0: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 4/4 [00:02<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CA state and defaulted loans rows: 20761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for row in arrow_ds_pipe.iter_rows():\n",
    "    results.append(row)\n",
    "print(f\"Total CA state and defaulted loans rows: {sum(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df42a9ca-54a9-4b93-8dc0-dcc18b5636a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
