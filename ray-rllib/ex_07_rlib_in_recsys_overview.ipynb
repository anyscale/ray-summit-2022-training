{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05. (Take-home) Advanced Topic: Adding an in-game Recommender using RLlib\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn how to:\n",
    "\n",
    " * [Intro RecSys with RL](#recsys_rl)\n",
    " * [Create a RecSys RL environment](#recsys_env)\n",
    " * [Train a Contextual Bandit on the environment](#cb)\n",
    " * [Train using a RL Offline algorithm on the environment](#offline)\n",
    " \n",
    " \n",
    " find RLlib algos to train policy models on environments.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro RecSys with RL <a class=\"anchor\" id=\"recsys_rl\"></a>\n",
    "\n",
    "A Recommender System <b>(RecSys)</b> suggests items that are most pertinent to a particular user.  Examples of recommender systems include:\n",
    "<ul>\n",
    "    <li>Movie/video/music recommendations (Netflix/YouTube/Spotify)</li>\n",
    "    <li>Online shopping recommendations (Amazon/Shopify)</li>\n",
    "    <li>Filtering your feed as you scroll (Twitter/Instagram)</li>\n",
    "</ul>\n",
    "\n",
    "<b>Two main approaches to training algorithms</b> for RecSys are: \n",
    "<ol>\n",
    "    <li>Traditional Machine Learning <b>(ML)</b></li>\n",
    "    <li>Reinforcement Learning <b>(RL)</b></li>\n",
    "    </ol>\n",
    "\n",
    "<b>In the traditional ML method</b>, data is gathered about users interactions with items.  Matrix factorization is often used to turn this data into trainable representations of users and items (\"embeddings\" or features or X's), and the views or actions by users of those items (dependent variable or y's). A ranking algorithm (e.g. collaborative filtering) is trained on all the data at once as if all the actions happend during the same time step.  Such a <b><i>static</i> </b>, fixed model is useful when there are millions of items and users, since learning from all data at once in <b>batch learning</b> is efficient.\n",
    "\n",
    "<b>In RL</b>, users interact with offers repeatedly over time.  This <b><i>dynamic</i></b> model is iteratively trained based on \n",
    "the last observation, action, and reward.  One caveat with RL, since a recommendation needs to be calculated at every time step, only a pre-selected handful of top candidate items per user (from the traditional ML ranking model) is presented in the simulation environment.  \n",
    "\n",
    "Both Online RL and Offline RL are important parts of modern Recommender Systems. \n",
    "<b>Online RL in RecSys</b> takes as input the top-K ranked recommendations per user, keeps the user and item feature embeddings as context, but optimizes a sequence of user in-session interactions <b>real-time</b> using live results.  \n",
    "\n",
    "<img src=\"./images/recsys_overview.png\" width=\"100%\" />\n",
    "\n",
    "<b>Offline RL in RecSys</b> typically uses log files to explore the last recommender model put into production, implicitly through the data.  ‚ÄúSerendipitous‚Äù aspects of user experience can be explored through offline RL, since random actions the user did not historically take can be tried in the simulation.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "    <b>üí° Online vs Offline RL, when the algorithm learning from an environment is: </b> <br><br>\n",
    "    ‚úîÔ∏è live (typically gaming platforms or complex systems simulations), this is called <b>online RL</b> and evaluation during training is <b>on-policy</b>. <br><br>\n",
    "    ‚úîÔ∏è trained using log files (users interactions with items), this is called <b>offline RL</b> and evaluation during training is <b>off-policy</b>, because the policy (RL word for model) used to log the data is different from the policy used to explore the data. </b> \n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a RecSys RL Environment <a class=\"anchor\" id=\"recsys_env\"></a>\n",
    "\n",
    "As we learned in the first 2 lessons, the first step to training a RL RecSys policy model is to create an <b>environment</b> that can interact with a RL Algorithm to train a recommender agent. \n",
    "\n",
    "In this notebook, we will use <b><a href=\"https://github.com/google-research/recsim\">Google's RecSim environment</a></b>, which was developed for the YouTube recommendation problem.  The environment is <i>Timelimit-based</i>, meaning the termination condition for an episode will be after a fixed number (60) of videos are watched. The RecSim environment consists of:\n",
    "\n",
    "<img src=\"./images/recsim_environment.png\" width=\"90%\" />\n",
    "\n",
    "* <b>Document Model</b>, in the range [0, 1].  \n",
    "<ul>\n",
    "    <li>On the 0-end of the scale, <b>\"sweet\"</b> documents lead to large amounts of <b>\"click bait\"</b> or immediate engagement. Sweetness values are drawn from ln Normal(Œºsweet, œÉsweet).</li>\n",
    "    <li>On the 1-end of the scale, documents termed <b>kale</b>, are less click-bait, but tend to <b>increase user long-term satisfaction</b>. Kale values are drawn from ln Normal(Œºkale, œÉkale)</li>\n",
    "    <li>Mixed doc values are drawn from linear interpolation between parameters of the two distributions in proportion to their kaleness.</li>\n",
    "    </ul>\n",
    "* <b>User Model</b>, empty vectors.  <i>Since user features are so business-dependent, RecSim out-of-the-box does not include any user features.</i>  User features, typically imported from real data, could contain: \n",
    "<ul>\n",
    "    <li>evolving, unknown contexts (interests, preferences, satisfaction, activity, mood)</li>\n",
    "    <li>unobservable events that could impact user behavior (personalized promotions, interuptions that cause turning off a video such as because someone rang their doorbell).</li>\n",
    "    </ul>\n",
    "* <b>Rewards</b>, or user satisfaction after their choice, modeled in the range [0, 1] that stochastically (and slowly) increases or decreases with the consumption of different types of content; kale or sweetness.  \n",
    "\n",
    "\n",
    "<b>RLlib comes with 3 RecSim environments</b>  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "üëâ <b>Long Term Satisfaction</b> (used in this tutorial) <br>\n",
    "- Interest Evolution <br>\n",
    "- Interest Exploration <br>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n",
      "tensorflow: 2.6.0\n",
      "gym: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from scipy.stats import linregress, sem\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# silence the many tensorflow warnings\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import recsim\n",
    "\n",
    "print(f\"tensorflow: {tf.__version__}\")\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <_RecSimEnv<MultiDiscreteToDiscreteActionWrapper<RecSimObservationSpaceWrapper<RecSimResetWrapper<RecSimGymEnv instance>>>>>\n",
      "\n",
      "action space: Discrete(10)\n",
      "\n",
      "observation space: Dict(user:Box([], [], (0,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), engagement:Box(0.0, 100.0, (), float32))))\n"
     ]
    }
   ],
   "source": [
    "# Create a RecSim instance using the following config parameters \n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # The number of possible documents/videos/candidates that we can recommend\n",
    "    \"slate_size\": 1, # The number of recommendations that we will be making\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {lts_10_1_env}\")\n",
    "\n",
    "# print gym Spaces\n",
    "if isinstance(lts_10_1_env.action_space, gym.spaces.Space):\n",
    "    print()\n",
    "    print(f\"action space: {lts_10_1_env.action_space}\")\n",
    "if isinstance(lts_10_1_env.observation_space, gym.spaces.Space):\n",
    "    print()\n",
    "    print(f\"observation space: {lts_10_1_env.observation_space}\") \n",
    "        \n",
    "# Note: the action space is discrete with 10 possible actions.\n",
    "# Note: observations consist of 1 user, 10 docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified-lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified-lts\", \n",
    "                  lambda env_config: LTSWithStrongerDissatisfactionEffect(\n",
    "                      LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified-lts' to be used in RLlib configs (see below)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.LTSWithStrongerDissatisfactionEffect'>\n"
     ]
    }
   ],
   "source": [
    "# How to make RecSim environments?\n",
    "\n",
    "# 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# Try to strengthen the dissatisfaction effect on the engagement\n",
    "\n",
    "# 1. define a config dictionary\n",
    "env_config_20 = \\\n",
    "    {\n",
    "        # The number of possible documents/videos/candidates that we can recommend\n",
    "        \"num_candidates\": 20,  \n",
    "        # The number of recommendations that we will be making\n",
    "        \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "        # Set to False for re-using the same candidate doecuments each timestep.\n",
    "        \"resample_documents\": True,\n",
    "        # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "        \"wrap_for_bandits\": True,}\n",
    "\n",
    "# 2. create a RecSim environment\n",
    "lts_20_2_env = LongTermSatisfactionRecSimEnv(env_config_20)\n",
    "\n",
    "# 3. create a modified RecSim environment\n",
    "# LTSWithStrongerDissatisfactionEffect(recsim_env)\n",
    "modified_lts_20_2_env = \\\n",
    "    LTSWithStrongerDissatisfactionEffect(lts_20_2_env)\n",
    "\n",
    "print(type(modified_lts_20_2_env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Check intuition about environment</b>\n",
    "\n",
    "In the cell below, we run for 1K episodes and record the reward slopes for 3 scenarios:\n",
    "    <ul>\n",
    "        <li>Random policy: pick randomly every time</li>\n",
    "        <li>Greedy sweet policy: always pick the sweetest item </li>\n",
    "        <li>Greedy kale policy: always pick the kaleist item </li>\n",
    "    </ul>\n",
    "\n",
    "The result below shows random is worst; greedy sweet is next worst with negative reward slope; and greedy kale is best with very small positive reward slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1000 episodes, reward slope: -8.737139146441298e-05\n"
     ]
    }
   ],
   "source": [
    "# Check slopes with more complicated env\n",
    "lts_20_2_env = \\\n",
    "    LTSWithStrongerDissatisfactionEffect(\n",
    "        LongTermSatisfactionRecSimEnv(env_config_20))\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "\n",
    "# Run 1000 episodes\n",
    "num_episodes = 1000\n",
    "for _ in range(num_episodes):\n",
    "    obs = lts_20_2_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax(obs['item'])\n",
    "    action_kaleiest = np.argmin(obs['item'])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        action = lts_20_2_env.action_space.sample()\n",
    "        # action = action_sweetest\n",
    "        # action = action_kaleiest\n",
    "\n",
    "        obs, reward, done, _ = lts_20_2_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(f\"after {num_episodes} episodes, reward slope: {np.mean(slopes)}\")\n",
    "\n",
    "# random slope: 2.0507302101210928e-05          # smallest positive\n",
    "# greedy sweet slope: -0.00022956105559631047   # negative\n",
    "# greedy kale slope:  8.599181822029186e-05     # larger positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Get an environment baseline</b>\n",
    "\n",
    "Above we got slopes, to help with intution.  But, it is always best practice, before training an algorithm, to run through the environment and record the mean reward as a baseline.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° For this environment, we will calculate 3 baselines: <br>\n",
    "    <ul>\n",
    "        <li>Random baseline: pick randomly every time</li>\n",
    "        <li>Greedy sweet baseline: always pick the sweetest item </li>\n",
    "        <li>Greedy kale baseline: always pick the kaleist item </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, \n",
    "                                       baseline_type=\"random\",\n",
    "                                       episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce an action\n",
    "        action_random = env.action_space.sample()\n",
    "        action_sweetest = np.argmax(obs['item'])\n",
    "        action_kaleiest = np.argmin(obs['item'])\n",
    "        if baseline_type == \"random\":\n",
    "            action = action_random\n",
    "        elif baseline_type == \"sweetest\":\n",
    "            action = action_sweetest\n",
    "        elif baseline_type == \"kaleist\":\n",
    "            action = action_kaleiest\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 99 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 9 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "    env_sd_reward = sem(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean {baseline_type} baseline reward: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 .......... 99 \n",
      "\n",
      "Mean sweetest baseline reward: 1162.53+/-1.03\n"
     ]
    }
   ],
   "source": [
    "# Calculate a baseline type: random, sweetest, or kaleist\n",
    "lts_20_2_env_mean_random_reward, _ = \\\n",
    "    measure_random_performance_for_env(lts_20_2_env, \n",
    "                                       baseline_type=\"sweetest\",\n",
    "                                       episodes=100)\n",
    "\n",
    "# Random baseline: 1158.00+/-1.21    # next biggest\n",
    "# Sweetest baseline: 1164.24+/-1.18  # biggest\n",
    "# Kaleist baseline: 1085.04+/-1.05   # smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Contextual Bandit on the environment <a class=\"anchor\" id=\"cb\"></a>\n",
    "\n",
    "A Bandit session is one where we have an opportunity to recommend the user an item and observe their behaviour. We receive a reward if they click.\n",
    "\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>Bandits</b></i></li>\n",
    "    <li>On the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#bandits\">algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select RLlib Bandit algorithm w/Upper Confidence Bound (UCB) exploration\n",
    "# and find that algorithm's config class\n",
    "\n",
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.bandit import BanditLinUCBConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your config settings and instantiate a config object with those settings\n",
    "# Define algorithm config values\n",
    "env_name = \"modified-lts\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 2  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"my_LinUCB_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig'>\n"
     ]
    }
   ],
   "source": [
    "# # uncomment below to see the long list of default config values\n",
    "# print(f\"Bandit's default config is:\")\n",
    "# print(pretty_print(BanditLinUCBConfig().to_dict()))\n",
    "\n",
    "# Create a defaut config:\n",
    "bandit_config = BanditLinUCBConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "bandit_config.environment(env=env_name, env_config=env_config_20)\n",
    "\n",
    "# Decide if you want torch or tensorflow DL framework\n",
    "bandit_config.framework(\"tf2\", eager_tracing=True)\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "bandit_config.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=evaluation_interval,\n",
    "    # Use separate resources (RLlib rollout workers).\n",
    "    evaluation_num_workers=num_workers,\n",
    "    # Run 20 episodes per evaluation (per iteration) -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=evaluation_duration,\n",
    "    evaluation_duration_unit=\"timesteps\",\n",
    "    # # Run evaluation alternatingly with training (not in parallel).\n",
    "    # evaluation_parallel_to_training=False,\n",
    ")\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "bandit_config\\\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\\\n",
    "    .resources(num_gpus=num_gpus, )\\\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    "\n",
    "print(type(bandit_config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - why is ray tune for bandit so slow?\n",
    "\n",
    "# # Example using Ray tune API (`tune.run()`) until some stopping condition is met.\n",
    "# # This will create one (or more) Algorithms under the hood automatically w/o us having to\n",
    "# # build these algos from the config.\n",
    "\n",
    "# experiment_results = tune.run(\n",
    "#     \"BanditLinUCB\",\n",
    "\n",
    "#     # training config params (translated into a python dict!)\n",
    "#     config=bandit_config.to_dict(),\n",
    "\n",
    "#     # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "#     stop={\n",
    "#         \"training_iteration\": 10,     # stop after n training iterations (calls to `Algorithm.train()`)\n",
    "#         #\"episode_reward_mean\": 400, # stop if average (sum of) rewards in an episode is 400 or more\n",
    "#         #\"timesteps_total\": 100000,  # stop if reached 100,000 sampling timesteps\n",
    "#     },  \n",
    "\n",
    "#     # redirect logs instead of default ~/ray_results/\n",
    "#     local_dir=relative_checkpoint_dir,\n",
    "         \n",
    "#     # Every how many train() calls do we create a checkpoint?\n",
    "#     checkpoint_freq=checkpoint_freq,\n",
    "#     # Always save last checkpoint (no matter the frequency).\n",
    "#     checkpoint_at_end=checkpoint_at_end,\n",
    "\n",
    "#     ###############\n",
    "#     # Note about Ray Tune verbosity.\n",
    "#     # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "#     # 0 = silent\n",
    "#     # 1 = only status updates, no logging messages\n",
    "#     # 2 = status and brief trial results, includes logging messages\n",
    "#     # 3 = status and detailed trial results, includes logging messages\n",
    "#     # Defaults to 3.\n",
    "#     ###############\n",
    "#     verbose=3,\n",
    "                   \n",
    "#     # Define what we are comparing for, when we search for the\n",
    "#     # \"best\" checkpoint at the end.\n",
    "#     metric=\"episode_reward_mean\",\n",
    "#     mode=\"max\",\n",
    "# )\n",
    "\n",
    "# print(\"Training completed.\")\n",
    "# print(\"Best checkpoint: \", experiment_results.best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 08:16:41,512\tERROR services.py:1376 -- Failed to start the dashboard: Failed to start the dashboard, return code 1\n",
      " The last 10 lines of /tmp/ray/session_2022-08-05_08-16-39_599973_26052/logs/dashboard.log:\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1200, in add_routes\n",
      "    registered_routes.extend(route_def.register(self))\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_routedef.py\", line 98, in register\n",
      "    resource = router.add_static(self.prefix, self.path, **self.kwargs)\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1124, in add_static\n",
      "    resource = StaticResource(\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 559, in __init__\n",
      "    raise ValueError(f\"No directory exists at '{directory}'\") from error\n",
      "ValueError: No directory exists at '/Users/christy/Documents/ray/dashboard/client/build/static'\n",
      "2022-08-05 08:16:41,513\tERROR services.py:1377 -- Failed to start the dashboard, return code 1\n",
      " The last 10 lines of /tmp/ray/session_2022-08-05_08-16-39_599973_26052/logs/dashboard.log:\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1200, in add_routes\n",
      "    registered_routes.extend(route_def.register(self))\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_routedef.py\", line 98, in register\n",
      "    resource = router.add_static(self.prefix, self.path, **self.kwargs)\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1124, in add_static\n",
      "    resource = StaticResource(\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 559, in __init__\n",
      "    raise ValueError(f\"No directory exists at '{directory}'\") from error\n",
      "ValueError: No directory exists at '/Users/christy/Documents/ray/dashboard/client/build/static'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/christy/Documents/ray/python/ray/_private/services.py\", line 1362, in start_api_server\n",
      "    raise Exception(err_msg + last_log_str)\n",
      "Exception: Failed to start the dashboard, return code 1\n",
      " The last 10 lines of /tmp/ray/session_2022-08-05_08-16-39_599973_26052/logs/dashboard.log:\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1200, in add_routes\n",
      "    registered_routes.extend(route_def.register(self))\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_routedef.py\", line 98, in register\n",
      "    resource = router.add_static(self.prefix, self.path, **self.kwargs)\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1124, in add_static\n",
      "    resource = StaticResource(\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 559, in __init__\n",
      "    raise ValueError(f\"No directory exists at '{directory}'\") from error\n",
      "ValueError: No directory exists at '/Users/christy/Documents/ray/dashboard/client/build/static'\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/christy/Documents/ray/python/ray/dashboard/modules/reporter/reporter_agent.py:46: UserWarning: `gpustat` package is not installed. GPU monitoring is not available. To have full functionality of the dashboard please install `pip install ray[default]`.)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=26101)\u001b[0m Metal device set to: Apple M1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26099)\u001b[0m Metal device set to: Apple M1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26100)\u001b[0m Metal device set to: Apple M1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26098)\u001b[0m Metal device set to: Apple M1\n",
      "Metal device set to: Apple M1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26122)\u001b[0m Metal device set to: Apple M1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26125)\u001b[0m Metal device set to: Apple M1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26123)\u001b[0m Metal device set to: Apple M1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26124)\u001b[0m Metal device set to: Apple M1\n",
      "Algorithm type: <class 'ray.rllib.algorithms.bandit.bandit.BanditLinUCB'>\n",
      "\n",
      "Iteration=0, Mean Bandit Reward=nan+/-nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/numpy/core/_methods.py:194: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(\n",
      "/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration=99, Mean Bandit Reward=1164.80+/-1.48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    \n",
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "linucb_algo = bandit_config.build()\n",
    "print(f\"Algorithm type: {type(linucb_algo)}\")\n",
    "\n",
    "# train the Bandit Algorithm instance for n iterations (timesteps)\n",
    "rewards = []\n",
    "n_timesteps = 100\n",
    "for i in range(n_timesteps):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = linucb_algo.train()\n",
    "    \n",
    "    # Extract reward from results.\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "    if i % 99 == 0:\n",
    "        print(f\"\\nIteration={i}, \", end=\"\")\n",
    "        print(f\"Mean Bandit Reward={result['episode_reward_mean']:.2f}\",end=\"\")\n",
    "        try:\n",
    "            print(f\"+/-{np.std(rewards[4:]):.2f}\\n\")\n",
    "        except:\n",
    "            print()\n",
    "    elif num_episodes % 9 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "linucb_algo.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 200\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our algorithms over 100 timesteps:\n",
    "<ul>\n",
    "    <li>Kaleist baseline: 1084.49+/-1.04 </li>\n",
    "    <li>Random baseline: 1158.00+/-1.21 </li>\n",
    "    <b><li>Sweetest baseline: 1164.24+/-1.18 </li>\n",
    "        <li>Bandit mean reward = 1164.80+/-1.48</li></b>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "ü§î Notice that the bandit mean reward is suspiciously the same as the sweetest baseline! <br>\n",
    "</div>\n",
    "\n",
    "In this dummy Recsim environment, we did not have any user features.  This makes the contextual bandit without any user context, i.e. without any state.  A stateless bandit cannot remember things between timesteps, so it will be exactly the most greedy policy.  \n",
    "\n",
    "So, it makes sense that the Bandit reward = Sweetest baseline!  The context-less bandit was unable to learn anything!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using a RL Offline algorithm on the environment <a class=\"anchor\" id=\"offline\"></a>\n",
    "\n",
    "CRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the config class of the algorithm, we would like to train with: CRR.\n",
    "from ray.rllib.algorithms.crr import CRRConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.crr.crr.CRRConfig'>\n"
     ]
    }
   ],
   "source": [
    "# Create a defaut CRR config:\n",
    "config = CRRConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "# NOTE: We said above that we wouldn't really have an environment available (so how can\n",
    "# we set one up here??).\n",
    "# The following is only to tell the algorithm, which environment our offline data was actually taken from.\n",
    "config.environment(env=\"modified-lts\")\n",
    "# If you really really don't have an environment, set `env=None` here and additionally define your action- and\n",
    "# observation spaces.\n",
    "# config.environment(env=None, action_space=..., observation_space=...)\n",
    "\n",
    "#################################################\n",
    "# This is the most important piece of code \n",
    "# in this notebook:\n",
    "# It explains how to point your \n",
    "# algorithm to the correct offline data file\n",
    "# (instead of a live-environment).\n",
    "#################################################\n",
    "config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        # If you feel daring here, use the `pendulum_beginner.json` file instead of the expert one here.\n",
    "        # You may need to train a little longer, then, in order to get a decent policy.\n",
    "        # But since you have the actual Pendulum environment available for evaluation, you should be able\n",
    "        # to perfectly stop learning once a good episode reward (> -300.0) has been reached.\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/pendulum_expert.json\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    # The (continuous) actions in our input files are already normalized\n",
    "    # (meaning between -1.0 and 1.0) -> We don't have to do anything with them prior to\n",
    "    # computing losses.\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "# RLlib's CRR is a very new algorithm (since 1.13) and only supports\n",
    "# the PyTorch framework thus far. We'll provide a tf version in the near future.\n",
    "config.framework(\"torch\")\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "config.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=1,\n",
    "    # Use a separate resource (\"RLlib rollout worker\")\n",
    "    evaluation_num_workers=1,\n",
    "\n",
    "    # # Use separate resources (RLlib rollout workers).\n",
    "    # evaluation_num_workers=2,\n",
    "\n",
    "    # Run 20 episodes per evaluation (per iteration) -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=20,\n",
    "    evaluation_duration_unit=\"episodes\",\n",
    "\n",
    "    # Use a slightly different config for the evaluation:\n",
    "    evaluation_config={\n",
    "        # - Use a real environment (so we can fully trust the evaluation results, rewards, etc..)\n",
    "        \"input\": \"sampler\",\n",
    "        # - Switch off exploration for better (less stochastic) action computations.\n",
    "        \"explore\": False,\n",
    "    },\n",
    "\n",
    "    # Run evaluation alternatingly with training (not in parallel).\n",
    "    evaluation_parallel_to_training=False,\n",
    ")\n",
    "\n",
    "print(type(config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=26101)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26101)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26101)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26101)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26100)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26100)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26100)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26100)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26098)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26098)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26098)\u001b[0m \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluation_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m   \u001b[38;5;66;03m#100, num training episodes to run between eval steps\u001b[39;00m\n\u001b[1;32m      2\u001b[0m verbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# Tune screen verbosity\u001b[39;00m\n\u001b[1;32m      4\u001b[0m experiment_results \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      5\u001b[0m                     \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Stopping criteria whichever occurs first: average reward over training episodes, or ...\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     stop\u001b[38;5;241m=\u001b[39m{\u001b[38;5;66;03m#\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\u001b[39;00m\n\u001b[1;32m      8\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# stop if achieved 100 episodes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m           \u001b[38;5;66;03m# \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\u001b[39;00m\n\u001b[1;32m     10\u001b[0m           },  \n\u001b[1;32m     11\u001b[0m               \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# training config params\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m     14\u001b[0m                     \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#redirect logs instead of default ~/ray_results/\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     local_dir \u001b[38;5;241m=\u001b[39m relative_checkpoint_dir, \u001b[38;5;66;03m#relative path\u001b[39;00m\n\u001b[1;32m     17\u001b[0m          \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# set frequency saving checkpoints >= evaulation_interval\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     checkpoint_freq \u001b[38;5;241m=\u001b[39m checkpoint_freq,\n\u001b[1;32m     20\u001b[0m     checkpoint_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m          \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Reduce logging messages\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m verbosity,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=26122)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26122)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26122)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26122)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26125)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26125)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26125)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26125)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26123)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26123)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26123)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26123)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26124)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26124)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26099)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26099)\u001b[0m systemMemory: 16.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26099)\u001b[0m maxCacheSize: 5.33 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26099)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "evaluation_interval = 100   #100, num training episodes to run between eval steps\n",
    "verbosity = 2 # Tune screen verbosity\n",
    "\n",
    "experiment_results = tune.run(\"CRR\", \n",
    "                    \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={#\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\n",
    "          \"training_iteration\": 100,  # stop if achieved 100 episodes\n",
    "          # \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\n",
    "          },  \n",
    "              \n",
    "    # training config params\n",
    "    config = config.to_dict(),\n",
    "                    \n",
    "    #redirect logs instead of default ~/ray_results/\n",
    "    local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq = checkpoint_freq,\n",
    "    checkpoint_at_end=True,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    verbose = verbosity,\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
