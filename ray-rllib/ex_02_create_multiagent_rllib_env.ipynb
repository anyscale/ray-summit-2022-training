{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02. Create a Custom Multi-Agent RLlib Environment\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    " * [Code a custom RLlib Multi-agent environment](#multi_agent_env)\n",
    " * [Select an algorithm and instantiate a config object using that algorithm's config class](#rllib_algo)\n",
    " * [Train a RL model using a multi-agent algorithm from RLlib](#tune_run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# install libraries\n",
    "\n",
    "import time\n",
    "import ray, gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "import numpy as np\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code a custom RLlib Multi-agent environment <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "We will create the following (adversarial) multi-agent environment.  Wil will use this custom environment for the rest of this tutorial.\n",
    "\n",
    "<img src=\"images/custom_environment.png\" width=\"98%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review OpenAI Gym Environments\n",
    "\n",
    "We learned in the last lesson about OpenAI Gym Environments.  Specifically we covered Gym:\n",
    "<ul>\n",
    "    <li>Action Space</li>\n",
    "    <li>Observation Space</li>\n",
    "    <li>Rewards</li>\n",
    "    <li><i>`done`</i> signal</li>\n",
    "    <li>Gym Environment API methods:</li>\n",
    "    <ul>\n",
    "        <li>reset(self)</li>\n",
    "        <li>step(self, action: dict)</li>\n",
    "        <li>render(self, mode=None)</li>\n",
    "    </ul>\n",
    "    </ul>\n",
    "\n",
    "### RLlib MultiAgentEnv API\n",
    "\n",
    "RLlib supports environments created using the OpenAI Gym API. This means, to create a RLlib environment from scratch, you need to minimally implement the same methods [required by Gym](https://www.gymlibrary.ml/content/api/#standard-methods).  \n",
    "\n",
    "We want a multi-agent environment, so we will implement the [RLlib MultiAgentEnv base class](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) which requires a few extra methods in addition to the minimal Gym methods.\n",
    "    \n",
    "<ul>\n",
    "    <li>__init__(self)</li>\n",
    "    <li> _get_obs(self)</li>\n",
    "    <li>_move(self, coords, action, which_agent)</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's code our multi-agent environment\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "\n",
    "        # For rendering.\n",
    "        self.out = None\n",
    "        if config.get(\"render\"):\n",
    "            self.out = Output()\n",
    "            display.display(self.out)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Did we have a collision in recent step?\n",
    "        self.collision = False\n",
    "        # How many collisions in total have we had in this episode?\n",
    "        self.num_collisions = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "        if self.collision is True:\n",
    "            self.num_collisions += 1\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "\n",
    "        if self.out is not None:\n",
    "            self.out.clear_output(wait=True)\n",
    "\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"â¾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f} ({} collisions)\".format(self.agent2_R, self.num_collisions))\n",
    "        print()\n",
    "        time.sleep(0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the cell below:\n",
    "<ul>\n",
    "    <li>Initialize the environment</li>\n",
    "    <li>Make both agents take a few steps</li>\n",
    "    <li>Render the environment after each agent takes a step.</li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f106347653b8438b8f3ca80d8a0b5c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1's x/y position=[2, 2]\n",
      "Agent2's x/y position=[7, 7]\n",
      "Env timesteps=4\n"
     ]
    }
   ],
   "source": [
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs = env.reset()\n",
    "\n",
    "with env.out:\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an algorithm and instantiate a config object using that algorithm's config class <a class=\"anchor\" id=\"rllib_algo\"></a>\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">Open RLlib docs</a></li>\n",
    "    <li>Scroll down and click url of algo you're searching for, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sven's trainer config\n",
    "\n",
    "# TRAINER_CFG = {\n",
    "#     # Using our environment class defined above.\n",
    "#     \"env\": MultiAgentArena,\n",
    "#     # Use `framework=torch` here for PyTorch.\n",
    "#     \"framework\": \"tf\",\n",
    "\n",
    "#     # Run on 1 GPU on the \"learner\".\n",
    "#     \"num_gpus\": 1,\n",
    "#     # Use 15 ray-parallelized environment workers,\n",
    "#     # which collect samples to learn from. Each worker gets assigned\n",
    "#     # 1 CPU.\n",
    "#     \"num_workers\": 15,\n",
    "#     # Each of the 15 workers has 10 environment copies (\"vectorization\")\n",
    "#     # for faster (batched) forward passes.\n",
    "#     \"num_envs_per_worker\": 10,\n",
    "\n",
    "#     # Multi-agent setup: 2 policies.\n",
    "#     \"multiagent\": {\n",
    "#         \"policies\": {\"policy1\", \"policy2\"},\n",
    "#         \"policy_mapping_fn\": lambda agent_id: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>\n"
     ]
    }
   ],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "# Define algorithm config values\n",
    "env_name = MultiAgentArena\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"multiagent_PPO_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "config_train = (\n",
    "    PPOConfig()\n",
    "    .framework(framework='torch')\n",
    "    .environment(env=env_name, disable_env_checking=False)\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    "    .multi_agent(policies=[\"policy1\", \"policy2\"], \n",
    "                 policy_mapping_fn=lambda agent_id: \"policy1\" if agent_id == \"agent1\" else \"policy2\")\n",
    ")\n",
    "\n",
    "print(type(config_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RL model using a multi-agent algorithm from RLlib <a class=\"anchor\" id=\"tune_run\"></a>\n",
    "\n",
    "ð¡ <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sven's original tuning run\n",
    "\n",
    "# results = tune.run(\n",
    "#     # RLlib Trainer class (we use the \"PPO\" algorithm today).\n",
    "#     PPOTrainer,\n",
    "#     # Give our experiment a name (we will find results/checkpoints\n",
    "#     # under this name on the server's `~ray_results/` dir).\n",
    "#     name=f\"CUJ-RL\",\n",
    "#     # The RLlib config (defined in a cell above).\n",
    "#     config=TRAINER_CFG,\n",
    "#     # Take a snapshot every 2 iterations.\n",
    "#     checkpoint_freq=2,\n",
    "#     # Plus one at the very end of training.\n",
    "#     checkpoint_at_end=True,\n",
    "#     # Run for exactly 30 training iterations.\n",
    "#     stop={\"training_iteration\": 20},\n",
    "#     # Define what we are comparing for, when we search for the\n",
    "#     # \"best\" checkpoint at the end.\n",
    "#     metric=\"episode_reward_mean\",\n",
    "#     mode=\"max\")\n",
    "\n",
    "# print(\"Best checkpoint: \", results.best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 21:59:51,747\tINFO tune.py:862 -- Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`.\n",
      "2022-07-10 21:59:53,801\tERROR services.py:1494 -- Failed to start the dashboard: Failed to start the dashboard, return code 0\n",
      " The last 10 lines of /tmp/ray/session_2022-07-10_21-59-51_749470_76756/logs/dashboard.log:\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/head.py\", line 105, in _configure_http_server\n",
      "    http_server = HttpServerDashboardHead(\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 69, in __init__\n",
      "    raise ex\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 60, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 31, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm install && npm ci && npm run build): '/Users/christy/Documents/ray/python/ray/dashboard/client/build'\n",
      "2022-07-10 21:59:53,803\tERROR services.py:1495 -- Failed to start the dashboard, return code 0\n",
      " The last 10 lines of /tmp/ray/session_2022-07-10_21-59-51_749470_76756/logs/dashboard.log:\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/head.py\", line 105, in _configure_http_server\n",
      "    http_server = HttpServerDashboardHead(\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 69, in __init__\n",
      "    raise ex\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 60, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 31, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm install && npm ci && npm run build): '/Users/christy/Documents/ray/python/ray/dashboard/client/build'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/christy/Documents/ray/python/ray/_private/services.py\", line 1472, in start_dashboard\n",
      "    raise Exception(err_msg + last_log_str)\n",
      "Exception: Failed to start the dashboard, return code 0\n",
      " The last 10 lines of /tmp/ray/session_2022-07-10_21-59-51_749470_76756/logs/dashboard.log:\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/head.py\", line 105, in _configure_http_server\n",
      "    http_server = HttpServerDashboardHead(\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 69, in __init__\n",
      "    raise ex\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 60, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 31, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm install && npm ci && npm run build): '/Users/christy/Documents/ray/python/ray/dashboard/client/build'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-10 22:00:01 (running for 00:00:07.14)<br>Memory usage on this node: 12.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/7.29 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/multiagent_PPO_logs/PPO<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_4de8f_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_4de8f_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/multiagent_PPO_logs/PPO/PPO_MultiAgentArena_4de8f_00000_0_2022-07-10_21-59-54/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m 2022-07-10 21:59:58,298\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-07-10 22:00:01,980\tERROR trial_runner.py:920 -- Trial PPO_MultiAgentArena_4de8f_00000: Error processing event.\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trial PPO_MultiAgentArena_4de8f_00000 errored with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': ['policy1', 'policy2'], 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x15afc8700>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}. Error file: /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/multiagent_PPO_logs/PPO/PPO_MultiAgentArena_4de8f_00000_0_2022-07-10_21-59-54/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m 2022-07-10 22:00:01,971\tERROR worker.py:749 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=84310, ip=127.0.0.1, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/worker_set.py\", line 127, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     self.add_workers(\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/worker_set.py\", line 270, in add_workers\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     self.foreach_worker_with_index(\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/worker_set.py\", line 405, in foreach_worker_with_index\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     remote_results = ray.get(\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=84324, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17a237a30>)\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 578, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     self.policy_dict = _determine_spaces_for_multi_agent_dict(\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 1986, in _determine_spaces_for_multi_agent_dict\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     if isinstance(env, MultiAgentEnv) and env._spaces_in_preferred_format:\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m AttributeError: 'MultiAgentArena' object has no attribute '_spaces_in_preferred_format'\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m \u001b[36mray::PPO.__init__()\u001b[39m (pid=84310, ip=127.0.0.1, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/algorithms/algorithm.py\", line 291, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/tune/trainable/trainable.py\", line 156, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/algorithms/algorithm.py\", line 409, in setup\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m     raise e.args[0].args[2]\n",
      "\u001b[2m\u001b[36m(PPO pid=84310)\u001b[0m AttributeError: 'MultiAgentArena' object has no attribute '_spaces_in_preferred_format'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84322)\u001b[0m 2022-07-10 22:00:01,976\tERROR worker.py:749 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=84322, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1565f7a30>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84322)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 578, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84322)\u001b[0m     self.policy_dict = _determine_spaces_for_multi_agent_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84322)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 1986, in _determine_spaces_for_multi_agent_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84322)\u001b[0m     if isinstance(env, MultiAgentEnv) and env._spaces_in_preferred_format:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84322)\u001b[0m AttributeError: 'MultiAgentArena' object has no attribute '_spaces_in_preferred_format'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84321)\u001b[0m 2022-07-10 22:00:01,964\tERROR worker.py:749 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=84321, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1760a7a30>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84321)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 578, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84321)\u001b[0m     self.policy_dict = _determine_spaces_for_multi_agent_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84321)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 1986, in _determine_spaces_for_multi_agent_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84321)\u001b[0m     if isinstance(env, MultiAgentEnv) and env._spaces_in_preferred_format:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84321)\u001b[0m AttributeError: 'MultiAgentArena' object has no attribute '_spaces_in_preferred_format'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84324)\u001b[0m 2022-07-10 22:00:01,964\tERROR worker.py:749 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=84324, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17a237a30>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84324)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 578, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84324)\u001b[0m     self.policy_dict = _determine_spaces_for_multi_agent_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84324)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 1986, in _determine_spaces_for_multi_agent_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84324)\u001b[0m     if isinstance(env, MultiAgentEnv) and env._spaces_in_preferred_format:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84324)\u001b[0m AttributeError: 'MultiAgentArena' object has no attribute '_spaces_in_preferred_format'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84323)\u001b[0m 2022-07-10 22:00:01,964\tERROR worker.py:749 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=84323, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x179b0fa30>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84323)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 578, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84323)\u001b[0m     self.policy_dict = _determine_spaces_for_multi_agent_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84323)\u001b[0m   File \"/Users/christy/Documents/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 1986, in _determine_spaces_for_multi_agent_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84323)\u001b[0m     if isinstance(env, MultiAgentEnv) and env._spaces_in_preferred_format:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84323)\u001b[0m AttributeError: 'MultiAgentArena' object has no attribute '_spaces_in_preferred_format'\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_MultiAgentArena_4de8f_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m evaluation_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m   \u001b[38;5;66;03m#100, num training episodes to run between eval steps\u001b[39;00m\n\u001b[1;32m     17\u001b[0m verbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# Tune screen verbosity\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Stopping criteria whichever occurs first: average reward over training episodes, or ...\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;66;43;03m#\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_iteration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# stop if achieved 200 episodes\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m         \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m              \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# training config params\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                    \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#redirect logs instead of default ~/ray_results/\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelative_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#relative path\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m         \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# set frequency saving checkpoints >= evaulation_interval\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m         \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Reduce logging messages\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                   \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Define what we are comparing for, when we search for the\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# \"best\" checkpoint at the end.\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest checkpoint: \u001b[39m\u001b[38;5;124m\"\u001b[39m, trainer\u001b[38;5;241m.\u001b[39mbest_checkpoint)\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/tune.py:731\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_MultiAgentArena_4de8f_00000])"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RAY TUNE API .run() IN A LOOP UNTIL STOP CONDITION\n",
    "# Note about Ray Tune verbosity.\n",
    "# Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "# 0 = silent\n",
    "# 1 = only status updates, no logging messages\n",
    "# 2 = status and brief trial results, includes logging messages\n",
    "# 3 = status and detailed trial results, includes logging messages\n",
    "# Defaults to 3.\n",
    "###############\n",
    "\n",
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "evaluation_interval = 100   #100, num training episodes to run between eval steps\n",
    "verbosity = 2 # Tune screen verbosity\n",
    "\n",
    "trainer = tune.run(\"PPO\", \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={#\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\n",
    "          \"training_iteration\": 20,  # stop if achieved 200 episodes\n",
    "          # \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\n",
    "         },  \n",
    "              \n",
    "    # training config params\n",
    "    config = config_train.to_dict(),\n",
    "                    \n",
    "    #redirect logs instead of default ~/ray_results/\n",
    "    local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq = checkpoint_freq,\n",
    "    checkpoint_at_end=True,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    verbose = verbosity,\n",
    "                   \n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")\n",
    "print(\"Best checkpoint: \", trainer.best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " * \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
