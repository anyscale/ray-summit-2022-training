{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02. Create a Custom Multi-Agent RLlib Environment\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    " * [Code a custom RLlib Multi-agent environment](#multi_agent_env)\n",
    " * [Select an algorithm and instantiate a config object using that algorithm's config class](#rllib_algo)\n",
    " * [Train a RL model using a multi-agent algorithm from RLlib](#rllib_run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "\n",
    "import time\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "\n",
    "import ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code a custom RLlib Multi-agent environment <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "We will create the following (adversarial) multi-agent environment.  Wil will use this custom environment for the rest of this tutorial.\n",
    "\n",
    "<img src=\"images/custom_environment.png\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review OpenAI Gym Environments\n",
    "\n",
    "We learned in the last lesson about OpenAI Gym Environments.  Specifically we covered Gym:\n",
    "<ul>\n",
    "    <li>Action Space</li>\n",
    "    <li>Observation Space</li>\n",
    "    <li>Rewards</li>\n",
    "    <li><i>`done`</i> signal</li>\n",
    "    <li>Gym Environment API methods:</li>\n",
    "    <ul>\n",
    "        <li>reset(self)</li>\n",
    "        <li>step(self, action: dict)</li>\n",
    "        <li>render(self, mode=None)</li>\n",
    "    </ul>\n",
    "    </ul>\n",
    "\n",
    "### RLlib MultiAgentEnv API\n",
    "\n",
    "RLlib supports environments created using the OpenAI Gym API. This means, to create a RLlib environment from scratch, you need to minimally implement the same methods [required by Gym](https://www.gymlibrary.ml/content/api/#standard-methods).  \n",
    "\n",
    "We want a multi-agent environment, so we will implement the [RLlib MultiAgentEnv base class](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) which requires a few extra methods in addition to the minimal Gym methods.\n",
    "    \n",
    "<ul>\n",
    "    <li>__init__(self)</li>\n",
    "    <li> _get_obs(self)</li>\n",
    "    <li>_move(self, coords, action, which_agent)</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's code our multi-agent environment\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "        \n",
    "        # For rendering.\n",
    "        self.out = None\n",
    "        if config.get(\"render\"):\n",
    "            self.out = Output()\n",
    "            display.display(self.out)\n",
    "\n",
    "        self._spaces_in_preferred_format = False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Did we have a collision in recent step?\n",
    "        self.collision = False\n",
    "        # How many collisions in total have we had in this episode?\n",
    "        self.num_collisions = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "        if self.collision is True:\n",
    "            self.num_collisions += 1\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "\n",
    "        if self.out is not None:\n",
    "            self.out.clear_output(wait=True)\n",
    "\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"â¾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f} ({} collisions)\".format(self.agent2_R, self.num_collisions))\n",
    "        print()\n",
    "        time.sleep(0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the cell below:\n",
    "<ul>\n",
    "    <li>Initialize the environment</li>\n",
    "    <li>Make both agents take a few steps</li>\n",
    "    <li>Render the environment after each agent takes a step.</li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e494488a888d4367856a8d348983ff42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1's x/y position=[2, 2]\n",
      "Agent2's x/y position=[7, 7]\n",
      "Env timesteps=4\n"
     ]
    }
   ],
   "source": [
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs = env.reset()\n",
    "\n",
    "with env.out:\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an algorithm and instantiate a config object using that algorithm's config class <a class=\"anchor\" id=\"rllib_algo\"></a>\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">Open RLlib docs</a></li>\n",
    "    <li>Scroll down and click url of algo you're searching for, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7fee73910460>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "config = PPOConfig()\n",
    "config.environment(env=MultiAgentArena)\n",
    "config.rollouts(num_rollout_workers=4, num_envs_per_worker=1)\n",
    "config.training(lr=0.00005, train_batch_size=4000)  # default values for this algorithm\n",
    "config.multi_agent(\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "config.debugging(log_level=\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RL model using a multi-agent algorithm from RLlib <a class=\"anchor\" id=\"rllib_run\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 11:27:29,136\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-15 11:27:31,373\tINFO services.py:1477 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-07-15 11:27:44,938\tINFO trainable.py:160 -- Trainable.setup took 15.805 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-07-15 11:27:44,940\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.187500000000005\n",
      "-4.755000000000001\n",
      "-2.234999999999991\n",
      "-1.0949999999999867\n",
      "0.13500000000001325\n",
      "0.5760000000000118\n",
      "0.936000000000009\n",
      "1.7310000000000076\n",
      "3.1410000000000036\n",
      "5.7899999999999965\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ppo = config.build()\n",
    "\n",
    "for _ in range(10):\n",
    "    result = ppo.train()\n",
    "    print(result[\"episode_reward_mean\"])\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "ppo.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    " <img src=\"images/exercise_env_loop.png\" width=500>\n",
    " \n",
    "In the cell above, we performed a `reset()` and a single `step()` call. To walk through an entire episode, one would normally call `step()` repeatedly (with different actions) until the returned `done` dict has the \"agent1\" or \"agent2\" (or \"__all__\") key set to True.\n",
    "Your task is to write an \"environment loop\" that runs for exactly one episode using our `MultiAgentArena` class.\n",
    "\n",
    "Follow these instructions here to get this done:\n",
    "\n",
    "1. `reset` the already created (variable `env`) environment to get the first (initial) observation.\n",
    "1. Enter an infinite while loop.\n",
    "1. Compute the actions for \\\"agent1\\\" and \\\"agent2\\\" calling `DummyTrainer.compute_action([obs])` twice (once for each agent).\n",
    "1. Put the results of the action computations into an action dict (`{\\\"agent1\\\": ..., \\\"agent2\\\": ...}`).\n",
    "1. Pass this action dict into the env's `step()` method, just like it's done in the above cell (where we do a single `step()`).\n",
    "1. Check the returned `dones` dict for True (yes, episode is terminated) and if True, break out of the loop.\n",
    "\n",
    "**Good luck! :)**\n",
    "\n",
    "\n",
    "Write your solution code into this following python cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.,\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # 1) Reset the env ...\n",
    "\n",
    "    # 2) Enter an infinite while loop (to step through the episode) ...\n",
    "\n",
    "        # 3) Calculate both agents' actions individually, using dummy_trainer.compute_action([individual agent's obs])...\n",
    "\n",
    "        # 4) Compile the actions dict from both individual agents' actions ...\n",
    "\n",
    "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts ...\n",
    "\n",
    "        # 6) We'll do this together: Render the env.\n",
    "        # Don't write any code here (skip directly to 7).\n",
    "        out.clear_output(wait=True),\n",
    "        time.sleep(0.08),\n",
    "        env.render(),\n",
    "\n",
    "        # 7) Check, whether the episde is done (take a look at the\n",
    "        # `dones` dict returned from `step()`)\n",
    "        # If yes, break out of the while loop we entered in step 2).\n",
    "\n",
    "\n",
    "# 8) Run it! :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " * \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
