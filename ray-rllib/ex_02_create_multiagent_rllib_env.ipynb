{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02. Create a Custom Multi-Agent RLlib Environment\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    " * [Code a custom RLlib Multi-agent environment](#multi_agent_env)\n",
    " * [Select an algorithm and instantiate a config object using that algorithm's config class](#rllib_algo)\n",
    " * [Train a RL model using a multi-agent capable algorithm from RLlib](#rllib_run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "\n",
    "import time\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "\n",
    "import ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code a custom RLlib Multi-agent environment <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "We will create the following (adversarial) multi-agent environment.  Wil will use this custom environment for the rest of this tutorial.\n",
    "\n",
    "<img src=\"images/multi_agent_arena_0.png\" width=\"80%\">\n",
    "<img src=\"images/multi_agent_arena_1.png\" width=\"80%\">\n",
    "<img src=\"images/multi_agent_arena_2.png\" width=\"80%\">\n",
    "<img src=\"images/multi_agent_arena_3.png\" width=\"80%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review OpenAI Gym Environments\n",
    "\n",
    "We learned in the last lesson about OpenAI Gym Environments.  Specifically we covered Gym:\n",
    "<ul>\n",
    "    <li>Action Space</li>\n",
    "    <li>Observation Space</li>\n",
    "    <li>Rewards</li>\n",
    "    <li><i>`done`</i> signal</li>\n",
    "    <li>Gym Environment API methods:</li>\n",
    "    <ul>\n",
    "        <li>reset(self)</li>\n",
    "        <li>step(self, action: dict)</li>\n",
    "        <li>render(self, mode=None)</li>\n",
    "    </ul>\n",
    "    </ul>\n",
    "\n",
    "### RLlib MultiAgentEnv API\n",
    "\n",
    "RLlib supports environments created using the OpenAI Gym API. This means, to create a RLlib environment from scratch, you need to minimally implement the same methods [required by Gym](https://www.gymlibrary.ml/content/api/#standard-methods).  \n",
    "\n",
    "We want a multi-agent environment, so we will implement the [RLlib MultiAgentEnv base class](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) which requires a few extra methods in addition to the minimal Gym methods.\n",
    "    \n",
    "<ul>\n",
    "    <li>__init__(self)</li>\n",
    "    <li> _get_obs(self)</li>\n",
    "    <li>_move(self, coords, action, which_agent)</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's code our multi-agent environment\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):  # MultiAgentEnv is a gym.Env sub-class\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "        \n",
    "        # For rendering.\n",
    "        self.out = None\n",
    "        if config.get(\"render\"):\n",
    "            self.out = Output()\n",
    "            display.display(self.out)\n",
    "\n",
    "        self._spaces_in_preferred_format = False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Did we have a collision in recent step?\n",
    "        self.collision = False\n",
    "        # How many collisions in total have we had in this episode?\n",
    "        self.num_collisions = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "        if self.collision is True:\n",
    "            self.num_collisions += 1\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "\n",
    "        if self.out is not None:\n",
    "            self.out.clear_output(wait=True)\n",
    "\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"â¾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f} ({} collisions)\".format(self.agent2_R, self.num_collisions))\n",
    "        print()\n",
    "        time.sleep(0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the cell below:\n",
    "<ul>\n",
    "    <li>Initialize the environment</li>\n",
    "    <li>Make both agents take a few steps</li>\n",
    "    <li>Render the environment after each agent takes a step.</li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77e150c91da44938c6c2b98a560a42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1's x/y position=[2, 2]\n",
      "Agent2's x/y position=[7, 7]\n",
      "Env timesteps=4\n"
     ]
    }
   ],
   "source": [
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs = env.reset()\n",
    "\n",
    "with env.out:\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an algorithm and instantiate a config object using that algorithm's config class <a class=\"anchor\" id=\"rllib_algo\"></a>\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">Open RLlib docs</a></li>\n",
    "    <li>Scroll down and click url of algo you're searching for, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7fee75325130>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "config = PPOConfig()\n",
    "config.environment(env=MultiAgentArena)\n",
    "config.rollouts(num_rollout_workers=4, num_envs_per_worker=1)\n",
    "config.training(lr=0.00005, train_batch_size=4000)  # default values for this algorithm\n",
    "config.multi_agent(\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "config.debugging(log_level=\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RL model using a multi-agent algorithm from RLlib <a class=\"anchor\" id=\"rllib_run\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 15:19:56,429\tINFO trainable.py:160 -- Trainable.setup took 12.896 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-07-24 15:19:56,431\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards for both agents R=-12.855000000000008\n",
      "Agent1 R=-4.0375\n",
      "Agent2 R=-8.817499999999983\n",
      "\n",
      "Sum of rewards for both agents R=-6.81\n",
      "Agent1 R=1.58125\n",
      "Agent2 R=-8.391249999999985\n",
      "\n",
      "Sum of rewards for both agents R=-3.050999999999993\n",
      "Agent1 R=5.42\n",
      "Agent2 R=-8.470999999999984\n",
      "\n",
      "Sum of rewards for both agents R=0.28800000000001186\n",
      "Agent1 R=8.605\n",
      "Agent2 R=-8.316999999999984\n",
      "\n",
      "Sum of rewards for both agents R=0.45600000000001295\n",
      "Agent1 R=8.355\n",
      "Agent2 R=-7.898999999999986\n",
      "\n",
      "Sum of rewards for both agents R=1.7160000000000153\n",
      "Agent1 R=8.79\n",
      "Agent2 R=-7.073999999999987\n",
      "\n",
      "Sum of rewards for both agents R=2.4420000000000126\n",
      "Agent1 R=9.395\n",
      "Agent2 R=-6.952999999999987\n",
      "\n",
      "Sum of rewards for both agents R=1.9860000000000084\n",
      "Agent1 R=8.785\n",
      "Agent2 R=-6.798999999999986\n",
      "\n",
      "Sum of rewards for both agents R=3.141000000000005\n",
      "Agent1 R=9.665\n",
      "Agent2 R=-6.523999999999989\n",
      "\n",
      "Sum of rewards for both agents R=3.9270000000000045\n",
      "Agent1 R=9.67\n",
      "Agent2 R=-5.742999999999991\n",
      "\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "#if ray.is_initialized():\n",
    "#    ray.shutdown()\n",
    "\n",
    "ppo = config.build()\n",
    "\n",
    "for _ in range(10):\n",
    "    result = ppo.train()\n",
    "    print(f\"Sum of rewards for both agents R={result['episode_reward_mean']}\")\n",
    "    print(f\"Agent1 R={result['policy_reward_mean']['policy1']}\")\n",
    "    print(f\"Agent2 R={result['policy_reward_mean']['policy2']}\")\n",
    "    print()\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note</b> that in an adversarial multi-agent setup, an agent benefits from the other agent's failures and vice-versa: agents get harmed more (receive negative rewards) the better the other agent is doing.\n",
    "    <br/>\n",
    "This highlights some important aspects of multi-agent training:\n",
    "    <br/>\n",
    "<ul>\n",
    "<li>From each agent's perspective, the environment is not as static as in respective single-agent scenarios (the other agent's behavior is probably harder to predict than the environment's own inherent dynamics/physics).</li>\n",
    "<li>As one agent learns how to behave more intelligently, the other agent has to counter this new behavior of its opponent and become smarter as well, asoasf.</li>\n",
    "    </ul>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "ppo.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    " <img src=\"images/exercise_env_loop.png\" width=500>\n",
    " \n",
    "We already learned how to use an environment's `reset()` and `step()` calls to walk through a single agent environment: Call `reset()` once, continue using the returned observations to compute actions, pass these actions into consecutive calls to `step()` and stop when `step()` returns the `done=True` flag.\n",
    "\n",
    "Let's do the same thing now in the multi-agent setting, using our `MultiAgentArena` class.\n",
    "Remember that everything, from observations, over rewards, dones, and actions now become dictionaries mapping agent IDs to the individual agent's observation, reward, action, and done information.\n",
    "\n",
    "Follow these instructions here to get this done:\n",
    "\n",
    "1. `reset` the already created (variable `env`) environment to get the first (initial) observations for \"agent1\" and \"agent2\".\n",
    "1. Enter an infinite while loop, in which you ..\n",
    "1. .. compute the actions for \"agent1\" and \"agent2\" (using random sampling).\n",
    "1. .. put the results of the action computations into an action dict (`{\"agent1\": [action1], \"agent2\": [action2]}`).\n",
    "1. .. pass this action dict into the env's `step()` method.\n",
    "1. .. check the returned `dones` dict for True (yes, episode is terminated) and if True, break out of the loop. Note here that you may also check the `dones` dict for the special \"__all__\" key and if `dones['__all__'] is True` -> the episode has ended for all agents.\n",
    "\n",
    "**Good luck! :)**\n",
    "\n",
    "\n",
    "Write your solution code into this following python cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.,\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # 1) Reset the env ...\n",
    "\n",
    "    # 2) Enter an infinite while loop (in order to step through one episode) ...\n",
    "\n",
    "        # 3) Calculate both agents' actions individually, using random sampling with the \n",
    "        # action space: e.g. `a1 = env.action_space.sample()`.\n",
    "\n",
    "        # 4) Compile the actions dict from both individual agents' actions ...\n",
    "\n",
    "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts ...\n",
    "\n",
    "        # 6) We'll do this together: Render the env.\n",
    "        # Don't write any code here (skip directly to 7).\n",
    "        out.clear_output(wait=True),\n",
    "        time.sleep(0.08),\n",
    "        env.render(),\n",
    "\n",
    "        # 7) Check, whether the episde is done (take a look at the\n",
    "        # `dones` dict returned from `step()`)\n",
    "        # If dones[\"__all__\"], break out of the while loop we entered in step 2).\n",
    "\n",
    "\n",
    "# 8) Run it! :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " * [Hands-on RL with Rayâs RLlib](https://github.com/sven1977/rllib_tutorials/tree/main/ray_summit_2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
