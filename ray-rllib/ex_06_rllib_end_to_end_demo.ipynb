{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a2f968-af93-4759-b6dc-ba5e5dcd3332",
   "metadata": {},
   "source": [
    "# Notebook 06. End-to-end demo: Learning a multiplayer game and a in-game item price recommendation system with RLlib, Ray Tune, and Ray Serve\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "* Recycle our multi-player game from a previous notebook in this tutorial\n",
    "* The game will be interrupted in the middle of an episode by an in-game item sale (a power-up is offered to both players at a price determined by a trained RecSys model served via Ray Serve)\n",
    "* A user model decides whether to buy the item or not\n",
    "* The game continues with or without the bought item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e78d6-61fb-4e8f-bb54-d09080a6a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.examples.env.random_env import RandomEnv\n",
    "\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78f7eb-936c-47a1-922a-9bd4beab2e3b",
   "metadata": {},
   "source": [
    "## Modifying our Game\n",
    "\n",
    "So far, we have been using our own custom `MultiAgentEnv` sub-class to define our game and asked RLlib to train two policies (one for each agent/player in the game) on how to play the game close to optimal.\n",
    "\n",
    "In this end-to-end example, we would like to extend this idea and include an in-game power-up (item) sale in the middle of the episode.\n",
    "The type of the offered item is fixed and always the same for both players. Buying it will allow the respectve agent to move twice as fast as before.\n",
    "Remember that each episode had a fixed number of timesteps (configurable via the `timestep_limit` constructor argument). We will now add some logic such that the game will pause after half of this number of timesteps and ask the in the to \n",
    "\n",
    "<img src=\"images/multi_agent_arena_3.png\" width=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafd4eb-cc44-4acf-9f19-2f7e4d30957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this simple script to generate some RecSys (price recommender) offline data:\n",
    "\n",
    "dummy_config = PPOConfig().environment(env=RandomEnv, env_config={\n",
    "    # Observation space: agent1 total reward, agent2 total reward\n",
    "    \"observation_space\": gym.spaces.Box(-100, 100.0, (2, ), np.float32),\n",
    "    # Price for the offered item (between $0 and $100).\n",
    "    \"action_space\": gym.spaces.Box(0.0, 100.0, (1,), np.float32),\n",
    "    \"reward_space\": gym.spaces.Box(0.0, 1.0, (), np.float32),\n",
    "    \"p_done\": 0.0,\n",
    "    # One-step episode len:\n",
    "    # reset() -> obs=game state\n",
    "    # step(action=recommended price) -> reward=bought or not + done?\n",
    "    \"max_episode_len\": 1,\n",
    "}).offline_data(output=\"offline_rl_data\")\n",
    "\n",
    "# Uncomment to train and generate the json output.\n",
    "\"\"\"\n",
    "algo = dummy_config.build()\n",
    "\n",
    "for _ in range(4):\n",
    "    algo.train()\n",
    "\"\"\"\n",
    "\n",
    "dummy_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ec786-dc51-45b1-a788-c7f8e83c7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first take a look at some of this (JSON) data using pandas:\n",
    "json_file = \"offline_rl_data/in_game_item_price_recsys.json\"\n",
    "dataframe = pandas.read_json(json_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39d301-c849-4c25-bbd4-cb0a5d05e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crr_config = CRRConfig()\n",
    "\n",
    "crr_config.environment(\n",
    "    env=None,\n",
    "    observation_space=dummy_config.env_config[\"observation_space\"],\n",
    "    action_space=dummy_config.env_config[\"action_space\"],\n",
    ")\n",
    "\n",
    "crr_config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        # If you feel daring here, use the `pendulum_beginner.json` file instead of the expert one here.\n",
    "        # You may need to train a little longer, then, in order to get a decent policy.\n",
    "        # But since you have the actual Pendulum environment available for evaluation, you should be able\n",
    "        # to perfectly stop learning once a good episode reward (> -300.0) has been reached.\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/in_game_item_price_recsys.json\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "crr_config.framework(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132acdd0-15b2-4aa9-98ef-b7f94685c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tune.run(\n",
    "    # Registered name for the CRR Algorithm.\n",
    "    \"CRR\",\n",
    "    # Use our config -> converted to python dict.\n",
    "    config=crr_config.to_dict(),\n",
    "    # Stopping criteria -> As we are learning from dummy data, just train for a few iterations.\n",
    "    stop={\n",
    "        \"training_iteration\": 3,\n",
    "    },\n",
    "    # Create checkpoint every iteration.\n",
    "    checkpoint_freq=3,\n",
    "    local_dir=\"results\",\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d74ff1-5429-4535-abd4-c6d762a48a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trial (there is only one) and last checkpoint.\n",
    "best_trial = results.get_best_trial()\n",
    "last_checkpoint = results.get_last_checkpoint(trial=best_trial)\n",
    "print(f\"Last checkpoint from training: {last_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbadcf-57c1-434a-9ed4-724f6a45997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/in-game-recommendations\")\n",
    "class ServeModel:\n",
    "    def __init__(self, config, checkpoint) -> None:\n",
    "        # Create new algo from scratch.\n",
    "        self.algo = config.build()\n",
    "        # Restore state of algo to a already trained one (using a checkpoint).\n",
    "        self.algo.restore(checkpoint)\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        json_input = await request.json()\n",
    "        # Extract observation from input.\n",
    "        obs = json_input[\"observation\"]\n",
    "        # Translate obs back to np.arrays.\n",
    "        np_obs = np.array(obs)\n",
    "        action = self.algo.compute_single_action(np_obs, explore=False)\n",
    "        return {\"action\": action}\n",
    "\n",
    "serve_model = ServeModel.bind(crr_config, last_checkpoint)\n",
    "serve.run(serve_model)\n",
    "    \n",
    "# That's it: Deployment created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe9b6e-13b7-4609-99d6-09676177e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convenience function to send action requests to the service.\n",
    "def get_price(rewards1, rewards2):\n",
    "    obs = np.array([rewards1, rewards2])\n",
    "    # Convert numpy array to list (needed for http transfer).\n",
    "    obs = obs.tolist()\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/in-game-recommendations\", json={\"observation\": obs}\n",
    "    )\n",
    "    response_json = resp.json()\n",
    "    price = response_json[\"action\"][0]\n",
    "    return price\n",
    "\n",
    "# Test our deployment\n",
    "get_price(0.0, -10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15093b3c-52ee-4a8b-a3cf-e9821176db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentArenaWithItemSale(MultiAgentArena):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config=config)\n",
    "        \n",
    "        self.sell_item_at_ts = self.timestep_limit // 2\n",
    "\n",
    "    def reset(self):\n",
    "        obs = super().reset()\n",
    "        self.agent1_moves_first = False\n",
    "        self.agent2_double_speed = False\n",
    "        return obs\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        # Increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        ######################\n",
    "        # NEW BEHAVIOR\n",
    "        ######################\n",
    "        # It's time to do the item sale.\n",
    "        price_agent1_item = price_agent2_item = 0.0\n",
    "        if self.timesteps == self.sell_item_at_ts:\n",
    "            # Send a price request to our price service.\n",
    "            price_agent1_item = get_price(self.agent1_R, self.agent2_R)\n",
    "            price_agent2_item = get_price(self.agent2_R, self.agent1_R)\n",
    "            \n",
    "            # User model agent1: User of agent1 buys if item price < 50.0.\n",
    "            if price_agent1_item < 50.0:\n",
    "                print(\"User1 bought power-up!\")\n",
    "                time.sleep(1.0)\n",
    "                self.agent1_moves_first = True\n",
    "            # User model agent2: User of agent2 buys if item price < 45.0.\n",
    "            if price_agent2_item < 45.0:\n",
    "                print(\"User2 bought power-up!\")\n",
    "                time.sleep(1.0)\n",
    "                self.agent2_double_speed = True\n",
    "        \n",
    "        # Who moves first?\n",
    "        # events = [collision|agent1_new_field]\n",
    "        if self.agent1_moves_first:\n",
    "            events = self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "            events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            # Agent2 is allowed to move twice (double the speed).\n",
    "            if self.agent2_double_speed:\n",
    "                events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        else:\n",
    "            events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            # Agent2 is allowed to move twice (double the speed).\n",
    "            if self.agent2_double_speed:\n",
    "                events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Determine rewards based on the collected events AND on the prices paid:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "        r1 -= price_agent1_item / 10.0\n",
    "        r2 -= price_agent2_item / 10.0\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        ######################\n",
    "        # END: NEW BEHAVIOR\n",
    "        ######################\n",
    "\n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "        if self.collision is True:\n",
    "            self.num_collisions += 1    \n",
    "\n",
    "        return self._get_obs(), rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a8cfa-e0e4-4a7f-9214-b780807c10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MultiAgentArenaWithItemSale(config={\"render\": True, \"width\": 5, \"height\": 5, \"timestep_limit\": 10})\n",
    "obs = env.reset()\n",
    "\n",
    "with env.out:\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves left, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 3, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae42430-c321-4aa6-8919-b6a2385f217b",
   "metadata": {},
   "source": [
    "<img src=\"images/end_to_end_example.png\" width=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce6522-a71e-4c6f-bd83-29c7dbb9846d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
