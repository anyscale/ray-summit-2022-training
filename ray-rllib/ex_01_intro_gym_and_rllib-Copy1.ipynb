{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c385108a",
   "metadata": {},
   "source": [
    "# Exercise 02. Introduction to the OpenAI Gym Environment and RLlib Algorithm top-level APIs\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, we will learn about:\n",
    " * [What is an Environment in RL](#intro_env)\n",
    " * [Overview of RL terminology](#intro_rl)\n",
    " * [What is OpenAI Gym](#intro_gym)\n",
    " * [High-level OpenAI Gym API calls](#intro_gym_api)\n",
    " * [Introduction to RLlib](#intro_rllib)\n",
    " * [How to train a RLlib model on a Gym environment](#intro_rllib_api)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2644aa5",
   "metadata": {},
   "source": [
    "## What is an environment in RL? <a class=\"anchor\" id=\"intro_env\"></a>\n",
    "\n",
    "Solving a problem in RL begins with an **environment**. In the simplest definition of RL:\n",
    "\n",
    "> An **agent** interacts with an **environment** and receives a reward.\n",
    "\n",
    "An environment in RL is the agent's world, it is a simulation of the problem to be solved. \n",
    "\n",
    "<img src=\"images/env_key_concept1.png\" width=\"50%\">\n",
    "\n",
    "The environment simulator might be a:\n",
    "<ul>\n",
    "    <li>real, physical situation such as a gas turbine</li>\n",
    "    <li>virtual sytem on a computer such as a board game or video game</li>\n",
    "    </ul>\n",
    "Why bother with an Agent and Environment?  \n",
    "\n",
    "> RL is useful when you have sequential decisions that need to be optimized over time. \n",
    "\n",
    "Traditional supervised learning views the world as more of a one-shot training, not as action -> fedback -> improved action -> repeat.\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03bd0cb",
   "metadata": {},
   "source": [
    "## Overview of RL terminology <a class=\"anchor\" id=\"intro_rl\"></a>\n",
    "\n",
    "An RL environment consists of: \n",
    "\n",
    "1. all possible actions (**action space**)\n",
    "2. a complete omniscient description of the environment, nothing hidden (**state space**)\n",
    "3. an observation by the agent of certain parts of the state (**observation space**)\n",
    "4. **reward**, which is the only feedback the agent receives per action.\n",
    "\n",
    "The model that tries to maximize the expected sum over all future rewards is called a **policy**. The policy is a function mapping the environment's observations to an action to take, usually written **π** (s(t)) -> a(t).\n",
    "\n",
    "Below is a high-level image of how the Agent and Environment work together in a RL simulation feedback loop in RLlib.\n",
    "\n",
    "<img src=\"images/env_key_concept2.png\" width=\"98%\">\n",
    "\n",
    "The **RL simulation feedback loop** repeatedly collects data, for one (single-agent case) or multiple (multi-agent case) policies, trains the policies on these collected data, and makes sure the policies' weights are kept in synch. Thereby, the collected environment data contains observations, taken actions, received rewards and so-called **done** flags, indicating the boundaries of different episodes the agents play through in the simulation.\n",
    "\n",
    "The simulation iterations of action -> reward -> next state -> train -> repeat, until the end state, is called an **episode**, or in RLlib, a **rollout**\n",
    "\n",
    "Per episode, the RL simulation feedback loop repeats up to some specified end state (termination state or timesteps). Examples of termination could be:\n",
    "<ul>\n",
    "    <li>the end of a maze (termination state)</li>  \n",
    "    <li>the player died in a game (termination state)</li>\n",
    "    <li>after 60 videos watched in a recommender system (timesteps).</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e038",
   "metadata": {},
   "source": [
    "## OpenAI Gym example: frozen lake <a class=\"anchor\" id=\"intro_gym\"></a>\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) is a well-known reference library of RL environments. \n",
    "\n",
    "#### 1. import gym\n",
    "\n",
    "Below is how you would import gym and view all available environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246bb39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "Num Gym Environments: 103\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import gym\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# List all available gym environments\n",
    "all_env  =  list(gym.envs.registry.all())\n",
    "print(f'Num Gym Environments: {len(all_env)}')\n",
    "\n",
    "# # You could loop through and list all environments if you wanted\n",
    "# [print(e) for e in all_env]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd7c45",
   "metadata": {},
   "source": [
    "#### 2. Instatiate your Gym object\n",
    "\n",
    "The way you instantiate a Gym environment is with the **make()** function.\n",
    "\n",
    "The .make() function takes arguments:\n",
    "- **name of the Gym environment**, type: str, Required.\n",
    "- **runtime parameter values**, Optional.\n",
    "\n",
    "For the required string argument, you need to know the Gym name.  You can find the Gym name in the Gym documentation for environments, either:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "    \n",
    "Below is an example of how to create a basic Gym environment, [frozen lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/).  We can see below that the termination condition of an episode will be time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9a8aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n",
      "env_spec: EnvSpec(FrozenLake-v1)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"FrozenLake-v1\"\n",
    "env_runtime_param_value = False\n",
    "\n",
    "# Instantiate gym env object with a runtime parameter value\n",
    "env = gym.make(\n",
    "        env_name, \n",
    "        is_slippery=env_runtime_param_value)\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89213b",
   "metadata": {},
   "source": [
    "#### 3. Inspect the environment action and observations spaces\n",
    "\n",
    "Gym Environments can be deterministic or stochastic.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Deterministic</b> if the current state + selected action determines the next state of the environment.  Chess is a deterministic environment, since all possible states/action combinations can be described as a discrete set of rules with states bounded by the pieces and size of the board.</li>\n",
    "    <li>\n",
    "        <b>Stochastic</b> if the policy output action is a probability distribution over a set of possible actions at time step t. In this case the agent needs to compute its action from the policy in two steps. i) sample actions from the policy according to the probability distribution, ii) compute log likelihoods of the actions. Stochastic environments are random in nature.  Random visitors to a website is an example of a stochastic environment. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Gym actions.</b> The action_space describes the numerical structure of the legitimate actions that can be applied to the environment. \n",
    "\n",
    "For example, if we have 4 possible discrete actions, we could encode them as:\n",
    "<ul>\n",
    "    <li>0: LEFT</li>\n",
    "    <li>1: DOWN</li>\n",
    "    <li>2: RIGHT</li>\n",
    "    <li>3: UP</li>\n",
    "</ul>\n",
    "\n",
    "<b>Gym observations.</b>  The observation_space defines the structure as well as the legitimate values for the observation of the state of the environment.  \n",
    "\n",
    "For example, if we have a 4x4 grid, we could encode them as {0,1,2,3, 4, … ,16} for grid positions ((0,0), (0,1), (0,2), (0,3), …. (3,3)).\n",
    "\n",
    "\n",
    "From the Gym [documentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) about the frozen lake environment, we see: <br>\n",
    "\n",
    "|Frozen Lake      | Gym space   |\n",
    "|---------------- | ----------- |\n",
    "|Action Space     | Discrete(4) |\n",
    "|Observation Space| Discrete(16)|\n",
    "\n",
    "\n",
    " \n",
    "<b><a href=\"https://github.com/openai/gym/tree/master/gym/spaces\">Gym spaces</a></b> are gym data types.  The main types are `Discrete` for discrete numbers and `Box` for continuous numbers.  \n",
    "\n",
    "Gym Space `Discrete` elements are Python type `int`, and Gym Space `Box` are Python type `float32`.\n",
    "\n",
    "Below is an example how to inspect the environment action and observations spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da84376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a gym environment.\n",
      "\n",
      "gym action space: Discrete(4)\n",
      "gym observation space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# check if it is a gym instance\n",
    "if isinstance(env, gym.Env):\n",
    "    print(\"This is a gym environment.\")\n",
    "    print()\n",
    "\n",
    "    # print gym Spaces\n",
    "    if isinstance(env.action_space, gym.spaces.Space):\n",
    "        print(f\"gym action space: {env.action_space}\")\n",
    "    if isinstance(env.observation_space, gym.spaces.Space):\n",
    "        print(f\"gym observation space: {env.observation_space}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9c3e0",
   "metadata": {},
   "source": [
    "#### 4. Inspect gym environment parameters\n",
    "\n",
    "Gym environments contain 2 sets of configuration parameters that are set after the environment object is instantiated.\n",
    "<ul>\n",
    "    <li><b>Runtime parameters</b> are passed into the make() function as **kwargs.</li>\n",
    "    <li><b>Default parameters</b> are fixed in the Gym environment code.</li>\n",
    "    </ul>\n",
    "\n",
    "Below is an example of how to inspect the environment parameters.  \n",
    "\n",
    "Notice we can tell from the parameters that our frozen lake environment is: \n",
    "1) Deterministic, and \n",
    "2) Episode terminates with time step condition max_episode_steps = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e5130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime spec params...\n",
      "map_name: 4x4\n",
      "is_slippery: False\n",
      "\n",
      "Default spec params...\n",
      "id: FrozenLake-v1\n",
      "entry_point: gym.envs.toy_text:FrozenLakeEnv\n",
      "reward_threshold: 0.7\n",
      "nondeterministic: False\n",
      "max_episode_steps: 100\n",
      "order_enforce: True\n"
     ]
    }
   ],
   "source": [
    "# inspect env.spec parameters\n",
    " \n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print(\"Runtime spec params...\")\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    " \n",
    "# View default env spec params\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "print(f\"entry_point: {env_spec.entry_point}\")\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\")\n",
    "\n",
    "# We can tell that our frozen lake environment is: \n",
    "# 1) Deterministic, and \n",
    "# 2) Episode terminates with condition max_episode_steps = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd2c68",
   "metadata": {},
   "source": [
    "#### 5. Perform some basic Gym API calls <a class=\"anchor\" id=\"intro_gym_api\"></a>\n",
    "\n",
    "The most basic Gym API methods are:\n",
    "<ul>\n",
    "    <li><b>env.reset()</b> <br>Reset the environment to an initial state, this is how you initialize an environment so you can run a simulation on it.  You should call this method every time to initiate a new episode.</li>\n",
    "    <li><b>env.render()</b>  <br>Visually inspect the environment anytime. Note you cannot inspect an environment before it has been initialized with env.reset().</li>\n",
    "    <li><b>env.step(action)</b> <br>Take an action from the possible action space values.  It accepts an action, computes the state of the environment after applying that action and returns the 4-tuple (observation, reward, done, info).</li>\n",
    "    <li><b>env.close()</b> <br>Close an environment.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802f6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Print the starting observation.  Recall possible observations are between 0-16.\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c36f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 1, reward: 0.0, done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Take an action\n",
    "# Recall the possible actions are: 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "\n",
    "new_obs, reward, done, _ = env.step(2) #Right\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()\n",
    "new_obs, reward, done, _ = env.step(1) #Down\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c91c07",
   "metadata": {},
   "source": [
    "We can also try to run an action in the frozen lake environment which is outside the defined number range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3db698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comment this cell if you want whole notebook to run without errors\n",
    "\n",
    "# # Try to take an invalid action\n",
    "\n",
    "# env.step(4) # invalid\n",
    "\n",
    "# # should see KeyError below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54a6486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 200\n",
      "obs: 5, reward: 1.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 400\n",
      "obs: 5, reward: 3.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 600\n",
      "obs: 5, reward: 9.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 800\n",
      "obs: 5, reward: 11.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Putting the simple API methods together.\n",
    "# Here is a pattern for running a bunch of episodes.\n",
    " \n",
    "num_episodes = 1000 # Number of episodes you want to run the agent\n",
    "render_freq = 200  # Render every X number of episodes \n",
    "total_reward = 0  # Initialize reward to 0\n",
    "\n",
    "# Loop through episodes\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Loop through time steps per episode\n",
    "    while True:\n",
    "        # take random action, but you can also do something more intelligent \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # If the epsiode is up, then start another one\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Render the env only every render_freq episodes\n",
    "    if ep % render_freq == 0:\n",
    "        print(f\"episode: {ep}\")\n",
    "        print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "        env.render()\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d495a",
   "metadata": {},
   "source": [
    "## Overview of RLlib <a class=\"anchor\" id=\"intro_rllib\"></a>\n",
    "\n",
    "<img width=\"7%\" src=\"images/rllib-logo.png\"> is the most comprehensive open-source Reinforcement Learning framework. **[RLlib](https://github.com/ray-project/ray/tree/master/rllib)** is built on top of **[Ray](https://docs.ray.io/en/latest/)**, an easy-to-use, open-source, distributed computing framework for Python that can handle complex, heterogeneous applications. Ray and RLlib run on compute clusters on any cloud without vendor lock.\n",
    "\n",
    "RLlib includes 25+ available [algorithms](https://docs.ray.io/en/master/rllib/rllib-algorithms.html), converted to both <img width=\"3%\" src=\"images/tensorflow-logo.png\">_TensorFlow_ and <img width=\"3%\" src=\"images/pytorch-logo.png\">_PyTorch_, covering different sub-categories of RL: _model-based_, _model-free_, and _Offline RL_. Almost any RLlib algorithm can learn in a multi-agent setting. Many algorithms support RNNs and LSTMs.\n",
    "\n",
    "**Environments in RLlib**\n",
    "\n",
    "To take advantage of Ray distributed parallel processing and vectorization, we can implement any environment as inheriting from [rllib.env.BaseEnv](https://github.com/ray-project/ray/blob/master/rllib/env/base_env.py).  See the [API documentation](https://docs.ray.io/en/latest/rllib/package_ref/env.html).  \n",
    "\n",
    "By default, all OpenAI Gym environments are automatically implemented as RLlib BaseEnv. RLlib environments use a Gym wrapper, which means **Gym APIs can be used in RLlib**.\n",
    "\n",
    "\n",
    "\n",
    "#### 1.  Import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb8ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39886fa1",
   "metadata": {},
   "source": [
    "#### 2. Check environment for errors\n",
    "\n",
    "Before you start training, it is a good idea to check the environment for errors.  We can use a convenient [RLlib function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) for this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4374df63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "\n",
    "# How to check you do not have any environment errors\n",
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    ray.rllib.utils.pre_checks.env.check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9cb0c",
   "metadata": {},
   "source": [
    "## Train a Gym environment using an algorithm from RLlib <a class=\"anchor\" id=\"intro_rllib_api\"></a>\n",
    "\n",
    "Roughly, RLlib is organized by **environments**, **algorithms**, **examples**, and **tuned_examples**.  \n",
    "\n",
    "    ray\n",
    "    |- rllib\n",
    "    |  |- env \n",
    "    |  |- algorithms \n",
    "    |  |  |- alpha_zero \n",
    "    |  |  |- appo \n",
    "    |  |  |- ppo \n",
    "    |  |  |- ... \n",
    "    |  |- examples \n",
    "    |  |- tuned_examples\n",
    "\n",
    "Within **_examples_** you will find faq code patterns.  \n",
    "\n",
    "Within **_tuned_examples_**, you will find, sorted by algorithm, suggested hyperparameter value choices within .yaml files. Ray RLlib team ran simulations/benchmarks to find suggested hyperparameter value choices.  These files used for daily testing, and weekly hard-task testing to make sure they all run at speed, for both TF and Torch.  Helps you with leg-up with parameter choices!\n",
    "\n",
    "In this tutorial, we will mainly focus on **_algorithms_**, where we will find RLlib algorithms to train RLlib models on environments.\n",
    "\n",
    "#### 3.  Select an algorithm and instantiate that algorithm's config object  \n",
    "\n",
    "To find it:\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">Open RLlib docs</a></li>\n",
    "    <li>Scroll down and click url of algo you're searching for, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i></li>\n",
    "    <li>Search github file for the word <i><b>trainer</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30536d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>\n"
     ]
    }
   ],
   "source": [
    "# NEW WAY TO TRAIN SINCE Ray version >= 1.13\n",
    "# config is an object instead of a dictionary\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# Define algorithm config values\n",
    "env_name = \"CartPole-v1\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 4  #4, num vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "relative_checkpoint_dir = \"my_PPO_logs\" # redirect logs instead of ~/ray_results/\n",
    "\n",
    "# uncomment below to see the long list of default algorithm config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "config_train = (\n",
    "    PPOConfig()\n",
    "    .framework(framework='torch')\n",
    "    .environment(env=env_name, disable_env_checking=False)\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    ")\n",
    "\n",
    "print(type(config_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ae0cd",
   "metadata": {},
   "source": [
    "#### 4. Instantiate a Trainer from the config object\n",
    "\n",
    "**Three ways to train RLlib models**\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/package_ref/index.html\">RLlib API.</a> The main methods we will use in this tutorial are:</li>\n",
    "    <ul>\n",
    "        <li>evaluate()</li>\n",
    "        <li>save()</li>\n",
    "        <li>restore()</li>\n",
    "    </ul>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/tune/api_docs/overview.html\">Ray Tune API.</a>  The main methods we will use in this tutorial are:</li>\n",
    "        <ul>\n",
    "        <li>run()</li>\n",
    "    </ul>\n",
    "    <li>RLlib CLI from command line: <i>rllib train -f [myfile_name].yml</i></li>\n",
    "    </ol>\n",
    "    \n",
    "We will cover Options 1-2 below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebbbb4",
   "metadata": {},
   "source": [
    "<b>Example Option 1: train RLlib using RLlib API .train() method</b>\n",
    "\n",
    "The code below shows how to instantiate the trainer and train the algorithm on the environment for 1 single episode.  \n",
    "\n",
    "To train for N number of episodes, you would put _.train()_ into a loop, similar to the way we ran env.step() in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd24eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e99bdc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 15:46:20,516\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.ppo.ppo.PPO'>\n"
     ]
    }
   ],
   "source": [
    "# Here is how you build a RLlib trainer and train it using RLlib API\n",
    "\n",
    "# Use .build() similar to how gym environments are passed to the gym .make() method.\n",
    "trainer = config_train.build(env=env_name)\n",
    "\n",
    "print(type(trainer))\n",
    "\n",
    "# run the trainer for 1 episode\n",
    "# trainer.train()\n",
    "\n",
    "# Below, you will see a lot of output.  In fact you will see the output 100 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d52c21",
   "metadata": {},
   "source": [
    "<b>Example Option 2: train RLlib using Ray Tune API .run() method</b>\n",
    "\n",
    "From the above cell, you can see how to train a RLlib algorithm 1 episode at a time.  But it is more practical to train RLlib algorithms using Ray Tune.\n",
    "\n",
    "Many more options are available for training using Ray Tune.  For example, in the code below, we specify a stopping criteria, instead of having to specify an exact number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e30029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98df8cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-25 16:46:38 (running for 00:01:03.36)<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.59 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=54685)\u001b[0m 2022-06-25 16:45:38,553\tWARNING ppo.py:350 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=4 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 250.\n",
      "\u001b[2m\u001b[36m(PPO pid=54685)\u001b[0m 2022-06-25 16:45:38,554\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=54685)\u001b[0m 2022-06-25 16:45:38,554\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=54685)\u001b[0m 2022-06-25 16:45:41,508\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=54685)\u001b[0m 2022-06-25 16:45:41,676\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=54696)\u001b[0m 2022-06-25 16:45:41,671\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "2022-06-25 16:46:39,441\tINFO tune.py:737 -- Total run time: 63.88 seconds (63.34 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Here is how you instantiate a Ray Tune trainer for RLlib and train a RLlib model\n",
    "\n",
    "trainer = tune.run(\"PPO\", \n",
    "                    \n",
    "        # Stopping criteria: average reward over training episodes\n",
    "        stop={\"episode_reward_mean\": 400},  #better is 400 out of max 500\n",
    "                    \n",
    "         # training config params\n",
    "         config = config_train.to_dict(),\n",
    "        \n",
    "         # # OLD WAY TO PASS training config params\n",
    "         # config = config_train,\n",
    "                    \n",
    "         #redirect logs instead of default ~/ray_results/\n",
    "         local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "         # set frequency saving checkpoints >= evaulation_interval\n",
    "         checkpoint_freq = checkpoint_freq,\n",
    "         \n",
    "         # Reduce logging messages\n",
    "         verbose = 1,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5439eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray if you are done\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306e4fe",
   "metadata": {},
   "source": [
    "#### 4. Understand the results of training\n",
    "\n",
    "How long did it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec3b8ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  60.38 seconds,    1.01 minutes\n"
     ]
    }
   ],
   "source": [
    "stats = trainer.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fad4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 407)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['episode_reward_max', 'episode_reward_min', 'episode_reward_mean',\n",
       "       'episode_len_mean', 'episodes_this_iter', 'num_healthy_workers',\n",
       "       'num_agent_steps_sampled', 'num_agent_steps_trained',\n",
       "       'num_env_steps_sampled', 'num_env_steps_trained',\n",
       "       ...\n",
       "       'info/learner/default_policy/learner_stats/total_loss',\n",
       "       'info/learner/default_policy/learner_stats/policy_loss',\n",
       "       'info/learner/default_policy/learner_stats/vf_loss',\n",
       "       'info/learner/default_policy/learner_stats/vf_explained_var',\n",
       "       'info/learner/default_policy/learner_stats/kl',\n",
       "       'info/learner/default_policy/learner_stats/entropy',\n",
       "       'info/learner/default_policy/learner_stats/entropy_coeff',\n",
       "       'config/evaluation_config/tf_session_args/gpu_options/allow_growth',\n",
       "       'config/evaluation_config/tf_session_args/device_count/CPU',\n",
       "       'config/evaluation_config/multiagent/policies/default_policy'],\n",
       "      dtype='object', length=407)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Tune training results in a pandas dataframe\n",
    "df = trainer.results_df\n",
    "print(df.shape)  #Only 1 trial\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535239e9",
   "metadata": {},
   "source": [
    "For how many episodes did training run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49d88a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of episodes for the 1st trial\n",
    "df.iloc[0,:].episodes_this_iter  \n",
    "\n",
    "#Answer is 14 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6bc2e",
   "metadata": {},
   "source": [
    "What were the best parameter values?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8edf7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out, the following cell output is long because there are many parameters!\n",
    "\n",
    "# trainer.get_best_config(metric=\"episode_reward_mean\", mode=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb478f",
   "metadata": {},
   "source": [
    "Where is the best model checkpoint file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "634c88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_a4705_00000_0_2022-06-25_15-46-25'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best checkpoint path\n",
    "logdir = trainer.get_best_logdir(metric=\"evaluation_reward_mean\", mode=\"max\")\n",
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d95c9a",
   "metadata": {},
   "source": [
    "#### 5. Visualize the training progress in TensorBoard\n",
    "\n",
    "RLlib automatically creates logs for your trained RLlib models that can be visualized in TensorBoard.  To visualize the performance of your RL model:\n",
    "\n",
    "<ol>\n",
    "    <li>Open a terminal</li>\n",
    "    <li><i><b>cd</b></i> into the logdir path from the above cell's output.</li>\n",
    "    <li><i><b>ls</b></i></li>\n",
    "    <li>You should see files that look like: checkpoint_NNNNNN</li>\n",
    "    <li>To be able to compare all your experiments, cd one dir level up.\n",
    "    <li><i><b>cd ..</b></i>  \n",
    "    <li><i><b>tensorboard --logdir . </b></i></li>\n",
    "    <li>Look at the url in the message, and open it in a browser</li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f2557",
   "metadata": {},
   "source": [
    "#### Screenshot of Tensorboard\n",
    "\n",
    "TensorBoard will give you many pages of charts.  Below displaying just Train/Eval mean and min rewards.\n",
    "\n",
    "<b>Train Performance:</b> <br>\n",
    "\n",
    "<img src=\"images/ppo_cartpole_training_rewards.png\" width=\"80%\">\n",
    "\n",
    "<b>Eval Performance:</b> <br>\n",
    "<img src=\"images/ppo_cartpole_evaluation_rewards.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9bb62",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc087967",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Look at the Gym [documentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) for the frozen lake environment.  Can you figure out how to change the environment to be **stochastic?**  Hint: change the kwarg `is_slippery`.\n",
    "2. Look at the runtime parameters for the Gym frozen lake environment.  How would you change the number of time steps per episode to be 200 instead of 100?\n",
    "3. How would you change the choice of RLlib algorithm from PPO to DQN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c3b4f",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
