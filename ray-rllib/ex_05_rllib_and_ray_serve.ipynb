{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05. Using RLlib with Ray Serve to deploy a policy into production\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this notebook, you will learn:\n",
    "\n",
    " * [How to deploy a trained policy into production using Ray Serve](#ray_serve)\n",
    " * [How to send requests to a deployed policy via HTTP](#ray_serve_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests import Request\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ray Serve to deploy a trained policy into production <a class=\"anchor\" id=\"ray_serve\"></a>\n",
    "\n",
    "<img src=\"images/rllib_and_ray_serve.png\" width=800 />\n",
    "\n",
    "This is a quick demo on how to use our already (offline RL) trained CRR policy and deploy it using Ray Serve.\n",
    "All you need to run the following code and produce a serve deployment is one of the checkpoints files from the previous notebook. Let's take checkpoint number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/serve-deployment\")  # , num_replicas=2)\n",
    "class ServeModel:\n",
    "    def __init__(self, config, checkpoint) -> None:\n",
    "        # Create new algo from scratch.\n",
    "        self.algo = config.build()\n",
    "        # Restore state of algo to a already trained one (using a checkpoint).\n",
    "        self.algo.restore(checkpoint)\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        json_input = await request.json()\n",
    "        # Extract observation from input.\n",
    "        obs = json_input[\"observation\"]\n",
    "        # Translate obs back to np.arrays.\n",
    "        np_obs = np.array(obs)\n",
    "        action = self.algo.compute_single_action(np_obs, explore=False)\n",
    "        return {\"action\": action}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, you will be asked to provide the path to an already existing checkpoint file.\n",
    "\n",
    "We have created Pendulum-v1 CRR checkpoints in the previous notebook and you should by now be able to navigate to one of these checkpoints using your notebook file browser on the left side:\n",
    "\n",
    "<img src=\"images/copy_checkpoint_path.png\" width=400>\n",
    "\n",
    "Once you copied the path, you can paste it into the `checkpoint = \"...\"` code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to create a deployment from the tagged class above, simply `.bind()`\n",
    "# the deployments with its constructor arguments:\n",
    "\n",
    "# Create config to use. Same as before, but lighter:\n",
    "# 1) No evaluation necessary (model only used for inference).\n",
    "# 2) No `offline_data` settings necessary (model only used for inference).\n",
    "\n",
    "config = CRRConfig().environment(env=\"Pendulum-v1\").framework(\"torch\").rollouts(num_rollout_workers=0)\n",
    "\n",
    "# Pick a solid checkpoint from the previous notebook's CRR experiment:\n",
    "#checkpoint = \"results/CRR/CRR_Pendulum-v1_93a30_00000_0_2022-07-26_23-26-14/checkpoint_000028/checkpoint-28\"\n",
    "checkpoint = \"/home/ray/Ray-Tutorial/ray-summit-2022-training/ray-rllib/offline_rl_data/pendulum_checkpoint/checkpoint-2\"\n",
    "\n",
    "serve_model = ServeModel.bind(config, checkpoint)\n",
    "serve.run(serve_model)\n",
    "    \n",
    "# That's it: Deployment created!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the deployment for serving actions <a class=\"anchor\" id=\"ray_serve_requests\"></a>\n",
    "\n",
    "Now let's send action inference requests to the existing deployment.\n",
    "We'll be using a test `Pendulum-v1` environment here, pretending we are some client that would like to query the server for good Pendulum actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a environment so we can step through episodes using requested, served actions.\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "# Get the initial observation.\n",
    "obs = env.reset()\n",
    "\n",
    "# Request 5 actions of an episode from served policy and step through the env using the received actions.\n",
    "for _ in range(3):\n",
    "    # Convert numpy array to list (needed for http transfer).\n",
    "    obs = obs.tolist()\n",
    "\n",
    "    print(f\"-> Sending observation {obs}\")\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/serve-deployment\", json={\"observation\": obs}\n",
    "    )\n",
    "    # Convert to response to JSON.\n",
    "    # The received JSON should include an \"action\" field (see our ServeModel class for details).\n",
    "    response_json = resp.json()\n",
    "    print(f\"<- got {response_json}\")\n",
    "\n",
    "    # Convert to numpy array.\n",
    "    action = np.array(response_json[\"action\"])\n",
    "\n",
    "    # Perform a step with the served action.\n",
    "    obs, _, _, _ = env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt:\n",
    "\n",
    "* How to create a Ray Serve \"deployment\" using a custom `@serve.deployment`-tagged class\n",
    "* How to query actions from this custom deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 05\n",
    "\n",
    "#### a) Run a complete episode using our Policy deployment\n",
    "\n",
    "Use the code in the cell above and now run a complete episode through a Pendulum environment, then report the episode's rewards at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new environment using gym.make():\n",
    "# ...\n",
    "\n",
    "# Get the initial observation via the env's `reset()` method.\n",
    "# ...\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "# Loop through an entire episode using a while loop.\n",
    "# while True:\n",
    "\n",
    "    # Remember to convert all np-arrays to lists prior to sending them via the Convert numpy array to list (needed for http transfer).\n",
    "    # obs = obs.tolist()\n",
    "\n",
    "    # Send the action request using `resp = request.get([address], json={\"observation\": [current observation]})`.\n",
    "    # ...\n",
    "\n",
    "    # Convert response to JSON and extract the \"action\" field, then convert the action to a numpy array.\n",
    "    # ...\n",
    "\n",
    "    # Perform an env step with the served action (using the env's `step([some action])` method).\n",
    "    # obs, reward, done, info = env.step(...)\n",
    "    \n",
    "    # Add up reward.\n",
    "    \n",
    "    # Check done flag; if True -> break out of while loop.\n",
    "\n",
    "    \n",
    "print(f\"Played one episode entirely using served actions; Total episode reward is {total_reward}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) What would happen, if we had a \"stateful\" policy?\n",
    "\n",
    "Think about the problem of having trained a neural network that uses one or more memory-capable (stateful) layers, like an LSTM layer.\n",
    "Given the problem of distributed deployment (across several endpoints accessible through different addresses/ports), what do you think would be the best solution for keeping or passing around the layer's state (e.g. the LSTM's internal c- and h- states)?\n",
    "\n",
    "* Should the server side (Ray Serve deployment) keep these tensors in between action requests?\n",
    "* Or should the clients handle these states and pass them back via each new requests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    " * [Ray Serve: Scalable and Programmable Serving](https://docs.ray.io/en/latest/serve/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è [Link to previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "‚û°Ô∏è [Link to next notebook](./ex_06_rllib_in_recsys_overview.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
