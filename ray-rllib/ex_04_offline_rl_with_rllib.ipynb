{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook 04. Introduction to Offline RL with RLlib\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "‚¨ÖÔ∏è  [Previous notebook](./ex_03_train_tune_rllib_model.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this notebook, you will learn:\n",
    " * [What's offline RL (aka \"batch RL\")?](#offline_rl)\n",
    " * [How to configure RLlib for offline RL](#offline_rl_with_rllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import os\n",
    "import pandas\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "import ray\n",
    "# Import the config class of the algorithm, we would like to train with: CRR.\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "from ray import tune\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's offline RL (aka \"batch RL\")? <a class=\"anchor\" id=\"offline_rl\"></a>\n",
    "\n",
    "So far, we have dealt with a so-called \"online\" setting for RL, in which we had direct control over a live environment (or a simulator). We were able to send arbitrary actions to this simulator and collect its responses (rewards and observations), thereby learning \"as we go\". This setup is called \"online\" RL:\n",
    "\n",
    "<img src=\"images/online_rl.png\" width=800 />\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src=\"images/exploration.png\" width=400 />\n",
    "\n",
    "However, often and especially in real-life industry settings, we are faced with the problem of not having a simulator at hand.\n",
    "In this case, we need to fall back to offline RL:\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Due to the dynamic nature of adversarial multi-agent scenarios, we will cover the topic of\n",
    "of offline RL here only for the single-agent case.\n",
    "Research on multi-agent offline RL is bleeding edge and not well explored by RLlib thus far (see references for more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL comes in two flavours:\n",
    "\n",
    "#### 1) Pure imitation learning\n",
    "\n",
    "The agent will try to imitate 100% the actions/behavior that it finds in the offline data).\n",
    "This setup is nothing else but supervised learning with a `-log(p)` loss function and only really makes sense if we would like to\n",
    "imitate an expert.\n",
    "\n",
    "#### 2) Imitation learning plus improvement over the recorded behavior\n",
    "\n",
    "The agent will partly imitate the offline, recorded behavior, but also try to improve over it, learning a policy that will\n",
    "perform better in the actual environment. This is achieved by focusing on those actions within the distribution that seem more \n",
    "promising, e.g. via weighting based on the received rewards.\n",
    "\n",
    "### If we don't have a live environment, how do we know, how well our trained policy will perform?\n",
    "\n",
    "One of the challenges in offline RL is the evaluation of the trained policy. In online RL (when a simulator\n",
    "is available), one can either use the data collected for training to compute episode total rewards. Remember\n",
    "that observations, actions, rewards, and done flags are all part of this training data. Alternatively,\n",
    "one could run a separate worker (with the same trained policy) and run it on a fresh evaluation-only environment.\n",
    "In this latter case, we would also have the chance to switch off any exploratory behavior (e.g. stochasticity used\n",
    "for better action entropy).\n",
    "\n",
    "In offline RL, no such data from a live environment is available to us. There are two common ways of addressing this dilemma:\n",
    "\n",
    "1) We deploy the learnt policy/ies into production, or maybe just a portion of our production system (similar to A/B testing), and see what happens.\n",
    "\n",
    "2) We use a method called \"off policy evaluation\" (OPE) to compute an estimate on how the new policy would perform if we were to deploy it into a real environment. There are different OPE methods available in RLlib off-the-shelf.\n",
    "\n",
    "3) The third option - which we will use here - is kind of cheating and only possible if you actually do have a simulator available (but you only want to use it for evaluation, not for training, because you want to see how cool offline RL is :) )\n",
    "\n",
    "### An example offline RL experiment\n",
    "Modern offline RL algorithms are capable of learning to perfectly play e.g. the Pendulum environment, when only behavioral data from a beginner agent is available (not completely random, but nowhere perfectly solving the problem either)! We'll explore this right now using RLlib's new CRR algorithm.\n",
    "\n",
    "The Pendulum-v1 environment looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"images/pendulum.gif\", width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continuous actions between -2.0 and 2.0 encode torques that will be applied to the hinge of a freely rotating pole.\n",
    "- The observations are x- and y- positions as well as the angular velocity.\n",
    "- The goal is to apply torques to the hinge such that the pendulum balances in an upright position.\n",
    "\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem available (the Pendulum-v1 problem), however, let's assume we possess a lot of pre-recorded, historic data from two legacy (non-RL) systems:\n",
    "- A **beginner system** that only knew how to get to a low episode reward. We recorded ~4500 timesteps from such a policy.\n",
    "- An **expert system** that was able to solve the Pendulum-v1 environment perfectly. We recorded ~100k timesteps from such a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first take a look at some of this (JSON) data using pandas:\n",
    "json_file = \"offline_rl_data/pendulum_beginner.json\"\n",
    "dataframe = pandas.read_json(json_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 1 timestep.\n",
    "print(f\"Read {len(dataframe)} records from JSON file\")\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning a decent policy using offline RL requires specialized RL algorithms.\n",
    "# Examples of offline RL algos are RLlib's \"CRR\", \"MARWIL\", or \"CQL\".\n",
    "# For this example, we'll use the \"Pendulum-v0\" environment and have the \"CRR\"\n",
    "# (critic regularized regression) algorithm learn how to solve this environment, purely from\n",
    "# data recorded from a random/beginner agent.\n",
    "\n",
    "# Create a defaut CRR config:\n",
    "config = CRRConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "# NOTE: We said above that we wouldn't really have an environment available (so how can\n",
    "# we set one up here??).\n",
    "# The following is only to tell the algorithm, which environment our offline data was actually taken from.\n",
    "config.environment(env=\"Pendulum-v1\")\n",
    "# If you really really don't have an environment, set `env=None` here and additionally define your action- and\n",
    "# observation spaces.\n",
    "# config.environment(env=None, action_space=..., observation_space=...)\n",
    "\n",
    "#################################################\n",
    "# This is the most important piece of code \n",
    "# in this notebook:\n",
    "# It explains how to point your \n",
    "# algorithm to the correct offline data file\n",
    "# (instead of a live-environment).\n",
    "#################################################\n",
    "config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        # If you feel daring here, use the `pendulum_beginner.json` file instead of the expert one here.\n",
    "        # You may need to train a little longer, then, in order to get a decent policy.\n",
    "        # But since you have the actual Pendulum environment available for evaluation, you should be able\n",
    "        # to perfectly stop learning once a good episode reward (> -300.0) has been reached.\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/pendulum_expert.json\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    # The (continuous) actions in our input files are already normalized\n",
    "    # (meaning between -1.0 and 1.0) -> We don't have to do anything with them prior to\n",
    "    # computing losses.\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "# RLlib's CRR is a very new algorithm (since 1.13) and only supports\n",
    "# the PyTorch framework thus far. We'll provide a tf version in the near future.\n",
    "config.framework(\"torch\")\n",
    "\n",
    "config.rollouts(num_rollout_workers=3)\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "config.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=1,\n",
    "\n",
    "    # Use separate resources (RLlib rollout workers).\n",
    "    evaluation_num_workers=1,\n",
    "\n",
    "    # Run 10 episodes per evaluation (per iteration) -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=10,\n",
    "    evaluation_duration_unit=\"episodes\",\n",
    "\n",
    "    # Use a slightly different config for the evaluation:\n",
    "    evaluation_config={\n",
    "        # - Use a real environment (so we can fully trust the evaluation results, rewards, etc..)\n",
    "        \"input\": \"sampler\",\n",
    "        # - Switch off exploration for better (less stochastic) action computations.\n",
    "        \"explore\": False,\n",
    "    },\n",
    "\n",
    "    # Run evaluation alternatingly with training (not in parallel).\n",
    "    evaluation_parallel_to_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt:\n",
    "\n",
    "* What offline RL is, how it differs from \"normal\" RL, and when you should use offline RL\n",
    "* How you can evaluate a offline-RL-learnt policy, even if you don't have a environment available (e.g. via OPE)\n",
    "* How to configure an RLlib offline-capable algorithm (e.g. CQL, CRR, or MARWIL) to read data from a datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 04\n",
    "\n",
    "#### a) Finish our CRR configuration\n",
    "\n",
    "Keep configuring our CRR algorithm by calling the config object's `training()` method and passing the following settings into that call.\n",
    "\n",
    "**NOTE:** Most of the following settings are CRR specific (except for gamma and train_batch_size, which are both universal RL/RLlib settings).\n",
    "\n",
    "```\n",
    "gamma: 0.99\n",
    "train_batch_size: 1024\n",
    "target_network_update_freq: 1\n",
    "tau: 0.0001\n",
    "weight_type: \"exp\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the `training()` call on your config here in this cell:\n",
    "config.training(\n",
    "    gamma=0.99,\n",
    "    # <- complete the other arguments to configure our CRR algo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Use `tune.run()` to kick off the experiment\n",
    "\n",
    "Similar to how we did it in the previous notebook, use `tune.run()` to kick off our offline RL learning experiment.\n",
    "Let's see how fast CRR can learn to play pendulum from beginner's data!\n",
    "\n",
    "- As stopping criteria, use `timesteps_total=2000000` and `evaluation/episode_reward_mean=-300`.\n",
    "- Also, make sure checkpoints are created every iteration (`checkpoint_freq=1`).\n",
    "- Set the output directory (`local_dir` arg) to \"results\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform the `tune.run()` call here:\n",
    "results = tune.run(\n",
    "    \"CRR\",\n",
    "    # config=...  <- check out the previous notebook on how to use tune.run() with an RLlib config object\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best checkpoint from our experiment.\n",
    "\n",
    "# We only had a single trial (one Algorithm instance), so this should be returned here.\n",
    "best_trial = results.get_best_trial()\n",
    "\n",
    "# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\n",
    "best_checkpoint = results.get_best_checkpoint(trial=best_trial, metric=\"evaluation/episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# We would expect this to be either the very last checkpoint or one close to it:\n",
    "print(f\"Best checkpoint from training: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Record our trained algorithm on a live Pendulum environment\n",
    "\n",
    "Analogous to how episode recording for FrozenLake was done in a previous notebook here, we will now\n",
    "restore a CRR Algorithm from one of the checkpoints created during the above `tune.run()` experiment (we will chose\n",
    "a checkpoint that was showing good mean rewards on the evaluation live-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a brand new CRR Algorithm using our existing config.\n",
    "crr = config.build()\n",
    "\n",
    "# Override the new CRR's state by restoring from one of our checkpoints.\n",
    "# Here, we use the best checkpoint (according to the `evaluation/episode_reward_mean` criterium).\n",
    "crr.restore(best_checkpoint)\n",
    "\n",
    "print(\"CRR Algorithm restored from checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this\n",
    "restored algorithm, we will record a single episode as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual (we are in a notebook) display, so we can record the env.\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "# Wrap a new Pendulum-v1 env with the gym VideoRecorder.\n",
    "env = RecordVideo(gym.make(\"Pendulum-v1\"), \"videos\")\n",
    "# Reset the env.\n",
    "obs = env.reset()\n",
    "\n",
    "# Run a single episode using actions computed by our trained CRR.\n",
    "while True:\n",
    "    action = crr.compute_single_action(observation=obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# Play the recorded video.\n",
    "Video(\"videos/rl-video-episode-0.mp4\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (release resources for other notebooks to come).\n",
    "crr.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems (by Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, 2020)](https://arxiv.org/abs/2005.01643)\n",
    "* [Batch Reinforcement Learning (by Sascha Lange, Thomas Gabel, Martin Riedmiller, 2012)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.229.787)\n",
    "\n",
    "##### Early Work\n",
    "* [Least-squares policy iteration (by Michail G. Lagoudakis, Ronald Parr, 2003)](http://www.jmlr.org/papers/v4/lagoudakis03a.html)\n",
    "* [Tree-based batch mode reinforcement learning (by Damien Ernst, Pierre Geurts, Louis Wehenkel, 2005)](https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÖÔ∏è [Previous notebook](./ex_03_train_tune_rllib_model.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
