{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 04. Introduction to Offline RL\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "‚¨ÖÔ∏è  [Previous notebook](./ex_03_train_tune_rllib_model.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn:\n",
    " * [What's offline RL (aka \"batch RL\")?](#offline_rl)\n",
    " * [How to configure RLlib for offline RL](#offline_rl_with_rllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import os\n",
    "\n",
    "import ray\n",
    "# Import the config class of the algorithm, we would like to train with: CRR.\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "from ray import tune\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's offline RL (aka \"batch RL\")? <a class=\"anchor\" id=\"offline_rl\"></a>\n",
    "\n",
    "So far, we have dealt with a so-called \"online\" setting for RL, in which we had direct control over a live environment (or a simulator). We were able to send arbitrary actions to this simulator and collect its responses (rewards and observations), thereby learning \"as we go\". This setup is called \"online\" RL:\n",
    "\n",
    "<img src=\"images/online_rl.png\" width=\"80%\"></img>\n",
    "\n",
    "However, often and especially in real-life industry settings, we are faced with the problem of not having a simulator at hand.\n",
    "In this case, we need to fall back to offline RL:\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=\"70%\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Due to the dynamic nature of adversarial multi-agent scenarios, we will cover the topic of\n",
    "of offline RL here only for the single-agent case.\n",
    "Research on multi-agent offline RL is bleeding edge and not well explored by RLlib thus far (see references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL comes in two flavours:\n",
    "\n",
    "#### 1) Pure imitation learning\n",
    "\n",
    "The agent will try to imitate 100% the actions/behavior that it finds in the offline data).\n",
    "This setup is nothing else but supervised learning with a `-log(p)` loss function.\n",
    "\n",
    "#### 2) Imitation learning plus improvement over the recorded behavior\n",
    "\n",
    "The agent will partly imitate the offline, recorded behavior, but also try to improve over it, learning a policy that will\n",
    "perform better in the actual environment. This is achieved by focusing on those actions within the distribution that seem more \n",
    "promising, e.g. via weighting based on the received rewards.\n",
    "\n",
    "### If we don't have a live environment, how do we know, how well our trained policy will perform?\n",
    "\n",
    "One of the challenges in offline RL is the evaluation of the trained policy. In online RL (when a simulator\n",
    "is available), one can either use the data collected for training to compute episode total rewards. Remember\n",
    "that observations, actions, rewards, and done flags are all part of this training data. Alternatively,\n",
    "one could run a separate worker (with the same trained policy) and run it on a fresh evaluation-only environment.\n",
    "In this latter case, we would also have the chance to switch off any exploratory behavior (e.g. stochasticity used\n",
    "for better action entropy).\n",
    "\n",
    "In offline RL, no such data from a live environment is available to us. There are two common ways of addressing this dilemma:\n",
    "\n",
    "1) We deploy the learnt policy/ies into production, or maybe just a portion of our production system (similar to A/B testing), and see what happens.\n",
    "\n",
    "2) We use a method called \"off policy evaluation\" (OPE) to compute an estimate on how the new polcy would perform if we were to deploy it into a real environment. There are different OPE methods available in RLlib off-the-shelf. In one of the exercises below, we will ask you to look\n",
    "\n",
    "3) The third option - which we will use here - is kind of cheating and only possible if you actually do have a simulator available (but you only want to use it for evaluation, not for training, because you want to see how cool offline RL is :) )    \"### Using an (offline) input file with an offline RL algorithm.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?\"\n",
    "\n",
    "\n",
    "\n",
    "### An example offline RL experiment\n",
    "Modern offline RL algorithms are capable of learning to perfectly play e.g. the Pendulum environment, when only behavioral data from a randomly acting agent is available! We'll explore this right now using RLlib's new CRR algorithm.\n",
    "\n",
    "The Pendulum-v1 environment looks as follows:\n",
    "- Continuous actions between -2.0 and 2.0 encode torques that will be applied to the hinge of a freely rotating pole.\n",
    "- The observations are x- and y- positions as well as the angular velocity.\n",
    "- The goal is to apply torques to the hinge such that the pendulum balances in an upright position.\n",
    "\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem available (the Pendulum-v1 problem), however, let's assume we possess a lot of pre-recorded, historic data from two legacy (non-RL) systems:\n",
    "- A **beginner system** that only knew how to get to a low episode reward.\n",
    "- An **expert system** that was able to solve the Pendulum-v1 environment perfectly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning a decent policy using offline RL requires specialized RL algorithms.\n",
    "# Examples of offline RL algos are RLlib's \"CRR\", \"MARWIL\", or \"CQL\".\n",
    "# For this example, we'll use the \"Pendulum-v0\" environment and have the \"CRR\"\n",
    "# (critic regularized regression) algorithm learn how to solve this environment, purely from\n",
    "# data recorded from a random/beginner agent.\n",
    "\n",
    "# Create a defaut CRR config:\n",
    "config = CRRConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "# NOTE: We said above that we wouldn't really have an environment available (so how can\n",
    "# we set one up here??).\n",
    "# The following is only to tell the algorithm, which environment our offline data was actually taken from.\n",
    "config.environment(env=\"Pendulum-v1\")\n",
    "# If you really really don't have an environment, set `env=None` here and additionally define your action- and\n",
    "# observation spaces.\n",
    "# config.environment(env=None, action_space=..., observation_space=...)\n",
    "\n",
    "#################################################\n",
    "# This is the most important piece of code \n",
    "# in this notebook:\n",
    "# It explains how to point your \n",
    "# algorithm to the correct offline data file\n",
    "# (instead of a live-environment).\n",
    "#################################################\n",
    "config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        # If you feel daring here, use the `pendulum_beginner.json` file instead of the expert one here.\n",
    "        # You may need to train a little longer, then, in order to get a decent policy.\n",
    "        # But since you have the actual Pendulum environment available for evaluation, you should be able\n",
    "        # to perfectly stop learning once a good episode reward (> -300.0) has been reached.\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/pendulum_expert.json\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    # The (continuous) actions in our input files are already normalized\n",
    "    # (meaning between -1.0 and 1.0) -> We don't have to do anything with them prior to\n",
    "    # computing losses.\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "# RLlib's CRR is a very new algorithm (since 1.13) and only supports\n",
    "# the PyTorch framework thus far. We'll provide a tf version in the near future.\n",
    "config.framework(\"torch\")\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "config.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=1,\n",
    "    # Use a separate resource (\"RLlib rollout worker\")\n",
    "    evaluation_num_workers=1,\n",
    "\n",
    "    # Use separate resources (RLlib rollout workers).\n",
    "    evaluation_num_workers=2,\n",
    "\n",
    "    # Run 20 episodes per evaluation (per iteration) -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=20,\n",
    "    evaluation_duration_unit=\"episodes\",\n",
    "\n",
    "    # Use a slightly different config for the evaluation:\n",
    "    evaluation_config={\n",
    "        # - Use a real environment (so we can fully trust the evaluation results, rewards, etc..)\n",
    "        \"input\": \"sampler\",\n",
    "        # - Switch off exploration for better (less stochastic) action computations.\n",
    "        \"explore\": False,\n",
    "    },\n",
    "\n",
    "    # Run evaluation alternatingly with training (not in parallel).\n",
    "    evaluation_parallel_to_training=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### 1) Finish CRR configuration\n",
    "\n",
    "Keep configuring our CRR algorithm by calling the config object's `training()` method and passing the following settings into that call:\n",
    "\n",
    "```\n",
    "gamma: 0.99\n",
    "train_batch_size: 1024\n",
    "target_network_update_freq: 1\n",
    "tau: 0.0001\n",
    "weight_type: \"exp\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the `training()` call on your config here in this cell:\n",
    "config.training(\n",
    "    gamma=0.99,\n",
    "    # <- complete the other arguments to configure our CRR algo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use `tune.run()` to kick off the experiment\n",
    "\n",
    "Similar to how we did it in the previous notebook, use `tune.run()` to kick off our offline RL learning experiment.\n",
    "Let's see how fast CRR can learn to play pendulum from scratch (from beginner's data)!\n",
    "\n",
    "- As stopping criteria, use `timesteps_total=2000000` and `evaluation/episode_reward_mean=-300`.\n",
    "- Also, make sure checkpoints are created every iteration (`checkpoint_freq=1`).\n",
    "- Set the output directory (`local_dir` arg) to \"results\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the `tune.run()` call here:\n",
    "tune.run(\n",
    "    \"CRR\",\n",
    "    # config=...  <- check out the previous notebook on how to use tune.run() with an RLlib config object\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove\n",
    "tune.run(\n",
    "    \"CRR\",\n",
    "    config=config.to_dict(),\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True,\n",
    "    local_dir=\"results\",\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Let's record our trained algorithm on a live Pendulum environment\n",
    "\n",
    "Analogous to how episode recording for CartPole was done in a previous notebook here, we will now\n",
    "restore a CRR Algorithm from one of the checkpoints created during the above `tune.run()` experiment (we will chose\n",
    "a checkpoint that was showing good mean rewards on the evaluation live-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a brand new CRR Algorithm using our existing config.\n",
    "crr = config.build()\n",
    "# Override the new CRR's state by restoring from one of our checkpoints.\n",
    "# Here, we use checkpoing 18, but you should simply pick the one that performed best on the\n",
    "# evaluation track (using the live environment).\n",
    "crr.restore(\"results/CRR/CRR_Pendulum-v1_e47f1_00000_0_2022-07-26_18-35-00/checkpoint_000038/checkpoint-38\")\n",
    "\n",
    "print(\"CRR Algorithm restored from checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this\n",
    "restored algorithm, we will record a single episode as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap a new Pendulum-v1 env with the gym VideoRecorder.\n",
    "env = RecordVideo(gym.make(\"Pendulum-v1\"), \"crr_video\")\n",
    "# Reset the env.\n",
    "obs = env.reset()\n",
    "\n",
    "# Run a single episode using actions computed by our trained CRR.\n",
    "while True:\n",
    "    action = crr.compute_single_action(observation=obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# Play the recorded video.\n",
    "Video(\"crr_video/rl-video-episode-0.mp4\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems (by Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, 2020)](https://arxiv.org/abs/2005.01643)\n",
    "* [Batch Reinforcement Learning (by Sascha Lange, Thomas Gabel, Martin Riedmiller, 2012)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.229.787)\n",
    "\n",
    "##### Early Work\n",
    "* [Least-squares policy iteration (by Michail G. Lagoudakis, Ronald Parr, 2003)](http://www.jmlr.org/papers/v4/lagoudakis03a.html)\n",
    "* [Tree-based batch mode reinforcement learning (by Damien Ernst, Pierre Geurts, Louis Wehenkel, 2005)](https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_03_train_tune_rllib_model.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
