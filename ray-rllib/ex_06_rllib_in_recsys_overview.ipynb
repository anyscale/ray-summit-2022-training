{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06. Introduction to RL applied to Recommender Systems\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_07_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn how to:\n",
    "\n",
    " * [Understand the Benefits of RL in a Recommender System](#recsys_rl)\n",
    " * [Create a RecSys RL environment](#recsys_env)\n",
    " * [Train a Contextual Bandit on the environment](#cb)\n",
    " * [Train using a SlateQ algorithm on the environment](#slateq)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of RL in a Recommender System <a class=\"anchor\" id=\"recsys_rl\"></a>\n",
    "\n",
    "A Recommender System <b>(RecSys)</b> suggests items that are most pertinent to a particular user.  Examples of recommender systems include:\n",
    "<ul>\n",
    "    <li>Movie/video/music recommendations (Netflix/YouTube/Spotify)</li>\n",
    "    <li>Online shopping recommendations (Amazon/Shopify)</li>\n",
    "    <li>Filtering your feed as you scroll (Twitter/Instagram)</li>\n",
    "</ul>\n",
    "\n",
    "<b>Two main approaches to training algorithms</b> for RecSys are: \n",
    "<ol>\n",
    "    <li>Traditional Machine Learning <b>(ML)</b></li>\n",
    "    <li>Reinforcement Learning <b>(RL)</b></li>\n",
    "    </ol>\n",
    "\n",
    "In the <b>traditional ML</b> method, data is gathered about users interactions with items.  Often this is represented as a <b>large, sparse matrix A</b> (rows are users, items are columns, elements of aij are the interactions).  <b>Matrix factorization (A = U * VT)</b> is often used to transform A into <i>trainable representations of users (U) and items (V)</i>, where U,V can be thought of as \"embeddings\" or features or X's.  Solving the matrix factorization problem turns into solving the quadratic minimization of frobenius-norm(A - U*VT).  Computationally this is solved using SGD Stochastic gradient descent (SGD), or Weighted Alternating Least Squares (WALS) which is the more parallelizable method of the two.  <br>\n",
    "\n",
    "Solving the matrix factorization problem is often referred to as <b><i>collaborative filtering</i></b>.  The algorithm is trained on all the data at once, which is extremely efficient.  But this means the traditional supervised learning model is <b>static with respect to time</b>. This can produce models that are:\n",
    "- too shortsighted or \n",
    "- overlook important and changing user intents or business conditions such as seasonality or promotional campaigns.\n",
    "\n",
    "<b>In RL</b>, users interact with offers repeatedly over time.  This <b><i>dynamic</i></b> model is iteratively trained based on observations, actions, and rewards.  \n",
    "> RL algorithms have become the de facto standard ML approach for solving sequential decision-making problems based on multiple goals that can be short-term and long-term.\n",
    " \n",
    "One caveat with RL, due to the sequential decision making process, <i>RL needs to calculate a new recommendation at every time step</i>.  This means to use RL efficiently, it is best to <b>restrict the data in a RL environment to only a pre-selected handful of top candidate items per user</b>.  <br>\n",
    "\n",
    "This makes RL a good fit in a Recommender system where a traditional ML ranking model already exists.  RL can be plugged into the last, interactive part of the RecSys system, as either <b>online</b> or <b>offline RL</b>.  \n",
    "> With both types of RL, ‚Äúserendipitous‚Äù aspects of user experience can be explored, since random actions the user did not historically take can be tried in the simulation.\n",
    "\n",
    "<img src=\"./images/recsys_overview.png\" width=\"100%\"/>\n",
    "\n",
    "- <b>Online RL in RecSys</b> takes as input the top-K ranked and ordered recommendations per user, keeps the user and item feature embeddings as context, and optimizes a sequence of user in-session interactions using <b> real-time, live results</b>. \n",
    "\n",
    "- <b>Offline RL in RecSys</b> uses offline data to explore a previous training run (or production) recommender model policy, implicitly through the data.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "    <b>üí° Online vs Offline RL, when the algorithm learning from an environment is: </b> <br><br>\n",
    "    ‚úîÔ∏è live (typically gaming platforms or complex systems simulations), this is called <b>online RL</b> and evaluation during training is <b>on-policy</b>. The live environment could be used for simulation learning purposes or it could be in production. <br><br>\n",
    "    ‚úîÔ∏è trained using data (could be log files of users interactions with items) converted into trajectory sequences of tuples, this is called <b>offline RL</b> and evaluation during training is <b>off-policy</b>, because the policy (RL word for model) used to log the data is different from the policy used to explore the data. Off-policy evaluation is also called <b>counterfactual evaluation</b>.</b> \n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a RecSys RL Environment <a class=\"anchor\" id=\"recsys_env\"></a>\n",
    "\n",
    "As we learned in the first 2 lessons, the first step to learning a RL policy is to create an <b>environment</b> that can be used to train a RL algorithm. \n",
    "\n",
    "In this notebook, we will use <b><a href=\"https://github.com/google-research/recsim\">Google's RecSim environment</a></b>, which was developed for the YouTube recommendation problem.  It is a configurable environment, where ideally you would plug in your own users, products, and embedding features.\n",
    "\n",
    "- <a href=\"https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/recsim.py\">RLlib implementation</a>\n",
    "- <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/recommender_system_with_recsim_and_slateq.py\">RLlib example</a>\n",
    "- <a href=\"https://github.com/google-research/recsim\">RecSim github</a>\n",
    "- <a href=\"https://arxiv.org/pdf/1909.04847.pdf\">RecSim paper</a>\n",
    "\n",
    "The environment is <i>Timelimit-based</i>, meaning the termination condition for an episode will be after a fixed number (60) of videos are watched. The RecSim environment consists of:\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/recsim_environment.png\" width=\"100%\" />\n",
    "\n",
    "- <b>Documents</b>, with features sampled in the range [0, 1] .  In this tutorial, we use <b>1 single feature \"sweetness\"</b> drawn from a gaussian distribution.\n",
    "> - The documents can be different at each step (produced by some \"candidate generation\" process), or fixed throughout the simulation.\n",
    "> - The recommendation algorithm observes the D candidate documents.  It then makes a selection (possibly ordered) of k documents and presents them in a \"slate\" to the user.\n",
    "\n",
    "- <b>Users</b>, with no features in this tutorial.\n",
    "> - The user examines a \"slate\" of recommended items and makes a choice of one item. After making their choice, the user emits an observation, which the recommender can use to learn for the next recommendation. \n",
    "> - The <b>user's long-term satisfaction is explicitly revealed as a numerical reward</b> at each time step of the simulation.\n",
    "- <b>Satisfaction</b> mechanism.  Think of this like a \"sugar-level\".  Based on the user's current sugar level, consuming a sweet item may or may not increase their long-term satisfaction (LTS).  <b>LTS stochastically (and slowly) decreases with the consumption of items containing high sweetness</b>. \n",
    "\n",
    "<b>RLlib comes with 3 RecSim environments</b>  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "üëâ <b>Long Term Satisfaction</b> (used in this tutorial) <br>\n",
    "- Interest Evolution <br>\n",
    "- Interest Exploration <br>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 2.0.0rc1\n",
      "tensorflow: 2.9.1\n",
      "gym: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.stats import linregress, sem\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "from pprint import pprint\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# silence the many tensorflow warnings\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import recsim\n",
    "\n",
    "print(f\"tensorflow: {tf.__version__}\")\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.env.wrappers.recsim.make_recsim_env.<locals>._RecSimEnv'>\n"
     ]
    }
   ],
   "source": [
    "# How to make RecSim environments?\n",
    "\n",
    "# Env config with 10 candidate items and a slate-size of 1.\n",
    "\n",
    "# Step 1. define a config dictionary\n",
    "env_config_10 = {\n",
    "    # The number of possible documents/videos/candidates that we can recommend\n",
    "    # no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "    \"num_candidates\": 10,  \n",
    "    # The number of recommendations that we will be making\n",
    "    \"slate_size\": 1,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": True,\n",
    "    # Use consistent seeds for the environment ...\n",
    "    \"seed\": 0,}\n",
    "\n",
    "# Step 2. create a RecSim environment\n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv(env_config_10)\n",
    "\n",
    "print(type(lts_10_1_env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "action space: MultiDiscrete([10])\n",
      "\n",
      "('observation space = Dict(user:Box([], [], (0,), float32), '\n",
      " 'doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), '\n",
      " '2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), '\n",
      " '4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), '\n",
      " '6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), '\n",
      " '8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32)), '\n",
      " 'response:Tuple(Dict(click:Discrete(2), engagement:Box(0.0, 100.0, (), '\n",
      " 'float32))))')\n",
      "\n",
      "(\"initial obs = OrderedDict([('user', array([], dtype=float32)), ('doc', {'0': \"\n",
      " \"array([0.79172504], dtype=float32), '1': array([0.5288949], dtype=float32), \"\n",
      " \"'2': array([0.56804454], dtype=float32), '3': array([0.92559665], \"\n",
      " \"dtype=float32), '4': array([0.07103606], dtype=float32), '5': \"\n",
      " \"array([0.0871293], dtype=float32), '6': array([0.0202184], dtype=float32), \"\n",
      " \"'7': array([0.83261985], dtype=float32), '8': array([0.77815676], \"\n",
      " \"dtype=float32), '9': array([0.87001216], dtype=float32)}), ('response', \"\n",
      " \"(OrderedDict([('click', 0), ('engagement', array(25.640253, \"\n",
      " 'dtype=float32))]),))])')\n",
      "engagement=25.6403\n",
      "\n",
      "Take a random action...\n",
      "action = [8]; reward = 20.58; done = False\n",
      "(\"next obs = OrderedDict([('user', array([], dtype=float32)), ('doc', {'0': \"\n",
      " \"array([0.9786183], dtype=float32), '1': array([0.7991586], dtype=float32), \"\n",
      " \"'2': array([0.46147937], dtype=float32), '3': array([0.7805292], \"\n",
      " \"dtype=float32), '4': array([0.11827443], dtype=float32), '5': \"\n",
      " \"array([0.639921], dtype=float32), '6': array([0.14335328], dtype=float32), \"\n",
      " \"'7': array([0.9446689], dtype=float32), '8': array([0.5218483], \"\n",
      " \"dtype=float32), '9': array([0.41466194], dtype=float32)}), ('response', \"\n",
      " \"({'click': 1, 'engagement': array(20.57586, dtype=float32)},))])\")\n",
      "engagement=20.5759\n"
     ]
    }
   ],
   "source": [
    "# print gym Spaces\n",
    "if isinstance(lts_10_1_env.action_space, gym.spaces.Space):\n",
    "    print()\n",
    "    print(f\"action space: {lts_10_1_env.action_space}\")\n",
    "if isinstance(lts_10_1_env.observation_space, gym.spaces.Space):\n",
    "    print()\n",
    "    pprint(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "\n",
    "# Start a new episode and look at an observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "print()\n",
    "pprint(f\"initial obs = {obs}\")\n",
    "print(f\"engagement={np.sum(obs['response'][0]['engagement']):.4f}\")\n",
    "\n",
    "action = lts_10_1_env.action_space.sample()\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "print()\n",
    "print(\"Take a random action...\")\n",
    "print(f\"action = {action}; reward = {reward:.2f}; done = {done}\")\n",
    "pprint(f\"next obs = {next_obs}\")\n",
    "print(f\"engagement={np.sum(next_obs['response'][0]['engagement']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us manually recommend some items (select some actions), send them to the environment, and record the rewards and next observations.  \n",
    "\n",
    "It is always a good idea to make sure you understand your environment well before using it to train a RL algorithm.\n",
    "\n",
    "> üëâ Exercise: Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's step() method. Each time, look at the returned next observation, reward, and done flag and write down what you find interesting about the dynamics and observations of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.2645556], dtype=float32),\n",
      "               '1': array([0.7742337], dtype=float32),\n",
      "               '2': array([0.45615032], dtype=float32),\n",
      "               '3': array([0.56843394], dtype=float32),\n",
      "               '4': array([0.0187898], dtype=float32),\n",
      "               '5': array([0.6176355], dtype=float32),\n",
      "               '6': array([0.6120957], dtype=float32),\n",
      "               '7': array([0.616934], dtype=float32),\n",
      "               '8': array([0.94374806], dtype=float32),\n",
      "               '9': array([0.6818203], dtype=float32)}),\n",
      "             ('response',\n",
      "              ({'click': 1, 'engagement': array(25.61927, dtype=float32)},))])\n",
      "reward = 25.62; done = False\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = [3]  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What did you learn from experimenting with the environment?</b>\n",
    "\n",
    "* User's state (if any) is hidden to agent (not part of observation).\n",
    "* Episodes seem to last about 60 time steps -> user seems to have some time budget to spend.\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 10.0+.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Weak suspicions</b><br>\n",
    "ü§î If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes. <br>\n",
    "ü§î If we always recommend the item with the lowest feature value, rewards seem to increase over time.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What the environment actually does under the hood</b>\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<img src=\"./images/long_term_satisfaction_env.png\" width=100%>\n",
    "\n",
    "<b>There is a double objective built into the env </b> (a. sweetness -> engagement; b. sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying the environment. As said above, the effect is very weak and almost not measurable, which is a problem on the env's side. \n",
    "\n",
    "We can use this following gym.ObservationWrapper class in the cell below to \"fix\" that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified-lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified-lts\", \n",
    "                  lambda env_config: LTSWithStrongerDissatisfactionEffect(\n",
    "                      LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified-lts' to be used in RLlib configs (see below)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.env.wrappers.recsim.make_recsim_env.<locals>._RecSimEnv'>\n",
      "<class '__main__.LTSWithStrongerDissatisfactionEffect'>\n"
     ]
    }
   ],
   "source": [
    "# How to make wrapped RecSim environments?\n",
    "\n",
    "# Env config with 20 candidate items and a slate-size of 2.\n",
    "\n",
    "# Step 1. define a config dictionary\n",
    "env_config_20 = {\n",
    "    # The number of possible documents/videos/candidates that we can recommend\n",
    "    # no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "    \"num_candidates\": 20,  \n",
    "    # The number of recommendations that we will be making\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    # Set to False for re-using the same candidate documents each timestep.\n",
    "    \"resample_documents\": True,\n",
    "    # # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": False,\n",
    "    # Use consistent seeds for the environment ...\n",
    "    \"seed\": 0,}\n",
    "\n",
    "# Step 2. create a RecSim environment\n",
    "lts_20_2_env = LongTermSatisfactionRecSimEnv(env_config_20)\n",
    "\n",
    "print(type(lts_20_2_env))\n",
    "\n",
    "# step 3. create a modified RecSim environment\n",
    "# LTSWithStrongerDissatisfactionEffect(recsim_env)\n",
    "modified_lts_20_2_env = \\\n",
    "    LTSWithStrongerDissatisfactionEffect(lts_20_2_env)\n",
    "\n",
    "print(type(modified_lts_20_2_env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "action space: Discrete(400)\n",
      "\n",
      "('observation space = Dict(user:Box([0.], [1.], (1,), float32), '\n",
      " 'doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), '\n",
      " '2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), '\n",
      " '4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), '\n",
      " '6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), '\n",
      " '8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), '\n",
      " '10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), '\n",
      " '12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), '\n",
      " '14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), '\n",
      " '16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), '\n",
      " '18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), '\n",
      " 'response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), '\n",
      " 'float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), '\n",
      " 'float32))))')\n",
      "\n",
      "(\"initial obs = OrderedDict([('user', array([0.51039932])), ('doc', {'0': \"\n",
      " \"array([0.9786183], dtype=float32), '1': array([0.7991586], dtype=float32), \"\n",
      " \"'2': array([0.46147937], dtype=float32), '3': array([0.7805292], \"\n",
      " \"dtype=float32), '4': array([0.11827443], dtype=float32), '5': \"\n",
      " \"array([0.639921], dtype=float32), '6': array([0.14335328], dtype=float32), \"\n",
      " \"'7': array([0.9446689], dtype=float32), '8': array([0.5218483], \"\n",
      " \"dtype=float32), '9': array([0.41466194], dtype=float32), '10': \"\n",
      " \"array([0.2645556], dtype=float32), '11': array([0.7742337], dtype=float32), \"\n",
      " \"'12': array([0.45615032], dtype=float32), '13': array([0.56843394], \"\n",
      " \"dtype=float32), '14': array([0.0187898], dtype=float32), '15': \"\n",
      " \"array([0.6176355], dtype=float32), '16': array([0.6120957], dtype=float32), \"\n",
      " \"'17': array([0.616934], dtype=float32), '18': array([0.94374806], \"\n",
      " \"dtype=float32), '19': array([0.6818203], dtype=float32)}), ('response', \"\n",
      " \"(OrderedDict([('click', 1), ('watch_time', array(97.67094, \"\n",
      " \"dtype=float32))]), OrderedDict([('click', 0), ('watch_time', \"\n",
      " 'array(21.992516, dtype=float32))])))])')\n",
      "\n",
      "Take a random action...\n",
      "action = 239; reward = 11.65; done = False\n",
      "(\"next obs = OrderedDict([('user', array([0.50253614])), ('doc', {'0': \"\n",
      " \"array([0.3595079], dtype=float32), '1': array([0.43703195], dtype=float32), \"\n",
      " \"'2': array([0.6976312], dtype=float32), '3': array([0.06022547], \"\n",
      " \"dtype=float32), '4': array([0.6667667], dtype=float32), '5': \"\n",
      " \"array([0.67063785], dtype=float32), '6': array([0.21038257], dtype=float32), \"\n",
      " \"'7': array([0.12892629], dtype=float32), '8': array([0.31542835], \"\n",
      " \"dtype=float32), '9': array([0.36371076], dtype=float32), '10': \"\n",
      " \"array([0.57019675], dtype=float32), '11': array([0.43860152], \"\n",
      " \"dtype=float32), '12': array([0.9883738], dtype=float32), '13': \"\n",
      " \"array([0.10204481], dtype=float32), '14': array([0.20887676], \"\n",
      " \"dtype=float32), '15': array([0.16130951], dtype=float32), '16': \"\n",
      " \"array([0.6531083], dtype=float32), '17': array([0.2532916], dtype=float32), \"\n",
      " \"'18': array([0.46631077], dtype=float32), '19': array([0.2444256], \"\n",
      " \"dtype=float32)}), ('response', ({'click': 1, 'watch_time': array(11.647298, \"\n",
      " \"dtype=float32)}, {'click': 0, 'watch_time': array(0., dtype=float32)}))])\")\n"
     ]
    }
   ],
   "source": [
    "# print gym Spaces\n",
    "if isinstance(modified_lts_20_2_env.action_space, gym.spaces.Space):\n",
    "    print()\n",
    "    print(f\"action space: {modified_lts_20_2_env.action_space}\")\n",
    "if isinstance(modified_lts_20_2_env.observation_space, gym.spaces.Space):\n",
    "    print()\n",
    "    pprint(f\"observation space = {modified_lts_20_2_env.observation_space}\")\n",
    "\n",
    "# Start a new episode and look at an observation.\n",
    "obs = modified_lts_20_2_env.reset()\n",
    "print()\n",
    "pprint(f\"initial obs = {obs}\")\n",
    "\n",
    "\n",
    "action = modified_lts_20_2_env.action_space.sample()\n",
    "next_obs, reward, done, _ = modified_lts_20_2_env.step(action)\n",
    "print()\n",
    "print(\"Take a random action...\")\n",
    "print(f\"action = {action}; reward = {reward:.2f}; done = {done}\")\n",
    "pprint(f\"next obs = {next_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the RecSim documentation, we can see that:\n",
    "\n",
    "<ul>\n",
    "    <li><b>action</b>, is a number between 0 and num_products - 1 that represents index of products clicked, between 0 and 399 in this case. </li>\n",
    "    <li><b>observation</b> will be the next session of 20 candidate products user sees, based on the user's action.  Only 1 product feature is displayed: the sugar-value of the product.\n",
    "    </li>\n",
    "    <li><b>reward</b> is the engagement value (between 0 and 20) of the product user clicked.  RecSim has a simplified assumption that the products are ordered on the slates, and user always clicks the 1st-position slate.</li>\n",
    "    <li><b>done</b> is a True/False flag indicating if the episode or user's timeline (fixed 60 sessions) is over.</li>\n",
    "    <li><b>info</b> currently not used, so it is always an empty dictionary.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment by itself took 4.37 seconds\n",
      "after 100 episodes, reward slope: 0.00024281131238526826\n"
     ]
    }
   ],
   "source": [
    "# CHECK SLOPES OF ORIGINAL LTS\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "\n",
    "# Run 100 episodes\n",
    "num_episodes = 100\n",
    "for _ in range(num_episodes):\n",
    "    obs = lts_20_2_env.reset()  # Reset environment to get initial observation:\n",
    "    doc_list = [t[1][0] for t in list(obs['doc'].items()) ]\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    # action_sweetest = np.argmax(obs['item'])\n",
    "    # action_kaleiest = np.argmin(obs['item'])     \n",
    "    action_sweetest = np.argmax(doc_list)\n",
    "    action_kaleiest = np.argmin(doc_list)\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        action = lts_20_2_env.action_space.sample()\n",
    "        action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "\n",
    "        obs, reward, done, _ = lts_20_2_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(f\"Environment by itself took {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"after {num_episodes} episodes, reward slope: {np.mean(slopes)}\")\n",
    "\n",
    "# random slope: 0.0001578753656547918         #positive\n",
    "# greedy sweet slope: -0.0004637595769463624  # negative\n",
    "# greedy kale slope: -4.93173502101358e-05    # less negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment by itself took 4.42 seconds\n",
      "after 100 episodes, reward slope: 0.0004655441935348558\n"
     ]
    }
   ],
   "source": [
    "# CHECK SLOPES OF MODIFIED LTS\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "\n",
    "# Run 100 episodes\n",
    "num_episodes = 100\n",
    "for _ in range(num_episodes):\n",
    "    obs = modified_lts_20_2_env.reset()  # Reset environment to get initial observation:\n",
    "    doc_list = [t[1][0] for t in list(obs['doc'].items()) ]\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    # action_sweetest = np.argmax(obs['item'])\n",
    "    # action_kaleiest = np.argmin(obs['item'])     \n",
    "    action_sweetest = np.argmax(doc_list)\n",
    "    action_kaleiest = np.argmin(doc_list)\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        action = modified_lts_20_2_env.action_space.sample()\n",
    "        # action = action_sweetest\n",
    "        # action = action_kaleiest\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_20_2_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(f\"Environment by itself took {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"after {num_episodes} episodes, reward slope: {np.mean(slopes)}\")\n",
    "\n",
    "# random slope: 0.0003322163874076709         # less positive\n",
    "# greedy sweet slope: -7.708065223943293e-05  # negative\n",
    "# greedy kale slope: 0.0004774986787622364    # more positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept, the_label):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    x_vals = np.array(ax.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    ax.plot(x_vals, y_vals, '--', label=the_label)\n",
    "    # Add legend\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    # Add titles\n",
    "    plt.title(\"Satisfaction Slopes\")\n",
    "    # Hide x-axis labels\n",
    "    ax.tick_params(labelbottom=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAEWCAYAAABIYLz4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAABtYElEQVR4nO3deVxU9frA8c8My7CDiGyKiisq5C5imRuKW0lpqXETFbO6abesTLul1a1reatb3ix/N7duaZktViYkbmlKuCso7rsyICKrrDPf3x+TB0dxFwfxeb9e84o55znnPGdM5+F7votOKaUQQgghhKiG9LZOQAghhBDicqRQEUIIIUS1JYWKEEIIIaotKVSEEEIIUW1JoSKEEEKIaksKFSGEEEJUW1KoCCGEEKLakkJFCCGEENWWFCpCCCGEqLakUBFCWFmzZg06nY41a9Zc13Hl5eVMnDiRoKAg9Ho90dHRVZJfZW4059vpTshRiOpIChUh7hApKSkMGTKEBg0a4OTkRN26denduzf/+c9/buh8n3zyCfPnz79l+c2dO5d//etfDBkyhM8//5znn3/+lp37vFud861gNpv53//+R3h4ON7e3ri7u9OsWTNGjBjBH3/8Yev0hLjj6WStHyGqvw0bNtCjRw/q169PbGws/v7+HD9+nD/++IODBw9y4MCB6z5naGgoPj4+l/yGbzabKS0txdHREb3+2n+XGTZsGL///jsnTpy47lyu1a3O+VYYN24cM2fOZNCgQfTs2RN7e3v27t1LfHw8jz32GK+//jpgaVHp0aMHq1evpnv37rc1RyHuZPa2TkAIcXVvv/02np6ebNq0CS8vL6t9mZmZt/Raer0eJyen6z4uMzPzktxulxvN+WZlZGTwySef8MQTT/Df//7Xat+HH37I6dOnb3tOQtQ08uhHiDvAwYMHadWqVaWFgK+vr9X7efPm0bNnT3x9fTEYDLRs2ZJPP/3UKqZhw4bs2rWL3377DZ1Oh06n037Lr6wvxf79+xk8eDD+/v44OTlRr149hg0bRm5uLkeOHEGn07F69Wp27dqlne/88e+99x5dunShdu3aODs70759e7799ttK7/PLL7+kU6dOuLi4UKtWLe6//36WL19+QzkDLF68mPbt2+Ps7IyPjw9/+ctfOHnypFXMyJEjcXNz4+TJk0RHR+Pm5kadOnV48cUXMZlMV/hTgcOHD6OU4t57771kn06nu+TPpjLXk+OhQ4eIiorC1dWVwMBA3nzzTS5uFDebzXz44Ye0atUKJycn/Pz8ePLJJzl79qxV3ObNm4mKisLHxwdnZ2eCg4MZPXr0VfMV4naTFhUh7gANGjQgKSmJ1NRUQkNDrxj76aef0qpVKx588EHs7e35+eef+etf/4rZbOaZZ54BLL/tjx8/Hjc3N/7+978D4OfnV+n5SktLiYqKoqSkhPHjx+Pv78/JkydZunQpOTk51KlThy+++IK3336bgoICpk2bBkCLFi0A+Oijj3jwwQeJiYmhtLSUr7/+mkceeYSlS5cyYMAA7TpvvPEGr7/+Ol26dOHNN9/E0dGR5ORkVq1aRZ8+fa4rZ4D58+czatQoOnbsyLRp08jIyOCjjz5i/fr1bNu2zaroM5lMREVFER4eznvvvceKFSt4//33ady4MU8//fQV/1zAUmw88sgjuLi4XPHP5mZz7Nu3L507d2b69OkkJCQwdepUysvLefPNN7W4J598Ujvvs88+y+HDh/n444/Ztm0b69evx8HBgczMTPr06UOdOnWYNGkSXl5eHDlyhO+///668hfitlBCiGpv+fLlys7OTtnZ2amIiAg1ceJE9euvv6rS0tJLYs+dO3fJtqioKNWoUSOrba1atVLdunW7JHb16tUKUKtXr1ZKKbVt2zYFqMWLF18xx27duqlWrVpdNZ/S0lIVGhqqevbsqW3bv3+/0uv16qGHHlImk8kq3mw2X3fOpaWlytfXV4WGhqqioiItbunSpQpQU6ZM0bbFxsYqQL355ptW52zbtq1q3779Fe9ZKaVGjBihAFWrVi310EMPqffee0+lpaVVSY7jx4+3+lwGDBigHB0d1enTp5VSSq1bt04BasGCBVbXTkhIsNr+ww8/KEBt2rTpqvcnhK3Jox8h7gC9e/cmKSmJBx98kB07djB9+nSioqKoW7cuP/30k1Wss7Oz9nNubi5ZWVl069aNQ4cOkZube93X9vT0BODXX3/l3Llz1338hfmcPXuW3NxcunbtytatW7XtS5YswWw2M2XKlEs6w+p0uuu+5ubNm8nMzOSvf/2rVd+VAQMGEBISwi+//HLJMU899ZTV+65du3Lo0KGrXmvevHl8/PHHBAcH88MPP/Diiy/SokULevXqdckjnJvNcdy4cdrPOp2OcePGUVpayooVKwBLy46npye9e/cmKytLe7Vv3x43NzdWr14NoLXULF26lLKysqveoxC2JIWKEHeIjh078v3333P27Fk2btzI5MmTyc/PZ8iQIezevVuLW79+PZGRkbi6uuLl5UWdOnV45ZVXAG6oUAkODmbChAnMnj0bHx8foqKimDlz5jWfa+nSpXTu3BknJye8vb2pU6cOn376qdXxBw8eRK/X07Jly+vOrzJHjx4FoHnz5pfsCwkJ0faf5+TkRJ06day21apV65J+HZXR6/U888wzbNmyhaysLH788Uf69evHqlWrGDZs2C3LUa/X06hRI6ttzZo1A+DIkSOApS9Rbm4uvr6+1KlTx+pVUFCgdbzu1q0bgwcP5o033sDHx4dBgwYxb948SkpKrnq/QtxuUqgIcYdxdHSkY8eO/POf/+TTTz+lrKyMxYsXA5Yv/F69epGVlcUHH3zAL7/8QmJiojanidlsvqFrvv/+++zcuZNXXnmFoqIinn32WVq1anXVocjr1q3jwQcfxMnJiU8++YRly5aRmJjIY489dkknUFuys7O7JeepXbs2Dz74IMuWLaNbt278/vvvlxQcVclsNuPr60tiYmKlr/N9WXQ6Hd9++y1JSUmMGzeOkydPMnr0aNq3b09BQcFty1eIayGdaYW4g3Xo0AGA9PR0AH7++WdKSkr46aefqF+/vhZ3vsn/Qtf7SCUsLIywsDBeffVVNmzYwL333susWbN46623LnvMd999h5OTE7/++isGg0HbPm/ePKu4xo0bYzab2b17N23atLns+a415/OdXPfu3UvPnj2t9u3du1fbX5U6dOjAb7/9Rnp6eqXXu94czWYzhw4d0lpRAPbt2wdYRkSB5XNcsWIF9957r9Ujt8vp3LkznTt35u2332bhwoXExMTw9ddfM2bMmOu6VyGqkrSoCHEHWL16daUtEMuWLQMqHh+cbxm4MDY3N/eSwgDA1dWVnJycq147Ly+P8vJyq21hYWHo9fqrPiqws7NDp9NZDfM9cuQIS5YssYqLjo5Gr9fz5ptvXtLqc+G9XGvOHTp0wNfXl1mzZlnlGB8fT1pamtVoo5thNBqtHrudV1paysqVK9Hr9TRp0uSW5fjxxx9rPyul+Pjjj3FwcKBXr14APProo5hMJv7xj39ccmx5ebn22Z09e/aS/5/OF4jy+EdUN9KiIsQdYPz48Zw7d46HHnqIkJAQSktL2bBhA4sWLaJhw4aMGjUKgD59+uDo6MgDDzzAk08+SUFBAZ999hm+vr5aq8t57du359NPP+Wtt96iSZMm+Pr6XvKbPcCqVasYN24cjzzyCM2aNaO8vJwvvvgCOzs7Bg8efMW8BwwYwAcffEDfvn157LHHyMzMZObMmTRp0oSdO3dqcU2aNOHvf/87//jHP+jatSsPP/wwBoOBTZs2ERgYqA15vtacHRwcePfddxk1ahTdunVj+PDh2tDfhg0b3rLp/U+cOEGnTp3o2bMnvXr1wt/fn8zMTL766it27NjBc889h4+PT6XHXm+OTk5OJCQkEBsbS3h4OPHx8fzyyy+88sorWv+abt268eSTTzJt2jS2b99Onz59cHBwYP/+/SxevJiPPvpIW+Lgk08+4aGHHqJx48bk5+fz2Wef4eHhQf/+/W/JZyPELWPLIUdCiGsTHx+vRo8erUJCQpSbm5tydHRUTZo0UePHj1cZGRlWsT/99JO65557lJOTk2rYsKF699131dy5cxWgDh8+rMUZjUY1YMAA5e7urgBt2O/Fw2gPHTqkRo8erRo3bqycnJyUt7e36tGjh1qxYoXVdS83PHnOnDmqadOmymAwqJCQEDVv3jw1depUVdk/P3PnzlVt27ZVBoNB1apVS3Xr1k0lJiZed87nLVq0SDuft7e3iomJUSdOnLCKiY2NVa6urpfkcrkcL5SXl6c++ugjFRUVperVq6ccHByUu7u7ioiIUJ999pnV0OpbkePBgwdVnz59lIuLi/Lz81NTp069ZDi3Ukr997//Ve3bt1fOzs7K3d1dhYWFqYkTJ6pTp04ppZTaunWrGj58uKpfv74yGAzK19dXDRw4UG3evPmK9yuELchaP0IIUc2NHDmSb7/9Vjq6iruS9FERQgghRLUlhYoQQgghqi0pVIQQQghRbUkfFSGEEEJUW9KiIoQQQohqSwoVIYQQQlRbMuHbTTCbzZw6dQp3d/cbWuFVCCGEuFsppcjPzycwMPCSVdMvJIXKTTh16hRBQUG2TkMIIYS4Yx0/fpx69epddr8UKjfB3d0dsHzIHh4eNs5GCCGEuHPk5eURFBSkfZdejhQqN+H84x4PDw8pVIQQQogbcLWuE9KZVgghhBDVlhQqQgghhKi2pFARQgghRLUlhYoQQgghqi0pVIQQQghRbUmhIoQQQohqSwoVIYQQQlzW6XOnKTWV2uz6UqgIIYQQ4hLH847zZtKb9P2uLz8d/MlmeciEb0IIIYSwcvrcaR5c8iDlqhyAzRmbGdJsiE1ykUJFCCGEEBzNO0oDjwYA1HGpQ7egbhSbiokLjaODXweb5SWFiqg2dDodP/zwA9HR0bZORQgh7gpKKdadXMfslNnsPL2TXx7+hbpudQH41/3/wsHOwcYZSh8VcZGRI0ei0+nQ6XQ4ODgQHBzMxIkTKS4utnVqQgghbpFycznLDi1jyM9DeGblM2zL3IZOp2N75nYtpjoUKSAtKqISffv2Zd68eZSVlbFlyxZiY2PR6XS8++67tk5NCCHETSg1lbLkwBLmpc7jRMEJAFzsXXik2SM83vJx/Fz9bJzhpW5Li8rMmTNp2LAhTk5OhIeHs3HjxivGL168mJCQEJycnAgLC2PZsmVW+5VSTJkyhYCAAJydnYmMjGT//v1WMdnZ2cTExODh4YGXlxdxcXEUFBRo+48cOaK1HFz4+uOPP27djd+hDAYD/v7+BAUFER0dTWRkJImJiQCcOXOG4cOHU7duXVxcXAgLC+Orr76yOr579+48++yzTJw4EW9vb/z9/Xn99detYvbv38/999+Pk5MTLVu21M5/oZSUFHr27ImzszO1a9dm7NixVn+GI0eOJDo6mn/+85/4+fnh5eXFm2++SXl5OS+99BLe3t7Uq1ePefPm3foPSQgh7kDFpmI+2PIBJwpO4GXw4pk2z7B8yHJe7PhitSxS4DYUKosWLWLChAlMnTqVrVu30rp1a6KiosjMzKw0fsOGDQwfPpy4uDi2bdtGdHQ00dHRpKamajHTp09nxowZzJo1i+TkZFxdXYmKirJ6PBETE8OuXbtITExk6dKlrF27lrFjx15yvRUrVpCenq692rdvf+s/hAucKy2/7Ku4zHTLY29WamoqGzZswNHREYDi4mLat2/PL7/8QmpqKmPHjuXxxx+/pPj8/PPPcXV1JTk5menTp/Pmm29qxYjZbObhhx/G0dGR5ORkZs2axcsvv2x1fGFhIVFRUdSqVYtNmzaxePFiVqxYwbhx46ziVq1axalTp1i7di0ffPABU6dOZeDAgdSqVYvk5GSeeuopnnzySU6cOHHTn4UQQtxpzhSdYWHaQpRSAHg4evB066eZ1GkSvw7+ladaP4WnwdPGWV6ZTp3PvoqEh4fTsWNHPv74Y8DyJRUUFMT48eOZNGnSJfFDhw6lsLCQpUuXats6d+5MmzZtmDVrFkopAgMDeeGFF3jxxRcByM3Nxc/Pj/nz5zNs2DDS0tJo2bIlmzZtokMHS0/lhIQE+vfvz4kTJwgMDOTIkSMEBwezbds22rRpc0P3lpeXh6enJ7m5uXh4eFzTMQ0n/XLZfT2a12HeqE7a+xavJVB0UUFyXniwN4uejNDet/tHItmFl07Ic+SdAdeU13kjR47kyy+/xMnJifLyckpKStDr9XzzzTcMHjy40mMGDhxISEgI7733HmBpUTGZTKxbt06L6dSpEz179uSdd95h+fLlDBgwgKNHjxIYGAhY/nz69eundab97LPPePnllzl+/Diurq4ALFu2jAceeIBTp07h5+fHyJEjWbNmDYcOHUKvt9TcISEh+Pr6snbtWgBMJhOenp7Mnj2bYcOGXddnIYQQd6pTBaeYv2s+P+z/gWJTMbP7zCY8INzWaVm51u/QKm1RKS0tZcuWLURGRlZcUK8nMjKSpKSkSo9JSkqyigeIiorS4g8fPozRaLSK8fT0JDw8XItJSkrCy8tLK1IAIiMj0ev1JCcnW537wQcfxNfXl/vuu4+ffrryhDYlJSXk5eVZvWqiHj16sH37dpKTk4mNjWXUqFFakWIymfjHP/5BWFgY3t7euLm58euvv3Ls2DGrc9xzzz1W7wMCArRWtLS0NIKCgrQiBSAiIsIqPi0tjdatW2tFCsC9996L2Wxm79692rZWrVppRQqAn58fYWFh2ns7Oztq16592RY8IYSoSQ7mHOSVda/Q//v+fLXnK4pNxYTWDsVBXz06xt6IKu1Mm5WVhclkws/P+rmXn58fe/bsqfQYo9FYabzRaNT2n992pRhfX1+r/fb29nh7e2sxbm5uvP/++9x7773o9Xq+++47oqOjWbJkCQ8++GCluU2bNo033njjWm79sna/GXXZfXqdzur9ltciLxN5aezvL/e4qbwu5OrqSpMmTQCYO3curVu3Zs6cOcTFxfGvf/2Ljz76iA8//JCwsDBcXV157rnnKC21bs1xcLD+S6HT6TCbzbcsxytd53ZdWwghqovcklxeW/8aq4+v1rZ1DujMmLAxdPLvhO6i74w7yV076sfHx4cJEyZo7zt27MipU6f417/+ddlCZfLkyVbH5OXlERQUdF3XdXG89o+8qmKvh16v55VXXmHChAk89thjrF+/nkGDBvGXv/wFsDzK27dvHy1btrzmc7Zo0YLjx4+Tnp5OQEAAwCWdmFu0aMH8+fMpLCzUWlXWr1+PXq+nefPmt+juhBCiZnB3dOdw7mF06IhsEMno0NGE+oTaOq1bokof/fj4+GBnZ0dGRobV9oyMDPz9/Ss9xt/f/4rx5/97tZiLm/rLy8vJzs6+7HXB0p/mwIEDl91vMBjw8PCwet0NHnnkEezs7Jg5cyZNmzYlMTGRDRs2kJaWxpNPPnnJn8XVREZG0qxZM2JjY9mxYwfr1q3j73//u1VMTEwMTk5OxMbGkpqayurVqxk/fjyPP/74Ja1pQghxNzGZTSw/spynVzxNiakEAL1Oz+tdXmdJ9BI+6P5BjSlSoIoLFUdHR9q3b8/KlSu1bWazmZUrV17SJ+G8iIgIq3iAxMRELT44OBh/f3+rmLy8PJKTk7WYiIgIcnJy2LJlixazatUqzGYz4eGX70y0fft27Td8UcHe3p5x48Yxffp0XnjhBdq1a0dUVBTdu3fH39//umeS1ev1/PDDDxQVFdGpUyfGjBnD22+/bRXj4uLCr7/+SnZ2Nh07dmTIkCH06tVL65QthBB3mzJTGT/s/4HoH6N54bcX+P3k7/x44Edtf3u/9jTybGTDDKuIqmJff/21MhgMav78+Wr37t1q7NixysvLSxmNRqWUUo8//riaNGmSFr9+/Xplb2+v3nvvPZWWlqamTp2qHBwcVEpKihbzzjvvKC8vL/Xjjz+qnTt3qkGDBqng4GBVVFSkxfTt21e1bdtWJScnq99//101bdpUDR8+XNs/f/58tXDhQpWWlqbS0tLU22+/rfR6vZo7d+4131tubq4CVG5u7s18REIIIcRlFZYWqs9TP1c9v+mpQueHqtD5oarLwi7q420fq+yibFund8Ou9Tu0yvuoDB06lNOnTzNlyhSMRiNt2rQhISFBa74/duyY1aiNLl26sHDhQl599VVeeeUVmjZtypIlSwgNrWjGmjhxIoWFhYwdO5acnBzuu+8+EhIScHJy0mIWLFjAuHHj6NWrF3q9nsGDBzNjxgyr3P7xj39w9OhR7O3tCQkJYdGiRQwZYpvVIYUQQoiL5ZbkMvCHgeSU5ADg6+zLiFYjGNJsCK4Orlc+uIao8nlUarIbmUdFCCGEuJKC0gLcHN2098+sfIYjuUcYHTqaBxo/gKOdow2zu3Wu9Tv0rh31I4QQQlQnh3MPMy91HglHEvgp+if8XS2DP9669y08HD2w09vZOEPbkEJFCCGEsKFdZ3YxJ2UOK46uQGF5yLHy2EpiWsQAUMupli3TszkpVIQQQojbTCnFRuNG5qTMISm9Yqb27vW6ExcWRxvfNrZLrpqRQkUIIYS4zfLL8hm/ajxF5UXY6ezoF9yP0aGjaVqrqa1Tq3akUBFCCCGqWJm5jPUn19M9qDtgWcX4Ly3+Ql5pHiNbjaSeez3bJliNSaEihBBCVJGi8iK+3/89n+/6nPTCdD7v+znt/NoB8Gy7Z22c3Z1BChUhhBDiFsstyWXR3kUsSFtAdnE2AN5O3pwpPmPjzO48UqgIIYQQt8i5snPM2jGLb/Z9Q2FZIQB13eoyqtUoBjUZhJO901XOIC5WpWv9iDvT6dOnefrpp6lfvz4GgwF/f3+ioqJYv369TfIZOXLkda8ndDWvv/46bdq0uaXnFEIIRztHEo8mUlhWSNNaTXmn6zssfWgpQ0OGSpFyg6RFRVxi8ODBlJaW8vnnn9OoUSMyMjJYuXIlZ85Ik6UQQlxob/Zevtn7DS93ehlHO0fs9fa83Oll9Do9Xet2RafT2TrFO9/tWHiopqqJixKePXtWAWrNmjWV7n/hhRfUgAEDtPf//ve/FaDi4+O1bY0bN1afffaZ9v6zzz5TISEhymAwqObNm6uZM2danfPYsWPqkUceUZ6enqpWrVrqwQcfVIcPH1ZKKTV16lQFWL1Wr1591eOUUmr16tWqY8eOysXFRXl6eqouXbqoI0eOqHnz5l1yznnz5t3cByeEuKtsNm5WTyc+rS0S+N2+72yd0h2n2ixKKC5SWnj5fTo7cHC6xlg9ODhfPdbx+hatcnNzw83NjSVLltC5c2cMBoPV/m7dujF79mxMJhN2dnb89ttv+Pj4sGbNGvr27cvJkyc5ePAg3bt3ByyLQ06ZMoWPP/6Ytm3bsm3bNp544glcXV2JjY2lrKyMqKgoIiIiWLduHfb29rz11lv07duXnTt38uKLL5KWlkZeXh7z5s0DwNvb+6rH6fV6oqOjeeKJJ/jqq68oLS1l48aN6HQ6hg4dSmpqKgkJCaxYsQIAT0/P6/qchBB3H6UU606uY3bKbLZlbgNAr9MT1SCKUJ/QqxwtbpQUKrfbPwMvv69pH4hZXPH+X02g7FzlsQ3ug1G/VLz/MAzOVfJo5vXc60rP3t6e+fPn88QTTzBr1izatWtHt27dGDZsGPfccw9du3YlPz+fbdu20b59e9auXctLL73EkiVLAFizZg1169alSZMmAEydOpX333+fhx9+GIDg4GB2797N//3f/xEbG8uiRYswm83Mnj1bayKdN28eXl5erFmzhj59+uDs7ExJSQn+/v5anl9++eUVj+vQoQO5ubkMHDiQxo0bA9CiRQvteDc3N+zt7a3OKYQQl1NYVsiI+BHsO7sPAAe9A4OaDGJUq1HU96hv4+xqNulMKy4xePBgTp06xU8//UTfvn1Zs2YN7dq1Y/78+Xh5edG6dWvWrFlDSkoKjo6OjB07lm3btlFQUMBvv/1Gt27dACgsLOTgwYPExcVpLTVubm689dZbHDx4EIAdO3Zw4MAB3N3dtf3e3t4UFxdrMZW52nHe3t6MHDmSqKgoHnjgAT766CPS09Nvy+cnhKgZzMqs/ezq4Eptp9q42LswqtUofh38K1MjpkqRchtIi8rt9sqpy+/TXbQy5ksHrhB7UY35XMqN51QJJycnevfuTe/evXnttdcYM2YMU6dOZeTIkXTv3p01a9ZgMBjo1q0b3t7etGjRgt9//53ffvuNF154AYCCggIAPvvsM8LDw63Ob2dnp8W0b9+eBQsWXJJDnTp1LpvftRw3b948nn32WRISEli0aBGvvvoqiYmJdO7c+cY+FCHEXaGgtIBFexexeN9ivuz/JT7OPgC8FvEaHo4eeBrkUfHtJIXK7XY9fUaqKvYGtGzZUnu8061bN+bOnYu9vT19+/YFoHv37nz11Vfs27dP65/i5+dHYGAghw4dIiYmptLztmvXjkWLFuHr64uHh0elMY6OjphMpus+DqBt27a0bduWyZMnExERwcKFC+ncuXOl5xRC3N3OFJ1hQdoCvt7zNfll+QAs3reYp1s/DUCQe5At07tryaMfYeXMmTP07NmTL7/8kp07d3L48GEWL17M9OnTGTRoEAD3338/+fn5LF26VCtKunfvzoIFCwgICKBZs2ba+d544w2mTZvGjBkz2LdvHykpKcybN48PPvgAgJiYGHx8fBg0aBDr1q3j8OHDrFmzhmeffZYTJ04A0LBhQ3bu3MnevXvJysqirKzsqscdPnyYyZMnk5SUxNGjR1m+fDn79+/X+qk0bNiQw4cPs337drKysigpKbmNn7IQojo5WXCSfyb/k6jvovgs5TPyy/Jp5NmIt+59izFhY2ydnrg9g5Bqppo4PLm4uFhNmjRJtWvXTnl6eioXFxfVvHlz9eqrr6pz585pca1bt1b+/v7a+zNnziidTqeGDRt2yTkXLFig2rRpoxwdHVWtWrXU/fffr77//nttf3p6uhoxYoTy8fFRBoNBNWrUSD3xxBPa55qZmal69+6t3NzcrIYnX+k4o9GooqOjVUBAgHJ0dFQNGjRQU6ZMUSaTSbvPwYMHKy8vLxmeLMRdrLC0UIUvCNeGGQ9fOlytOLpCmcwmW6dW413rd6hOKaVsWyrdufLy8vD09CQ3N/eKjx+EEEJUHwfOHqBJrSba+38m/5PDuYcZEzaGTv6dZJK22+Rav0Olj4oQQogaTynFhlMbmJM6h03GTSzov4B76twDwMSOE7HXy9dhdSV/MkIIIWosk9nEimMrmJMyh7TsNADs9fbsPrNbK1SkSKne5E9HCCFEjVNmLuOnAz8xb9c8juYdBcDZ3pnBTQcT2yoWf1eZ7PFOIYWKEEKIGun/dv4f6YXpeDh68FiLx3gs5DFqOdWydVriOt2W4ckzZ86kYcOGODk5ER4ezsaNG68Yv3jxYkJCQnByciIsLIxly5ZZ7VdKMWXKFAICAnB2diYyMpL9+/dbxWRnZxMTE4OHhwdeXl7ExcVpE5Bd7PwMp15eXjd1n0IIIWzjbPFZ5qbOpcxcBlimuB/fdjwvdniRxCGJPNPmGSlS7lBVXqgsWrSICRMmMHXqVLZu3Urr1q2JiooiMzOz0vgNGzYwfPhw4uLi2LZtG9HR0URHR5OamqrFTJ8+nRkzZjBr1iySk5NxdXUlKiqK4uJiLSYmJoZdu3aRmJjI0qVLWbt2LWPHjr3kemVlZQwfPpyuXbve+psXQghRpYyFRt7d+C5R30Xx7y3/JuFwgrbvgcYPENsqFhcHFxtmKG5WlQ9PDg8Pp2PHjnz88ccAmM1mgoKCGD9+PJMmTbokfujQoRQWFrJ06VJtW+fOnWnTpg2zZs1CKUVgYCAvvPACL774IgC5ubn4+fkxf/58hg0bRlpaGi1btmTTpk106NABgISEBPr378+JEycIDKxYGPDll1/m1KlT9OrVi+eee46cnJxrvjcZniyEELZxKPcQ81LnsfTQUsrN5QC08G7Bc+2eo0vdLjbOTlyLa/0OrdIWldLSUrZs2UJkZGTFBfV6IiMjSUpKqvSYpKQkq3iAqKgoLf7w4cMYjUarGE9PT8LDw7WYpKQkvLy8tCIFIDIyEr1eT3JysrZt1apVLF68mJkzZ978zQohhKhyJaYSnl/9PNFLollyYAnl5nI6+ndkVuQsFg1cJEVKDVSlnWmzsrIwmUz4+flZbffz82PPnj2VHmM0GiuNNxqN2v7z264U4+vra7Xf3t4eb29vLebMmTOMHDmSL7/88ppbQ0pKSqymWs/Ly7um44QQQtwaBjsDOSU5KBTdg7ozJmwMreu0tnVaogrdtWv9PPHEEzz22GPcf//913zMtGnT8PT01F5BQbJA1cXmz59/3Z2SdTqdtuChEEKcZ1ZmVh5dyaiEUZwpOqNtf7nTy3z/4Pf8p+d/pEi5C1RpoeLj44OdnR0ZGRlW2zMyMvD3r3wMu7+//xXjz//3ajEXd9YtLy8nOztbi1m1ahXvvfce9vb22NvbExcXR25uLvb29sydO7fS3CZPnkxubq72On78+LV8DHeUkSNHEh0dbbXt22+/xcnJiffff79Krpmenk6/fv2uKVaKGiFqvjJzGT8e+JGHfnyI59Y8x+aMzSzcs1DbH+IdQtNaTW2YobidqrRQcXR0pH379qxcuVLbZjabWblyJREREZUeExERYRUPkJiYqMUHBwfj7+9vFZOXl0dycrIWExERQU5ODlu2bNFiVq1ahdlsJjw8HLD0Y9m+fbv2evPNN3F3d2f79u089NBDleZmMBjw8PCwetV0s2fPJiYmhk8//ZQXXnihSq7h7++PwWCoknMLIe4cReVFLEhbQP/v+/Pq+lc5lHsIdwd3ngh7gsdCHrN1esJWqnp1xK+//loZDAY1f/58tXv3bjV27Fjl5eWljEajUkqpxx9/XE2aNEmLX79+vbK3t1fvvfeeSktLU1OnTlUODg4qJSVFi3nnnXeUl5eX+vHHH9XOnTvVoEGDVHBwsCoqKtJi+vbtq9q2bauSk5PV77//rpo2baqGDx9+2TznzZunPD09r+veauLqybGxsWrQoEFKKaXeffdd5eTkZLXS8fvvv69CQ0OVi4uLqlevnnr66adVfn6+tr+yz3HJkiWqbdu2ymAwqODgYPX666+rsrIybT+gfvjhB6WUUiUlJeqZZ55R/v7+ymAwqPr166t//vOfSimlGjRooADt1aBBgyr5DIQQt19JeYnq9U0vbRXjbl93U3NS5qj8kvyrHyzuSNf6HVrlM9MOHTqU06dPM2XKFIxGI23atCEhIUHrDHvs2DH0+oqGnS5durBw4UJeffVVXnnlFZo2bcqSJUsIDQ3VYiZOnEhhYSFjx44lJyeH++67j4SEBJycnLSYBQsWMG7cOHr16oVer2fw4MHMmDGjqm/3qs6VnbvsPju9HQY7wzXF6nV6nOydrhp7o/MHvPzyy3zyyScsXbqUXr16VVxXr2fGjBkEBwdz6NAh/vrXvzJx4kQ++eSTSs+zbt06RowYwYwZM+jatSsHDx7U5rOZOnXqJfEzZszgp59+4ptvvqF+/focP35ce8S2adMmfH19mTdvHn379sXOzu6G7k0IUT3kluTiafAEwNHOka71uvLHqT8YFTqKQU0GWf17KO5eVT6PSk12I/OohH0edtl9Xet25ZPIii/8Tgs6UVReVGlsB78OzOs7T3t//9f3c7bk7CVxKbEp15TXeSNHjuSrr76itLSUlStX0rNnzyvGf/vttzz11FNkZWUBls60F85HExkZSa9evZg8ebJ2zJdffsnEiRM5deoUYOl38sMPPxAdHc2zzz7Lrl27WLFiRaVLrV8YK4S4Mx3LO8bc1Ln8fPBnvuj/BS1rtwQgvzQfZ3tnWSTwLnGt36Hyf4O4xD333ENWVhZTp06lU6dOuLm5aftWrFjBtGnT2LNnD3l5eZSXl1NcXMy5c+dwcbm09WbHjh2sX7+et99+W9tmMpkue8zIkSPp3bs3zZs3p2/fvgwcOJA+ffpU3c0KIW6bPdl7mJMyh+VHl2NWZgBWHlupFSruju62TE9UU1Ko3GbJjyVfdp+d3vpRxppH11w2Vq+z7gedMDjhMpHXr27dunz77bf06NGDvn37Eh8fj7u7O0eOHGHgwIE8/fTTvP3223h7e/P7778TFxdHaWlppYVKQUEBb7zxBg8//PAl+y58VHdeu3btOHz4MPHx8axYsYJHH32UyMhIvv3221t2f0KI20cpxZaMLcxJncPvJ3/Xtnet25UxYWNo59fOhtmJO4EUKrfZ9fQZqarYa9GgQQN+++03rVhJSEhgy5YtmM1m3n//fa1f0TfffHPF87Rr1469e/fSpEmTa762h4cHQ4cOZejQoQwZMoS+ffuSnZ2Nt7c3Dg4OmEymm7o3IcTtU24u5+W1L5NZlIlepyeqYRRxoXE0925u69TEHUIKFXFZQUFBrFmzhh49ehAVFcWnn35KWVkZ//nPf3jggQdYv349s2bNuuI5pkyZwsCBA6lfvz5DhgxBr9ezY8cOUlNTeeutty6J/+CDDwgICKBt27bo9XoWL16Mv7+/Nolcw4YNWblyJffeey8Gg4FatWQ1VCGqk3JzOauPr6ZHUA/s9fY42Dkw5p4x7D+7n1GtRhHkIRNliutz185MK65NvXr1WLNmDVlZWTz11FO8/vrrvPvuu4SGhrJgwQKmTZt2xeOjoqJYunQpy5cvp2PHjnTu3Jl///vfNGjQoNJ4d3d3pk+fTocOHejYsSNHjhxh2bJlWgvO+++/T2JiIkFBQbRt2/aW368Q4saUmEpYtGcRA38YyIQ1E0g8mqjtGx4ynCkRU6RIETdERv3cBFk9WQhxtysoLWDR3kV8sfsLzhRbprmvZajFhA4TiG4SbdvkRLUmo36EEEJUmTJzGZ9s/4RFexaRX5YPQIBrACNbjeShpg/hbO9s4wxFTSGFihBCiOtmr7Nns3Ez+WX5NPJsRFxYHP2C++Ggd7B1aqKGkUJFCCHEVe0/u58vdn/BCx1ewNPgiU6n4/n2z3O25Cw9gnpcMmWCELeKFCpCCCEua3vmduakzGHNiTUABLgF8HTrpwFkDhRxW0ihIoQQwopSivWn1jMnZQ6bMzYDoENHZINIutfrbtvkxF1HChUhhBCacnM5sfGx7MzaCYC93p4HGj3AqNBRBHsG2zg7cTeSQkUIIe5y5eZybSFAe709jbwasT9nP4ObDia2VSz+rv42zlDczWQelZsg86gIIe5khWWFLN67mC92f8GnvT+lWa1mAJw+dxoHvQNeTl62TVDUaDKPihBCiEqdLT7LgrQFfLXnK/JK8wD4Zu83vNr5VQDquNSxZXpCWJFCRQgh7hLGQiOf7/qc7/Z/R1F5EQANPRoyKnQUAxsNtHF2QlROChUhhLgLmMwmHo9/HGOhEYAW3i0YEzaGXvV7Yae3s3F2osxkZsPBM8SnpFPbzZGXokJsnVK1IYWKEELUUGln0mhWqxl2ejvs9HYMbT6UpFNJxIXFEREQgU6ns3WK4k/vxu9h9u+HAfBxMzChd3Ps9PLnA7J6shBC1ChKKf5I/4Mxy8fw6NJHWX18tbZvdOho5kTNoUtgFylSbKS4zMTyXUaeX7SdzUeyte29W/rh42bgL53rM2NYG+RPp4K0qAghRA1gVmZWH1vN7JTZpJ5JBSzr8RzKPaTFyDT3tnGutJzf9p5mWaqRVWkZFJaaAPBwsqdDQ28AOjb0JvmVXtKKUgkpVIQQ4g5mVmZ+OvgTc1PncjjX8ujAyc6Jh5s+TGyrWALdAm2c4d0rr7iMSd/tZPWe0xSVmbTtgZ5O9AsL4MHWFX82eilQLksKFSGEuIPp0LFozyIO5x7G3dGd4SHDiWkRg7eTt61Tu+vkFZdxILOAdvVrAeBusGf7sRyKykwEeTvTPzSAfmEBtK7nKY/eroMUKkIIcQfJLcnlm73fMCxkGO6O7uh0Op5p+wz7z+7nkWaP4OboZusU7yo550pJ3J1BfKqR3/dn4WKwY9PfI3Gw06PT6XjroVB83Z1oFeghxckNkkJFCCHuAJnnMvli9xd8s/cbzpWfQ6fTMSZsDAD31b2P++reZ+MM7x7ZhaUs32VkWaqRDQeyKDdXTPAe5OpMek4x9Wu7ANAzxM9WadYYt6Vn1cyZM2nYsCFOTk6Eh4ezcePGK8YvXryYkJAQnJycCAsLY9myZVb7lVJMmTKFgIAAnJ2diYyMZP/+/VYx2dnZxMTE4OHhgZeXF3FxcRQUFGj79+7dS48ePfDz88PJyYlGjRrx6quvUlZWdutuXAghbtKxvGO8vuF1+n7Xl/m75nOu/BzNajWjkWcjW6d215r7+2EmfZ/C2n2nKTcrQvzdeT6yGYnP38/KF7prRYq4Naq8RWXRokVMmDCBWbNmER4ezocffkhUVBR79+7F19f3kvgNGzYwfPhwpk2bxsCBA1m4cCHR0dFs3bqV0NBQAKZPn86MGTP4/PPPCQ4O5rXXXiMqKordu3fj5OQEQExMDOnp6SQmJlJWVsaoUaMYO3YsCxcuBMDBwYERI0bQrl07vLy82LFjB0888QRms5l//vOfVf2xCCHEFZmVmcnrJpNwJAGzMgPQzrcdcWFxdK3bVR4j3AYZecX8usvIspR0nujaiF4tLK0j/cL8Wb03k/5hAfQN9adxHXncVpWqfFHC8PBwOnbsyMcffwyA2WwmKCiI8ePHM2nSpEvihw4dSmFhIUuXLtW2de7cmTZt2jBr1iyUUgQGBvLCCy/w4osvApCbm4ufnx/z589n2LBhpKWl0bJlSzZt2kSHDh0ASEhIoH///pw4cYLAwMp7wU+YMIFNmzaxbt26a7o3WZRQCFGVJq2bxC+HfuH+evcTFxpHO792tk6pxjuVU0RCqpH41HQ2Hz3L+W/Ih9vW5YOhbWyaW01zrd+hVfrop7S0lC1bthAZGVlxQb2eyMhIkpKSKj0mKSnJKh4gKipKiz98+DBGo9EqxtPTk/DwcC0mKSkJLy8vrUgBiIyMRK/Xk5ycXOl1Dxw4QEJCAt26dbvs/ZSUlJCXl2f1EkKIm2VWZn47/hux8bEcyqmY9+SZ1s/w7QPfMrPXTClSqlhBSTkPfbKeLu+s4s2lu9l0xFKktKvvxasDWjChTzNbp3jXqtJHP1lZWZhMJvz8rDsT+fn5sWfPnkqPMRqNlcYbjUZt//ltV4q5+LGSvb093t7eWsx5Xbp0YevWrZSUlDB27FjefPPNy97PtGnTeOONNy67Xwghrke5uZyEIwnMSZnDgZwDAMzfNZ8377X8OxTkEWTL9Gq0I1mF7DHm0zfUHwA3gz3nSkzodNCxgTf9wvzpG+pPgKezjTMVd/2on0WLFpGfn8+OHTt46aWXeO+995g4cWKlsZMnT2bChAna+7y8PIKC5B8SIcT1KS4vZsmBJczfNZ+TBScBcHVw5dHmj/J4i8dtnF3NdfB0AfEp6SxLMbI7PQ9nBzvubxaJi6Plq3D6kHsI8HTC18PJxpmKC1VpoeLj44OdnR0ZGRlW2zMyMvD396/0GH9//yvGn/9vRkYGAQEBVjFt2rTRYjIzM63OUV5eTnZ29iXXPV9otGzZEpPJxNixY3nhhRews7t0NVGDwYDBYLjabQshxGUppRj+y3CtBcXbyZu/tPgLQ0OG4uEofd1utUOnC/hpxyniU4zszcjXttvpdXRoWIszBaW4eFu+ClsHedkoS3ElVdpHxdHRkfbt27Ny5Uptm9lsZuXKlURERFR6TEREhFU8QGJiohYfHByMv7+/VUxeXh7JyclaTEREBDk5OWzZskWLWbVqFWazmfDw8MvmazabKSsrw2w2X//NCiHEZZwpOqON3NHpdEQ1jCLANYDJnSaTMDiBJ+55QoqUW0QphemCeU2W787gwxX72ZuRj4Odju7N6zB98D1s/nskX8SFE+QtQ4mrPVXFvv76a2UwGNT8+fPV7t271dixY5WXl5cyGo1KKaUef/xxNWnSJC1+/fr1yt7eXr333nsqLS1NTZ06VTk4OKiUlBQt5p133lFeXl7qxx9/VDt37lSDBg1SwcHBqqioSIvp27evatu2rUpOTla///67atq0qRo+fLi2/8svv1SLFi1Su3fvVgcPHlSLFi1SgYGBKiYm5prvLTc3VwEqNzf3Zj4iIUQNdTzvuPpH0j9Uu/+1U2uOrdG2nys7p0pNpTbMrGYxm81q5/Ec9U58muo2fZX6futxbd+RrAI1et5G9e3m4yqnUD7z6uRav0OrvI/K0KFDOX36NFOmTMFoNNKmTRsSEhK0zrDHjh1Dr69o2OnSpQsLFy7k1Vdf5ZVXXqFp06YsWbJEm0MFYOLEiRQWFjJ27FhycnK47777SEhI0OZQAViwYAHjxo2jV69e6PV6Bg8ezIwZM7T99vb2vPvuu+zbtw+lFA0aNGDcuHE8//zzVf2RCCFquH1n9zE3dS4JhxMwKctidOtOrqNbkGVUobO9dNC8WUopth/PIT7VMs/JibNF2r5fUzN4qG09ABrUdmXOyI62SlPcAlU+j0pNJvOoCCEutD1zO7NTZvPbid+0bV0CuzAmbAwd/DrIJG23yLnScnp/sJaTORXFibODHT1DfOkX5k+P5r64Gu76sSLV3rV+h8qfpBBC3AJKKd5Ofps92XvQoSOyQSRxYXG0qt3K1qnd0UxmxaYj2ew15hPbpSEALo72+Hs6kXOulF4t/Ogf5k+3Zr44O146CELc+aRQEUKIG2Aym0g8lkjXul1xdXBFp9Mx9p6x/H7yd0a1GkVDz4a2TvGOVW4y88ehbJalprN8l5GsglLs9DoebB1ILVdHAD4c2oY67gacHKQ4qemkUBFCiOtQairlx4M/Mi91Hsfzj/NihxeJbRULQO8GvendoLeNM7xzbT+ew1fJx1i+28jZcxULxHo6O9CnpR9FZSZq/blNRuvcPaRQEUKIa1BYVsjivYv53+7/cbroNACeBk/s9fLP6I0qKTehFFqrSOrJXBZtPg6At6sjUa386BcaQETj2jjYVelsGqIak79hQghxBUopZu2YxZdpX5JXalnfy8/Fj9hWsQxuOhgXB/nN/noUl5lYu+808alGVuzOYGLf5jwe0RCAqFb+pKXnMSAsgE7B3thLcSKQQkUIIa5Ip9NxMPcgeaV5NPRoyOjQ0QxsNBAHOwdbp3bHKCo1sWZvJstSjaxKy6Cw1KTt23DwjFao1HE38PZDYTbKUlRXUqgIIcQFDuUcYm7qXMbeM5b6HvUBeOqep4hqGEXPoJ7Y6aXz5vUoLjPR6Z8ryC8u17YFejrRLyyA/mH+tA2qdYWjhZBCRQghAEjNSmV2ymxWHVuFQuFg58DUiKkANKnVhCa1mtg4w+ovr7iMVWmZpBnzmNyvBWDpf9Kufi0Oni6gf1gA/cMCaF3PU+aUEddMChUhxF1LKcUf6X8wJ3UOyenJ2vZe9XvxcJOHbZjZnSP3XBmJaRnEp6Szbn8WpSbLmkaxEQ0J9LLMwDtjeFs8nOylOBE3RAoVIcRdSSnFUyueYsOpDQDY6+zp36g/caFxNPJqZOPsqr/1B7L479pDrD+QRfkFiwA2ruNK/7AA7O0qihJPZ+nPI26cFCpCiLtGmakMe73lN3udTkeoTyhbM7bycNOHiW0VS6BboK1TrLZO55dgr9dpE65lFZTw2z7LMO0Qf3f6hVr6nDT1c7dlmqIGkrV+boKs9SPEneFc2Tm+3/89n+/+nNcjXufeuvcCkFeaR7m5HG8nbxtnWD1l5BXz6y7Lon8bD2fzYlRz/trd0lcnv7iMzzccoV9YAI3ruNk4U3EnkrV+hBB3vdySXBbuWcjCtIXklOQA8N3+77RCxcNRfsG42KmcIhJSjcSnprP56Fku/FX2YGah9rO7kwPjeja1QYbibiOFihCixskozOCL3V+weN9izpWfAyDIPYhRoaN4sPGDNs6u+iotN9P7g9+s5jlpV9+L/mEBRLXyl2nrhU1IoSKEqHHGrxpPWnYaAM1rNScuLI7eDXrLdPcXOJJVSHyqkV2ncvn4sXYAONrr6dnCj4zcYvqF+dM31J8AT2cbZyrudvK3Vghxx0s7k0YDjwbadPaPt3ycb/d9y5iwMdxX9z4ZFvung6cLiE9JZ1mKkd3pedr2v/XK1zrBfji0DXZ6+bxE9SGFihDijqSUYnPGZuakzGH9qfVM6jSJmBYxAAxsNJAHGj9g4wyrj+W7jLy/fB97M/K1bXZ6HV0a16ZfaAC+Hk5W24WoTqRQEULcUczKzG/Hf2NO6hx2nN4BgF6nx1ho1GLu5hYUpRRp6fl4ujhQ988J1+z0OvZm5ONgp+PeJj70Dw2gd0s/baixENWZFCpCiDvGL4d+YXbKbA7kHADAUe/IQ00fIrZVLEHuQTbOznaUUqSezGNZajrxKekcOXOOv3ZvzMS+IQDc19SH9x9pTWQLPzxdZPI1cWeRQkUIccdYfmQ5B3IO4OrgytDmQ3m85eP4OPvYOi2bUEqx/XgO8amWeU5OnC3S9hns9RSVmS54b8fg9vVskaYQN00KFSFEtZRXmseiPYvo36g/dd3qAjD2nrGE+oQyNGToXT8HismsGD1/E2fPlQHg7GBHzxBf+oX506O5L64G+edd1Azyf7IQolrJKsrii91f8M3ebygoKyDzXCZ/7/x3AFr5tKKVTysbZ3h7mcyKTUeyiU9JZ8eJXL5/ugt6vQ57Oz0Pt6tHZn4JA8L86dbMF2dHO1unK8QtJ4WKEKJaOJ5/nPmp81lyYAml5lIAGns2pp1fOxtndvuVm8wkH85mWUo6v+4yklVQqu3bcSKHtvVrAfDawJa2SlGI20Z/Oy4yc+ZMGjZsiJOTE+Hh4WzcuPGK8YsXLyYkJAQnJyfCwsJYtmyZ1X6lFFOmTCEgIABnZ2ciIyPZv3+/VUx2djYxMTF4eHjg5eVFXFwcBQUF2v41a9YwaNAgAgICcHV1pU2bNixYsODW3bQQ4pq99cdbDPxhIN/s+4ZScyn31LmHGT1m8P2g7+kX3M/W6d1WP+04Rce3VxAzO5kFycfIKijF09mBIe3rMXdkB1oG3t2PvMTdp8oLlUWLFjFhwgSmTp3K1q1bad26NVFRUWRmZlYav2HDBoYPH05cXBzbtm0jOjqa6OhoUlNTtZjp06czY8YMZs2aRXJyMq6urkRFRVFcXKzFxMTEsGvXLhITE1m6dClr165l7NixVte55557+O6779i5cyejRo1ixIgRLF26tOo+DCFEpVwdXDErM/cG3svcqLl82e9LetTvgV53W36XspmSchOr9mSw74L5TfzcDZw9V4a3qyPDOwXxv9Gd2PxqJO890pqeIX4Y7OXxjrjLqCrWqVMn9cwzz2jvTSaTCgwMVNOmTas0/tFHH1UDBgyw2hYeHq6efPJJpZRSZrNZ+fv7q3/961/a/pycHGUwGNRXX32llFJq9+7dClCbNm3SYuLj45VOp1MnT568bK79+/dXo0aNuuZ7y83NVYDKzc295mOEuJuZzWa19vhaNWLZCPXHqT+07VnnstSurF02zOz2KSotV7+mpqvnvt6mQqckqAYvL1WvLUnR9pebzGr9/tOqrNxkwyyFqHrX+h1apb+ulJaWsmXLFiIjI7Vter2eyMhIkpKSKj0mKSnJKh4gKipKiz98+DBGo9EqxtPTk/DwcC0mKSkJLy8vOnTooMVERkai1+tJTk6+bL65ubl4e8ty70LcaiazifjD8Tzy8yP8deVf2Zq5lXm75mn7azvXpmXtmtvfwmRWxKekM/6rbbT/RyJjv9jCD9tOkl9Sjp+HAe8LJl6z0+vo0sQHe7ua3ZokxLWq0s60WVlZmEwm/Pz8rLb7+fmxZ8+eSo8xGo2VxhuNRm3/+W1XivH19bXab29vj7e3txZzsW+++YZNmzbxf//3f5e9n5KSEkpKSrT3eXl5l40VQkCJqYSfDv7EvNR5HM8/DoCzvTOPNnuUx1s+buPsqla5yawVG3odvPVLGidzLHOdBHo60S8sgP5h/rQNqoVepq0X4rJk1A+wevVqRo0axWeffUarVpcf+jht2jTeeOON25iZEHe2p1c8zSbjJgC8DF7EtIhheMhwPA2eNs6sauQVl7EqLZNlKensPJHLupd74GCnR6fTMSKiAdmFpfQLC6B1Pc+7epp/Ia5HlRYqPj4+2NnZkZGRYbU9IyMDf3//So/x9/e/Yvz5/2ZkZBAQEGAV06ZNGy3m4s665eXlZGdnX3Ld3377jQceeIB///vfjBgx4or3M3nyZCZMmKC9z8vLIyjo7p22W4iLZRdn4+rgisHOAMADjR7gWN4xRrYaycNNH9ZWN65Jcs+VkZiWQXxKOuv2Z1FqMmv7Nh85S0Tj2gA82a2xrVIU4o5WpQ9BHR0dad++PStXrtS2mc1mVq5cSURERKXHREREWMUDJCYmavHBwcH4+/tbxeTl5ZGcnKzFREREkJOTw5YtW7SYVatWYTabCQ8P17atWbOGAQMG8O6771qNCLocg8GAh4eH1UsIAekF6UxLnkbUt1H8eOBHbfvAxgOJfziev7T8S40sUhZvPk77txJ5cfEOVu7JpNRkpnEdV57t2YT4v3WlcyPp8ybEzaryRz8TJkwgNjaWDh060KlTJz788EMKCwsZNWoUACNGjKBu3bpMmzYNgL/97W9069aN999/nwEDBvD111+zefNm/vvf/wKWVVGfe+453nrrLZo2bUpwcDCvvfYagYGBREdHA9CiRQv69u3LE088waxZsygrK2PcuHEMGzaMwMBAwPK4Z+DAgfztb39j8ODBWt8VR0dH6VArxDU6mHOQualzWXZoGeWqHIDk9GQebf4oAA76mrMA3un8EpbvNhLi7077BpZ/I1oGelBuVjT3c6f/n31Omvq52zhTIWqY2zEE6T//+Y+qX7++cnR0VJ06dVJ//FExLLFbt24qNjbWKv6bb75RzZo1U46OjqpVq1bql19+sdpvNpvVa6+9pvz8/JTBYFC9evVSe/futYo5c+aMGj58uHJzc1MeHh5q1KhRKj8/X9sfGxurgEte3bp1u+b7kuHJ4m61M3Onenblsyp0fqj2ivs1Tm04uUGZzWZbp3fLGHOL1OcbDquh/7dBBU9aqhq8vFQ99/U2bb/ZbFaHThfYLkEh7mDX+h2qU0opG9ZJd7S8vDw8PT3Jzc2Vx0DirvLMymdYe2ItAL3q9yIuNI6wOmE2zurWMJsV8zccIT41nc1Hz3Lhv5D31PNkcLt6xHZpaLP8hKgprvU7VEb9CCGuyGQ2ser4KsJ8wvB3tXRGfyLsCbwMXsSFxtHIq5GNM7x5ZwtLqfXnXCZ6vY5Fm46z98/ZYtvW96J/aAB9Q/0J8q55/WyEqO6kUBFCVKrMVMbSQ0uZmzqXI3lH+EuLv/Byp5cBaOPbhja+bWyb4E06klVIfKqR+NR09mcUsOW1SFwcLf8kjr2/EblFZfQN9SfQy9nGmQpxd5NCRQhh5VzZOb7d9y2f7/6czHOWYf7uju7Udq5t48xu3sHTBcSnpLMsxcju9IoJG/U62H48hy6NfQAY3L6erVIUQlxEChUhhGZe6jzmpM4htyQXgDrOdRjRcgSPNH8EVwdXG2d3c77aeIzJ36do7+30Oro0rk2/0AD6tPLDx81gw+yEEJcjhYoQQnO66DS5JbkEuQcxKnQUgxoPwtHO8eoHViNKKfYY84lPSadNfS96hliW27i3sQ8OdjrubeJD/9AAerf00/qlCCGqLylUhLhLHck9wrxd8xjUeBDt/NoBENsylnt87qF3g97Y6e1snOG1U0qx61Qey1LSiU81cjirEIDeLf20QqV+bRe2vtYbd6eaM7eLEHcDKVSEuMvsPrOb2SmzWXF0BQrF6XOn+cTvEwD8XP3oG9zXxhleO6UU7yTsYVlKOsezi7TtBns93ZvX4cHWda3ipUgR4s4jhYoQdwGlFJszNjM7ZTYbTm3Qtner140xYWNsmNn1MZsV+zMLaO5vmf1Vp9Ox+chZjmcX4exgR4+QOvQPC6BHc19cDfLPmxA1gfxNFuIu8PK6l4k/HA+Anc6OvsF9GR06mma1mtk4s6szmRWbjmQTn5JOwi4jWQWlbP57pNa/ZFyPJhSXmejWvI42vFgIUXPI32ohaqAycxlQsdZOB78OrDy6koeaPkRsq1iC3Kv3qt/lJjPJh7NZlpLOr38WJ+e5G+zZm5FP50aW4dI9QnxtlaYQ4jaQQkWIGqSovIgf9v/A57s+58nWT/Jw04cBGNRkED3r98TH2cfGGV6bxVtOWA0l9nCyp08rf/qH+XNvEx8M9ndOR18hxM2RQkWIGiCvNI9FexbxZdqXZBdnA/DjgR+1QsVgZ8DgXP3mCSkpN7H+QBbLUox0blSbIX9OtBbZwo/33fYR2cKXfmEBRDSqjaO93sbZCiFsQQoVIe5gWUVZ/G/3//hm7zcUllmG5NZ1q8vIViOJbhJt2+Quo7jMxNp9p4lPNbJidwb5JeUAnDh7TitU6rgb2PhKL/R6nS1TFUJUA1KoCHEHm7phqraKcROvJowOHU3f4L5a35TqRCnFC4t38GuqkcJSk7bd191Av1B/+ocFWMVLkSKEAClUhLij7M3ei4+zj7buTmzLWHJLchkTNob7692PXld9Ho8UlpSz+ehZujWrA1iGEmcXllJYaiLA04l+oQH0D/OnXf1aUpQIIS5Lp5RStk7iTpWXl4enpye5ubl4eHjYOh1Rg23N2Mqc1DmsPbGWUaGjmNB+AmBppQBLEVAd5BWXsSotk2Up6fy27zQl5WY2TOqprUC8/XgOSila1/OS4kSIu9y1fodKi4oQ1ZRSinUn1zEnZQ5bM7cCoNfpySupWPW3OhQouUVlJO7OID4lnXX7syg1mbV9DWu7cDKnSCtU2gR52ShLIcSdSgoVIaqhFUdXMGvHLPae3QtY5kN5sPGDjA4dTX2P+jbOztqavZm8uHiH9r5xHVcGhAXQLyyAEH/3alFMCSHuXFKoCFENbTRuZO/ZvbjYu/Bo80d5vOXj+LrYdmKzrIISlu/KID41nfua+PBkt8YA9AzxJayuJ5Et/Ogf5k9TP3eb5imEqFmkUBHCxgpKC1i8bzGd/DvRyqcVACNbjcTbyZvhIcPxNHjaLLfMvGISdhlZlpLOxsPZmP/s0ZZdWKoVKu5ODvw8/j6b5SiEqNmkUBHCRs4UnWFB2gK+3vs1+aX59AzqyUc9PwIg0C2Qp1o/ZbPclFKMmr+J3/ad5sLu9q3redIvLIB+of42y00IcXeRQkWI2+xUwSnm75rPD/t/oNhUDECwZzA96/e0WU7Hs8/x+4Eshney9H/R6XS4GexRCtrW96J/aAB9Q/0J8naxWY5CiLuTFCpC3EYfbvmQ+bvmY1KWCc9Ca4cyJmwMPer3uO1zoBzJKiQ+1Uh8ajo7T+QC0KFBLa2PyYTezXilfwttxI4QQtiCFCpCVDGllDbypY5LHUzKRHhAOGPCxhDuH35bR8WczCnih60nWJZiZHd6xTBnvQ46BXtz7oIZYxvVcbtteQkhxOVU+a9wM2fOpGHDhjg5OREeHs7GjRuvGL948WJCQkJwcnIiLCyMZcuWWe1XSjFlyhQCAgJwdnYmMjKS/fv3W8VkZ2cTExODh4cHXl5exMXFUVBQoO0vLi5m5MiRhIWFYW9vT3R09C27XyHA8v/phpMbGP3raJYeWqptf7jpwyzsv5DZfWbTOaBzlRcpSilKyyvmNdlrzOO95fvYnZ6HnV7HfU18ePuhUJJfieTrsRG0lnlOhBDVTJUWKosWLWLChAlMnTqVrVu30rp1a6KiosjMzKw0fsOGDQwfPpy4uDi2bdtGdHQ00dHRpKamajHTp09nxowZzJo1i+TkZFxdXYmKiqK4uFiLiYmJYdeuXSQmJrJ06VLWrl3L2LFjtf0mkwlnZ2eeffZZIiMjq+4DEHcdk9nEr0d+ZejSoTy54kk2GTfxv93/02aQdbZ3JqxOWJXmoJRi96k83l++l8gPfuPfK/Zp++5t4kOfln68OziMTX+P5Msx4cSEN6COe/VbWVkIIaCKp9APDw+nY8eOfPzxxwCYzWaCgoIYP348kyZNuiR+6NChFBYWsnRpxW+gnTt3pk2bNsyaNQulFIGBgbzwwgu8+OKLAOTm5uLn58f8+fMZNmwYaWlptGzZkk2bNtGhQwcAEhIS6N+/PydOnCAwMNDqmiNHjiQnJ4clS5Zc9/3JFPrivDJTGT8f+pl5qfM4kncEsBQlg5sOJrZVLP6uVTtKRilF6sk8lqWmE5+SzpEz57R9zfzcWP58tyq9vhBCXC+bT6FfWlrKli1bmDx5srZNr9cTGRlJUlJSpcckJSUxYcIEq21RUVFaEXH48GGMRqNVK4inpyfh4eEkJSUxbNgwkpKS8PLy0ooUgMjISPR6PcnJyTz00EM3fE8lJSWUlJRo7/Py8q4QLe4mk3+fzK9HfgXA3dGdx0IeI6ZFDLWcat2W6z/0yQa2H8/R3hvs9XRvXof+YQH0DLHtRHFCCHEzqqxQycrKwmQy4efnZ7Xdz8+PPXv2VHqM0WisNN5oNGr7z2+7Uoyvr/U/zPb29nh7e2sxN2ratGm88cYbN3UOUTPkFOeg1+vxcLT8FvBwk4fZmrGV2FaxDGk2BFcH1yq5rtms2HrsLL/tO83zkc20hf1aBLiz15hPzxBf+oX506O5L64G6SsvhLjzyb9k12Hy5MlWLT55eXkEBQXZMCNxuxkLjfxv9//4dt+3PN7ycca3HQ9ARGAECYMTcLRzvOXXNJkVm45kE5+STsIuIxl5lla9HiG+tKtvabGZ0Ls5Uwa2wtnR7pZfXwghbKnKChUfHx/s7OzIyMiw2p6RkYG/f+XP6/39/a8Yf/6/GRkZBAQEWMW0adNGi7m4s255eTnZ2dmXve61MhgMGAzS6fBudDj3MPNS5/HzoZ8pN5cDsCNzhzb0WKfT3fIi5UBmPvPWH+HXXUayCkq17e4GeyJb+uFyQVEinWGFEDVVlY36cXR0pH379qxcuVLbZjabWblyJREREZUeExERYRUPkJiYqMUHBwfj7+9vFZOXl0dycrIWExERQU5ODlu2bNFiVq1ahdlsJjw8/Jbdn7g77DqziwlrJjBoySB+OPAD5eZyOvh14NPIT/msz2e3dHhxmclM7rky7X12YRkLko+RVVCKh5M9Q9rXY+7IDmx+LZJ/D21DiL904BZC1HxV+uhnwoQJxMbG0qFDBzp16sSHH35IYWEho0aNAmDEiBHUrVuXadOmAfC3v/2Nbt268f777zNgwAC+/vprNm/ezH//+1/AMq33c889x1tvvUXTpk0JDg7mtddeIzAwUJsLpUWLFvTt25cnnniCWbNmUVZWxrhx4xg2bJjViJ/du3dTWlpKdnY2+fn5bN++HUBrmREC4Nt935J4NBGA7kHdiQuNo41vm1t2/pJyE+sPZLEsxUji7gwebB3IP6JDAWjfoBaj7m1I9+a+RDSqjaP97Z25VgghqoMqLVSGDh3K6dOnmTJlCkajkTZt2pCQkKB1hj127Bh6fcU/vl26dGHhwoW8+uqrvPLKKzRt2pQlS5YQGhqqxUycOJHCwkLGjh1LTk4O9913HwkJCTg5OWkxCxYsYNy4cfTq1Qu9Xs/gwYOZMWOGVW79+/fn6NGj2vu2bdsCUIWjtUU1Z1ZmVh9fTX33+jSt1RSA0a1GU1JewqjQUdq2m1VcZmLtvtPEpxpZsTuD/JJybd/mo2e1n+30OqY+0OqWXFMIIe5UVTqPSk0n86jUDGXmMuIPxzM3ZS4Hcw/Sp0Ef3u/+fpVdr++Ha9ljzNfe+7ob6BfqT7+wADo29MZOf/um1BdCCFux+TwqQlR3ReVFfL//ez7f9TnphekAuDm40dCzodX6PDeqsKScVXsyWb0nk3eH3IODnaX1sGtTH3KLyugXGkD/MH/a1a+lDTMWQghhTQoVcVdatGcRn+z4hOzibABqO9Xm8ZaP82jzR3F3dL/h8+YVl7EqLZNlKen8tu80JX+usxPdti73N6sDWIYST+7XQooTIYS4BlKoiLtSsamY7OJs6rrVZVSrUQxqMggne6erH3gZqSdz+XfiPtbtz6LUVLEIYMPaLvQLC6BBbRdtm8x1IoQQ104KFVHjHc87zrxd8+gc0Jk+DfsA8EizR/Bx9iGqYRT2+uv/a3C2sJTC0nLq1bIUIHqdjpV7LPP3NK7jSv+wAPqFBtAiwL3KV0gWQoiaTAoVUWPtzd7LnNQ5/HrkV8zKzM7TO+ndoDc6nQ4XBxcGNBpwXefLKihh+a4M4lPT2XDwDA+2DuTfQ9sAlinsXx3Qgvub1aGZ340/OhJCCGFNChVR42zN2MrslNmsO7lO23Zf3fsYEzbmuls3MvOK+XWXkWUpRpIPn8F8wRi59Nwiq5lpx3RtdKtuQQghxJ+kUBE1yrTkaSzcsxAAvU5PnwZ9iAuLI8Q75IbOFztvE2npFatk31PPUxut06B21Sw8KIQQooIUKuKOVm4up9xcrnWE7RLYhcX7FjOoySBGtRpFfY/613Se49nnSEg1snJPBnNHdsTF0fJXo1+oP04OevqHBtA31J8gb5ernEkIIcStJBO+3QSZ8M12Skwl/HjgR+alzuPBJg/ydOunAcvMwmeKz+Dj7HPVcxw9U8iyFCPxqensPJGrbf8kph39wwK080lnWCGEuPVkwjdRIxWUFrBo7yK+2P0FZ4rPABB/OJ6n7nlK6ytytSJl67GzvPpDKrsveKSj10GnYG/6/zk77HlSpAghhG1JoSLuCGeKzrAgbQFf7/ma/DLL9PP+rv6MbDWSh5s+fMWCYn9GPmUmRctAS8Xu42pgd3oednodEY1q0y/Mnz4t/anjbrgt9yKEEOLaSaEi7gj/2fYfvtv/HQCNPBsxOnQ0/Rv1x0HvcEmsUoq09HziU9NZlpLOwdOFRLbwY3ZsBwDq13bhk5h2dG5UG29Xx9t6H0IIIa6PFCqiWjpw9gAGOwNBHkEAxLaKZf/Z/YwOHU2P+j3Q6/SXHJN6MpdfUtKJT0nnyJlz2nZHOz2O9jqr/ibn+6AIIYSo3qRQEdXKjtM7mJ0ymzXH1zCg0QDe6foOAMGewSwYsMAq9uKOrm/8vItNR84C4Givp3uzOvQPC6BnC188nC5teRFCCFH9SaEibE4pxYZTG5idMpvNGZsB0KHDZDZhVmar1hOzWbH12FmWpRhJTDPy0zP3UevPxzfRbetSx91Av9AAeoT44maQ/72FEOJOJ/+SC5tae2ItH2/7mLTsNADsdfYMbDyQUaGjaORpmenVZFZsPpJNfKplKHFGXol2fOLuDB7taHk8FBPegJjwBrf/JoQQQlQZKVSETe0/u5+07DSc7Z0Z3HQwsa1i8Xf11/ZvPJzNXxdsJaugojhxN9gT2dKPfqH+3N+sji3SFkIIcZtIoSJum3Nl51i8bzGNvRpzX937AHi0+aOUmksZ1nwYbg6eJB08wyG7LLo0tsyFEuzjSnZhCZ7ODvRu6Uf/MH/ubeKDwd7OlrcihBDiNpFCRVS5nOIcFuxZwMK0heSV5hFaO5R7A+9Fp9PhqHchxDCYf/58jOW7M8gtKqNzI2+tUKnjbuC7p7sQWtcTB7tLR/oIIYSo2aRQEVXGWGjk812f893+7ygqLwKggUcDhjQbwqo9Gfyy00hiWgb5xeXaMT5uBpr7uVuN6Glbv5ZN8hdCCGF7UqiIKjEnZQ4fb/+YcrOlCGleK4Qn7hlDZP1I7PR2jJy3kTV7TwPg626gX6g//f6cvt5OL9PWCyGEsJBCRdwyFw4lbuDRgHJzOcFu9+B6rg87NvsSev/92OktfUuGdgiikY8b/cP8aVe/FnopToQQQlRCChVxU5RSbDRuZHbKbMIDwnm0aSyr0jL5ZacHZcfGs7Ow7p+RZlbvyeQvnS3Dh/uFBdBPZocVQghxFbeld+LMmTNp2LAhTk5OhIeHs3HjxivGL168mJCQEJycnAgLC2PZsmVW+5VSTJkyhYCAAJydnYmMjGT//v1WMdnZ2cTExODh4YGXlxdxcXEUFBRYxezcuZOuXbvi5OREUFAQ06dPvzU3fBcwKzMrj64kZlkMY5aP4Y/0P/g8dQEd/vErzy3aTmLaaYoL69KgtgtPd2/Mz+PuIya8vq3TFkIIcYep8kJl0aJFTJgwgalTp7J161Zat25NVFQUmZmZlcZv2LCB4cOHExcXx7Zt24iOjiY6OprU1FQtZvr06cyYMYNZs2aRnJyMq6srUVFRFBcXazExMTHs2rWLxMREli5dytq1axk7dqy2Py8vjz59+tCgQQO2bNnCv/71L15//XX++9//Vt2HUQOUmcv48cCPPPhDNM+teY6UrBQMdgaGhwxnbp/PQWdHozqujO/ZhGXPdmXNi915uW8IYfU8r7jCsRBCCFEZnVJKVeUFwsPD6dixIx9//DEAZrOZoKAgxo8fz6RJky6JHzp0KIWFhSxdulTb1rlzZ9q0acOsWbNQShEYGMgLL7zAiy++CEBubi5+fn7Mnz+fYcOGkZaWRsuWLdm0aRMdOlhWzE1ISKB///6cOHGCwMBAPv30U/7+979jNBpxdLRMwT5p0iSWLFnCnj17rune8vLy8PT0JDc3Fw8Pj5v6nKqEUvDjM+BcC1y8wdkbXGpbfnapDW5+lp+vUVZBCS+ueoMtZ3+2nN7khGd5N356fDK1nWsDcCqniABPJylKhBBCXNG1fodWaR+V0tJStmzZwuTJk7Vter2eyMhIkpKSKj0mKSmJCRMmWG2LiopiyZIlABw+fBij0UhkZKS239PTk/DwcJKSkhg2bBhJSUl4eXlpRQpAZGQker2e5ORkHnroIZKSkrj//vu1IuX8dd59913Onj1LrVo1YEhscS5sX3D5/S0HwaP/s/xsNsPcPuDkVVHIOHuTa3Bhg7GctSdrs+hELXBsjHN9d8qy76WRY28GhgVTy1BR7AR6OVftPQkhhLirVGmhkpWVhclkws/Pz2q7n5/fZVstjEZjpfFGo1Hbf37blWJ8fX2t9tvb2+Pt7W0VExwcfMk5zu+rrFApKSmhpKRiKve8vLxK76Ha0NtB5Otw7gycO2v5b1H2n++zwfWC6edLcuHEJu3taTs9X3h48I2HG5GF57gvowlfqee4p05z+gTP4enMKOwcEmC/N5ysbVXc4B8KIQMqzp13ylIAObrctlsXQghRM8ion+swbdo03njjDVunce0M7nDf85fff+FTP3tnGPYVx3IOMi99LT/m7aMMMwB7XLx4oEkb1j3QgyBvFyg6C2tzLcVNzrFLz9syuqJQMZvh361AmS3XcPG2fgxVvzOEP1lx7KHfwMmjouhxdAV5jCSEEHetKi1UfHx8sLOzIyMjw2p7RkYG/v7+lR7j7+9/xfjz/83IyCAgIMAqpk2bNlrMxZ11y8vLyc7OtjpPZde58BoXmzx5stVjqby8PIKCgiqNvSNcUADszT/K7IzVLD+6HLOyFChtfdsyJmwMXet2te5zYvCA8VstrTIXttCcb7EJbFsRW5IHOjtLoVJeBHknLa/zlLmiUDGb4Itoy7bz7AwVrTWNe0Cftyr2bZoNju4XFT/elvykuBFCiBqhSgsVR0dH2rdvz8qVK4mOjgYsnWlXrlzJuHHjKj0mIiKClStX8txzz2nbEhMTiYiIACA4OBh/f39WrlypFSZ5eXkkJyfz9NNPa+fIyclhy5YttG/fHoBVq1ZhNpsJDw/XYv7+979TVlaGg4ODdp3mzZtftn+KwWDAYDDc1GdSXa0+vpqEIwkAdK3blbiwONr7ta88WG8HtRtbXlfj7AWvnYaS/AsePWVXFDa1GlbElp0D35Z/Fj5nwFQKphLIT7e8fJpWxJpN8MuLQCV9wfX2lhad8/1vAOIngYOz9SOq8x2LXX3AyfPq9yKEEOK2q/JHPxMmTCA2NpYOHTrQqVMnPvzwQwoLCxk1ahQAI0aMoG7dukybNg2Av/3tb3Tr1o3333+fAQMG8PXXX7N582Zt2LBOp+O5557jrbfeomnTpgQHB/Paa68RGBioFUMtWrSgb9++PPHEE8yaNYuysjLGjRvHsGHDCAwMBOCxxx7jjTfeIC4ujpdffpnU1FQ++ugj/v3vf1f1R2JzSinWnliLu6M77fzaATA8ZDjH848zouUImns3v7UX1Oksj3OcPIDgy8cZ3OHp9eeThNJC6xYbZ6+K2PISCH34gtacP1t3ys6BudzSinOe2QTJs6i0qAFo1B1G/FjxfsEjoHeoaKnRChtv8AyCgHtu7HMQQghx3aq8UBk6dCinT59mypQpGI1G2rRpQ0JCgtZx9dixY+j1FdO5dOnShYULF/Lqq6/yyiuv0LRpU5YsWUJoaKgWM3HiRAoLCxk7diw5OTncd999JCQk4OTkpMUsWLCAcePG0atXL/R6PYMHD2bGjBnafk9PT5YvX84zzzxD+/bt8fHxYcqUKVZzrdQ05eZyEo4kMCdlDgdyDtC6Tmu+6PcFOp0OT4Mnb9/3tq1TrKDTgcHN8vKqZKI4RxcYMvfS7WVFlqLlwkc/5nLoPvnSx1TnixuX2hWxpnLYn8jli5oeMGJJxfuP2oBObz3s+/xjKJ9m0GJgRey5bMtjKTvpGiaEENeqyudRqcmq/TwqfyouL2bJgSXM3zWfkwWW/iEu9i4MbT6U8W3H42DnYOMMbcxssjzOAkuhsmfpBUXNRaOlgsKh77SK2H/Uvvx5Ly5q3qlvGTLu5HnpnDb+90DEXytij2+yFGPnW3PsHS85vRBC3MmqxTwqwvZ+Pvgz729+nzPFZwCoZahFTIsYhoUMw9Mg/TKAiiIFLK0draKv7TidHv6aXNGnxqrFJht8QypiTeWWfjpgKVaKc+Hs4Yr9BRnWhcqXgy2jqs5zdAeXWpbCpV4n6H/Bcg87FoGdw6X9bxwqWhiFEOJOJYVKDeegd+BM8RkCXAOIbRXLw00fxtleJmW7JfR662LkSuzs4dXTUJxjPULq/M+e9SpizWZw97ccU3TWMgqqNN/yyjlmKUQutOwl66LmPAdXCO4Kjy2q2Lb6z9YgrTXngtFSLrUtw8GFEKIakUKlBjlZcJJ5qfNo5NmIx1o8BkDvBr2Zfv90IhtE4qC/yx/x2JqdvWWEkavPleP0ehj358KdZrOluCk6W9FaY3CviFUKGt1v3e/m3BlQJigrBFOZ9bn/+MQyZLwyge1g7OqK9989YRl55VLb+jGVszd4BIBfq+v+CIQQ4npJoVID7D+7n7mpc4k/HI9JmfB19mVIsyE42jlip7ejX3A/W6cobpReX9HyUdlwcJ0Ohn5pvU0py6OlouxLt4c/WfGo6sLRUufOXLru0954SytOZS4uamZHWkZpudT+c22pCwobrwbWnYpLCmQiPyHENZNC5Q62PXM7c1LmsObEGm1bREAEY8LGSOvJ3UynswzlvnA49/ntPV+t/BilLK0nF74f+O/L97+pc9EQ9sw9ly9q6ra3LlQ+jYDck5UslPnnSKku4ytijalg72TZ5+RlKdyEEHcVKVTuUJ/u+JRPtn8CgA4dkQ0iiQuNo5WPNMeLG6DTgb3B+v09j1z78aN+qaSVppJJ/cCyXZmg8LTldaG67a0Lla+GQ+6fyzTo9JbWmvPFjV8rGPhBRWza0j+LtAv63Dh5yXBwIe5w8jf4DmEymygqL8LN0Q2A7vW689nOz3ig8QOMbDWSYM8rTKQmRFULaH3tsS8dtC5kLhwK7nrRcG+Du2XEU2m+pVPx+RaeM/st8+NcKGES5B6/9HpOnpZlHS6c1G/Dx5ZZjy8eJn6+yLnbh+wLUY1IoVLNlZpK+fngz8zbNY+O/h2ZGjEVgBa1W7DykZXUcqp8un8hqi0HJ3AIBI/Aq8f+dYPlv+WllxY3Dhetxl23vWW01PmWneIcy/biXEv/mQslz6q8qAHwbgzPbq14v+wlyzm0QqZWxc9uvuDb4ppuWwhxY6RQqaYKywr5dt+3/G/X/8gssiywWFRWREmnEgx2liZ6KVLEXcPe0VKEuFe+YCgAj35u/d5U/udw8DPWC10CtB4GeemX9r8pOms9UzHAnmWQd6Lya9ZuAuO3VLxf8IhlTpyL15NyqQ3uAdZ9dcpLZSI/Ia6BFCrVzNnisyzcs5CFaQvJK7UMI/V18SW2ZSxDmg3RihQhxFVcaTj45ToVm02W9aIuFDkV8o2Xdig+dwa8L3rkmrHLenXwC9VuYl2ofNYTzhz4s5ipZV3ceAXBvX+riM06UDGpn6ObjJgSdxUpVKqZhXsWMmvHLAAaejRkdOhoBjQagKOd/OYlRJXT21nPUwNwz6PXfvzQL6Awq5L+N9ng5mcdW5QN5UWW1pqLW2xqN7UuVBbHQkaq5Wc7R+vWmloNYdDHFbEHV1kKLq1TsbdljSkpbsQdSgqVamZ4yHA2pm8kpkUMver3wu7C6d2FENVb3fbXHvvXPy6zplT2pcWSnSPYGSwdgE2lUGC0vMDyqOlCy1+rKGrO09v/uVBmUxi1rGL7ls8tEwBe2JH4/Fw4MhxcVBNSqFQz3k7efN7v86sHCiHubE4eltfFw7crM3a1ZW6bsnOXLsGgv+if8TohlqHc54eJl52zjJAqzLx0bp3k/4PMXZVf080PXtxX8X7VW5CfXnn/Gxcf8GlyPXcvxDWTQkUIIe4EOp1lRl9HV0sflssZMsf6fVlRRXFz8ZDuFg9Y5qPRWnb+bN0pzbe0qFxoz7LLFzWudeClAxXvv38Ssg9VsqZUbUtsSP+KWLNZWm7EFUmhIoQQNZmDM3jWtbwu1mNy5ceUl1w6pPvev1km37N6TPVn/5uLR0oZd0Lm7srP7eprXaj870FI33lph2KXP4d/3/d8RWzOcUunYmdvGTF1F5FCRQghhDV7g/VMxQCth1778Q/+x/KY6MKFMovO/tn/xs06tjDLsvp3SS6cPWK9z/WiQuWHJ+HoesvPju7WxY27P0R/UhF7LNnSWfnC/jcOsnL8nUgKFSGEELdWvQ7XHjtq2UULZV7QWnPxaEdltvS/UWbL46nSfMj5c4mFi0dVrXyjoqg5z8Gloqh5YmXF9pRv/1yYs5JFNR1cZMSUjUmhIoQQwnbO92Gh6dVjRydY+rQU5/zZQnNBcYOyjvVqYF34mMstHYvLzoG5zDp20xw4tqHyaxo8YNKximJl/QxLy09lSy+4eFuuK4XNLSWFihBCiDuHXl9RFNRufPm4hz6t+FkpyzDs85P1mUqsYxt1B7c6Fy2qecYyFPziFpW9y+BYUuXXtHOEVzMr3se/bFkBvLL+Ny61oUlv6Uh8DaRQEUIIUbPpdJbFKZ08L51NGKD7y5duU8rSobgkz3p7h9EQfH/li2raOVgXNae2w/E/Ks/JzgCvXjAHzqLHLQWQVTFzwWipiGcsExICFGRahqU7eVZsq8GkUBFCCCEuptNZOv5e3Pn3emYq7vMPy+KXFy67cL6wQWdd1BRkQuFpy+tidgboMr7i/U/Pwr54yzmca130CKo2PPCRZQmJGqLm3IkQQghRnQR1sryuxaP/sxQpF07md/4xlNlkXdSUF//5g7LsL8q2rBsFYO8E0TNv6W3YmhQqQgghhK25+1le12LEEsvq20VnL1os84xlDpwaRgoVIYQQ4k5j73h9xc0drMq6G2dnZxMTE4OHhwdeXl7ExcVRUFBwxWOKi4t55plnqF27Nm5ubgwePJiMDOsFt44dO8aAAQNwcXHB19eXl156ifJy62mh16xZQ7t27TAYDDRp0oT58+db7V+7di0PPPAAgYGB6HQ6lixZcituWQghhBC3WJUVKjExMezatYvExESWLl3K2rVrGTt27BWPef755/n5559ZvHgxv/32G6dOneLhhx/W9ptMJgYMGEBpaSkbNmzg888/Z/78+UyZMkWLOXz4MAMGDKBHjx5s376d5557jjFjxvDrr79qMYWFhbRu3ZqZM2vWczwhhBCixlFVYPfu3QpQmzZt0rbFx8crnU6nTp48WekxOTk5ysHBQS1evFjblpaWpgCVlJSklFJq2bJlSq/XK6PRqMV8+umnysPDQ5WUlCillJo4caJq1aqV1bmHDh2qoqKiKr0uoH744Ycbus/c3FwFqNzc3Bs6XgghhLhbXet3aJW0qCQlJeHl5UWHDhXTKEdGRqLX60lOTq70mC1btlBWVkZkZKS2LSQkhPr165OUlKSdNywsDD+/imdyUVFR5OXlsWvXLi3mwnOcjzl/jptRUlJCXl6e1UsIIYQQVadKChWj0Yivr6/VNnt7e7y9vTEajZc9xtHRES8vL6vtfn5+2jFGo9GqSDm///y+K8Xk5eVRVFR0w/cEMG3aNDw9PbVXUNAVlloXQgghxE27rkJl0qRJ6HS6K7727NlTVbna3OTJk8nNzdVex48ft3VKQgghRI12XcOTX3jhBUaOHHnFmEaNGuHv709mZqbV9vLycrKzs/H396/0OH9/f0pLS8nJybFqVcnIyNCO8ff3Z+PGjVbHnR8VdGHMxSOFMjIy8PDwwNn55pb4NhgMGAyGqwcKIYQQ4pa4rkKlTp061KlT56pxERER5OTksGXLFtq3bw/AqlWrMJvNhIeHV3pM+/btcXBwYOXKlQwePBiAvXv3cuzYMSIiIrTzvv3222RmZmqPlhITE/Hw8KBly5ZazLJly6zOnZiYqJ3jVlLKslqn9FURQgghrs/5787z36WXVVW9efv27avatm2rkpOT1e+//66aNm2qhg8fru0/ceKEat68uUpOTta2PfXUU6p+/fpq1apVavPmzSoiIkJFRERo+8vLy1VoaKjq06eP2r59u0pISFB16tRRkydP1mIOHTqkXFxc1EsvvaTS0tLUzJkzlZ2dnUpISNBi8vPz1bZt29S2bdsUoD744AO1bds2dfTo0eu6x+PHjyssa4vLS17ykpe85CWvG3gdP378it+1OqWuVsrcmOzsbMaNG8fPP/+MXq9n8ODBzJgxAzc3ywJPR44cITg4mNWrV9O9e3fAMuHbCy+8wFdffUVJSQlRUVF88sknVo+Ljh49ytNPP82aNWtwdXUlNjaWd955B3v7isahNWvW8Pzzz7N7927q1avHa6+9ZvXIas2aNfTo0eOSnGNjYy+ZHO5KzGYzp06dwt3dHd2F6zDUIHl5eQQFBXH8+HE8PDxsnU61IZ9L5eRzqZx8LpWTz6Vyd8vnopQiPz+fwMBA9PrLd5mtskJF1Ax5eXl4enqSm5tbo//CXC/5XConn0vl5HOpnHwulZPPxVqVzUwrhBBCCHGzpFARQgghRLUlhYq4IoPBwNSpU2VY9kXkc6mcfC6Vk8+lcvK5VE4+F2vSR0UIIYQQ1Za0qAghhBCi2pJCRQghhBDVlhQqQgghhKi2pFARQgghRLUlhYoQQgghqi0pVIQQQghRbUmhIoQQQohqSwoVIYQQQlRb/w8B74WjEbAjTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(6,3))\n",
    "\n",
    "abline(0.0003322163874076709, 0, \"Random\")\n",
    "abline(-7.708065223943293e-05, 0, \"Sweetest\")\n",
    "abline(0.0004774986787622364 , 0, \"Kaleist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Get an environment baseline</b>\n",
    "\n",
    "It is always best practice, before training an algorithm, to run through the environment and record the mean reward as a baseline.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° For this environment, we will calculate 3 baselines: <br>\n",
    "    <ul>\n",
    "        <li>Random baseline: pick randomly every time</li>\n",
    "        <li>Greedy sweet baseline: always pick the sweetest item </li>\n",
    "        <li>Greedy kale baseline: always pick the kaleist item </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def calc_baseline(env, \n",
    "                  baseline_type=\"random\",\n",
    "                  episodes=100, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    obs = env.reset()\n",
    "    doc_list = [t[1][0] for t in list(obs['doc'].items()) ]\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "    # Keep track of sugar consumed\n",
    "    episode_sugar = 0.0\n",
    "    doc_sugar = []\n",
    "    \n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce an action\n",
    "        # action_random = env.action_space.sample()\n",
    "        action_random = random.randint(0, len(doc_list)-1)\n",
    "        action_sweetest = np.argmax(doc_list)\n",
    "        action_kaleiest = np.argmin(doc_list)\n",
    "        if baseline_type == \"random\":\n",
    "            action = action_random\n",
    "        elif baseline_type == \"sweetest\":\n",
    "            action = action_sweetest\n",
    "        elif baseline_type == \"kaleist\":\n",
    "            action = action_kaleiest\n",
    "        sugar = doc_list[action]\n",
    "        # # debugging\n",
    "        # print(f\"doc_val={doc_list[action]}\")\n",
    "        # print(np.argmax(doc_list), doc_list)\n",
    "        \n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        doc_list = [t[1][0] for t in list(obs['doc'].items()) ]\n",
    "        episode_reward += reward\n",
    "        episode_sugar += sugar\n",
    "        \n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 99 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 9 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "                \n",
    "            # increment on end of episode\n",
    "            num_episodes += 1\n",
    "            obs = env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            doc_sugar.append(episode_sugar)\n",
    "            episode_sugar = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_reward = np.mean(episode_rewards)\n",
    "    env_sd_reward = np.std(episode_rewards)\n",
    "    \n",
    "    # Print out sugar consumed\n",
    "    env_mean_sugar_consumed = np.mean(doc_sugar)\n",
    "    env_sd_sugar = np.std(doc_sugar)\n",
    "\n",
    "    print(f\"\\nMean {baseline_type} baseline reward: {env_mean_reward:.2f}+/-{env_sd_reward:.2f}\")\n",
    "    print(f\"Mean {baseline_type} sugar consumed: {env_mean_sugar_consumed:.2f}+/-{env_sd_sugar:.2f}\")\n",
    "\n",
    "    return env_mean_reward, episode_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 ...\n",
      "Mean kaleist baseline reward: 1085.47+/-8.01\n",
      "Mean kaleist sugar consumed: 5.64+/-0.47\n",
      " 0 ...\n",
      "Mean random baseline reward: 1156.37+/-13.32\n",
      "Mean random sugar consumed: 60.60+/-3.74\n",
      " 0 ...\n",
      "Mean sweetest baseline reward: 1165.55+/-12.36\n",
      "Mean sweetest sugar consumed: 114.34+/-0.51\n",
      "Environment by itself took 4.13 seconds\n"
     ]
    }
   ],
   "source": [
    "# Calculate a baseline type: random, sweetest, or kaleist\n",
    "start_time = time.time()\n",
    "\n",
    "lts_20_2_mean_kaleist_reward, lts_20_2_mean_kaleist_rewards = \\\n",
    "    calc_baseline(modified_lts_20_2_env, \n",
    "                  baseline_type=\"kaleist\",\n",
    "                  episodes=100)\n",
    "lts_20_2_mean_random_reward, lts_20_2_mean_random_rewards = \\\n",
    "    calc_baseline(modified_lts_20_2_env, \n",
    "                  baseline_type=\"random\",\n",
    "                  episodes=100)\n",
    "lts_20_2_mean_sweetest_reward, lts_20_2_mean_sweetest_rewards = \\\n",
    "    calc_baseline(modified_lts_20_2_env, \n",
    "                  baseline_type=\"sweetest\",\n",
    "                  episodes=100)\n",
    "\n",
    "print(f\"Environment by itself took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Mean kaleist baseline reward: 1085.47+/-10.61  #lowest  \n",
    "# Mean kaleist sugar consumed: 5.64+/-0.47\n",
    "\n",
    "# Mean random baseline reward: 1156.15+/-12.21   #next biggest \n",
    "# Mean random sugar consumed: 59.84+/-3.15\n",
    "\n",
    "# Mean sweetest baseline reward: 1162.98+/-9.70  #biggest \n",
    "# Mean sweetest sugar consumed: 114.34+/-0.51\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot per-timestep (episode) rewards.\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,4))\n",
    "\n",
    "# collect plot data from rewards\n",
    "start_at = 4\n",
    "smoothing_win = 100\n",
    "\n",
    "# plot rewards\n",
    "x = list(range(start_at, len(lts_20_2_mean_sweetest_rewards)-1))\n",
    "y = [np.nanmean(lts_20_2_mean_sweetest_rewards[max(i - smoothing_win, 0):i + 1]) \n",
    "     for i in range(start_at, len(lts_20_2_mean_sweetest_rewards)-1)]\n",
    "ax.plot(x, y, label=\"Sweetest\", color=\"orange\")\n",
    "\n",
    "x = list(range(start_at, len(lts_20_2_mean_random_rewards)-1))\n",
    "y = [np.nanmean(lts_20_2_mean_random_rewards[max(i - smoothing_win, 0):i + 1]) \n",
    "     for i in range(start_at, len(lts_20_2_mean_random_rewards)-1)]\n",
    "ax.plot(x, y, label=\"Random\", color=\"grey\")\n",
    "\n",
    "x = list(range(start_at, len(lts_20_2_mean_kaleist_rewards)-1))\n",
    "y = [np.nanmean(lts_20_2_mean_kaleist_rewards[max(i - smoothing_win, 0):i + 1]) \n",
    "     for i in range(start_at, len(lts_20_2_mean_kaleist_rewards)-1)]\n",
    "ax.plot(x, y, label=\"Kaleist\", color=\"green\")\n",
    "\n",
    "# # Add mean random baseline reward (red line).\n",
    "# plt.axhline(y=lts_20_2_mean_random_reward, \n",
    "#             color=\"r\", \n",
    "#             linestyle=\"-\",\n",
    "#             label=\"Random baseline\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='center', frameon=True)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"Environment-only Mean Reward\")\n",
    "plt.xlabel(\"Training iterations\")\n",
    "\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Contextual Bandit on the environment <a class=\"anchor\" id=\"cb\"></a>\n",
    "\n",
    "A Bandit session is one where we have an opportunity to recommend to the user an item and receive a reward if they click.\n",
    "\n",
    "<img src=\"./images/simple_n_armed_bandit.png\" width=\"90%\" />\n",
    "\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>Bandits</b></i></li>\n",
    "    <li>On the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#bandits\">algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select RLlib Bandit algorithm w/Upper Confidence Bound (UCB) exploration\n",
    "# and find that algorithm's config class\n",
    "\n",
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.bandit.bandit import BanditLinUCBConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig at 0x7f16159f21d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # uncomment below to see the long list of default config values\n",
    "# print(f\"Bandit's default config is:\")\n",
    "# print(pretty_print(BanditLinUCBConfig().to_dict()))\n",
    "\n",
    "# Create a Bandit object\n",
    "bandit_config = BanditLinUCBConfig()\n",
    "\n",
    "# Setup our config object to use our environment\n",
    "env_config_20 = \\\n",
    "    {\n",
    "        # The number of possible documents/videos/candidates that we can recommend\n",
    "        \"num_candidates\": 20,  \n",
    "        # The number of recommendations that we will be making\n",
    "        \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "        # Set to False for re-using the same candidate doecuments each timestep.\n",
    "        \"resample_documents\": True,\n",
    "        # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": 0,}\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "bandit_config.environment(env=\"modified-lts\", env_config=env_config_20)\n",
    "\n",
    "# Decide if you want torch or tensorflow DL framework.  Default is \"tf\"\n",
    "bandit_config.framework(\"torch\")\n",
    "\n",
    "# Setup evaluation\n",
    "# Explicitly set \"explore\"=False to override default\n",
    "bandit_config.evaluation(\n",
    "    evaluation_interval=10, \n",
    "    evaluation_duration=20, \n",
    "    evaluation_duration_unit=\"timesteps\",)\n",
    "    # evaluation_config = {\"explore\" : False})\n",
    "\n",
    "# Setup sampling rollout workers\n",
    "# +1 for head node, num parallel workers or actors for rollouts\n",
    "bandit_config.rollouts(\\\n",
    "    num_rollout_workers=1,\n",
    "    num_envs_per_worker=1,)\n",
    "    # enable_connectors=True)\n",
    "\n",
    "bandit_config.resources(num_gpus=0)\n",
    "\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "# seed the algorithm / policy\n",
    "bandit_config.debugging(seed=415, log_level=\"ERROR\")\n",
    "\n",
    "# for increasing logging frequency in the terminal for this tutorial\n",
    "bandit_config.reporting(\n",
    "    min_train_timesteps_per_iteration=1,\n",
    "    metrics_num_episodes_for_smoothing=2) #200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm type: <class 'ray.rllib.algorithms.bandit.bandit.BanditLinUCB'>\n",
      "agent_timesteps_total: 100\n",
      "counters:\n",
      "  num_agent_steps_sampled: 100\n",
      "  num_agent_steps_trained: 100\n",
      "  num_env_steps_sampled: 100\n",
      "  num_env_steps_trained: 100\n",
      "custom_metrics: {}\n",
      "date: 2022-08-20_20-16-54\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: db57622f7ac04f3ab0dbe5d5844203df\n",
      "hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        update_latency: 0.0003199577331542969\n",
      "  num_agent_steps_sampled: 100\n",
      "  num_agent_steps_trained: 100\n",
      "  num_env_steps_sampled: 100\n",
      "  num_env_steps_trained: 100\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.18.12.66\n",
      "num_agent_steps_sampled: 100\n",
      "num_agent_steps_trained: 100\n",
      "num_env_steps_sampled: 100\n",
      "num_env_steps_sampled_this_iter: 100\n",
      "num_env_steps_trained: 100\n",
      "num_env_steps_trained_this_iter: 100\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 100\n",
      "perf:\n",
      "  cpu_util_percent: 19.65\n",
      "  ram_util_percent: 16.8\n",
      "pid: 235355\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  hist_stats:\n",
      "    episode_lengths: []\n",
      "    episode_reward: []\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "time_since_restore: 0.7849292755126953\n",
      "time_this_iter_s: 0.7849292755126953\n",
      "time_total_s: 0.7849292755126953\n",
      "timers:\n",
      "  learn_throughput: 1654.036\n",
      "  learn_time_ms: 0.605\n",
      "  load_throughput: 5563.475\n",
      "  load_time_ms: 0.18\n",
      "  synch_weights_time_ms: 0.758\n",
      "  training_iteration_time_ms: 7.548\n",
      "timestamp: 1661051814\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "warmup_time: 6.364623308181763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SINGLE .TRAIN() OUTPUT\n",
    "\n",
    "# instantiate an algo instance\n",
    "linucb_algo = bandit_config.build()\n",
    "print(f\"Algorithm type: {type(linucb_algo)}\")\n",
    "\n",
    "# Perform single `.train() iteration` call\n",
    "# Result is a Python dict object\n",
    "result = linucb_algo.train()\n",
    "\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm type: <class 'ray.rllib.algorithms.bandit.bandit.BanditLinUCB'>\n",
      "Iteration=10, Mean Reward=1148.91+/-7.92\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000011\n",
      "Iteration=20, Mean Reward=1167.48+/-8.15\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000021\n",
      "Iteration=30, Mean Reward=1165.22+/-6.77\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000031\n",
      "Iteration=40, Mean Reward=1148.79+/-6.84\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000041\n",
      "Iteration=50, Mean Reward=1165.07+/-7.02\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000051\n",
      "Iteration=60, Mean Reward=1169.95+/-6.74\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000061\n",
      "Iteration=70, Mean Reward=1156.46+/-7.29\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000071\n",
      "Iteration=80, Mean Reward=1164.03+/-7.42\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000081\n",
      "Iteration=90, Mean Reward=1158.23+/-7.18\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000091\n",
      "Iteration=99, Mean Reward=1166.01+/-6.94\n",
      "Checkpoints saved at results/bandit_recsim/checkpoint_000100\n",
      "Bandit Return 2332.0176404013196 over 200 episodes (24000 timesteps)\n",
      "Approx 11.66 Return per episode\n",
      "Training took 90.04 seconds\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RLLIB API .train() IN A LOOP\n",
    "# To train for N number of episodes, you put .train() into a loop, \n",
    "# similar to the way we ran the Gym env.step() in a loop.\n",
    "###############\n",
    "# # start fresh in case ray already running\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "linucb_algo = bandit_config.build()\n",
    "print(f\"Algorithm type: {type(linucb_algo)}\")\n",
    "\n",
    "# train the Algorithm instance for 100 iterations\n",
    "num_iterations = 100\n",
    "rewards = []\n",
    "checkpoint_dir = \"results/bandit_recsim/\"\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Call its `train()` method\n",
    "    result = linucb_algo.train()\n",
    "    \n",
    "    # Extract reward from results.\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "    # print something every 10 episodes\n",
    "    if (((i % 10 == 0) and (i >=4)) or (i == num_iterations-1)):\n",
    "        print(f\"Iteration={i}, Mean Reward={result['episode_reward_mean']:.2f}\",end=\"\")\n",
    "        try:\n",
    "            print(f\"+/-{np.std(rewards[4:]):.2f}\")\n",
    "        except:\n",
    "            print(\"\")\n",
    "        # save checkpoint file\n",
    "        checkpoint_file = linucb_algo.save(checkpoint_dir)\n",
    "        print(f\"Checkpoints saved at {checkpoint_file}\")\n",
    "        # evaluate the policy\n",
    "        eval_result = linucb_algo.evaluate()\n",
    "\n",
    "# convert num_iterations to num_episodes\n",
    "num_episodes = len(result[\"hist_stats\"][\"episode_lengths\"]) * num_iterations\n",
    "# convert num_iterations to num_timesteps\n",
    "num_timesteps = sum(result[\"hist_stats\"][\"episode_lengths\"] * num_iterations)\n",
    "# calculate cumulative sum rewards\n",
    "total_return = np.sum(result[\"hist_stats\"][\"episode_reward\"])\n",
    "\n",
    "# train time\n",
    "print(f\"Bandit Return {total_return} over {num_episodes} episodes ({num_timesteps} timesteps)\") \n",
    "print(f\"Approx {total_return/num_episodes:.2f} Return per episode\")\n",
    "print(f\"Training took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# # Uncomment to train and generate the json output.\n",
    "# \"\"\"\n",
    "# algo = dummy_config.build()\n",
    "\n",
    "# for _ in range(4):\n",
    "#     algo.train()\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Compare the Bandit Training results to Baseline <br></b>\n",
    "- Bandit Mean Reward=~1166.01+/-6.94. \n",
    "> Kaleist Baseline Mean Reward = ~1083.06+/-9.58 <br>\n",
    "Random Baseline Mean Reward = ~1157.19+/-11.19 <br>\n",
    "Sweetest Baseline Mean Reward = ~1164.25+/-11.00 <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    ü§î  <b>Bandit mean reward is approx the same as the sweetest baseline!</b> <br>\n",
    "</div>  \n",
    "\n",
    "<br>\n",
    "Try the code block below to compare what the bandit recommends with what is the sweetest item... you will see that the bandit always recommends the sweetest item!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.LTSWithStrongerDissatisfactionEffect'>\n",
      "action's feature value=0.978618323802948; max-choc-feature=0.978618323802948; \n",
      "action's feature value=0.9883738160133362; max-choc-feature=0.9883738160133362; \n",
      "action's feature value=0.9767611026763916; max-choc-feature=0.9767611026763916; \n",
      "action's feature value=0.9292961955070496; max-choc-feature=0.9292961955070496; \n",
      "action's feature value=0.9621885418891907; max-choc-feature=0.9621885418891907; \n",
      "action's feature value=0.9560836553573608; max-choc-feature=0.9560836553573608; \n",
      "action's feature value=0.9988470077514648; max-choc-feature=0.9988470077514648; \n",
      "action's feature value=0.9755215048789978; max-choc-feature=0.9755215048789978; \n",
      "action's feature value=0.9443724155426025; max-choc-feature=0.9443724155426025; \n",
      "action's feature value=0.990338921546936; max-choc-feature=0.990338921546936; \n",
      "action's feature value=0.9527916312217712; max-choc-feature=0.9527916312217712; \n",
      "action's feature value=0.961936354637146; max-choc-feature=0.961936354637146; \n",
      "action's feature value=0.9774951338768005; max-choc-feature=0.9774951338768005; \n",
      "action's feature value=0.9818294048309326; max-choc-feature=0.9818294048309326; \n",
      "action's feature value=0.9065554738044739; max-choc-feature=0.9065554738044739; \n",
      "action's feature value=0.7885454893112183; max-choc-feature=0.7885454893112183; \n",
      "action's feature value=0.9594333171844482; max-choc-feature=0.9594333171844482; \n",
      "action's feature value=0.9591665863990784; max-choc-feature=0.9591665863990784; \n",
      "action's feature value=0.9589827060699463; max-choc-feature=0.9589827060699463; \n",
      "action's feature value=0.9453015327453613; max-choc-feature=0.9453015327453613; \n",
      "action's feature value=0.9903450012207031; max-choc-feature=0.9903450012207031; \n",
      "action's feature value=0.9920112490653992; max-choc-feature=0.9920112490653992; \n",
      "action's feature value=0.9944007992744446; max-choc-feature=0.9944007992744446; \n",
      "action's feature value=0.96779465675354; max-choc-feature=0.96779465675354; \n",
      "action's feature value=0.9241587519645691; max-choc-feature=0.9241587519645691; \n",
      "action's feature value=0.9488610029220581; max-choc-feature=0.9488610029220581; \n",
      "action's feature value=0.9627703428268433; max-choc-feature=0.9627703428268433; \n",
      "action's feature value=0.9805801510810852; max-choc-feature=0.9805801510810852; \n",
      "action's feature value=0.997962236404419; max-choc-feature=0.997962236404419; \n",
      "action's feature value=0.9992780089378357; max-choc-feature=0.9992780089378357; \n",
      "action's feature value=0.9762256741523743; max-choc-feature=0.9762256741523743; \n",
      "action's feature value=0.9834262132644653; max-choc-feature=0.9834262132644653; \n",
      "action's feature value=0.857124924659729; max-choc-feature=0.857124924659729; \n",
      "action's feature value=0.95914226770401; max-choc-feature=0.95914226770401; \n",
      "action's feature value=0.8562761545181274; max-choc-feature=0.8562761545181274; \n",
      "action's feature value=0.987348735332489; max-choc-feature=0.987348735332489; \n",
      "action's feature value=0.9707314372062683; max-choc-feature=0.9707314372062683; \n",
      "action's feature value=0.9918903112411499; max-choc-feature=0.9918903112411499; \n",
      "action's feature value=0.9758838415145874; max-choc-feature=0.9758838415145874; \n",
      "action's feature value=0.9543338418006897; max-choc-feature=0.9543338418006897; \n",
      "action's feature value=0.9323939681053162; max-choc-feature=0.9323939681053162; \n",
      "action's feature value=0.9195073843002319; max-choc-feature=0.9195073843002319; \n",
      "action's feature value=0.9631972908973694; max-choc-feature=0.9631972908973694; \n",
      "action's feature value=0.8667885661125183; max-choc-feature=0.8667885661125183; \n",
      "action's feature value=0.9804856777191162; max-choc-feature=0.9804856777191162; \n",
      "action's feature value=0.9031496644020081; max-choc-feature=0.9031496644020081; \n",
      "action's feature value=0.9738187193870544; max-choc-feature=0.9738187193870544; \n",
      "action's feature value=0.9998085498809814; max-choc-feature=0.9998085498809814; \n",
      "action's feature value=0.9384120106697083; max-choc-feature=0.9384120106697083; \n",
      "action's feature value=0.9536756873130798; max-choc-feature=0.9536756873130798; \n",
      "action's feature value=0.9958152770996094; max-choc-feature=0.9958152770996094; \n",
      "action's feature value=0.9692058563232422; max-choc-feature=0.9692058563232422; \n",
      "action's feature value=0.9750946760177612; max-choc-feature=0.9750946760177612; \n",
      "action's feature value=0.9673377871513367; max-choc-feature=0.9673377871513367; \n",
      "action's feature value=0.9851086735725403; max-choc-feature=0.9851086735725403; \n",
      "action's feature value=0.9979940056800842; max-choc-feature=0.9979940056800842; \n",
      "action's feature value=0.9555683732032776; max-choc-feature=0.9555683732032776; \n",
      "action's feature value=0.983853816986084; max-choc-feature=0.983853816986084; \n",
      "action's feature value=0.9279761910438538; max-choc-feature=0.9279761910438538; \n",
      "action's feature value=0.9421847462654114; max-choc-feature=0.9421847462654114; \n",
      "action's feature value=0.992667019367218; max-choc-feature=0.992667019367218; \n",
      "action's feature value=0.9717630743980408; max-choc-feature=0.9717630743980408; \n",
      "action's feature value=0.9792863130569458; max-choc-feature=0.9792863130569458; \n",
      "action's feature value=0.9960712790489197; max-choc-feature=0.9960712790489197; \n",
      "action's feature value=0.9942330718040466; max-choc-feature=0.9942330718040466; \n",
      "action's feature value=0.9824448823928833; max-choc-feature=0.9824448823928833; \n",
      "action's feature value=0.9492799043655396; max-choc-feature=0.9492799043655396; \n",
      "action's feature value=0.9568705558776855; max-choc-feature=0.9568705558776855; \n",
      "action's feature value=0.9853785634040833; max-choc-feature=0.9853785634040833; \n",
      "action's feature value=0.9903685450553894; max-choc-feature=0.9903685450553894; \n",
      "action's feature value=0.9980227947235107; max-choc-feature=0.9980227947235107; \n",
      "action's feature value=0.9376630783081055; max-choc-feature=0.9376630783081055; \n",
      "action's feature value=0.9834339618682861; max-choc-feature=0.9834339618682861; \n",
      "action's feature value=0.9832748770713806; max-choc-feature=0.9832748770713806; \n",
      "action's feature value=0.9804664850234985; max-choc-feature=0.9804664850234985; \n",
      "action's feature value=0.9825738668441772; max-choc-feature=0.9825738668441772; \n",
      "action's feature value=0.9983548521995544; max-choc-feature=0.9983548521995544; \n",
      "action's feature value=0.7697890400886536; max-choc-feature=0.7697890400886536; \n",
      "action's feature value=0.9985265731811523; max-choc-feature=0.9985265731811523; \n",
      "action's feature value=0.9906516671180725; max-choc-feature=0.9906516671180725; \n",
      "action's feature value=0.9890884160995483; max-choc-feature=0.9890884160995483; \n",
      "action's feature value=0.9605224132537842; max-choc-feature=0.9605224132537842; \n",
      "action's feature value=0.9260265231132507; max-choc-feature=0.9260265231132507; \n",
      "action's feature value=0.9829264879226685; max-choc-feature=0.9829264879226685; \n",
      "action's feature value=0.9454309940338135; max-choc-feature=0.9454309940338135; \n",
      "action's feature value=0.9270205497741699; max-choc-feature=0.9270205497741699; \n",
      "action's feature value=0.9585323333740234; max-choc-feature=0.9585323333740234; \n",
      "action's feature value=0.9738933444023132; max-choc-feature=0.9738933444023132; \n",
      "action's feature value=0.9793245196342468; max-choc-feature=0.9793245196342468; \n",
      "action's feature value=0.9890880584716797; max-choc-feature=0.9890880584716797; \n",
      "action's feature value=0.9670467972755432; max-choc-feature=0.9670467972755432; \n",
      "action's feature value=0.9703752994537354; max-choc-feature=0.9703752994537354; \n",
      "action's feature value=0.9550641179084778; max-choc-feature=0.9550641179084778; \n",
      "action's feature value=0.9574885368347168; max-choc-feature=0.9574885368347168; \n",
      "action's feature value=0.9229142665863037; max-choc-feature=0.9229142665863037; \n",
      "action's feature value=0.9832027554512024; max-choc-feature=0.9832027554512024; \n",
      "action's feature value=0.998198926448822; max-choc-feature=0.998198926448822; \n",
      "action's feature value=0.9642097353935242; max-choc-feature=0.9642097353935242; \n",
      "action's feature value=0.9417421221733093; max-choc-feature=0.9417421221733093; \n",
      "action's feature value=0.9040508270263672; max-choc-feature=0.9040508270263672; \n",
      "action's feature value=0.9835554361343384; max-choc-feature=0.9835554361343384; \n",
      "action's feature value=0.9747744202613831; max-choc-feature=0.9747744202613831; \n",
      "action's feature value=0.8408302664756775; max-choc-feature=0.8408302664756775; \n",
      "action's feature value=0.9682117700576782; max-choc-feature=0.9682117700576782; \n",
      "action's feature value=0.974723219871521; max-choc-feature=0.974723219871521; \n",
      "action's feature value=0.9895250797271729; max-choc-feature=0.9895250797271729; \n",
      "action's feature value=0.9818642139434814; max-choc-feature=0.9818642139434814; \n",
      "action's feature value=0.987434983253479; max-choc-feature=0.987434983253479; \n",
      "action's feature value=0.9674006104469299; max-choc-feature=0.9674006104469299; \n",
      "action's feature value=0.9522157907485962; max-choc-feature=0.9522157907485962; \n",
      "action's feature value=0.9649279713630676; max-choc-feature=0.9649279713630676; \n",
      "action's feature value=0.9949018955230713; max-choc-feature=0.9949018955230713; \n",
      "action's feature value=0.9630938768386841; max-choc-feature=0.9630938768386841; \n",
      "action's feature value=0.9801590442657471; max-choc-feature=0.9801590442657471; \n",
      "action's feature value=0.9809793829917908; max-choc-feature=0.9809793829917908; \n",
      "action's feature value=0.9905391931533813; max-choc-feature=0.9905391931533813; \n",
      "action's feature value=0.9747872352600098; max-choc-feature=0.9747872352600098; \n",
      "action's feature value=0.9961004257202148; max-choc-feature=0.9961004257202148; \n",
      "action's feature value=0.9473085999488831; max-choc-feature=0.9473085999488831; \n",
      "action's feature value=0.9615751504898071; max-choc-feature=0.9615751504898071; \n"
     ]
    }
   ],
   "source": [
    "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
    "\n",
    "# Instantiate a new environment, same as bandit algorithm was trained on\n",
    "# Step 1. define a config dictionary\n",
    "env_config_20 = {\n",
    "    # The number of possible documents/videos/candidates that we can recommend\n",
    "    # no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "    \"num_candidates\": 20,  \n",
    "    # The number of recommendations that we will be making\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": True,\n",
    "    # # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": True,\n",
    "    # Use consistent seeds for the environment ...\n",
    "    \"seed\": 0,}\n",
    "# Step 2. create a RecSim environment\n",
    "lts_20_2_env = LongTermSatisfactionRecSimEnv(env_config_20)\n",
    "# step 3. create a modified RecSim environment\n",
    "# LTSWithStrongerDissatisfactionEffect(recsim_env)\n",
    "modified_lts_20_2_env = \\\n",
    "    LTSWithStrongerDissatisfactionEffect(lts_20_2_env)\n",
    "print(type(modified_lts_20_2_env))\n",
    "\n",
    "# Reset the environment\n",
    "obs = modified_lts_20_2_env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    action = linucb_algo.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"item\"][action][0]\n",
    "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dummy Recsim environment, we did not have any user features.  This makes the contextual bandit without any user context, i.e. without any state.  A stateless bandit cannot remember things between timesteps, so it will be exactly the most greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot per-timestep (episode) rewards.\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,4))\n",
    "\n",
    "# collect plot data from bandit rewards\n",
    "start_at = 4  #bandit rewards are nan for 1st 3 iterations\n",
    "smoothing_win = 100\n",
    "x = list(range(start_at, len(rewards)-1))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) \n",
    "     for i in range(start_at, len(rewards)-1)]\n",
    "\n",
    "# plot bandit rewards\n",
    "ax.plot(x, y, label=\"LinUCBBandit\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_mean_sweetest_reward, \n",
    "            color=\"r\", \n",
    "            linestyle=\"-\",\n",
    "            label=\"Sweetest baseline\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='center right', frameon=True)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"Bandit Training Mean Reward\")\n",
    "plt.xlabel(\"Training iterations\")\n",
    "\n",
    "plt.plot();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "linucb_algo.stop()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using a RL SlateQ algorithm on the environment <a class=\"anchor\" id=\"slateq\"></a>\n",
    "\n",
    "The RLlib team has implemented the Slate-Q algorithm from Google - Deep Q-Learning (DQN) designed for k-slate, long time horizon, and dynamic user recommendation problems.\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9f91de1fa0ac351ecb12e4062a37afb896aa1463.pdf\">Slate-Q Paper</a> <br>\n",
    "<a href=\"https://slideslive.com/38917655/reinforcement-learning-in-recommender-systems-some-challenges\">Author video</a> about Slate-Q <br>\n",
    "<a href=\"https://www.anyscale.com/blog/reinforcement-learning-with-deep-q-networks\">Anyscale tutorial blog explanation of Deep Q-Learning (DQN)</a>\n",
    "\n",
    "<img src=\"./images/slateq_with_DQN_equations.png\" width=\"100%\" />\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.slateq.slateq import SlateQConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.slateq.slateq.SlateQConfig at 0x7f16f45e1790>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SlateQConfig object\n",
    "slateq_config = SlateQConfig()\n",
    "\n",
    "# Setup our config object to use our environment\n",
    "slateq_config.environment(\n",
    "        env=\"modified-lts\", \n",
    "        env_config={\n",
    "            # The number of possible documents/videos/candidates that we can recommend\n",
    "            \"num_candidates\": 20,  \n",
    "            # The number of recommendations that we will be making\n",
    "            \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "            # Set to False for re-using the same candidate doecuments each timestep.\n",
    "            \"resample_documents\": True,\n",
    "            # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "            # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "            \"convert_to_discrete_action_space\": False,\n",
    "            # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "            # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
    "            \"wrap_for_bandits\": False,\n",
    "            # Use consistent seeds for the environment ...\n",
    "            \"seed\": 0,})\n",
    "\n",
    "# Decide if you want torch or tensorflow DL framework.  Default is \"tf\"\n",
    "slateq_config.framework(framework=\"torch\")\n",
    "\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "# seed the algorithm / policy\n",
    "slateq_config.debugging(seed=415, log_level=\"INFO\")\n",
    "\n",
    "# Setup evaluation\n",
    "# Explicitly set \"explore\"=False to override default\n",
    "slateq_config.evaluation(\n",
    "    # evaluation_interval=10, \n",
    "    # evaluation_duration=20, \n",
    "    # evaluation_duration_unit=\"timesteps\",\n",
    "    evaluation_config = {\"explore\" : False})\n",
    "\n",
    "# Setup sampling rollout workers\n",
    "# +1 for head node, num parallel workers or actors for rollouts\n",
    "slateq_config.rollouts(\n",
    "    num_rollout_workers=1,\n",
    "    num_envs_per_worker=1,)\n",
    "    # enable_connectors=True)\n",
    "\n",
    "slateq_config.resources(num_gpus=0)\n",
    "\n",
    "# Setup target network training\n",
    "slateq_config.training(target_network_update_freq=3200,)\n",
    "                       # lr=0.0003,\n",
    "                       # train_batch_size=32)\n",
    "\n",
    "# Setup exploration\n",
    "slateq_config.exploration(explore=True,\n",
    "                 exploration_config={\n",
    "                  # You could use Ray Tune to run 3 parallel tuning trials\n",
    "                  # epsilon_timesteps=tune.grid_search([30000, 60000, 3000]),  # default: 250000\n",
    "                  \"epsilon_timesteps\": 60000,  \n",
    "                 })\n",
    "\n",
    "# for increasing logging frequency in the terminal for this tutorial\n",
    "slateq_config.reporting(\n",
    "    min_train_timesteps_per_iteration=1,\n",
    "    metrics_num_episodes_for_smoothing=100) #200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed we have setup the Trainer correctly, let's call train() on it for one iteration to inspect a single result.\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # SINGLE .TRAIN() OUTPUT\n",
    "\n",
    "# # instantiate an algo instance\n",
    "# slateq_algo = slateq_config.build()\n",
    "# print(f\"Algorithm type: {type(slateq_algo)}\")\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Perform single `.train() iteration` call\n",
    "# # Result is a Python dict object\n",
    "# result = slateq_algo.train()\n",
    "\n",
    "# # train time\n",
    "# print(f\"Training took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# # Erase config dict from result (for better overview).\n",
    "# del result[\"hist_stats\"]\n",
    "# del result[\"config\"]\n",
    "# # Print out training iteration results.\n",
    "# print(pretty_print(result))\n",
    "\n",
    "# # Training took 168.93 seconds for 1 single iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed we have setup the Trainer correctly, let's call train() on it 30 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fresh in case ray already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-20 21:09:36 (running for 00:19:34.41)<br>Memory usage on this node: 9.1/31.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/13.63 GiB heap, 0.0/6.82 GiB objects<br>Current best trial: 55930_00000 with episode_reward_mean=1165.8204422369952 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'modified-lts', 'env_config': {'num_candidates': 20, 'slate_size': 2, 'resample_documents': True, 'convert_to_discrete_action_space': False, 'wrap_for_bandits': False, 'seed': 0}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 4, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.00025, 'train_batch_size': 32, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'SlateEpsilonGreedy', 'warmup_timesteps': 20000, 'epsilon_timesteps': 60000, 'final_epsilon': 0.01}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'modified-lts', 'env_config': {'num_candidates': 20, 'slate_size': 2, 'resample_documents': True, 'convert_to_discrete_action_space': False, 'wrap_for_bandits': False, 'seed': 0}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 4, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.00025, 'train_batch_size': 32, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': False, 'exploration_config': {'type': 'SlateEpsilonGreedy', 'warmup_timesteps': 20000, 'epsilon_timesteps': 60000, 'final_epsilon': 0.01}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 1, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'fcnet_hiddens_per_candidate': [256, 32], 'target_network_update_freq': 3200, 'tau': 1.0, 'use_huber': False, 'huber_threshold': 1.0, 'training_intensity': None, 'lr_schedule': None, 'lr_choice_model': 0.001, 'rmsprop_epsilon': 1e-05, 'grad_clip': None, 'n_step': 1, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'capacity': 100000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'learning_starts': 20000, 'replay_mode': 'independent'}, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x7f160c340ed0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 1, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': True, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'fcnet_hiddens_per_candidate': [256, 32], 'target_network_update_freq': 3200, 'tau': 1.0, 'use_huber': False, 'huber_threshold': 1.0, 'training_intensity': None, 'lr_schedule': None, 'lr_choice_model': 0.001, 'rmsprop_epsilon': 1e-05, 'grad_clip': None, 'n_step': 1, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'capacity': 100000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'learning_starts': 20000, 'replay_mode': 'independent'}, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x7f16f4416110>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}<br>Result logdir: /home/ray/Ray-Tutorial/ray-summit-2022-training/ray-rllib/results/slateq_recsim/SlateQ<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_wor...</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_modified-lts_55930_00000</td><td>TERMINATED</td><td>172.18.12.66:249606</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         1154.61</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\"> 1165.82</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">             1191.32</td><td style=\"text-align: right;\">             1142.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:11,976\tWARNING deprecation.py:48 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:11,977\tINFO simple_q.py:294 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,324\tWARNING env.py:143 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,380\tINFO policy.py:941 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,380\tINFO torch_policy.py:178 -- Found 0 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,439\tINFO worker_set.py:164 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict(user:Box([0.], [1.], (1,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), 10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), 12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), 14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), 16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), 18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)))), MultiDiscrete([20 20])), '__env__': (Dict(user:Box([0.], [1.], (1,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), 10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), 12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), 14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), 16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), 18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)))), MultiDiscrete([20 20]))}\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m   return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,482\tINFO policy.py:941 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,482\tINFO torch_policy.py:178 -- Found 0 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,515\tINFO rollout_worker.py:1802 -- Built policy map: {}\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,515\tINFO rollout_worker.py:1803 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,515\tINFO rollout_worker.py:654 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fafc41d7590>}\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,526\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,588\tWARNING deprecation.py:48 -- DeprecationWarning: `ReplayBuffer.add_batch()` has been deprecated. Use `ReplayBuffer.add()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:50:19,615\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0269 GB (100000.0 batches of size 1, 269 bytes each), available system memory is 33.673457664 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,541\tINFO rollout_worker.py:802 -- Generating sample batch of size 4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,544\tINFO sampler.py:677 -- Raw obs from env: { 0: { 'agent0': { 'doc': { '0': np.ndarray((1,), dtype=float32, min=0.159, max=0.159, mean=0.159),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '1': np.ndarray((1,), dtype=float32, min=0.11, max=0.11, mean=0.11),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '10': np.ndarray((1,), dtype=float32, min=0.976, max=0.976, mean=0.976),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '11': np.ndarray((1,), dtype=float32, min=0.469, max=0.469, mean=0.469),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '12': np.ndarray((1,), dtype=float32, min=0.977, max=0.977, mean=0.977),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '13': np.ndarray((1,), dtype=float32, min=0.605, max=0.605, mean=0.605),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '14': np.ndarray((1,), dtype=float32, min=0.739, max=0.739, mean=0.739),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '15': np.ndarray((1,), dtype=float32, min=0.039, max=0.039, mean=0.039),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '16': np.ndarray((1,), dtype=float32, min=0.283, max=0.283, mean=0.283),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '17': np.ndarray((1,), dtype=float32, min=0.12, max=0.12, mean=0.12),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '18': np.ndarray((1,), dtype=float32, min=0.296, max=0.296, mean=0.296),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '19': np.ndarray((1,), dtype=float32, min=0.119, max=0.119, mean=0.119),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '2': np.ndarray((1,), dtype=float32, min=0.656, max=0.656, mean=0.656),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '3': np.ndarray((1,), dtype=float32, min=0.138, max=0.138, mean=0.138),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '4': np.ndarray((1,), dtype=float32, min=0.197, max=0.197, mean=0.197),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '5': np.ndarray((1,), dtype=float32, min=0.369, max=0.369, mean=0.369),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '6': np.ndarray((1,), dtype=float32, min=0.821, max=0.821, mean=0.821),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '7': np.ndarray((1,), dtype=float32, min=0.097, max=0.097, mean=0.097),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '8': np.ndarray((1,), dtype=float32, min=0.838, max=0.838, mean=0.838),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                             '9': np.ndarray((1,), dtype=float32, min=0.096, max=0.096, mean=0.096)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                    'response': ( { 'click': 1,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                    'watch_time': np.ndarray((), dtype=float32, min=91.777, max=91.777, mean=91.777)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                  { 'click': 1,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                    'watch_time': np.ndarray((), dtype=float32, min=11.905, max=11.905, mean=11.905)}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                    'user': np.ndarray((1,), dtype=float64, min=0.505, max=0.505, mean=0.505)}}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,545\tINFO sampler.py:678 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,546\tINFO sampler.py:940 -- Filtered obs: { 'doc': { '0': np.ndarray((1,), dtype=float32, min=0.159, max=0.159, mean=0.159),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '1': np.ndarray((1,), dtype=float32, min=0.11, max=0.11, mean=0.11),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '10': np.ndarray((1,), dtype=float32, min=0.976, max=0.976, mean=0.976),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '11': np.ndarray((1,), dtype=float32, min=0.469, max=0.469, mean=0.469),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '12': np.ndarray((1,), dtype=float32, min=0.977, max=0.977, mean=0.977),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '13': np.ndarray((1,), dtype=float32, min=0.605, max=0.605, mean=0.605),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '14': np.ndarray((1,), dtype=float32, min=0.739, max=0.739, mean=0.739),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '15': np.ndarray((1,), dtype=float32, min=0.039, max=0.039, mean=0.039),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '16': np.ndarray((1,), dtype=float32, min=0.283, max=0.283, mean=0.283),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '17': np.ndarray((1,), dtype=float32, min=0.12, max=0.12, mean=0.12),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '18': np.ndarray((1,), dtype=float32, min=0.296, max=0.296, mean=0.296),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '19': np.ndarray((1,), dtype=float32, min=0.119, max=0.119, mean=0.119),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '2': np.ndarray((1,), dtype=float32, min=0.656, max=0.656, mean=0.656),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '3': np.ndarray((1,), dtype=float32, min=0.138, max=0.138, mean=0.138),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '4': np.ndarray((1,), dtype=float32, min=0.197, max=0.197, mean=0.197),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '5': np.ndarray((1,), dtype=float32, min=0.369, max=0.369, mean=0.369),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '6': np.ndarray((1,), dtype=float32, min=0.821, max=0.821, mean=0.821),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '7': np.ndarray((1,), dtype=float32, min=0.097, max=0.097, mean=0.097),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '8': np.ndarray((1,), dtype=float32, min=0.838, max=0.838, mean=0.838),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            '9': np.ndarray((1,), dtype=float32, min=0.096, max=0.096, mean=0.096)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'response': ( { 'click': 1,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                   'watch_time': np.ndarray((), dtype=float32, min=91.777, max=91.777, mean=91.777)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                 { 'click': 1,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                   'watch_time': np.ndarray((), dtype=float32, min=11.905, max=11.905, mean=11.905)}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'user': np.ndarray((1,), dtype=float64, min=0.505, max=0.505, mean=0.505)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,548\tINFO sampler.py:1187 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                   'obs': { 'doc': { '0': np.ndarray((1,), dtype=float32, min=0.159, max=0.159, mean=0.159),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '1': np.ndarray((1,), dtype=float32, min=0.11, max=0.11, mean=0.11),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '10': np.ndarray((1,), dtype=float32, min=0.976, max=0.976, mean=0.976),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '11': np.ndarray((1,), dtype=float32, min=0.469, max=0.469, mean=0.469),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '12': np.ndarray((1,), dtype=float32, min=0.977, max=0.977, mean=0.977),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '13': np.ndarray((1,), dtype=float32, min=0.605, max=0.605, mean=0.605),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '14': np.ndarray((1,), dtype=float32, min=0.739, max=0.739, mean=0.739),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '15': np.ndarray((1,), dtype=float32, min=0.039, max=0.039, mean=0.039),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '16': np.ndarray((1,), dtype=float32, min=0.283, max=0.283, mean=0.283),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '17': np.ndarray((1,), dtype=float32, min=0.12, max=0.12, mean=0.12),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '18': np.ndarray((1,), dtype=float32, min=0.296, max=0.296, mean=0.296),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '19': np.ndarray((1,), dtype=float32, min=0.119, max=0.119, mean=0.119),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '2': np.ndarray((1,), dtype=float32, min=0.656, max=0.656, mean=0.656),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '3': np.ndarray((1,), dtype=float32, min=0.138, max=0.138, mean=0.138),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '4': np.ndarray((1,), dtype=float32, min=0.197, max=0.197, mean=0.197),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '5': np.ndarray((1,), dtype=float32, min=0.369, max=0.369, mean=0.369),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '6': np.ndarray((1,), dtype=float32, min=0.821, max=0.821, mean=0.821),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '7': np.ndarray((1,), dtype=float32, min=0.097, max=0.097, mean=0.097),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '8': np.ndarray((1,), dtype=float32, min=0.838, max=0.838, mean=0.838),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                     '9': np.ndarray((1,), dtype=float32, min=0.096, max=0.096, mean=0.096)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                            'response': ( { 'click': 1,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                            'watch_time': np.ndarray((), dtype=float32, min=91.777, max=91.777, mean=91.777)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                          { 'click': 1,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                                            'watch_time': np.ndarray((), dtype=float32, min=11.905, max=11.905, mean=11.905)}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                            'user': np.ndarray((1,), dtype=float64, min=0.505, max=0.505, mean=0.505)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                   'prev_action': None,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                   'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         'type': '_PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,554\tINFO sampler.py:1215 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=int32, min=5.0, max=6.0, mean=5.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 380), dtype=float32, min=-0.037, max=0.061, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0)})}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,579\tINFO simple_list_collector.py:510 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m { 'agent0': { 'actions': np.ndarray((4, 2), dtype=int32, min=0.0, max=13.0, mean=5.875),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'agent_index': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'dones': np.ndarray((4,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'eps_id': np.ndarray((4,), dtype=int64, min=544577571.0, max=544577571.0, mean=544577571.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'infos': np.ndarray((4,), dtype=object, head={'env': <recsim.simulator.environment.SingleUserEnvironment object at 0x7fc853d97f10>}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'new_obs': { 'doc': { '0': np.ndarray((4, 1), dtype=float32, min=0.318, max=0.897, mean=0.654),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '1': np.ndarray((4, 1), dtype=float32, min=0.27, max=0.501, mean=0.388),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '10': np.ndarray((4, 1), dtype=float32, min=0.149, max=0.618, mean=0.383),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '11': np.ndarray((4, 1), dtype=float32, min=0.429, max=0.868, mean=0.703),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '12': np.ndarray((4, 1), dtype=float32, min=0.132, max=0.699, mean=0.282),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '13': np.ndarray((4, 1), dtype=float32, min=0.297, max=0.716, mean=0.482),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '14': np.ndarray((4, 1), dtype=float32, min=0.124, max=0.814, mean=0.449),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '15': np.ndarray((4, 1), dtype=float32, min=0.183, max=0.848, mean=0.505),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '16': np.ndarray((4, 1), dtype=float32, min=0.574, max=0.881, mean=0.712),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '17': np.ndarray((4, 1), dtype=float32, min=0.02, max=0.653, mean=0.456),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '18': np.ndarray((4, 1), dtype=float32, min=0.407, max=0.882, mean=0.692),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '19': np.ndarray((4, 1), dtype=float32, min=0.005, max=0.693, mean=0.299),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '2': np.ndarray((4, 1), dtype=float32, min=0.064, max=0.956, mean=0.548),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '3': np.ndarray((4, 1), dtype=float32, min=0.644, max=0.962, mean=0.798),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '4': np.ndarray((4, 1), dtype=float32, min=0.249, max=0.806, mean=0.511),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '5': np.ndarray((4, 1), dtype=float32, min=0.265, max=0.704, mean=0.538),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '6': np.ndarray((4, 1), dtype=float32, min=0.019, max=0.592, mean=0.309),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '7': np.ndarray((4, 1), dtype=float32, min=0.094, max=0.919, mean=0.472),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '8': np.ndarray((4, 1), dtype=float32, min=0.223, max=0.714, mean=0.543),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                     '9': np.ndarray((4, 1), dtype=float32, min=0.29, max=0.999, mean=0.793)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                            'response': ( { 'click': np.ndarray((4,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                            'watch_time': np.ndarray((4,), dtype=float32, min=7.433, max=9.015, mean=8.525)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                          { 'click': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                            'watch_time': np.ndarray((4,), dtype=float32, min=0.0, max=0.0, mean=0.0)}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                            'user': np.ndarray((4, 1), dtype=float32, min=0.501, max=0.512, mean=0.507)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'obs': { 'doc': { '0': np.ndarray((4, 1), dtype=float32, min=0.159, max=0.725, mean=0.47),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '1': np.ndarray((4, 1), dtype=float32, min=0.11, max=0.501, mean=0.324),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '10': np.ndarray((4, 1), dtype=float32, min=0.319, max=0.976, mean=0.59),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '11': np.ndarray((4, 1), dtype=float32, min=0.429, max=0.846, mean=0.603),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '12': np.ndarray((4, 1), dtype=float32, min=0.132, max=0.977, mean=0.486),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '13': np.ndarray((4, 1), dtype=float32, min=0.297, max=0.716, mean=0.479),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '14': np.ndarray((4, 1), dtype=float32, min=0.289, max=0.814, mean=0.603),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '15': np.ndarray((4, 1), dtype=float32, min=0.039, max=0.591, mean=0.302),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '16': np.ndarray((4, 1), dtype=float32, min=0.283, max=0.881, mean=0.581),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '17': np.ndarray((4, 1), dtype=float32, min=0.02, max=0.653, mean=0.344),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '18': np.ndarray((4, 1), dtype=float32, min=0.296, max=0.882, mean=0.665),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '19': np.ndarray((4, 1), dtype=float32, min=0.005, max=0.693, mean=0.312),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '2': np.ndarray((4, 1), dtype=float32, min=0.064, max=0.956, mean=0.603),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '3': np.ndarray((4, 1), dtype=float32, min=0.138, max=0.962, mean=0.609),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '4': np.ndarray((4, 1), dtype=float32, min=0.197, max=0.567, mean=0.359),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '5': np.ndarray((4, 1), dtype=float32, min=0.265, max=0.606, mean=0.454),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '6': np.ndarray((4, 1), dtype=float32, min=0.019, max=0.821, mean=0.489),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '7': np.ndarray((4, 1), dtype=float32, min=0.094, max=0.572, mean=0.266),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '8': np.ndarray((4, 1), dtype=float32, min=0.223, max=0.838, mean=0.574),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                 '9': np.ndarray((4, 1), dtype=float32, min=0.096, max=0.953, mean=0.567)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                        'response': ( { 'click': np.ndarray((4,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                        'watch_time': np.ndarray((4,), dtype=float32, min=7.433, max=91.777, mean=29.264)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                      { 'click': np.ndarray((4,), dtype=int64, min=0.0, max=1.0, mean=0.25),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                        'watch_time': np.ndarray((4,), dtype=float32, min=0.0, max=11.905, mean=2.976)}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                        'user': np.ndarray((4, 1), dtype=float32, min=0.501, max=0.508, mean=0.505)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'rewards': np.ndarray((4,), dtype=float32, min=7.433, max=9.015, mean=8.525),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               't': np.ndarray((4,), dtype=int64, min=0.0, max=3.0, mean=1.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m               'unroll_id': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m 2022-08-20 20:50:19,583\tINFO rollout_worker.py:838 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m { 'actions': np.ndarray((4, 2), dtype=int32, min=0.0, max=13.0, mean=5.875),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'agent_index': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'dones': np.ndarray((4,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'eps_id': np.ndarray((4,), dtype=int64, min=544577571.0, max=544577571.0, mean=544577571.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'infos': np.ndarray((4,), dtype=object, head={'env': <recsim.simulator.environment.SingleUserEnvironment object at 0x7fc853d97f10>}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'new_obs': { 'doc': { '0': np.ndarray((4, 1), dtype=float32, min=0.318, max=0.897, mean=0.654),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '1': np.ndarray((4, 1), dtype=float32, min=0.27, max=0.501, mean=0.388),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '10': np.ndarray((4, 1), dtype=float32, min=0.149, max=0.618, mean=0.383),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '11': np.ndarray((4, 1), dtype=float32, min=0.429, max=0.868, mean=0.703),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '12': np.ndarray((4, 1), dtype=float32, min=0.132, max=0.699, mean=0.282),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '13': np.ndarray((4, 1), dtype=float32, min=0.297, max=0.716, mean=0.482),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '14': np.ndarray((4, 1), dtype=float32, min=0.124, max=0.814, mean=0.449),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '15': np.ndarray((4, 1), dtype=float32, min=0.183, max=0.848, mean=0.505),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '16': np.ndarray((4, 1), dtype=float32, min=0.574, max=0.881, mean=0.712),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '17': np.ndarray((4, 1), dtype=float32, min=0.02, max=0.653, mean=0.456),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '18': np.ndarray((4, 1), dtype=float32, min=0.407, max=0.882, mean=0.692),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '19': np.ndarray((4, 1), dtype=float32, min=0.005, max=0.693, mean=0.299),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '2': np.ndarray((4, 1), dtype=float32, min=0.064, max=0.956, mean=0.548),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '3': np.ndarray((4, 1), dtype=float32, min=0.644, max=0.962, mean=0.798),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '4': np.ndarray((4, 1), dtype=float32, min=0.249, max=0.806, mean=0.511),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '5': np.ndarray((4, 1), dtype=float32, min=0.265, max=0.704, mean=0.538),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '6': np.ndarray((4, 1), dtype=float32, min=0.019, max=0.592, mean=0.309),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '7': np.ndarray((4, 1), dtype=float32, min=0.094, max=0.919, mean=0.472),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '8': np.ndarray((4, 1), dtype=float32, min=0.223, max=0.714, mean=0.543),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                         '9': np.ndarray((4, 1), dtype=float32, min=0.29, max=0.999, mean=0.793)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                'response': ( { 'click': np.ndarray((4,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                'watch_time': np.ndarray((4,), dtype=float32, min=7.433, max=9.015, mean=8.525)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                              { 'click': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                                'watch_time': np.ndarray((4,), dtype=float32, min=0.0, max=0.0, mean=0.0)}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                'user': np.ndarray((4, 1), dtype=float32, min=0.501, max=0.512, mean=0.507)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'obs': { 'doc': { '0': np.ndarray((4, 1), dtype=float32, min=0.159, max=0.725, mean=0.47),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '1': np.ndarray((4, 1), dtype=float32, min=0.11, max=0.501, mean=0.324),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '10': np.ndarray((4, 1), dtype=float32, min=0.319, max=0.976, mean=0.59),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '11': np.ndarray((4, 1), dtype=float32, min=0.429, max=0.846, mean=0.603),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '12': np.ndarray((4, 1), dtype=float32, min=0.132, max=0.977, mean=0.486),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '13': np.ndarray((4, 1), dtype=float32, min=0.297, max=0.716, mean=0.479),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '14': np.ndarray((4, 1), dtype=float32, min=0.289, max=0.814, mean=0.603),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '15': np.ndarray((4, 1), dtype=float32, min=0.039, max=0.591, mean=0.302),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '16': np.ndarray((4, 1), dtype=float32, min=0.283, max=0.881, mean=0.581),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '17': np.ndarray((4, 1), dtype=float32, min=0.02, max=0.653, mean=0.344),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '18': np.ndarray((4, 1), dtype=float32, min=0.296, max=0.882, mean=0.665),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '19': np.ndarray((4, 1), dtype=float32, min=0.005, max=0.693, mean=0.312),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '2': np.ndarray((4, 1), dtype=float32, min=0.064, max=0.956, mean=0.603),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '3': np.ndarray((4, 1), dtype=float32, min=0.138, max=0.962, mean=0.609),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '4': np.ndarray((4, 1), dtype=float32, min=0.197, max=0.567, mean=0.359),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '5': np.ndarray((4, 1), dtype=float32, min=0.265, max=0.606, mean=0.454),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '6': np.ndarray((4, 1), dtype=float32, min=0.019, max=0.821, mean=0.489),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '7': np.ndarray((4, 1), dtype=float32, min=0.094, max=0.572, mean=0.266),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '8': np.ndarray((4, 1), dtype=float32, min=0.223, max=0.838, mean=0.574),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                     '9': np.ndarray((4, 1), dtype=float32, min=0.096, max=0.953, mean=0.567)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            'response': ( { 'click': np.ndarray((4,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                            'watch_time': np.ndarray((4,), dtype=float32, min=7.433, max=91.777, mean=29.264)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                          { 'click': np.ndarray((4,), dtype=int64, min=0.0, max=1.0, mean=0.25),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m                            'watch_time': np.ndarray((4,), dtype=float32, min=0.0, max=11.905, mean=2.976)}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m            'user': np.ndarray((4, 1), dtype=float32, min=0.501, max=0.508, mean=0.505)},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'rewards': np.ndarray((4,), dtype=float32, min=7.433, max=9.015, mean=8.525),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   't': np.ndarray((4,), dtype=int64, min=0.0, max=3.0, mean=1.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m   'unroll_id': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=249697)\u001b[0m \n",
      "\u001b[2m\u001b[36m(SlateQ pid=249606)\u001b[0m 2022-08-20 20:53:07,509\tWARNING deprecation.py:48 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  counters:\n",
      "    last_target_update_ts: 20000\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 32\n",
      "    num_target_updates: 1\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1192.2370625634153\n",
      "  episode_reward_mean: 1158.3620514286292\n",
      "  episode_reward_min: 1124.7665796275885\n",
      "  episodes_this_iter: 166\n",
      "  episodes_total: 166\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 20000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 10.184002876281738\n",
      "          choice_loss: 1.0986121892929077\n",
      "          grad_gnorm: 12.331111907958984\n",
      "          mean_actions: 9.875\n",
      "          next_q_target_max: 0.08504226803779602\n",
      "          next_q_target_slate: 0.0009142349590547383\n",
      "          next_q_values: 0.0015279218787327409\n",
      "          q_clicked: 0.02682812511920929\n",
      "          q_loss: 109.21027374267578\n",
      "          q_values: 0.0015279218787327409\n",
      "          replay_click_q: 0.02682812511920929\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2476074695587158\n",
      "          slate_q_values: 0.012152591720223427\n",
      "          target: 10.268195152282715\n",
      "          target_clicked: 10.268195152282715\n",
      "        mean_td_error: 10.24136734008789\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [11.20054817199707, 14.890012741088867, 6.84792947769165, 10.241247177124023,\n",
      "          10.697001457214355, 9.197612762451172, 8.181028366088867, 9.563899993896484,\n",
      "          7.602230072021484, 8.65860366821289, 9.86634635925293, 10.763044357299805,\n",
      "          8.213456153869629, 12.192608833312988, 12.526931762695312, 7.12294340133667,\n",
      "          7.41319465637207, 10.138748168945312, 12.166614532470703, 10.603470802307129,\n",
      "          10.981623649597168, 11.11935043334961, 7.970088958740234, 9.257460594177246,\n",
      "          8.494342803955078, 11.323105812072754, 9.47072982788086, 10.903877258300781,\n",
      "          14.970022201538086, 10.727288246154785, 9.895734786987305, 14.522628784179688]\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 32\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 20000\n",
      "  num_env_steps_trained: 32\n",
      "  num_env_steps_trained_this_iter: 32\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 32\n",
      "  perf:\n",
      "    cpu_util_percent: 17.95504201680672\n",
      "    ram_util_percent: 20.657983193277303\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07368138978304993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.46973087556254534\n",
      "    mean_inference_ms: 3.8146622436868265\n",
      "    mean_raw_obs_processing_ms: 1.9719034819285644\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1192.2370625634153\n",
      "    episode_reward_mean: 1158.3620514286292\n",
      "    episode_reward_min: 1124.7665796275885\n",
      "    episodes_this_iter: 166\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1146.518507238592, 1165.0956172796775, 1179.4178404287336, 1147.5147049278844,\n",
      "        1168.1446026470887, 1174.0954674951222, 1164.3632690464244, 1138.466211720443,\n",
      "        1150.850637656586, 1134.7046768770385, 1164.317415069988, 1163.2103231885058,\n",
      "        1168.7133550153392, 1159.8633152760838, 1171.2937427825389, 1164.7257114798142,\n",
      "        1160.109852154277, 1167.2209728227501, 1150.0055854898974, 1171.4519866038904,\n",
      "        1146.2530306785334, 1168.2605907753257, 1154.9889460634708, 1166.6841759039266,\n",
      "        1150.2775064863238, 1160.3439644637735, 1147.327979252325, 1153.871326968745,\n",
      "        1145.2769089329606, 1169.0769190848753, 1159.6945203774158, 1151.6729619070827,\n",
      "        1151.3469195464809, 1124.7665796275885, 1161.441416533022, 1168.5818267753305,\n",
      "        1143.453309861732, 1162.4478603783796, 1162.5000173507042, 1169.5118317570768,\n",
      "        1165.5146438365082, 1158.8204262642528, 1150.702419986169, 1159.2168801076375,\n",
      "        1153.9140609577282, 1165.6886569595156, 1163.6375171798218, 1148.267968544193,\n",
      "        1154.9077577563412, 1174.2042689667346, 1151.0810288206947, 1131.4304239746036,\n",
      "        1168.8719870284199, 1158.1482430720312, 1136.4976516800418, 1149.153022760343,\n",
      "        1164.22389502251, 1155.2735924571373, 1151.9226677195454, 1141.3929767558993,\n",
      "        1147.801410184879, 1164.1702411722765, 1160.7813643685824, 1140.3040176627374,\n",
      "        1146.6667933561591, 1166.8164825125673, 1140.688121872114, 1152.1966671288233,\n",
      "        1164.2180212037465, 1157.2758498728274, 1147.7073001719573, 1144.7499004967062,\n",
      "        1161.4544666950396, 1159.0118221889827, 1146.2677405755974, 1155.3505958934857,\n",
      "        1144.091470648271, 1150.3965017764165, 1168.2587387890571, 1161.4534569199009,\n",
      "        1161.4438468402182, 1163.952970710218, 1160.8465875180902, 1129.8958324046298,\n",
      "        1174.0413360092566, 1140.5976064737165, 1164.028034279684, 1154.503489949859,\n",
      "        1136.7781071567551, 1147.0590657237951, 1172.1943517172642, 1186.4567924442993,\n",
      "        1147.8323992499884, 1149.1733357621304, 1159.4165965638033, 1139.8594842117157,\n",
      "        1166.5310325713117, 1156.4243308531825, 1164.8428539992196, 1135.0095524959377,\n",
      "        1165.385800344941, 1192.2370625634153, 1154.9204438102988, 1165.2919414997436,\n",
      "        1174.5764230206396, 1152.8899236148332, 1157.7186056935989, 1181.599543025826,\n",
      "        1170.2498395929365, 1164.8904627869856, 1175.8011071363633, 1174.515724997997,\n",
      "        1160.512611031629, 1166.1611112614526, 1155.0560705352318, 1152.715641083067,\n",
      "        1159.1349859563584, 1156.3723038330195, 1157.1029766358467, 1169.5429905880892,\n",
      "        1152.4853476928465, 1174.3602742641003, 1166.573436306293, 1167.6540783332298,\n",
      "        1162.768550636346, 1165.418703553838, 1158.5053125453057, 1148.396599384758,\n",
      "        1154.7583268000353, 1173.0105537337397, 1150.7207492570533, 1171.2986247447866,\n",
      "        1172.972268521348, 1140.218826815783, 1169.2559496329748, 1153.4782008219725,\n",
      "        1165.555784164654, 1175.3490066912873, 1168.5140368733912, 1160.552561306091,\n",
      "        1158.793833530738, 1154.767984399435, 1156.2638389591846, 1151.0717960753755,\n",
      "        1129.6501992832211, 1158.7279562522838, 1145.3691746298232, 1148.1098311338435,\n",
      "        1170.0869977346588, 1187.0482698841834, 1141.5364813776207, 1156.1390754863532,\n",
      "        1148.5036413532255, 1158.1779387658498, 1154.7465540481562, 1178.912916299883,\n",
      "        1154.9883919690078, 1157.4703867878054, 1171.245673604493, 1161.4057591796459,\n",
      "        1155.903163870535, 1164.3921068411369, 1165.7413533627173, 1171.7361662352237,\n",
      "        1159.1996365716934, 1148.3076002231874]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07368138978304993\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.46973087556254534\n",
      "      mean_inference_ms: 3.8146622436868265\n",
      "      mean_raw_obs_processing_ms: 1.9719034819285644\n",
      "  time_since_restore: 168.0737428665161\n",
      "  time_this_iter_s: 168.0737428665161\n",
      "  time_total_s: 168.0737428665161\n",
      "  timers:\n",
      "    learn_throughput: 1081.224\n",
      "    learn_time_ms: 29.596\n",
      "    load_throughput: 56465.178\n",
      "    load_time_ms: 0.567\n",
      "    synch_weights_time_ms: 4.045\n",
      "    training_iteration_time_ms: 39.605\n",
      "  timestamp: 1661053987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 1\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  counters:\n",
      "    last_target_update_ts: 20000\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 8032\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 8032\n",
      "    num_target_updates: 1\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1192.2370625634153\n",
      "  episode_reward_mean: 1159.2226382643462\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 175\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 20000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 12.63912582397461\n",
      "          choice_loss: 1.011768102645874\n",
      "          grad_gnorm: 13.337262153625488\n",
      "          mean_actions: 9.546875\n",
      "          next_q_target_max: 0.12407658994197845\n",
      "          next_q_target_slate: 0.0213877335190773\n",
      "          next_q_values: 0.03818820044398308\n",
      "          q_clicked: 2.3998348712921143\n",
      "          q_loss: 282.6170959472656\n",
      "          q_values: 0.6602727770805359\n",
      "          replay_click_q: 2.3998348712921143\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2578532695770264\n",
      "          slate_q_values: 1.7340868711471558\n",
      "          target: 12.761959075927734\n",
      "          target_clicked: 12.761959075927734\n",
      "        mean_td_error: 10.433403968811035\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [9.261489868164062, 11.143708229064941, 12.429916381835938, 8.535090446472168,\n",
      "          9.05744457244873, 8.947615623474121, 12.958941459655762, 11.8149995803833,\n",
      "          9.693572998046875, 12.791486740112305, 0.7913379669189453, 10.790786743164062,\n",
      "          80.56051635742188, 12.816694259643555, 1.1242179870605469, 3.4048871994018555,\n",
      "          0.3491086959838867, 4.45932674407959, 0.6072673797607422, 7.644071102142334,\n",
      "          9.70340347290039, 11.814009666442871, 2.3800439834594727, 9.686595916748047,\n",
      "          7.194888591766357, 7.941370010375977, 4.4545817375183105, 12.604747772216797,\n",
      "          9.011350631713867, 7.4101338386535645, 10.721019744873047, 11.764274597167969]\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 8032\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 8032\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 21000\n",
      "  num_agent_steps_trained: 8032\n",
      "  num_env_steps_sampled: 21000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 8032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.55813953488372\n",
      "    ram_util_percent: 22.837209302325583\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07371756554447174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.46986731524477804\n",
      "    mean_inference_ms: 3.8158357077860945\n",
      "    mean_raw_obs_processing_ms: 1.9721533797069926\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1192.2370625634153\n",
      "    episode_reward_mean: 1159.2226382643462\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1155.3505958934857, 1144.091470648271, 1150.3965017764165, 1168.2587387890571,\n",
      "        1161.4534569199009, 1161.4438468402182, 1163.952970710218, 1160.8465875180902,\n",
      "        1129.8958324046298, 1174.0413360092566, 1140.5976064737165, 1164.028034279684,\n",
      "        1154.503489949859, 1136.7781071567551, 1147.0590657237951, 1172.1943517172642,\n",
      "        1186.4567924442993, 1147.8323992499884, 1149.1733357621304, 1159.4165965638033,\n",
      "        1139.8594842117157, 1166.5310325713117, 1156.4243308531825, 1164.8428539992196,\n",
      "        1135.0095524959377, 1165.385800344941, 1192.2370625634153, 1154.9204438102988,\n",
      "        1165.2919414997436, 1174.5764230206396, 1152.8899236148332, 1157.7186056935989,\n",
      "        1181.599543025826, 1170.2498395929365, 1164.8904627869856, 1175.8011071363633,\n",
      "        1174.515724997997, 1160.512611031629, 1166.1611112614526, 1155.0560705352318,\n",
      "        1152.715641083067, 1159.1349859563584, 1156.3723038330195, 1157.1029766358467,\n",
      "        1169.5429905880892, 1152.4853476928465, 1174.3602742641003, 1166.573436306293,\n",
      "        1167.6540783332298, 1162.768550636346, 1165.418703553838, 1158.5053125453057,\n",
      "        1148.396599384758, 1154.7583268000353, 1173.0105537337397, 1150.7207492570533,\n",
      "        1171.2986247447866, 1172.972268521348, 1140.218826815783, 1169.2559496329748,\n",
      "        1153.4782008219725, 1165.555784164654, 1175.3490066912873, 1168.5140368733912,\n",
      "        1160.552561306091, 1158.793833530738, 1154.767984399435, 1156.2638389591846,\n",
      "        1151.0717960753755, 1129.6501992832211, 1158.7279562522838, 1145.3691746298232,\n",
      "        1148.1098311338435, 1170.0869977346588, 1187.0482698841834, 1141.5364813776207,\n",
      "        1156.1390754863532, 1148.5036413532255, 1158.1779387658498, 1154.7465540481562,\n",
      "        1178.912916299883, 1154.9883919690078, 1157.4703867878054, 1171.245673604493,\n",
      "        1161.4057591796459, 1155.903163870535, 1164.3921068411369, 1165.7413533627173,\n",
      "        1171.7361662352237, 1159.1996365716934, 1148.3076002231874, 1150.1153172047273,\n",
      "        1144.7809976227927, 1163.0801368007358, 1170.736368582094, 1146.5150176683303,\n",
      "        1141.8127559632514, 1143.1617450143522, 1139.5624772518045, 1165.2411504129357]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07371756554447174\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.46986731524477804\n",
      "      mean_inference_ms: 3.8158357077860945\n",
      "      mean_raw_obs_processing_ms: 1.9721533797069926\n",
      "  time_since_restore: 198.0816080570221\n",
      "  time_this_iter_s: 30.00786519050598\n",
      "  time_total_s: 198.0816080570221\n",
      "  timers:\n",
      "    learn_throughput: 1009.122\n",
      "    learn_time_ms: 31.711\n",
      "    load_throughput: 52679.852\n",
      "    load_time_ms: 0.607\n",
      "    synch_weights_time_ms: 3.82\n",
      "    training_iteration_time_ms: 121.709\n",
      "  timestamp: 1661054017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 2\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  counters:\n",
      "    last_target_update_ts: 20000\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 16032\n",
      "    num_env_steps_sampled: 22000\n",
      "    num_env_steps_trained: 16032\n",
      "    num_target_updates: 1\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-54-09\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1192.2370625634153\n",
      "  episode_reward_mean: 1159.483861259135\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 183\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 20000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 11.065163612365723\n",
      "          choice_loss: 0.9392691850662231\n",
      "          grad_gnorm: 25.963947296142578\n",
      "          mean_actions: 9.4375\n",
      "          next_q_target_max: 0.12639263272285461\n",
      "          next_q_target_slate: 0.021784016862511635\n",
      "          next_q_values: 0.03926672041416168\n",
      "          q_clicked: 1.4835097789764404\n",
      "          q_loss: 158.66917419433594\n",
      "          q_values: 0.7938125133514404\n",
      "          replay_click_q: 1.4835097789764404\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2528836727142334\n",
      "          slate_q_values: 1.5866179466247559\n",
      "          target: 11.190292358398438\n",
      "          target_clicked: 11.190292358398438\n",
      "        mean_td_error: 10.044279098510742\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [11.923771858215332, 8.035351753234863, 8.083907127380371, 8.464494705200195,\n",
      "          4.31952428817749, 1.0804357528686523, 8.976914405822754, 8.684403419494629,\n",
      "          7.830506801605225, 12.45758056640625, 10.294675827026367, 8.48408317565918,\n",
      "          4.751762390136719, 9.418177604675293, 8.0579252243042, 8.228574752807617,\n",
      "          50.268394470214844, 12.634224891662598, 5.898635387420654, 10.082836151123047,\n",
      "          7.725273609161377, 11.078115463256836, 9.6632719039917, 7.71389102935791,\n",
      "          9.017905235290527, 10.011641502380371, 8.014092445373535, 8.19395923614502,\n",
      "          10.70680046081543, 9.807901382446289, 12.50422477722168, 9.003694534301758]\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 16032\n",
      "    num_env_steps_sampled: 22000\n",
      "    num_env_steps_trained: 16032\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 22000\n",
      "  num_agent_steps_trained: 16032\n",
      "  num_env_steps_sampled: 22000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 16032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.24090909090909\n",
      "    ram_util_percent: 23.077272727272728\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07377436354859057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47007764099936156\n",
      "    mean_inference_ms: 3.818029080503356\n",
      "    mean_raw_obs_processing_ms: 1.9725105939826566\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1192.2370625634153\n",
      "    episode_reward_mean: 1159.483861259135\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1129.8958324046298, 1174.0413360092566, 1140.5976064737165, 1164.028034279684,\n",
      "        1154.503489949859, 1136.7781071567551, 1147.0590657237951, 1172.1943517172642,\n",
      "        1186.4567924442993, 1147.8323992499884, 1149.1733357621304, 1159.4165965638033,\n",
      "        1139.8594842117157, 1166.5310325713117, 1156.4243308531825, 1164.8428539992196,\n",
      "        1135.0095524959377, 1165.385800344941, 1192.2370625634153, 1154.9204438102988,\n",
      "        1165.2919414997436, 1174.5764230206396, 1152.8899236148332, 1157.7186056935989,\n",
      "        1181.599543025826, 1170.2498395929365, 1164.8904627869856, 1175.8011071363633,\n",
      "        1174.515724997997, 1160.512611031629, 1166.1611112614526, 1155.0560705352318,\n",
      "        1152.715641083067, 1159.1349859563584, 1156.3723038330195, 1157.1029766358467,\n",
      "        1169.5429905880892, 1152.4853476928465, 1174.3602742641003, 1166.573436306293,\n",
      "        1167.6540783332298, 1162.768550636346, 1165.418703553838, 1158.5053125453057,\n",
      "        1148.396599384758, 1154.7583268000353, 1173.0105537337397, 1150.7207492570533,\n",
      "        1171.2986247447866, 1172.972268521348, 1140.218826815783, 1169.2559496329748,\n",
      "        1153.4782008219725, 1165.555784164654, 1175.3490066912873, 1168.5140368733912,\n",
      "        1160.552561306091, 1158.793833530738, 1154.767984399435, 1156.2638389591846,\n",
      "        1151.0717960753755, 1129.6501992832211, 1158.7279562522838, 1145.3691746298232,\n",
      "        1148.1098311338435, 1170.0869977346588, 1187.0482698841834, 1141.5364813776207,\n",
      "        1156.1390754863532, 1148.5036413532255, 1158.1779387658498, 1154.7465540481562,\n",
      "        1178.912916299883, 1154.9883919690078, 1157.4703867878054, 1171.245673604493,\n",
      "        1161.4057591796459, 1155.903163870535, 1164.3921068411369, 1165.7413533627173,\n",
      "        1171.7361662352237, 1159.1996365716934, 1148.3076002231874, 1150.1153172047273,\n",
      "        1144.7809976227927, 1163.0801368007358, 1170.736368582094, 1146.5150176683303,\n",
      "        1141.8127559632514, 1143.1617450143522, 1139.5624772518045, 1165.2411504129357,\n",
      "        1163.7297633389403, 1152.5527430242103, 1174.3053856110491, 1159.0199610283596,\n",
      "        1148.6487276912617, 1169.085267853609, 1162.0806462066228, 1162.493973820481]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07377436354859057\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47007764099936156\n",
      "      mean_inference_ms: 3.818029080503356\n",
      "      mean_raw_obs_processing_ms: 1.9725105939826566\n",
      "  time_since_restore: 229.45788884162903\n",
      "  time_this_iter_s: 31.376280784606934\n",
      "  time_total_s: 229.45788884162903\n",
      "  timers:\n",
      "    learn_throughput: 853.274\n",
      "    learn_time_ms: 37.503\n",
      "    load_throughput: 51269.234\n",
      "    load_time_ms: 0.624\n",
      "    synch_weights_time_ms: 3.885\n",
      "    training_iteration_time_ms: 131.25\n",
      "  timestamp: 1661054049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 3\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  counters:\n",
      "    last_target_update_ts: 20000\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 24032\n",
      "    num_env_steps_sampled: 23000\n",
      "    num_env_steps_trained: 24032\n",
      "    num_target_updates: 1\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-54-41\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1192.2370625634153\n",
      "  episode_reward_mean: 1160.2342988496234\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 191\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 20000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.789778709411621\n",
      "          choice_loss: 0.9040689468383789\n",
      "          grad_gnorm: 6.496415615081787\n",
      "          mean_actions: 9.875\n",
      "          next_q_target_max: 0.11489363759756088\n",
      "          next_q_target_slate: 0.020127344876527786\n",
      "          next_q_values: 0.036352675408124924\n",
      "          q_clicked: 3.171496868133545\n",
      "          q_loss: 63.78840255737305\n",
      "          q_values: 0.9497378468513489\n",
      "          replay_click_q: 3.171496868133545\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2563608884811401\n",
      "          slate_q_values: 2.0817995071411133\n",
      "          target: 9.903523445129395\n",
      "          target_clicked: 9.903523445129395\n",
      "        mean_td_error: 7.067728519439697\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [7.6097025871276855, 2.166393280029297, 12.694096565246582, 6.797048568725586,\n",
      "          9.676448822021484, 9.898174285888672, 9.999433517456055, 12.912371635437012,\n",
      "          1.0500097274780273, 11.360916137695312, 8.886396408081055, 8.476643562316895,\n",
      "          1.4490118026733398, 0.43394947052001953, 9.347335815429688, 8.552629470825195,\n",
      "          2.742727279663086, 2.6285033226013184, 9.295729637145996, 10.38274097442627,\n",
      "          6.668235778808594, 0.2156848907470703, 8.555283546447754, 6.597139835357666,\n",
      "          7.393061637878418, 10.344880104064941, 10.159655570983887, 6.719049453735352,\n",
      "          6.784884452819824, 0.09449386596679688, 5.816222190856934, 10.458455085754395]\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 24032\n",
      "    num_env_steps_sampled: 23000\n",
      "    num_env_steps_trained: 24032\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 23000\n",
      "  num_agent_steps_trained: 24032\n",
      "  num_env_steps_sampled: 23000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 24032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.397872340425533\n",
      "    ram_util_percent: 23.300000000000004\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07385499574084824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47036953452343744\n",
      "    mean_inference_ms: 3.8217259026764467\n",
      "    mean_raw_obs_processing_ms: 1.9729719126439027\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1192.2370625634153\n",
      "    episode_reward_mean: 1160.2342988496234\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1186.4567924442993, 1147.8323992499884, 1149.1733357621304, 1159.4165965638033,\n",
      "        1139.8594842117157, 1166.5310325713117, 1156.4243308531825, 1164.8428539992196,\n",
      "        1135.0095524959377, 1165.385800344941, 1192.2370625634153, 1154.9204438102988,\n",
      "        1165.2919414997436, 1174.5764230206396, 1152.8899236148332, 1157.7186056935989,\n",
      "        1181.599543025826, 1170.2498395929365, 1164.8904627869856, 1175.8011071363633,\n",
      "        1174.515724997997, 1160.512611031629, 1166.1611112614526, 1155.0560705352318,\n",
      "        1152.715641083067, 1159.1349859563584, 1156.3723038330195, 1157.1029766358467,\n",
      "        1169.5429905880892, 1152.4853476928465, 1174.3602742641003, 1166.573436306293,\n",
      "        1167.6540783332298, 1162.768550636346, 1165.418703553838, 1158.5053125453057,\n",
      "        1148.396599384758, 1154.7583268000353, 1173.0105537337397, 1150.7207492570533,\n",
      "        1171.2986247447866, 1172.972268521348, 1140.218826815783, 1169.2559496329748,\n",
      "        1153.4782008219725, 1165.555784164654, 1175.3490066912873, 1168.5140368733912,\n",
      "        1160.552561306091, 1158.793833530738, 1154.767984399435, 1156.2638389591846,\n",
      "        1151.0717960753755, 1129.6501992832211, 1158.7279562522838, 1145.3691746298232,\n",
      "        1148.1098311338435, 1170.0869977346588, 1187.0482698841834, 1141.5364813776207,\n",
      "        1156.1390754863532, 1148.5036413532255, 1158.1779387658498, 1154.7465540481562,\n",
      "        1178.912916299883, 1154.9883919690078, 1157.4703867878054, 1171.245673604493,\n",
      "        1161.4057591796459, 1155.903163870535, 1164.3921068411369, 1165.7413533627173,\n",
      "        1171.7361662352237, 1159.1996365716934, 1148.3076002231874, 1150.1153172047273,\n",
      "        1144.7809976227927, 1163.0801368007358, 1170.736368582094, 1146.5150176683303,\n",
      "        1141.8127559632514, 1143.1617450143522, 1139.5624772518045, 1165.2411504129357,\n",
      "        1163.7297633389403, 1152.5527430242103, 1174.3053856110491, 1159.0199610283596,\n",
      "        1148.6487276912617, 1169.085267853609, 1162.0806462066228, 1162.493973820481,\n",
      "        1150.808373891986, 1155.2434592344266, 1166.9304253850923, 1157.0256505266002,\n",
      "        1168.0605158969788, 1177.6709912251386, 1148.766721409387, 1169.6354451942034]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07385499574084824\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47036953452343744\n",
      "      mean_inference_ms: 3.8217259026764467\n",
      "      mean_raw_obs_processing_ms: 1.9729719126439027\n",
      "  time_since_restore: 262.24792551994324\n",
      "  time_this_iter_s: 32.79003667831421\n",
      "  time_total_s: 262.24792551994324\n",
      "  timers:\n",
      "    learn_throughput: 811.462\n",
      "    learn_time_ms: 39.435\n",
      "    load_throughput: 51841.533\n",
      "    load_time_ms: 0.617\n",
      "    synch_weights_time_ms: 3.828\n",
      "    training_iteration_time_ms: 130.799\n",
      "  timestamp: 1661054081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 4\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    last_target_update_ts: 23200\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 32032\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 32032\n",
      "    num_target_updates: 2\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-55-17\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1192.2370625634153\n",
      "  episode_reward_mean: 1160.3296722232685\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 200\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 23200\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 12.191072463989258\n",
      "          choice_loss: 0.8716235756874084\n",
      "          grad_gnorm: 15.941347122192383\n",
      "          mean_actions: 10.203125\n",
      "          next_q_target_max: 3.7835776805877686\n",
      "          next_q_target_slate: 0.5840131044387817\n",
      "          next_q_values: 1.0324310064315796\n",
      "          q_clicked: 1.7450706958770752\n",
      "          q_loss: 423.01641845703125\n",
      "          q_values: 1.4128315448760986\n",
      "          replay_click_q: 1.7450706958770752\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2456560134887695\n",
      "          slate_q_values: 1.721737027168274\n",
      "          target: 15.936814308166504\n",
      "          target_clicked: 15.936814308166504\n",
      "        mean_td_error: 14.205438613891602\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [17.06346321105957, 11.952621459960938, 9.341789245605469, 13.40300464630127,\n",
      "          6.100309371948242, 3.5249385833740234, 14.377036094665527, 13.425211906433105,\n",
      "          11.529555320739746, 12.957045555114746, 17.381343841552734, 17.105945587158203,\n",
      "          11.344240188598633, 94.62805938720703, 13.045331001281738, 13.410605430603027,\n",
      "          8.9860258102417, 11.15681266784668, 10.767179489135742, 10.629737854003906,\n",
      "          13.982406616210938, 9.916268348693848, 15.448227882385254, 12.031871795654297,\n",
      "          10.126415252685547, 13.751463890075684, 11.709383010864258, 0.2191295623779297,\n",
      "          11.388611793518066, 9.367236137390137, 14.74581527709961, 9.756954193115234]\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 32032\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 32032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 32032\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 32032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.283673469387754\n",
      "    ram_util_percent: 23.51428571428571\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07397113270545673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4708042163393988\n",
      "    mean_inference_ms: 3.828088792715938\n",
      "    mean_raw_obs_processing_ms: 1.9736233478940506\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1192.2370625634153\n",
      "    episode_reward_mean: 1160.3296722232685\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1165.385800344941, 1192.2370625634153, 1154.9204438102988, 1165.2919414997436,\n",
      "        1174.5764230206396, 1152.8899236148332, 1157.7186056935989, 1181.599543025826,\n",
      "        1170.2498395929365, 1164.8904627869856, 1175.8011071363633, 1174.515724997997,\n",
      "        1160.512611031629, 1166.1611112614526, 1155.0560705352318, 1152.715641083067,\n",
      "        1159.1349859563584, 1156.3723038330195, 1157.1029766358467, 1169.5429905880892,\n",
      "        1152.4853476928465, 1174.3602742641003, 1166.573436306293, 1167.6540783332298,\n",
      "        1162.768550636346, 1165.418703553838, 1158.5053125453057, 1148.396599384758,\n",
      "        1154.7583268000353, 1173.0105537337397, 1150.7207492570533, 1171.2986247447866,\n",
      "        1172.972268521348, 1140.218826815783, 1169.2559496329748, 1153.4782008219725,\n",
      "        1165.555784164654, 1175.3490066912873, 1168.5140368733912, 1160.552561306091,\n",
      "        1158.793833530738, 1154.767984399435, 1156.2638389591846, 1151.0717960753755,\n",
      "        1129.6501992832211, 1158.7279562522838, 1145.3691746298232, 1148.1098311338435,\n",
      "        1170.0869977346588, 1187.0482698841834, 1141.5364813776207, 1156.1390754863532,\n",
      "        1148.5036413532255, 1158.1779387658498, 1154.7465540481562, 1178.912916299883,\n",
      "        1154.9883919690078, 1157.4703867878054, 1171.245673604493, 1161.4057591796459,\n",
      "        1155.903163870535, 1164.3921068411369, 1165.7413533627173, 1171.7361662352237,\n",
      "        1159.1996365716934, 1148.3076002231874, 1150.1153172047273, 1144.7809976227927,\n",
      "        1163.0801368007358, 1170.736368582094, 1146.5150176683303, 1141.8127559632514,\n",
      "        1143.1617450143522, 1139.5624772518045, 1165.2411504129357, 1163.7297633389403,\n",
      "        1152.5527430242103, 1174.3053856110491, 1159.0199610283596, 1148.6487276912617,\n",
      "        1169.085267853609, 1162.0806462066228, 1162.493973820481, 1150.808373891986,\n",
      "        1155.2434592344266, 1166.9304253850923, 1157.0256505266002, 1168.0605158969788,\n",
      "        1177.6709912251386, 1148.766721409387, 1169.6354451942034, 1170.6262338870927,\n",
      "        1146.421511206209, 1156.0785764084944, 1163.8704043976702, 1153.3209612431924,\n",
      "        1151.8499060395345, 1175.4501445243802, 1150.4085823017904, 1147.0573955077214]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07397113270545673\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4708042163393988\n",
      "      mean_inference_ms: 3.828088792715938\n",
      "      mean_raw_obs_processing_ms: 1.9736233478940506\n",
      "  time_since_restore: 297.41472721099854\n",
      "  time_this_iter_s: 35.1668016910553\n",
      "  time_total_s: 297.41472721099854\n",
      "  timers:\n",
      "    learn_throughput: 727.525\n",
      "    learn_time_ms: 43.985\n",
      "    load_throughput: 51749.587\n",
      "    load_time_ms: 0.618\n",
      "    synch_weights_time_ms: 3.889\n",
      "    training_iteration_time_ms: 139.902\n",
      "  timestamp: 1661054117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 5\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  counters:\n",
      "    last_target_update_ts: 23200\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 40032\n",
      "    num_env_steps_sampled: 25000\n",
      "    num_env_steps_trained: 40032\n",
      "    num_target_updates: 2\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.0482698841834\n",
      "  episode_reward_mean: 1159.0987817725943\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 208\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 23200\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 12.500337600708008\n",
      "          choice_loss: 0.8112215995788574\n",
      "          grad_gnorm: 69.02080535888672\n",
      "          mean_actions: 10.421875\n",
      "          next_q_target_max: 3.545844554901123\n",
      "          next_q_target_slate: 0.5609897971153259\n",
      "          next_q_values: 1.0025426149368286\n",
      "          q_clicked: 3.0437283515930176\n",
      "          q_loss: 328.5265197753906\n",
      "          q_values: 1.8073453903198242\n",
      "          replay_click_q: 3.0437283515930176\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2570425271987915\n",
      "          slate_q_values: 3.053826093673706\n",
      "          target: 15.801730155944824\n",
      "          target_clicked: 15.801730155944824\n",
      "        mean_td_error: 12.889183044433594\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [12.025050163269043, 13.680577278137207, 13.346054077148438, 10.56189250946045,\n",
      "          10.298654556274414, 12.966821670532227, 12.347755432128906, 9.035545349121094,\n",
      "          11.295153617858887, 13.645249366760254, 13.779891014099121, 7.4600138664245605,\n",
      "          0.4651823043823242, 81.08222961425781, 14.072652816772461, 12.172935485839844,\n",
      "          11.689918518066406, 3.6402482986450195, 13.69937801361084, 5.570635795593262,\n",
      "          11.159080505371094, 12.332429885864258, 13.381486892700195, 9.492460250854492,\n",
      "          11.107291221618652, 6.098838806152344, 14.21636962890625, 12.063344955444336,\n",
      "          12.504257202148438, 2.0988779067993164, 11.34135627746582, 13.822205543518066]\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 40032\n",
      "    num_env_steps_sampled: 25000\n",
      "    num_env_steps_trained: 40032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 25000\n",
      "  num_agent_steps_trained: 40032\n",
      "  num_env_steps_sampled: 25000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 40032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.089795918367347\n",
      "    ram_util_percent: 23.73469387755102\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07409664897122652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47127256502833526\n",
      "    mean_inference_ms: 3.836572518204014\n",
      "    mean_raw_obs_processing_ms: 1.9743207272946095\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.0482698841834\n",
      "    episode_reward_mean: 1159.0987817725943\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1170.2498395929365, 1164.8904627869856, 1175.8011071363633, 1174.515724997997,\n",
      "        1160.512611031629, 1166.1611112614526, 1155.0560705352318, 1152.715641083067,\n",
      "        1159.1349859563584, 1156.3723038330195, 1157.1029766358467, 1169.5429905880892,\n",
      "        1152.4853476928465, 1174.3602742641003, 1166.573436306293, 1167.6540783332298,\n",
      "        1162.768550636346, 1165.418703553838, 1158.5053125453057, 1148.396599384758,\n",
      "        1154.7583268000353, 1173.0105537337397, 1150.7207492570533, 1171.2986247447866,\n",
      "        1172.972268521348, 1140.218826815783, 1169.2559496329748, 1153.4782008219725,\n",
      "        1165.555784164654, 1175.3490066912873, 1168.5140368733912, 1160.552561306091,\n",
      "        1158.793833530738, 1154.767984399435, 1156.2638389591846, 1151.0717960753755,\n",
      "        1129.6501992832211, 1158.7279562522838, 1145.3691746298232, 1148.1098311338435,\n",
      "        1170.0869977346588, 1187.0482698841834, 1141.5364813776207, 1156.1390754863532,\n",
      "        1148.5036413532255, 1158.1779387658498, 1154.7465540481562, 1178.912916299883,\n",
      "        1154.9883919690078, 1157.4703867878054, 1171.245673604493, 1161.4057591796459,\n",
      "        1155.903163870535, 1164.3921068411369, 1165.7413533627173, 1171.7361662352237,\n",
      "        1159.1996365716934, 1148.3076002231874, 1150.1153172047273, 1144.7809976227927,\n",
      "        1163.0801368007358, 1170.736368582094, 1146.5150176683303, 1141.8127559632514,\n",
      "        1143.1617450143522, 1139.5624772518045, 1165.2411504129357, 1163.7297633389403,\n",
      "        1152.5527430242103, 1174.3053856110491, 1159.0199610283596, 1148.6487276912617,\n",
      "        1169.085267853609, 1162.0806462066228, 1162.493973820481, 1150.808373891986,\n",
      "        1155.2434592344266, 1166.9304253850923, 1157.0256505266002, 1168.0605158969788,\n",
      "        1177.6709912251386, 1148.766721409387, 1169.6354451942034, 1170.6262338870927,\n",
      "        1146.421511206209, 1156.0785764084944, 1163.8704043976702, 1153.3209612431924,\n",
      "        1151.8499060395345, 1175.4501445243802, 1150.4085823017904, 1147.0573955077214,\n",
      "        1154.2179070424274, 1141.6683838742279, 1152.3057675938212, 1141.7168694080988,\n",
      "        1160.0590250434752, 1148.0518592108374, 1155.1464241877188, 1168.3644621452552]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07409664897122652\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47127256502833526\n",
      "      mean_inference_ms: 3.836572518204014\n",
      "      mean_raw_obs_processing_ms: 1.9743207272946095\n",
      "  time_since_restore: 332.0153362751007\n",
      "  time_this_iter_s: 34.60060906410217\n",
      "  time_total_s: 332.0153362751007\n",
      "  timers:\n",
      "    learn_throughput: 746.419\n",
      "    learn_time_ms: 42.871\n",
      "    load_throughput: 50759.295\n",
      "    load_time_ms: 0.63\n",
      "    synch_weights_time_ms: 3.647\n",
      "    training_iteration_time_ms: 135.237\n",
      "  timestamp: 1661054151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 6\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  counters:\n",
      "    last_target_update_ts: 23200\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 48032\n",
      "    num_env_steps_sampled: 26000\n",
      "    num_env_steps_trained: 48032\n",
      "    num_target_updates: 2\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-56-26\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.0482698841834\n",
      "  episode_reward_mean: 1158.8258187370668\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 216\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 23200\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 10.229642868041992\n",
      "          choice_loss: 0.8260022401809692\n",
      "          grad_gnorm: 9.941327095031738\n",
      "          mean_actions: 9.140625\n",
      "          next_q_target_max: 3.5637412071228027\n",
      "          next_q_target_slate: 0.5606240630149841\n",
      "          next_q_values: 1.0164673328399658\n",
      "          q_clicked: 1.703131079673767\n",
      "          q_loss: 157.5757293701172\n",
      "          q_values: 2.224548816680908\n",
      "          replay_click_q: 1.703131079673767\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2506822347640991\n",
      "          slate_q_values: 2.8409385681152344\n",
      "          target: 13.757746696472168\n",
      "          target_clicked: 13.757746696472168\n",
      "        mean_td_error: 12.05461597442627\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [11.590078353881836, 11.4166898727417, 13.65442943572998, 14.998395919799805,\n",
      "          13.6449556350708, 15.212841987609863, 10.12725830078125, 15.184246063232422,\n",
      "          11.413041114807129, 14.171196937561035, 13.207633018493652, 11.180110931396484,\n",
      "          17.200626373291016, 12.045137405395508, 15.519002914428711, 14.131037712097168,\n",
      "          16.49091339111328, 11.285902976989746, 11.234663963317871, 6.545812606811523,\n",
      "          12.22259521484375, 12.100451469421387, 15.076532363891602, 10.208697319030762,\n",
      "          12.832152366638184, 0.9817600250244141, 11.509596824645996, 1.5518531799316406,\n",
      "          11.614910125732422, 10.952688217163086, 14.432384490966797, 12.01010513305664]\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 48032\n",
      "    num_env_steps_sampled: 26000\n",
      "    num_env_steps_trained: 48032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 26000\n",
      "  num_agent_steps_trained: 48032\n",
      "  num_env_steps_sampled: 26000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 48032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.656000000000002\n",
      "    ram_util_percent: 23.982\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07424187632905294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47183342753496615\n",
      "    mean_inference_ms: 3.847874376675411\n",
      "    mean_raw_obs_processing_ms: 1.9751506803528198\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.0482698841834\n",
      "    episode_reward_mean: 1158.8258187370668\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1159.1349859563584, 1156.3723038330195, 1157.1029766358467, 1169.5429905880892,\n",
      "        1152.4853476928465, 1174.3602742641003, 1166.573436306293, 1167.6540783332298,\n",
      "        1162.768550636346, 1165.418703553838, 1158.5053125453057, 1148.396599384758,\n",
      "        1154.7583268000353, 1173.0105537337397, 1150.7207492570533, 1171.2986247447866,\n",
      "        1172.972268521348, 1140.218826815783, 1169.2559496329748, 1153.4782008219725,\n",
      "        1165.555784164654, 1175.3490066912873, 1168.5140368733912, 1160.552561306091,\n",
      "        1158.793833530738, 1154.767984399435, 1156.2638389591846, 1151.0717960753755,\n",
      "        1129.6501992832211, 1158.7279562522838, 1145.3691746298232, 1148.1098311338435,\n",
      "        1170.0869977346588, 1187.0482698841834, 1141.5364813776207, 1156.1390754863532,\n",
      "        1148.5036413532255, 1158.1779387658498, 1154.7465540481562, 1178.912916299883,\n",
      "        1154.9883919690078, 1157.4703867878054, 1171.245673604493, 1161.4057591796459,\n",
      "        1155.903163870535, 1164.3921068411369, 1165.7413533627173, 1171.7361662352237,\n",
      "        1159.1996365716934, 1148.3076002231874, 1150.1153172047273, 1144.7809976227927,\n",
      "        1163.0801368007358, 1170.736368582094, 1146.5150176683303, 1141.8127559632514,\n",
      "        1143.1617450143522, 1139.5624772518045, 1165.2411504129357, 1163.7297633389403,\n",
      "        1152.5527430242103, 1174.3053856110491, 1159.0199610283596, 1148.6487276912617,\n",
      "        1169.085267853609, 1162.0806462066228, 1162.493973820481, 1150.808373891986,\n",
      "        1155.2434592344266, 1166.9304253850923, 1157.0256505266002, 1168.0605158969788,\n",
      "        1177.6709912251386, 1148.766721409387, 1169.6354451942034, 1170.6262338870927,\n",
      "        1146.421511206209, 1156.0785764084944, 1163.8704043976702, 1153.3209612431924,\n",
      "        1151.8499060395345, 1175.4501445243802, 1150.4085823017904, 1147.0573955077214,\n",
      "        1154.2179070424274, 1141.6683838742279, 1152.3057675938212, 1141.7168694080988,\n",
      "        1160.0590250434752, 1148.0518592108374, 1155.1464241877188, 1168.3644621452552,\n",
      "        1159.4587988651651, 1156.2417262259112, 1166.2319449464078, 1153.9971377397458,\n",
      "        1163.9812567919905, 1165.4796416164284, 1167.1479131368217, 1160.0678455504328]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07424187632905294\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47183342753496615\n",
      "      mean_inference_ms: 3.847874376675411\n",
      "      mean_raw_obs_processing_ms: 1.9751506803528198\n",
      "  time_since_restore: 366.8285713195801\n",
      "  time_this_iter_s: 34.81323504447937\n",
      "  time_total_s: 366.8285713195801\n",
      "  timers:\n",
      "    learn_throughput: 746.745\n",
      "    learn_time_ms: 42.853\n",
      "    load_throughput: 49391.966\n",
      "    load_time_ms: 0.648\n",
      "    synch_weights_time_ms: 3.634\n",
      "    training_iteration_time_ms: 139.913\n",
      "  timestamp: 1661054186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 7\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  counters:\n",
      "    last_target_update_ts: 26400\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 56032\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 56032\n",
      "    num_target_updates: 3\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-57-00\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.0482698841834\n",
      "  episode_reward_mean: 1158.5698092239797\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 225\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 26400\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 10.077130317687988\n",
      "          choice_loss: 0.8114432692527771\n",
      "          grad_gnorm: 15.127808570861816\n",
      "          mean_actions: 9.0\n",
      "          next_q_target_max: 6.594878196716309\n",
      "          next_q_target_slate: 1.281520962715149\n",
      "          next_q_values: 2.313570022583008\n",
      "          q_clicked: 3.6603851318359375\n",
      "          q_loss: 197.10963439941406\n",
      "          q_values: 2.82712984085083\n",
      "          replay_click_q: 3.6603851318359375\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2518351078033447\n",
      "          slate_q_values: 4.22747278213501\n",
      "          target: 16.606060028076172\n",
      "          target_clicked: 16.606060028076172\n",
      "        mean_td_error: 12.97890853881836\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [0.5317497253417969, 16.075618743896484, 16.180408477783203, 1.7845420837402344,\n",
      "          13.771716117858887, 15.600207328796387, 15.787714958190918, 16.161970138549805,\n",
      "          16.719100952148438, 18.42339324951172, 13.07089614868164, 3.6642379760742188,\n",
      "          14.237253189086914, 16.321372985839844, 14.894509315490723, 3.744015693664551,\n",
      "          15.22008228302002, 0.19192123413085938, 15.352505683898926, 14.004658699035645,\n",
      "          18.5253849029541, 15.8355131149292, 13.694904327392578, 14.695396423339844,\n",
      "          11.118756294250488, 15.764982223510742, 4.820323944091797, 16.526145935058594,\n",
      "          14.308775901794434, 18.58896255493164, 12.499960899353027, 17.20813751220703]\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 56032\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 56032\n",
      "    num_target_updates: 3\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 27000\n",
      "  num_agent_steps_trained: 56032\n",
      "  num_env_steps_sampled: 27000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 56032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.34375\n",
      "    ram_util_percent: 24.212500000000006\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07442720809300832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4725497942891968\n",
      "    mean_inference_ms: 3.863440756542572\n",
      "    mean_raw_obs_processing_ms: 1.9762408831292522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.0482698841834\n",
      "    episode_reward_mean: 1158.5698092239797\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1165.418703553838, 1158.5053125453057, 1148.396599384758, 1154.7583268000353,\n",
      "        1173.0105537337397, 1150.7207492570533, 1171.2986247447866, 1172.972268521348,\n",
      "        1140.218826815783, 1169.2559496329748, 1153.4782008219725, 1165.555784164654,\n",
      "        1175.3490066912873, 1168.5140368733912, 1160.552561306091, 1158.793833530738,\n",
      "        1154.767984399435, 1156.2638389591846, 1151.0717960753755, 1129.6501992832211,\n",
      "        1158.7279562522838, 1145.3691746298232, 1148.1098311338435, 1170.0869977346588,\n",
      "        1187.0482698841834, 1141.5364813776207, 1156.1390754863532, 1148.5036413532255,\n",
      "        1158.1779387658498, 1154.7465540481562, 1178.912916299883, 1154.9883919690078,\n",
      "        1157.4703867878054, 1171.245673604493, 1161.4057591796459, 1155.903163870535,\n",
      "        1164.3921068411369, 1165.7413533627173, 1171.7361662352237, 1159.1996365716934,\n",
      "        1148.3076002231874, 1150.1153172047273, 1144.7809976227927, 1163.0801368007358,\n",
      "        1170.736368582094, 1146.5150176683303, 1141.8127559632514, 1143.1617450143522,\n",
      "        1139.5624772518045, 1165.2411504129357, 1163.7297633389403, 1152.5527430242103,\n",
      "        1174.3053856110491, 1159.0199610283596, 1148.6487276912617, 1169.085267853609,\n",
      "        1162.0806462066228, 1162.493973820481, 1150.808373891986, 1155.2434592344266,\n",
      "        1166.9304253850923, 1157.0256505266002, 1168.0605158969788, 1177.6709912251386,\n",
      "        1148.766721409387, 1169.6354451942034, 1170.6262338870927, 1146.421511206209,\n",
      "        1156.0785764084944, 1163.8704043976702, 1153.3209612431924, 1151.8499060395345,\n",
      "        1175.4501445243802, 1150.4085823017904, 1147.0573955077214, 1154.2179070424274,\n",
      "        1141.6683838742279, 1152.3057675938212, 1141.7168694080988, 1160.0590250434752,\n",
      "        1148.0518592108374, 1155.1464241877188, 1168.3644621452552, 1159.4587988651651,\n",
      "        1156.2417262259112, 1166.2319449464078, 1153.9971377397458, 1163.9812567919905,\n",
      "        1165.4796416164284, 1167.1479131368217, 1160.0678455504328, 1156.0702914215542,\n",
      "        1146.953123537582, 1157.6710239517486, 1179.5060561335365, 1168.7513120802607,\n",
      "        1179.2765000434354, 1147.3670004056676, 1155.219264243292, 1149.5794211203795]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07442720809300832\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4725497942891968\n",
      "      mean_inference_ms: 3.863440756542572\n",
      "      mean_raw_obs_processing_ms: 1.9762408831292522\n",
      "  time_since_restore: 401.00149941444397\n",
      "  time_this_iter_s: 34.17292809486389\n",
      "  time_total_s: 401.00149941444397\n",
      "  timers:\n",
      "    learn_throughput: 799.504\n",
      "    learn_time_ms: 40.025\n",
      "    load_throughput: 52176.072\n",
      "    load_time_ms: 0.613\n",
      "    synch_weights_time_ms: 3.556\n",
      "    training_iteration_time_ms: 134.133\n",
      "  timestamp: 1661054220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 8\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  counters:\n",
      "    last_target_update_ts: 26400\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 64032\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 64032\n",
      "    num_target_updates: 3\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.0482698841834\n",
      "  episode_reward_mean: 1158.7495660303525\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 233\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 26400\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 13.237151145935059\n",
      "          choice_loss: 0.8001852631568909\n",
      "          grad_gnorm: 83.52560424804688\n",
      "          mean_actions: 9.828125\n",
      "          next_q_target_max: 6.740317344665527\n",
      "          next_q_target_slate: 1.2929199934005737\n",
      "          next_q_values: 2.329836368560791\n",
      "          q_clicked: 4.1626434326171875\n",
      "          q_loss: 542.3362426757812\n",
      "          q_values: 3.5546162128448486\n",
      "          replay_click_q: 4.1626434326171875\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2483150959014893\n",
      "          slate_q_values: 4.245466232299805\n",
      "          target: 19.910064697265625\n",
      "          target_clicked: 19.910064697265625\n",
      "        mean_td_error: 15.747421264648438\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [7.521313667297363, 15.267929077148438, 17.319814682006836, 14.008959770202637,\n",
      "          106.87748718261719, 15.06333065032959, 16.03689956665039, 0.344635009765625,\n",
      "          15.312114715576172, 16.003509521484375, 18.07053565979004, 13.19762134552002,\n",
      "          14.146383285522461, 8.125673294067383, 15.454245567321777, 1.3824577331542969,\n",
      "          0.49846363067626953, 16.198652267456055, 13.905847549438477, 17.59791374206543,\n",
      "          17.878034591674805, 2.908785820007324, 14.61839485168457, 14.624972343444824,\n",
      "          14.950821876525879, 15.505961418151855, 12.576957702636719, 13.486108779907227,\n",
      "          17.45639991760254, 17.483413696289062, 6.4417009353637695, 13.6521577835083]\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 64032\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 64032\n",
      "    num_target_updates: 3\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 64032\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 64032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.31875\n",
      "    ram_util_percent: 24.429166666666664\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0746093178365788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4732631112606237\n",
      "    mean_inference_ms: 3.879714979123128\n",
      "    mean_raw_obs_processing_ms: 1.9773444694932878\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.0482698841834\n",
      "    episode_reward_mean: 1158.7495660303525\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1140.218826815783, 1169.2559496329748, 1153.4782008219725, 1165.555784164654,\n",
      "        1175.3490066912873, 1168.5140368733912, 1160.552561306091, 1158.793833530738,\n",
      "        1154.767984399435, 1156.2638389591846, 1151.0717960753755, 1129.6501992832211,\n",
      "        1158.7279562522838, 1145.3691746298232, 1148.1098311338435, 1170.0869977346588,\n",
      "        1187.0482698841834, 1141.5364813776207, 1156.1390754863532, 1148.5036413532255,\n",
      "        1158.1779387658498, 1154.7465540481562, 1178.912916299883, 1154.9883919690078,\n",
      "        1157.4703867878054, 1171.245673604493, 1161.4057591796459, 1155.903163870535,\n",
      "        1164.3921068411369, 1165.7413533627173, 1171.7361662352237, 1159.1996365716934,\n",
      "        1148.3076002231874, 1150.1153172047273, 1144.7809976227927, 1163.0801368007358,\n",
      "        1170.736368582094, 1146.5150176683303, 1141.8127559632514, 1143.1617450143522,\n",
      "        1139.5624772518045, 1165.2411504129357, 1163.7297633389403, 1152.5527430242103,\n",
      "        1174.3053856110491, 1159.0199610283596, 1148.6487276912617, 1169.085267853609,\n",
      "        1162.0806462066228, 1162.493973820481, 1150.808373891986, 1155.2434592344266,\n",
      "        1166.9304253850923, 1157.0256505266002, 1168.0605158969788, 1177.6709912251386,\n",
      "        1148.766721409387, 1169.6354451942034, 1170.6262338870927, 1146.421511206209,\n",
      "        1156.0785764084944, 1163.8704043976702, 1153.3209612431924, 1151.8499060395345,\n",
      "        1175.4501445243802, 1150.4085823017904, 1147.0573955077214, 1154.2179070424274,\n",
      "        1141.6683838742279, 1152.3057675938212, 1141.7168694080988, 1160.0590250434752,\n",
      "        1148.0518592108374, 1155.1464241877188, 1168.3644621452552, 1159.4587988651651,\n",
      "        1156.2417262259112, 1166.2319449464078, 1153.9971377397458, 1163.9812567919905,\n",
      "        1165.4796416164284, 1167.1479131368217, 1160.0678455504328, 1156.0702914215542,\n",
      "        1146.953123537582, 1157.6710239517486, 1179.5060561335365, 1168.7513120802607,\n",
      "        1179.2765000434354, 1147.3670004056676, 1155.219264243292, 1149.5794211203795,\n",
      "        1157.9823424963902, 1170.3034692764174, 1179.8151225156312, 1168.0594050845705,\n",
      "        1153.8313801046131, 1171.8166447733145, 1161.3251618972924, 1149.9232930299254]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0746093178365788\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4732631112606237\n",
      "      mean_inference_ms: 3.879714979123128\n",
      "      mean_raw_obs_processing_ms: 1.9773444694932878\n",
      "  time_since_restore: 435.0155358314514\n",
      "  time_this_iter_s: 34.014036417007446\n",
      "  time_total_s: 435.0155358314514\n",
      "  timers:\n",
      "    learn_throughput: 811.789\n",
      "    learn_time_ms: 39.419\n",
      "    load_throughput: 51992.147\n",
      "    load_time_ms: 0.615\n",
      "    synch_weights_time_ms: 3.514\n",
      "    training_iteration_time_ms: 132.797\n",
      "  timestamp: 1661054255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 9\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  counters:\n",
      "    last_target_update_ts: 26400\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 72032\n",
      "    num_env_steps_sampled: 29000\n",
      "    num_env_steps_trained: 72032\n",
      "    num_target_updates: 3\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-58-08\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1158.6528242812692\n",
      "  episode_reward_min: 1129.6501992832211\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 241\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 26400\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.477968215942383\n",
      "          choice_loss: 0.7756021022796631\n",
      "          grad_gnorm: 17.552221298217773\n",
      "          mean_actions: 10.1875\n",
      "          next_q_target_max: 6.810735702514648\n",
      "          next_q_target_slate: 1.3088845014572144\n",
      "          next_q_values: 2.343076229095459\n",
      "          q_clicked: 3.5584893226623535\n",
      "          q_loss: 183.07925415039062\n",
      "          q_values: 4.13966178894043\n",
      "          replay_click_q: 3.5584893226623535\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2571769952774048\n",
      "          slate_q_values: 4.70017671585083\n",
      "          target: 16.220596313476562\n",
      "          target_clicked: 16.220596313476562\n",
      "        mean_td_error: 12.877973556518555\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [17.33273696899414, 14.911877632141113, 15.798175811767578, 19.091794967651367,\n",
      "          6.301108360290527, 11.678940773010254, 11.319093704223633, 17.247482299804688,\n",
      "          12.395994186401367, 14.549612998962402, 14.02039623260498, 12.98377513885498,\n",
      "          15.956188201904297, 14.761116981506348, 4.559619903564453, 12.879695892333984,\n",
      "          14.434062004089355, 14.017087936401367, 14.98522663116455, 14.737801551818848,\n",
      "          14.12489128112793, 12.381061553955078, 1.3926277160644531, 11.59434700012207,\n",
      "          15.067041397094727, 13.12639045715332, 18.523757934570312, 15.962646484375,\n",
      "          8.757428169250488, 2.0612316131591797, 12.186408996582031, 12.955525398254395]\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 72032\n",
      "    num_env_steps_sampled: 29000\n",
      "    num_env_steps_trained: 72032\n",
      "    num_target_updates: 3\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 29000\n",
      "  num_agent_steps_trained: 72032\n",
      "  num_env_steps_sampled: 29000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 72032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.287499999999998\n",
      "    ram_util_percent: 24.65625\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07480704300268477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4740381239374916\n",
      "    mean_inference_ms: 3.8982503427131237\n",
      "    mean_raw_obs_processing_ms: 1.9785182501998135\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1158.6528242812692\n",
      "    episode_reward_min: 1129.6501992832211\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1154.767984399435, 1156.2638389591846, 1151.0717960753755, 1129.6501992832211,\n",
      "        1158.7279562522838, 1145.3691746298232, 1148.1098311338435, 1170.0869977346588,\n",
      "        1187.0482698841834, 1141.5364813776207, 1156.1390754863532, 1148.5036413532255,\n",
      "        1158.1779387658498, 1154.7465540481562, 1178.912916299883, 1154.9883919690078,\n",
      "        1157.4703867878054, 1171.245673604493, 1161.4057591796459, 1155.903163870535,\n",
      "        1164.3921068411369, 1165.7413533627173, 1171.7361662352237, 1159.1996365716934,\n",
      "        1148.3076002231874, 1150.1153172047273, 1144.7809976227927, 1163.0801368007358,\n",
      "        1170.736368582094, 1146.5150176683303, 1141.8127559632514, 1143.1617450143522,\n",
      "        1139.5624772518045, 1165.2411504129357, 1163.7297633389403, 1152.5527430242103,\n",
      "        1174.3053856110491, 1159.0199610283596, 1148.6487276912617, 1169.085267853609,\n",
      "        1162.0806462066228, 1162.493973820481, 1150.808373891986, 1155.2434592344266,\n",
      "        1166.9304253850923, 1157.0256505266002, 1168.0605158969788, 1177.6709912251386,\n",
      "        1148.766721409387, 1169.6354451942034, 1170.6262338870927, 1146.421511206209,\n",
      "        1156.0785764084944, 1163.8704043976702, 1153.3209612431924, 1151.8499060395345,\n",
      "        1175.4501445243802, 1150.4085823017904, 1147.0573955077214, 1154.2179070424274,\n",
      "        1141.6683838742279, 1152.3057675938212, 1141.7168694080988, 1160.0590250434752,\n",
      "        1148.0518592108374, 1155.1464241877188, 1168.3644621452552, 1159.4587988651651,\n",
      "        1156.2417262259112, 1166.2319449464078, 1153.9971377397458, 1163.9812567919905,\n",
      "        1165.4796416164284, 1167.1479131368217, 1160.0678455504328, 1156.0702914215542,\n",
      "        1146.953123537582, 1157.6710239517486, 1179.5060561335365, 1168.7513120802607,\n",
      "        1179.2765000434354, 1147.3670004056676, 1155.219264243292, 1149.5794211203795,\n",
      "        1157.9823424963902, 1170.3034692764174, 1179.8151225156312, 1168.0594050845705,\n",
      "        1153.8313801046131, 1171.8166447733145, 1161.3251618972924, 1149.9232930299254,\n",
      "        1169.5627617671107, 1164.702107785678, 1146.9509086617184, 1159.19356383167,\n",
      "        1169.0416564941315, 1153.1568119069077, 1131.492356484583, 1187.943857996734]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07480704300268477\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4740381239374916\n",
      "      mean_inference_ms: 3.8982503427131237\n",
      "      mean_raw_obs_processing_ms: 1.9785182501998135\n",
      "  time_since_restore: 468.8101029396057\n",
      "  time_this_iter_s: 33.7945671081543\n",
      "  time_total_s: 468.8101029396057\n",
      "  timers:\n",
      "    learn_throughput: 776.109\n",
      "    learn_time_ms: 41.231\n",
      "    load_throughput: 51043.061\n",
      "    load_time_ms: 0.627\n",
      "    synch_weights_time_ms: 3.75\n",
      "    training_iteration_time_ms: 136.97\n",
      "  timestamp: 1661054288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 10\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    last_target_update_ts: 29600\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 80032\n",
      "    num_env_steps_sampled: 30000\n",
      "    num_env_steps_trained: 80032\n",
      "    num_target_updates: 4\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-58-44\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1159.517391410721\n",
      "  episode_reward_min: 1131.492356484583\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 250\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 29600\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 15.414586067199707\n",
      "          choice_loss: 0.7643043398857117\n",
      "          grad_gnorm: 36.531734466552734\n",
      "          mean_actions: 9.9375\n",
      "          next_q_target_max: 8.628763198852539\n",
      "          next_q_target_slate: 2.3746495246887207\n",
      "          next_q_values: 4.29224157333374\n",
      "          q_clicked: 5.080304145812988\n",
      "          q_loss: 949.3041381835938\n",
      "          q_values: 4.8375959396362305\n",
      "          replay_click_q: 5.080304145812988\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2460311651229858\n",
      "          slate_q_values: 6.158597946166992\n",
      "          target: 23.68655014038086\n",
      "          target_clicked: 23.68655014038086\n",
      "        mean_td_error: 18.606245040893555\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [16.551963806152344, 2.3831748962402344, 18.557098388671875, 14.708106994628906,\n",
      "          1.9615421295166016, 15.51994800567627, 4.5466508865356445, 12.692211151123047,\n",
      "          15.934231758117676, 5.206226348876953, 14.592726707458496, 17.252382278442383,\n",
      "          14.919464111328125, 2.2909812927246094, 17.416078567504883, 88.70631408691406,\n",
      "          15.227128982543945, 16.01376724243164, 13.066725730895996, 16.836101531982422,\n",
      "          14.240554809570312, 14.220874786376953, 14.926372528076172, 129.89041137695312,\n",
      "          13.046090126037598, 7.34598970413208, 19.86517333984375, 4.3614912033081055,\n",
      "          4.356601715087891, 20.48481559753418, 13.351290702819824, 14.927356719970703]\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 80032\n",
      "    num_env_steps_sampled: 30000\n",
      "    num_env_steps_trained: 80032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 80032\n",
      "  num_env_steps_sampled: 30000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 80032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.577551020408166\n",
      "    ram_util_percent: 24.871428571428574\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0750459808949509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47499238711809844\n",
      "    mean_inference_ms: 3.9215581923840324\n",
      "    mean_raw_obs_processing_ms: 1.9799601880151159\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1159.517391410721\n",
      "    episode_reward_min: 1131.492356484583\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1141.5364813776207, 1156.1390754863532, 1148.5036413532255, 1158.1779387658498,\n",
      "        1154.7465540481562, 1178.912916299883, 1154.9883919690078, 1157.4703867878054,\n",
      "        1171.245673604493, 1161.4057591796459, 1155.903163870535, 1164.3921068411369,\n",
      "        1165.7413533627173, 1171.7361662352237, 1159.1996365716934, 1148.3076002231874,\n",
      "        1150.1153172047273, 1144.7809976227927, 1163.0801368007358, 1170.736368582094,\n",
      "        1146.5150176683303, 1141.8127559632514, 1143.1617450143522, 1139.5624772518045,\n",
      "        1165.2411504129357, 1163.7297633389403, 1152.5527430242103, 1174.3053856110491,\n",
      "        1159.0199610283596, 1148.6487276912617, 1169.085267853609, 1162.0806462066228,\n",
      "        1162.493973820481, 1150.808373891986, 1155.2434592344266, 1166.9304253850923,\n",
      "        1157.0256505266002, 1168.0605158969788, 1177.6709912251386, 1148.766721409387,\n",
      "        1169.6354451942034, 1170.6262338870927, 1146.421511206209, 1156.0785764084944,\n",
      "        1163.8704043976702, 1153.3209612431924, 1151.8499060395345, 1175.4501445243802,\n",
      "        1150.4085823017904, 1147.0573955077214, 1154.2179070424274, 1141.6683838742279,\n",
      "        1152.3057675938212, 1141.7168694080988, 1160.0590250434752, 1148.0518592108374,\n",
      "        1155.1464241877188, 1168.3644621452552, 1159.4587988651651, 1156.2417262259112,\n",
      "        1166.2319449464078, 1153.9971377397458, 1163.9812567919905, 1165.4796416164284,\n",
      "        1167.1479131368217, 1160.0678455504328, 1156.0702914215542, 1146.953123537582,\n",
      "        1157.6710239517486, 1179.5060561335365, 1168.7513120802607, 1179.2765000434354,\n",
      "        1147.3670004056676, 1155.219264243292, 1149.5794211203795, 1157.9823424963902,\n",
      "        1170.3034692764174, 1179.8151225156312, 1168.0594050845705, 1153.8313801046131,\n",
      "        1171.8166447733145, 1161.3251618972924, 1149.9232930299254, 1169.5627617671107,\n",
      "        1164.702107785678, 1146.9509086617184, 1159.19356383167, 1169.0416564941315,\n",
      "        1153.1568119069077, 1131.492356484583, 1187.943857996734, 1167.6463231935545,\n",
      "        1165.4321160962988, 1160.5609910098847, 1167.472206065449, 1162.6347126792239,\n",
      "        1169.7491047711187, 1156.5952497742326, 1171.5012147456332, 1165.9608429618245]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0750459808949509\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47499238711809844\n",
      "      mean_inference_ms: 3.9215581923840324\n",
      "      mean_raw_obs_processing_ms: 1.9799601880151159\n",
      "  time_since_restore: 503.8986167907715\n",
      "  time_this_iter_s: 35.08851385116577\n",
      "  time_total_s: 503.8986167907715\n",
      "  timers:\n",
      "    learn_throughput: 820.643\n",
      "    learn_time_ms: 38.994\n",
      "    load_throughput: 52615.833\n",
      "    load_time_ms: 0.608\n",
      "    synch_weights_time_ms: 3.566\n",
      "    training_iteration_time_ms: 133.142\n",
      "  timestamp: 1661054324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 11\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  counters:\n",
      "    last_target_update_ts: 29600\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 88032\n",
      "    num_env_steps_sampled: 31000\n",
      "    num_env_steps_trained: 88032\n",
      "    num_target_updates: 4\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-59-17\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1159.408529914946\n",
      "  episode_reward_min: 1131.492356484583\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 258\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 29600\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 11.623719215393066\n",
      "          choice_loss: 0.7784885168075562\n",
      "          grad_gnorm: 25.79354476928711\n",
      "          mean_actions: 8.203125\n",
      "          next_q_target_max: 8.778149604797363\n",
      "          next_q_target_slate: 2.39451265335083\n",
      "          next_q_values: 4.313416004180908\n",
      "          q_clicked: 5.9604620933532715\n",
      "          q_loss: 359.66064453125\n",
      "          q_values: 5.8389997482299805\n",
      "          replay_click_q: 5.9604620933532715\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2479057312011719\n",
      "          slate_q_values: 5.252127170562744\n",
      "          target: 20.3140869140625\n",
      "          target_clicked: 20.3140869140625\n",
      "        mean_td_error: 14.36268424987793\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [15.107545852661133, 14.355217933654785, 17.426658630371094, 9.694055557250977,\n",
      "          15.985098838806152, 74.33739471435547, 14.796456336975098, 15.624944686889648,\n",
      "          15.404631614685059, 19.40872573852539, 17.157745361328125, 17.966413497924805,\n",
      "          3.6086349487304688, 0.7763214111328125, 0.8579483032226562, 0.3941764831542969,\n",
      "          16.463817596435547, 13.218427658081055, 3.3722028732299805, 17.829076766967773,\n",
      "          16.304447174072266, 1.1289596557617188, 16.282018661499023, 15.233976364135742,\n",
      "          0.1449451446533203, 17.35370445251465, 12.831642150878906, 14.984474182128906,\n",
      "          14.449971199035645, 18.0419864654541, 14.049318313598633, 15.014934539794922]\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 88032\n",
      "    num_env_steps_sampled: 31000\n",
      "    num_env_steps_trained: 88032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 31000\n",
      "  num_agent_steps_trained: 88032\n",
      "  num_env_steps_sampled: 31000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 88032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.441666666666666\n",
      "    ram_util_percent: 25.099999999999998\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07527261605322634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47590098301550077\n",
      "    mean_inference_ms: 3.9443350801265478\n",
      "    mean_raw_obs_processing_ms: 1.9812886563806074\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1159.408529914946\n",
      "    episode_reward_min: 1131.492356484583\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1171.245673604493, 1161.4057591796459, 1155.903163870535, 1164.3921068411369,\n",
      "        1165.7413533627173, 1171.7361662352237, 1159.1996365716934, 1148.3076002231874,\n",
      "        1150.1153172047273, 1144.7809976227927, 1163.0801368007358, 1170.736368582094,\n",
      "        1146.5150176683303, 1141.8127559632514, 1143.1617450143522, 1139.5624772518045,\n",
      "        1165.2411504129357, 1163.7297633389403, 1152.5527430242103, 1174.3053856110491,\n",
      "        1159.0199610283596, 1148.6487276912617, 1169.085267853609, 1162.0806462066228,\n",
      "        1162.493973820481, 1150.808373891986, 1155.2434592344266, 1166.9304253850923,\n",
      "        1157.0256505266002, 1168.0605158969788, 1177.6709912251386, 1148.766721409387,\n",
      "        1169.6354451942034, 1170.6262338870927, 1146.421511206209, 1156.0785764084944,\n",
      "        1163.8704043976702, 1153.3209612431924, 1151.8499060395345, 1175.4501445243802,\n",
      "        1150.4085823017904, 1147.0573955077214, 1154.2179070424274, 1141.6683838742279,\n",
      "        1152.3057675938212, 1141.7168694080988, 1160.0590250434752, 1148.0518592108374,\n",
      "        1155.1464241877188, 1168.3644621452552, 1159.4587988651651, 1156.2417262259112,\n",
      "        1166.2319449464078, 1153.9971377397458, 1163.9812567919905, 1165.4796416164284,\n",
      "        1167.1479131368217, 1160.0678455504328, 1156.0702914215542, 1146.953123537582,\n",
      "        1157.6710239517486, 1179.5060561335365, 1168.7513120802607, 1179.2765000434354,\n",
      "        1147.3670004056676, 1155.219264243292, 1149.5794211203795, 1157.9823424963902,\n",
      "        1170.3034692764174, 1179.8151225156312, 1168.0594050845705, 1153.8313801046131,\n",
      "        1171.8166447733145, 1161.3251618972924, 1149.9232930299254, 1169.5627617671107,\n",
      "        1164.702107785678, 1146.9509086617184, 1159.19356383167, 1169.0416564941315,\n",
      "        1153.1568119069077, 1131.492356484583, 1187.943857996734, 1167.6463231935545,\n",
      "        1165.4321160962988, 1160.5609910098847, 1167.472206065449, 1162.6347126792239,\n",
      "        1169.7491047711187, 1156.5952497742326, 1171.5012147456332, 1165.9608429618245,\n",
      "        1169.7868775127968, 1157.489281635276, 1160.1636269704593, 1135.4373108184816,\n",
      "        1167.6142987704247, 1151.727057625516, 1161.0540393492997, 1136.3167438281569]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07527261605322634\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47590098301550077\n",
      "      mean_inference_ms: 3.9443350801265478\n",
      "      mean_raw_obs_processing_ms: 1.9812886563806074\n",
      "  time_since_restore: 537.6792621612549\n",
      "  time_this_iter_s: 33.7806453704834\n",
      "  time_total_s: 537.6792621612549\n",
      "  timers:\n",
      "    learn_throughput: 813.307\n",
      "    learn_time_ms: 39.346\n",
      "    load_throughput: 51167.599\n",
      "    load_time_ms: 0.625\n",
      "    synch_weights_time_ms: 3.679\n",
      "    training_iteration_time_ms: 134.843\n",
      "  timestamp: 1661054357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 12\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    last_target_update_ts: 29600\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 96032\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 96032\n",
      "    num_target_updates: 4\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_20-59-51\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1158.883934223818\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 266\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 29600\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.703521728515625\n",
      "          choice_loss: 0.7415825724601746\n",
      "          grad_gnorm: 24.30400276184082\n",
      "          mean_actions: 8.78125\n",
      "          next_q_target_max: 8.5306978225708\n",
      "          next_q_target_slate: 2.351717948913574\n",
      "          next_q_values: 4.2523193359375\n",
      "          q_clicked: 5.937902927398682\n",
      "          q_loss: 187.195556640625\n",
      "          q_values: 6.522401332855225\n",
      "          replay_click_q: 5.937902927398682\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2461377382278442\n",
      "          slate_q_values: 6.2324137687683105\n",
      "          target: 18.14891242980957\n",
      "          target_clicked: 18.14891242980957\n",
      "        mean_td_error: 12.765079498291016\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [12.84190559387207, 15.697128295898438, 15.318863868713379, 14.127490997314453,\n",
      "          18.117748260498047, 16.87466812133789, 15.542072296142578, 11.674933433532715,\n",
      "          13.056424140930176, 16.420276641845703, 2.334254264831543, 0.4113655090332031,\n",
      "          13.190619468688965, 16.94880485534668, 13.997198104858398, 12.874564170837402,\n",
      "          5.169604301452637, 15.528666496276855, 14.405216217041016, 17.003253936767578,\n",
      "          16.69060516357422, 14.260464668273926, 12.180427551269531, 9.345674514770508,\n",
      "          13.121989250183105, 2.9342918395996094, 3.2841291427612305, 20.250717163085938,\n",
      "          13.51233959197998, 16.398120880126953, 16.853473663330078, 8.115226745605469]\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 96032\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 96032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 96032\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 96032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.242553191489367\n",
      "    ram_util_percent: 25.312765957446825\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07551348593102374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4768643177010483\n",
      "    mean_inference_ms: 3.96900144222387\n",
      "    mean_raw_obs_processing_ms: 1.9826885165627788\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1158.883934223818\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1150.1153172047273, 1144.7809976227927, 1163.0801368007358, 1170.736368582094,\n",
      "        1146.5150176683303, 1141.8127559632514, 1143.1617450143522, 1139.5624772518045,\n",
      "        1165.2411504129357, 1163.7297633389403, 1152.5527430242103, 1174.3053856110491,\n",
      "        1159.0199610283596, 1148.6487276912617, 1169.085267853609, 1162.0806462066228,\n",
      "        1162.493973820481, 1150.808373891986, 1155.2434592344266, 1166.9304253850923,\n",
      "        1157.0256505266002, 1168.0605158969788, 1177.6709912251386, 1148.766721409387,\n",
      "        1169.6354451942034, 1170.6262338870927, 1146.421511206209, 1156.0785764084944,\n",
      "        1163.8704043976702, 1153.3209612431924, 1151.8499060395345, 1175.4501445243802,\n",
      "        1150.4085823017904, 1147.0573955077214, 1154.2179070424274, 1141.6683838742279,\n",
      "        1152.3057675938212, 1141.7168694080988, 1160.0590250434752, 1148.0518592108374,\n",
      "        1155.1464241877188, 1168.3644621452552, 1159.4587988651651, 1156.2417262259112,\n",
      "        1166.2319449464078, 1153.9971377397458, 1163.9812567919905, 1165.4796416164284,\n",
      "        1167.1479131368217, 1160.0678455504328, 1156.0702914215542, 1146.953123537582,\n",
      "        1157.6710239517486, 1179.5060561335365, 1168.7513120802607, 1179.2765000434354,\n",
      "        1147.3670004056676, 1155.219264243292, 1149.5794211203795, 1157.9823424963902,\n",
      "        1170.3034692764174, 1179.8151225156312, 1168.0594050845705, 1153.8313801046131,\n",
      "        1171.8166447733145, 1161.3251618972924, 1149.9232930299254, 1169.5627617671107,\n",
      "        1164.702107785678, 1146.9509086617184, 1159.19356383167, 1169.0416564941315,\n",
      "        1153.1568119069077, 1131.492356484583, 1187.943857996734, 1167.6463231935545,\n",
      "        1165.4321160962988, 1160.5609910098847, 1167.472206065449, 1162.6347126792239,\n",
      "        1169.7491047711187, 1156.5952497742326, 1171.5012147456332, 1165.9608429618245,\n",
      "        1169.7868775127968, 1157.489281635276, 1160.1636269704593, 1135.4373108184816,\n",
      "        1167.6142987704247, 1151.727057625516, 1161.0540393492997, 1136.3167438281569,\n",
      "        1156.0183099922835, 1150.3416465863809, 1130.074408239591, 1160.4937820979442,\n",
      "        1179.2612343002804, 1155.4281371124441, 1165.9806350161869, 1147.8737374307136]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07551348593102374\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4768643177010483\n",
      "      mean_inference_ms: 3.96900144222387\n",
      "      mean_raw_obs_processing_ms: 1.9826885165627788\n",
      "  time_since_restore: 571.3099548816681\n",
      "  time_this_iter_s: 33.63069272041321\n",
      "  time_total_s: 571.3099548816681\n",
      "  timers:\n",
      "    learn_throughput: 818.125\n",
      "    learn_time_ms: 39.114\n",
      "    load_throughput: 52318.441\n",
      "    load_time_ms: 0.612\n",
      "    synch_weights_time_ms: 3.585\n",
      "    training_iteration_time_ms: 132.202\n",
      "  timestamp: 1661054391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 13\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  counters:\n",
      "    last_target_update_ts: 32800\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 104032\n",
      "    num_env_steps_sampled: 33000\n",
      "    num_env_steps_trained: 104032\n",
      "    num_target_updates: 5\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-00-25\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1159.9025571344212\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 275\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 32800\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.335627555847168\n",
      "          choice_loss: 0.7391660809516907\n",
      "          grad_gnorm: 29.0150089263916\n",
      "          mean_actions: 11.078125\n",
      "          next_q_target_max: 10.311507225036621\n",
      "          next_q_target_slate: 4.061761379241943\n",
      "          next_q_values: 7.289158821105957\n",
      "          q_clicked: 7.06642484664917\n",
      "          q_loss: 186.29165649414062\n",
      "          q_values: 7.557072639465332\n",
      "          replay_click_q: 7.06642484664917\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2577495574951172\n",
      "          slate_q_values: 7.286353588104248\n",
      "          target: 19.544021606445312\n",
      "          target_clicked: 19.544021606445312\n",
      "        mean_td_error: 12.685065269470215\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [17.206878662109375, 1.4256038665771484, 15.300809860229492, 12.8326416015625,\n",
      "          15.457191467285156, 2.7523460388183594, 0.5671730041503906, 15.538104057312012,\n",
      "          17.434253692626953, 16.011409759521484, 12.957937240600586, 13.5260009765625,\n",
      "          17.397045135498047, 7.717006683349609, 13.349531173706055, 13.809043884277344,\n",
      "          15.253372192382812, 16.000635147094727, 17.54688835144043, 14.65623664855957,\n",
      "          15.951650619506836, 4.982629776000977, 10.122435569763184, 17.273418426513672,\n",
      "          13.534528732299805, 10.003828048706055, 11.201330184936523, 17.431655883789062,\n",
      "          14.998369216918945, 2.372385025024414, 13.525716781616211, 17.78403091430664]\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 104032\n",
      "    num_env_steps_sampled: 33000\n",
      "    num_env_steps_trained: 104032\n",
      "    num_target_updates: 5\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 33000\n",
      "  num_agent_steps_trained: 104032\n",
      "  num_env_steps_sampled: 33000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 104032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.535416666666666\n",
      "    ram_util_percent: 25.53333333333333\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07576430959175351\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47787052284144566\n",
      "    mean_inference_ms: 3.9975808514283564\n",
      "    mean_raw_obs_processing_ms: 1.9841700166992835\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1159.9025571344212\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1163.7297633389403, 1152.5527430242103, 1174.3053856110491, 1159.0199610283596,\n",
      "        1148.6487276912617, 1169.085267853609, 1162.0806462066228, 1162.493973820481,\n",
      "        1150.808373891986, 1155.2434592344266, 1166.9304253850923, 1157.0256505266002,\n",
      "        1168.0605158969788, 1177.6709912251386, 1148.766721409387, 1169.6354451942034,\n",
      "        1170.6262338870927, 1146.421511206209, 1156.0785764084944, 1163.8704043976702,\n",
      "        1153.3209612431924, 1151.8499060395345, 1175.4501445243802, 1150.4085823017904,\n",
      "        1147.0573955077214, 1154.2179070424274, 1141.6683838742279, 1152.3057675938212,\n",
      "        1141.7168694080988, 1160.0590250434752, 1148.0518592108374, 1155.1464241877188,\n",
      "        1168.3644621452552, 1159.4587988651651, 1156.2417262259112, 1166.2319449464078,\n",
      "        1153.9971377397458, 1163.9812567919905, 1165.4796416164284, 1167.1479131368217,\n",
      "        1160.0678455504328, 1156.0702914215542, 1146.953123537582, 1157.6710239517486,\n",
      "        1179.5060561335365, 1168.7513120802607, 1179.2765000434354, 1147.3670004056676,\n",
      "        1155.219264243292, 1149.5794211203795, 1157.9823424963902, 1170.3034692764174,\n",
      "        1179.8151225156312, 1168.0594050845705, 1153.8313801046131, 1171.8166447733145,\n",
      "        1161.3251618972924, 1149.9232930299254, 1169.5627617671107, 1164.702107785678,\n",
      "        1146.9509086617184, 1159.19356383167, 1169.0416564941315, 1153.1568119069077,\n",
      "        1131.492356484583, 1187.943857996734, 1167.6463231935545, 1165.4321160962988,\n",
      "        1160.5609910098847, 1167.472206065449, 1162.6347126792239, 1169.7491047711187,\n",
      "        1156.5952497742326, 1171.5012147456332, 1165.9608429618245, 1169.7868775127968,\n",
      "        1157.489281635276, 1160.1636269704593, 1135.4373108184816, 1167.6142987704247,\n",
      "        1151.727057625516, 1161.0540393492997, 1136.3167438281569, 1156.0183099922835,\n",
      "        1150.3416465863809, 1130.074408239591, 1160.4937820979442, 1179.2612343002804,\n",
      "        1155.4281371124441, 1165.9806350161869, 1147.8737374307136, 1143.0514583296742,\n",
      "        1170.3665325237748, 1160.4988449114476, 1172.11059608468, 1169.8408111515282,\n",
      "        1171.4851715641241, 1163.296681506112, 1159.3619455286369, 1156.8562159813478]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07576430959175351\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47787052284144566\n",
      "      mean_inference_ms: 3.9975808514283564\n",
      "      mean_raw_obs_processing_ms: 1.9841700166992835\n",
      "  time_since_restore: 605.0064368247986\n",
      "  time_this_iter_s: 33.69648194313049\n",
      "  time_total_s: 605.0064368247986\n",
      "  timers:\n",
      "    learn_throughput: 816.913\n",
      "    learn_time_ms: 39.172\n",
      "    load_throughput: 52455.438\n",
      "    load_time_ms: 0.61\n",
      "    synch_weights_time_ms: 3.629\n",
      "    training_iteration_time_ms: 132.996\n",
      "  timestamp: 1661054425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 14\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  counters:\n",
      "    last_target_update_ts: 32800\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 112032\n",
      "    num_env_steps_sampled: 34000\n",
      "    num_env_steps_trained: 112032\n",
      "    num_target_updates: 5\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-00-59\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1159.6825182823852\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 283\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 32800\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 13.026041030883789\n",
      "          choice_loss: 0.768029510974884\n",
      "          grad_gnorm: 78.9549560546875\n",
      "          mean_actions: 9.875\n",
      "          next_q_target_max: 9.979615211486816\n",
      "          next_q_target_slate: 4.03533411026001\n",
      "          next_q_values: 7.312735557556152\n",
      "          q_clicked: 9.172432899475098\n",
      "          q_loss: 415.53070068359375\n",
      "          q_values: 9.00452709197998\n",
      "          replay_click_q: 9.172432899475098\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2454442977905273\n",
      "          slate_q_values: 9.770835876464844\n",
      "          target: 22.905860900878906\n",
      "          target_clicked: 22.905860900878906\n",
      "        mean_td_error: 14.205188751220703\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [14.122745513916016, 17.52495574951172, 14.10493278503418, 14.532247543334961,\n",
      "          14.418741226196289, 16.61997413635254, 11.24014663696289, 51.84725570678711,\n",
      "          13.354358673095703, 18.437349319458008, 8.712485313415527, 5.279885292053223,\n",
      "          3.23342227935791, 1.7998275756835938, 14.764894485473633, 4.205829620361328,\n",
      "          13.025471687316895, 17.571556091308594, 15.46141242980957, 10.839221954345703,\n",
      "          3.763946533203125, 15.07440185546875, 2.3948898315429688, 1.5410175323486328,\n",
      "          8.541810035705566, 13.28535270690918, 79.31597900390625, 9.809494018554688,\n",
      "          1.8124504089355469, 14.129958152770996, 10.702690124511719, 13.097349166870117]\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 112032\n",
      "    num_env_steps_sampled: 34000\n",
      "    num_env_steps_trained: 112032\n",
      "    num_target_updates: 5\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 34000\n",
      "  num_agent_steps_trained: 112032\n",
      "  num_env_steps_sampled: 34000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 112032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.34375\n",
      "    ram_util_percent: 25.7375\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07597430250608356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.47872730972254646\n",
      "    mean_inference_ms: 4.023438908232087\n",
      "    mean_raw_obs_processing_ms: 1.9853969282117188\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1159.6825182823852\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1150.808373891986, 1155.2434592344266, 1166.9304253850923, 1157.0256505266002,\n",
      "        1168.0605158969788, 1177.6709912251386, 1148.766721409387, 1169.6354451942034,\n",
      "        1170.6262338870927, 1146.421511206209, 1156.0785764084944, 1163.8704043976702,\n",
      "        1153.3209612431924, 1151.8499060395345, 1175.4501445243802, 1150.4085823017904,\n",
      "        1147.0573955077214, 1154.2179070424274, 1141.6683838742279, 1152.3057675938212,\n",
      "        1141.7168694080988, 1160.0590250434752, 1148.0518592108374, 1155.1464241877188,\n",
      "        1168.3644621452552, 1159.4587988651651, 1156.2417262259112, 1166.2319449464078,\n",
      "        1153.9971377397458, 1163.9812567919905, 1165.4796416164284, 1167.1479131368217,\n",
      "        1160.0678455504328, 1156.0702914215542, 1146.953123537582, 1157.6710239517486,\n",
      "        1179.5060561335365, 1168.7513120802607, 1179.2765000434354, 1147.3670004056676,\n",
      "        1155.219264243292, 1149.5794211203795, 1157.9823424963902, 1170.3034692764174,\n",
      "        1179.8151225156312, 1168.0594050845705, 1153.8313801046131, 1171.8166447733145,\n",
      "        1161.3251618972924, 1149.9232930299254, 1169.5627617671107, 1164.702107785678,\n",
      "        1146.9509086617184, 1159.19356383167, 1169.0416564941315, 1153.1568119069077,\n",
      "        1131.492356484583, 1187.943857996734, 1167.6463231935545, 1165.4321160962988,\n",
      "        1160.5609910098847, 1167.472206065449, 1162.6347126792239, 1169.7491047711187,\n",
      "        1156.5952497742326, 1171.5012147456332, 1165.9608429618245, 1169.7868775127968,\n",
      "        1157.489281635276, 1160.1636269704593, 1135.4373108184816, 1167.6142987704247,\n",
      "        1151.727057625516, 1161.0540393492997, 1136.3167438281569, 1156.0183099922835,\n",
      "        1150.3416465863809, 1130.074408239591, 1160.4937820979442, 1179.2612343002804,\n",
      "        1155.4281371124441, 1165.9806350161869, 1147.8737374307136, 1143.0514583296742,\n",
      "        1170.3665325237748, 1160.4988449114476, 1172.11059608468, 1169.8408111515282,\n",
      "        1171.4851715641241, 1163.296681506112, 1159.3619455286369, 1156.8562159813478,\n",
      "        1171.8336049337597, 1144.4817151438883, 1149.6009432809383, 1145.6040996383565,\n",
      "        1157.8552999108706, 1164.8152411483745, 1173.517106826634, 1162.204572488136]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07597430250608356\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.47872730972254646\n",
      "      mean_inference_ms: 4.023438908232087\n",
      "      mean_raw_obs_processing_ms: 1.9853969282117188\n",
      "  time_since_restore: 638.6441879272461\n",
      "  time_this_iter_s: 33.63775110244751\n",
      "  time_total_s: 638.6441879272461\n",
      "  timers:\n",
      "    learn_throughput: 791.385\n",
      "    learn_time_ms: 40.435\n",
      "    load_throughput: 49761.875\n",
      "    load_time_ms: 0.643\n",
      "    synch_weights_time_ms: 3.599\n",
      "    training_iteration_time_ms: 135.738\n",
      "  timestamp: 1661054459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 15\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  counters:\n",
      "    last_target_update_ts: 32800\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 120032\n",
      "    num_env_steps_sampled: 35000\n",
      "    num_env_steps_trained: 120032\n",
      "    num_target_updates: 5\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1160.4075751019504\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 291\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 32800\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.718790054321289\n",
      "          choice_loss: 0.7080008387565613\n",
      "          grad_gnorm: 28.28626251220703\n",
      "          mean_actions: 9.421875\n",
      "          next_q_target_max: 9.987022399902344\n",
      "          next_q_target_slate: 4.01856803894043\n",
      "          next_q_values: 7.26040506362915\n",
      "          q_clicked: 11.01149845123291\n",
      "          q_loss: 113.1905288696289\n",
      "          q_values: 10.241780281066895\n",
      "          replay_click_q: 11.01149845123291\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2519203424453735\n",
      "          slate_q_values: 11.18005084991455\n",
      "          target: 19.60594367980957\n",
      "          target_clicked: 19.60594367980957\n",
      "        mean_td_error: 9.129578590393066\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [5.68360710144043, 9.207802772521973, 17.18799591064453, 17.038806915283203,\n",
      "          8.566057205200195, 11.64738655090332, 4.4317169189453125, 14.091236114501953,\n",
      "          5.924018859863281, 17.654600143432617, 13.7374267578125, 3.6013612747192383,\n",
      "          13.768302917480469, 14.787351608276367, 0.7335224151611328, 0.21544837951660156,\n",
      "          8.444367408752441, 0.22286224365234375, 8.728079795837402, 3.5290088653564453,\n",
      "          3.2379751205444336, 12.754545211791992, 13.767871856689453, 12.171455383300781,\n",
      "          8.238139152526855, 7.577218055725098, 13.024070739746094, 9.018471717834473,\n",
      "          17.046527862548828, 1.864126205444336, 0.3356781005859375, 13.909481048583984]\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 120032\n",
      "    num_env_steps_sampled: 35000\n",
      "    num_env_steps_trained: 120032\n",
      "    num_target_updates: 5\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 35000\n",
      "  num_agent_steps_trained: 120032\n",
      "  num_env_steps_sampled: 35000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 120032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.195833333333336\n",
      "    ram_util_percent: 25.960416666666664\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07617155968117732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4795489302896138\n",
      "    mean_inference_ms: 4.049311636215617\n",
      "    mean_raw_obs_processing_ms: 1.9865572286673654\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1160.4075751019504\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1170.6262338870927, 1146.421511206209, 1156.0785764084944, 1163.8704043976702,\n",
      "        1153.3209612431924, 1151.8499060395345, 1175.4501445243802, 1150.4085823017904,\n",
      "        1147.0573955077214, 1154.2179070424274, 1141.6683838742279, 1152.3057675938212,\n",
      "        1141.7168694080988, 1160.0590250434752, 1148.0518592108374, 1155.1464241877188,\n",
      "        1168.3644621452552, 1159.4587988651651, 1156.2417262259112, 1166.2319449464078,\n",
      "        1153.9971377397458, 1163.9812567919905, 1165.4796416164284, 1167.1479131368217,\n",
      "        1160.0678455504328, 1156.0702914215542, 1146.953123537582, 1157.6710239517486,\n",
      "        1179.5060561335365, 1168.7513120802607, 1179.2765000434354, 1147.3670004056676,\n",
      "        1155.219264243292, 1149.5794211203795, 1157.9823424963902, 1170.3034692764174,\n",
      "        1179.8151225156312, 1168.0594050845705, 1153.8313801046131, 1171.8166447733145,\n",
      "        1161.3251618972924, 1149.9232930299254, 1169.5627617671107, 1164.702107785678,\n",
      "        1146.9509086617184, 1159.19356383167, 1169.0416564941315, 1153.1568119069077,\n",
      "        1131.492356484583, 1187.943857996734, 1167.6463231935545, 1165.4321160962988,\n",
      "        1160.5609910098847, 1167.472206065449, 1162.6347126792239, 1169.7491047711187,\n",
      "        1156.5952497742326, 1171.5012147456332, 1165.9608429618245, 1169.7868775127968,\n",
      "        1157.489281635276, 1160.1636269704593, 1135.4373108184816, 1167.6142987704247,\n",
      "        1151.727057625516, 1161.0540393492997, 1136.3167438281569, 1156.0183099922835,\n",
      "        1150.3416465863809, 1130.074408239591, 1160.4937820979442, 1179.2612343002804,\n",
      "        1155.4281371124441, 1165.9806350161869, 1147.8737374307136, 1143.0514583296742,\n",
      "        1170.3665325237748, 1160.4988449114476, 1172.11059608468, 1169.8408111515282,\n",
      "        1171.4851715641241, 1163.296681506112, 1159.3619455286369, 1156.8562159813478,\n",
      "        1171.8336049337597, 1144.4817151438883, 1149.6009432809383, 1145.6040996383565,\n",
      "        1157.8552999108706, 1164.8152411483745, 1173.517106826634, 1162.204572488136,\n",
      "        1186.25544641512, 1161.376347244283, 1165.530515390824, 1168.0472969980417,\n",
      "        1165.7151014737215, 1167.6263355156605, 1176.2459465573456, 1175.8502751253186]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07617155968117732\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4795489302896138\n",
      "      mean_inference_ms: 4.049311636215617\n",
      "      mean_raw_obs_processing_ms: 1.9865572286673654\n",
      "  time_since_restore: 673.7085294723511\n",
      "  time_this_iter_s: 35.06434154510498\n",
      "  time_total_s: 673.7085294723511\n",
      "  timers:\n",
      "    learn_throughput: 826.707\n",
      "    learn_time_ms: 38.708\n",
      "    load_throughput: 51990.133\n",
      "    load_time_ms: 0.616\n",
      "    synch_weights_time_ms: 3.707\n",
      "    training_iteration_time_ms: 134.229\n",
      "  timestamp: 1661054494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 16\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    last_target_update_ts: 36000\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 128032\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 128032\n",
      "    num_target_updates: 6\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-02-07\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1161.014618127275\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 300\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 36000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 10.992612838745117\n",
      "          choice_loss: 0.7450321912765503\n",
      "          grad_gnorm: 48.45651626586914\n",
      "          mean_actions: 10.921875\n",
      "          next_q_target_max: 10.038853645324707\n",
      "          next_q_target_slate: 4.0270094871521\n",
      "          next_q_values: 7.2957611083984375\n",
      "          q_clicked: 11.107316017150879\n",
      "          q_loss: 180.84628295898438\n",
      "          q_values: 11.455358505249023\n",
      "          replay_click_q: 11.107316017150879\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.247383713722229\n",
      "          slate_q_values: 11.582300186157227\n",
      "          target: 20.93107795715332\n",
      "          target_clicked: 20.93107795715332\n",
      "        mean_td_error: 10.257716178894043\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [11.10895824432373, 5.057163238525391, 52.35402297973633, 4.762275695800781,\n",
      "          8.536092758178711, 11.006573677062988, 11.672746658325195, 6.595526695251465,\n",
      "          15.675688743591309, 14.556930541992188, 15.072487831115723, 11.74003791809082,\n",
      "          0.9003067016601562, 11.020480155944824, 9.313994407653809, 1.882476806640625,\n",
      "          5.926900863647461, 0.9857864379882812, 7.839566230773926, 10.11335277557373,\n",
      "          2.575289726257324, 9.576176643371582, 4.2927398681640625, 12.034239768981934,\n",
      "          4.331302642822266, 5.058969497680664, 12.678888320922852, 11.072522163391113,\n",
      "          17.657264709472656, 11.827741622924805, 9.135469436645508, 11.88493537902832]\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 128032\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 128032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 128032\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 128032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.224999999999998\n",
      "    ram_util_percent: 26.16666666666667\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07638163323397909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.48042146811868847\n",
      "    mean_inference_ms: 4.077860164311576\n",
      "    mean_raw_obs_processing_ms: 1.9878333877080256\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1161.014618127275\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1154.2179070424274, 1141.6683838742279, 1152.3057675938212, 1141.7168694080988,\n",
      "        1160.0590250434752, 1148.0518592108374, 1155.1464241877188, 1168.3644621452552,\n",
      "        1159.4587988651651, 1156.2417262259112, 1166.2319449464078, 1153.9971377397458,\n",
      "        1163.9812567919905, 1165.4796416164284, 1167.1479131368217, 1160.0678455504328,\n",
      "        1156.0702914215542, 1146.953123537582, 1157.6710239517486, 1179.5060561335365,\n",
      "        1168.7513120802607, 1179.2765000434354, 1147.3670004056676, 1155.219264243292,\n",
      "        1149.5794211203795, 1157.9823424963902, 1170.3034692764174, 1179.8151225156312,\n",
      "        1168.0594050845705, 1153.8313801046131, 1171.8166447733145, 1161.3251618972924,\n",
      "        1149.9232930299254, 1169.5627617671107, 1164.702107785678, 1146.9509086617184,\n",
      "        1159.19356383167, 1169.0416564941315, 1153.1568119069077, 1131.492356484583,\n",
      "        1187.943857996734, 1167.6463231935545, 1165.4321160962988, 1160.5609910098847,\n",
      "        1167.472206065449, 1162.6347126792239, 1169.7491047711187, 1156.5952497742326,\n",
      "        1171.5012147456332, 1165.9608429618245, 1169.7868775127968, 1157.489281635276,\n",
      "        1160.1636269704593, 1135.4373108184816, 1167.6142987704247, 1151.727057625516,\n",
      "        1161.0540393492997, 1136.3167438281569, 1156.0183099922835, 1150.3416465863809,\n",
      "        1130.074408239591, 1160.4937820979442, 1179.2612343002804, 1155.4281371124441,\n",
      "        1165.9806350161869, 1147.8737374307136, 1143.0514583296742, 1170.3665325237748,\n",
      "        1160.4988449114476, 1172.11059608468, 1169.8408111515282, 1171.4851715641241,\n",
      "        1163.296681506112, 1159.3619455286369, 1156.8562159813478, 1171.8336049337597,\n",
      "        1144.4817151438883, 1149.6009432809383, 1145.6040996383565, 1157.8552999108706,\n",
      "        1164.8152411483745, 1173.517106826634, 1162.204572488136, 1186.25544641512,\n",
      "        1161.376347244283, 1165.530515390824, 1168.0472969980417, 1165.7151014737215,\n",
      "        1167.6263355156605, 1176.2459465573456, 1175.8502751253186, 1156.01727175097,\n",
      "        1152.9597062781856, 1163.908923946458, 1175.498594875335, 1172.7847671322634,\n",
      "        1169.6852294367152, 1160.610868142743, 1154.0478267751357, 1170.2748297107485]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07638163323397909\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.48042146811868847\n",
      "      mean_inference_ms: 4.077860164311576\n",
      "      mean_raw_obs_processing_ms: 1.9878333877080256\n",
      "  time_since_restore: 707.4176957607269\n",
      "  time_this_iter_s: 33.709166288375854\n",
      "  time_total_s: 707.4176957607269\n",
      "  timers:\n",
      "    learn_throughput: 817.253\n",
      "    learn_time_ms: 39.156\n",
      "    load_throughput: 51513.233\n",
      "    load_time_ms: 0.621\n",
      "    synch_weights_time_ms: 3.784\n",
      "    training_iteration_time_ms: 137.824\n",
      "  timestamp: 1661054527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 17\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  counters:\n",
      "    last_target_update_ts: 36000\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 136032\n",
      "    num_env_steps_sampled: 37000\n",
      "    num_env_steps_trained: 136032\n",
      "    num_target_updates: 6\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1161.2530141379893\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 308\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 36000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.090690612792969\n",
      "          choice_loss: 0.7250819802284241\n",
      "          grad_gnorm: 29.406723022460938\n",
      "          mean_actions: 10.015625\n",
      "          next_q_target_max: 12.325384140014648\n",
      "          next_q_target_slate: 6.413731098175049\n",
      "          next_q_values: 11.528822898864746\n",
      "          q_clicked: 12.909890174865723\n",
      "          q_loss: 105.15504455566406\n",
      "          q_values: 13.226324081420898\n",
      "          replay_click_q: 12.909890174865723\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2552906274795532\n",
      "          slate_q_values: 14.124029159545898\n",
      "          target: 20.953542709350586\n",
      "          target_clicked: 20.953542709350586\n",
      "        mean_td_error: 8.958776473999023\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [10.923369407653809, 9.843266487121582, 14.150166511535645, 20.156597137451172,\n",
      "          13.42415714263916, 14.74107551574707, 1.6233806610107422, 10.215604782104492,\n",
      "          7.483494758605957, 2.8543128967285156, 9.239206314086914, 19.03225326538086,\n",
      "          3.8714637756347656, 4.51225471496582, 8.271936416625977, 11.429804801940918,\n",
      "          10.568704605102539, 7.886133193969727, 7.774468421936035, 3.924592971801758,\n",
      "          3.1753463745117188, 2.457315444946289, 10.301131248474121, 14.960973739624023,\n",
      "          13.869579315185547, 1.6051483154296875, 6.316612243652344, 15.098624229431152,\n",
      "          12.308995246887207, 2.740114212036133, 7.164783477783203, 4.755978584289551]\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 136032\n",
      "    num_env_steps_sampled: 37000\n",
      "    num_env_steps_trained: 136032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 37000\n",
      "  num_agent_steps_trained: 136032\n",
      "  num_env_steps_sampled: 37000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 136032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.557142857142857\n",
      "    ram_util_percent: 26.38367346938776\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07655806038908058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4811686679374565\n",
      "    mean_inference_ms: 4.101931152598809\n",
      "    mean_raw_obs_processing_ms: 1.9889693026505604\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1161.2530141379893\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1159.4587988651651, 1156.2417262259112, 1166.2319449464078, 1153.9971377397458,\n",
      "        1163.9812567919905, 1165.4796416164284, 1167.1479131368217, 1160.0678455504328,\n",
      "        1156.0702914215542, 1146.953123537582, 1157.6710239517486, 1179.5060561335365,\n",
      "        1168.7513120802607, 1179.2765000434354, 1147.3670004056676, 1155.219264243292,\n",
      "        1149.5794211203795, 1157.9823424963902, 1170.3034692764174, 1179.8151225156312,\n",
      "        1168.0594050845705, 1153.8313801046131, 1171.8166447733145, 1161.3251618972924,\n",
      "        1149.9232930299254, 1169.5627617671107, 1164.702107785678, 1146.9509086617184,\n",
      "        1159.19356383167, 1169.0416564941315, 1153.1568119069077, 1131.492356484583,\n",
      "        1187.943857996734, 1167.6463231935545, 1165.4321160962988, 1160.5609910098847,\n",
      "        1167.472206065449, 1162.6347126792239, 1169.7491047711187, 1156.5952497742326,\n",
      "        1171.5012147456332, 1165.9608429618245, 1169.7868775127968, 1157.489281635276,\n",
      "        1160.1636269704593, 1135.4373108184816, 1167.6142987704247, 1151.727057625516,\n",
      "        1161.0540393492997, 1136.3167438281569, 1156.0183099922835, 1150.3416465863809,\n",
      "        1130.074408239591, 1160.4937820979442, 1179.2612343002804, 1155.4281371124441,\n",
      "        1165.9806350161869, 1147.8737374307136, 1143.0514583296742, 1170.3665325237748,\n",
      "        1160.4988449114476, 1172.11059608468, 1169.8408111515282, 1171.4851715641241,\n",
      "        1163.296681506112, 1159.3619455286369, 1156.8562159813478, 1171.8336049337597,\n",
      "        1144.4817151438883, 1149.6009432809383, 1145.6040996383565, 1157.8552999108706,\n",
      "        1164.8152411483745, 1173.517106826634, 1162.204572488136, 1186.25544641512,\n",
      "        1161.376347244283, 1165.530515390824, 1168.0472969980417, 1165.7151014737215,\n",
      "        1167.6263355156605, 1176.2459465573456, 1175.8502751253186, 1156.01727175097,\n",
      "        1152.9597062781856, 1163.908923946458, 1175.498594875335, 1172.7847671322634,\n",
      "        1169.6852294367152, 1160.610868142743, 1154.0478267751357, 1170.2748297107485,\n",
      "        1136.5200756954982, 1175.434487396968, 1156.4682247148828, 1176.6833525028326,\n",
      "        1143.8206771533423, 1166.202484870401, 1142.0570046736257, 1148.1839925697193]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07655806038908058\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4811686679374565\n",
      "      mean_inference_ms: 4.101931152598809\n",
      "      mean_raw_obs_processing_ms: 1.9889693026505604\n",
      "  time_since_restore: 741.4229807853699\n",
      "  time_this_iter_s: 34.005285024642944\n",
      "  time_total_s: 741.4229807853699\n",
      "  timers:\n",
      "    learn_throughput: 813.846\n",
      "    learn_time_ms: 39.319\n",
      "    load_throughput: 49357.455\n",
      "    load_time_ms: 0.648\n",
      "    synch_weights_time_ms: 3.603\n",
      "    training_iteration_time_ms: 133.612\n",
      "  timestamp: 1661054562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 18\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  counters:\n",
      "    last_target_update_ts: 36000\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 144032\n",
      "    num_env_steps_sampled: 38000\n",
      "    num_env_steps_trained: 144032\n",
      "    num_target_updates: 6\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1161.8625331066205\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 316\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 36000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 13.908208847045898\n",
      "          choice_loss: 0.7243267297744751\n",
      "          grad_gnorm: 53.13480758666992\n",
      "          mean_actions: 9.53125\n",
      "          next_q_target_max: 12.102155685424805\n",
      "          next_q_target_slate: 6.361093044281006\n",
      "          next_q_values: 11.458575248718262\n",
      "          q_clicked: 14.186046600341797\n",
      "          q_loss: 459.75616455078125\n",
      "          q_values: 14.142367362976074\n",
      "          replay_click_q: 14.186046600341797\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2485531568527222\n",
      "          slate_q_values: 13.652530670166016\n",
      "          target: 25.88934326171875\n",
      "          target_clicked: 25.88934326171875\n",
      "        mean_td_error: 11.916378021240234\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [8.757582664489746, 6.237561225891113, 8.245162010192871, 13.933431625366211,\n",
      "          18.603981018066406, 4.092561721801758, 5.32390022277832, 2.844087600708008,\n",
      "          58.40975570678711, 8.311352729797363, 11.001970291137695, 15.46865463256836,\n",
      "          3.413175582885742, 2.0376625061035156, 13.682014465332031, 1.661346435546875,\n",
      "          1.3880138397216797, 93.36944580078125, 3.637115478515625, 5.37092399597168,\n",
      "          14.136960983276367, 16.428829193115234, 7.690555572509766, 8.81205940246582,\n",
      "          10.971883773803711, 0.6816463470458984, 10.24367904663086, 1.3396492004394531,\n",
      "          0.9385185241699219, 5.101396560668945, 3.218303680419922, 15.970937728881836]\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 144032\n",
      "    num_env_steps_sampled: 38000\n",
      "    num_env_steps_trained: 144032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 38000\n",
      "  num_agent_steps_trained: 144032\n",
      "  num_env_steps_sampled: 38000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 144032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.568749999999998\n",
      "    ram_util_percent: 26.60625\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07672533200429095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4818655931505424\n",
      "    mean_inference_ms: 4.124525285072155\n",
      "    mean_raw_obs_processing_ms: 1.9900146707123412\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1161.8625331066205\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1156.0702914215542, 1146.953123537582, 1157.6710239517486, 1179.5060561335365,\n",
      "        1168.7513120802607, 1179.2765000434354, 1147.3670004056676, 1155.219264243292,\n",
      "        1149.5794211203795, 1157.9823424963902, 1170.3034692764174, 1179.8151225156312,\n",
      "        1168.0594050845705, 1153.8313801046131, 1171.8166447733145, 1161.3251618972924,\n",
      "        1149.9232930299254, 1169.5627617671107, 1164.702107785678, 1146.9509086617184,\n",
      "        1159.19356383167, 1169.0416564941315, 1153.1568119069077, 1131.492356484583,\n",
      "        1187.943857996734, 1167.6463231935545, 1165.4321160962988, 1160.5609910098847,\n",
      "        1167.472206065449, 1162.6347126792239, 1169.7491047711187, 1156.5952497742326,\n",
      "        1171.5012147456332, 1165.9608429618245, 1169.7868775127968, 1157.489281635276,\n",
      "        1160.1636269704593, 1135.4373108184816, 1167.6142987704247, 1151.727057625516,\n",
      "        1161.0540393492997, 1136.3167438281569, 1156.0183099922835, 1150.3416465863809,\n",
      "        1130.074408239591, 1160.4937820979442, 1179.2612343002804, 1155.4281371124441,\n",
      "        1165.9806350161869, 1147.8737374307136, 1143.0514583296742, 1170.3665325237748,\n",
      "        1160.4988449114476, 1172.11059608468, 1169.8408111515282, 1171.4851715641241,\n",
      "        1163.296681506112, 1159.3619455286369, 1156.8562159813478, 1171.8336049337597,\n",
      "        1144.4817151438883, 1149.6009432809383, 1145.6040996383565, 1157.8552999108706,\n",
      "        1164.8152411483745, 1173.517106826634, 1162.204572488136, 1186.25544641512,\n",
      "        1161.376347244283, 1165.530515390824, 1168.0472969980417, 1165.7151014737215,\n",
      "        1167.6263355156605, 1176.2459465573456, 1175.8502751253186, 1156.01727175097,\n",
      "        1152.9597062781856, 1163.908923946458, 1175.498594875335, 1172.7847671322634,\n",
      "        1169.6852294367152, 1160.610868142743, 1154.0478267751357, 1170.2748297107485,\n",
      "        1136.5200756954982, 1175.434487396968, 1156.4682247148828, 1176.6833525028326,\n",
      "        1143.8206771533423, 1166.202484870401, 1142.0570046736257, 1148.1839925697193,\n",
      "        1176.564785081392, 1160.7331769653413, 1186.1205696197187, 1165.3043900062098,\n",
      "        1175.9794958466293, 1171.380893554055, 1163.0653375768973, 1154.4095130857775]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07672533200429095\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4818655931505424\n",
      "      mean_inference_ms: 4.124525285072155\n",
      "      mean_raw_obs_processing_ms: 1.9900146707123412\n",
      "  time_since_restore: 775.2763087749481\n",
      "  time_this_iter_s: 33.85332798957825\n",
      "  time_total_s: 775.2763087749481\n",
      "  timers:\n",
      "    learn_throughput: 789.31\n",
      "    learn_time_ms: 40.542\n",
      "    load_throughput: 51548.845\n",
      "    load_time_ms: 0.621\n",
      "    synch_weights_time_ms: 3.794\n",
      "    training_iteration_time_ms: 135.273\n",
      "  timestamp: 1661054595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 19\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  counters:\n",
      "    last_target_update_ts: 36000\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 152032\n",
      "    num_env_steps_sampled: 39000\n",
      "    num_env_steps_trained: 152032\n",
      "    num_target_updates: 6\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-03-50\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1162.1041840934931\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 325\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 36000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 10.184026718139648\n",
      "          choice_loss: 0.7212508320808411\n",
      "          grad_gnorm: 35.36351776123047\n",
      "          mean_actions: 9.484375\n",
      "          next_q_target_max: 12.048288345336914\n",
      "          next_q_target_slate: 6.317941188812256\n",
      "          next_q_values: 11.426961898803711\n",
      "          q_clicked: 14.172219276428223\n",
      "          q_loss: 87.56580352783203\n",
      "          q_values: 15.026636123657227\n",
      "          replay_click_q: 14.172219276428223\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2462661266326904\n",
      "          slate_q_values: 15.111993789672852\n",
      "          target: 22.111831665039062\n",
      "          target_clicked: 22.111831665039062\n",
      "        mean_td_error: 8.364758491516113\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [17.936107635498047, 5.037082672119141, 13.08665657043457, 5.699892997741699,\n",
      "          4.495279312133789, 9.394692420959473, 7.180751800537109, 15.327391624450684,\n",
      "          11.63569164276123, 7.184861183166504, 6.856890678405762, 7.03879451751709,\n",
      "          3.4216232299804688, 3.0839595794677734, 4.303070068359375, 2.4992752075195312,\n",
      "          4.289836883544922, 10.479846954345703, 2.990337371826172, 7.1418256759643555,\n",
      "          5.405393600463867, 13.127095222473145, 9.317241668701172, 6.287868499755859,\n",
      "          16.4130916595459, 11.819908142089844, 12.502840995788574, 2.8219337463378906,\n",
      "          9.97744369506836, 12.105101585388184, 12.08080005645752, 6.729681015014648]\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 152032\n",
      "    num_env_steps_sampled: 39000\n",
      "    num_env_steps_trained: 152032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 39000\n",
      "  num_agent_steps_trained: 152032\n",
      "  num_env_steps_sampled: 39000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 152032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.2375\n",
      "    ram_util_percent: 26.84375\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07690175861782053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4826120273280155\n",
      "    mean_inference_ms: 4.148477883017211\n",
      "    mean_raw_obs_processing_ms: 1.9911019093944649\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1162.1041840934931\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1157.9823424963902, 1170.3034692764174, 1179.8151225156312, 1168.0594050845705,\n",
      "        1153.8313801046131, 1171.8166447733145, 1161.3251618972924, 1149.9232930299254,\n",
      "        1169.5627617671107, 1164.702107785678, 1146.9509086617184, 1159.19356383167,\n",
      "        1169.0416564941315, 1153.1568119069077, 1131.492356484583, 1187.943857996734,\n",
      "        1167.6463231935545, 1165.4321160962988, 1160.5609910098847, 1167.472206065449,\n",
      "        1162.6347126792239, 1169.7491047711187, 1156.5952497742326, 1171.5012147456332,\n",
      "        1165.9608429618245, 1169.7868775127968, 1157.489281635276, 1160.1636269704593,\n",
      "        1135.4373108184816, 1167.6142987704247, 1151.727057625516, 1161.0540393492997,\n",
      "        1136.3167438281569, 1156.0183099922835, 1150.3416465863809, 1130.074408239591,\n",
      "        1160.4937820979442, 1179.2612343002804, 1155.4281371124441, 1165.9806350161869,\n",
      "        1147.8737374307136, 1143.0514583296742, 1170.3665325237748, 1160.4988449114476,\n",
      "        1172.11059608468, 1169.8408111515282, 1171.4851715641241, 1163.296681506112,\n",
      "        1159.3619455286369, 1156.8562159813478, 1171.8336049337597, 1144.4817151438883,\n",
      "        1149.6009432809383, 1145.6040996383565, 1157.8552999108706, 1164.8152411483745,\n",
      "        1173.517106826634, 1162.204572488136, 1186.25544641512, 1161.376347244283, 1165.530515390824,\n",
      "        1168.0472969980417, 1165.7151014737215, 1167.6263355156605, 1176.2459465573456,\n",
      "        1175.8502751253186, 1156.01727175097, 1152.9597062781856, 1163.908923946458,\n",
      "        1175.498594875335, 1172.7847671322634, 1169.6852294367152, 1160.610868142743,\n",
      "        1154.0478267751357, 1170.2748297107485, 1136.5200756954982, 1175.434487396968,\n",
      "        1156.4682247148828, 1176.6833525028326, 1143.8206771533423, 1166.202484870401,\n",
      "        1142.0570046736257, 1148.1839925697193, 1176.564785081392, 1160.7331769653413,\n",
      "        1186.1205696197187, 1165.3043900062098, 1175.9794958466293, 1171.380893554055,\n",
      "        1163.0653375768973, 1154.4095130857775, 1182.237449494122, 1152.7768695896,\n",
      "        1166.299055072828, 1169.5100833812503, 1170.4274430944804, 1164.0003070221608,\n",
      "        1161.2194825885572, 1144.50647313447, 1153.58192824728]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07690175861782053\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4826120273280155\n",
      "      mean_inference_ms: 4.148477883017211\n",
      "      mean_raw_obs_processing_ms: 1.9911019093944649\n",
      "  time_since_restore: 809.386561870575\n",
      "  time_this_iter_s: 34.11025309562683\n",
      "  time_total_s: 809.386561870575\n",
      "  timers:\n",
      "    learn_throughput: 776.493\n",
      "    learn_time_ms: 41.211\n",
      "    load_throughput: 51707.72\n",
      "    load_time_ms: 0.619\n",
      "    synch_weights_time_ms: 3.818\n",
      "    training_iteration_time_ms: 138.325\n",
      "  timestamp: 1661054630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 20\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    last_target_update_ts: 39200\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 160032\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 160032\n",
      "    num_target_updates: 7\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1187.943857996734\n",
      "  episode_reward_mean: 1161.9844411972008\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 333\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 39200\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 12.252835273742676\n",
      "          choice_loss: 0.739122748374939\n",
      "          grad_gnorm: 57.46548080444336\n",
      "          mean_actions: 9.96875\n",
      "          next_q_target_max: 12.51734733581543\n",
      "          next_q_target_slate: 8.369868278503418\n",
      "          next_q_values: 15.13995361328125\n",
      "          q_clicked: 16.802963256835938\n",
      "          q_loss: 240.3114013671875\n",
      "          q_values: 16.225791931152344\n",
      "          replay_click_q: 16.802963256835938\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2385685443878174\n",
      "          slate_q_values: 16.90309715270996\n",
      "          target: 24.279687881469727\n",
      "          target_clicked: 24.279687881469727\n",
      "        mean_td_error: 9.49362850189209\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [11.258859634399414, 6.927045822143555, 39.72565460205078, 5.800436019897461,\n",
      "          9.286148071289062, 0.16675567626953125, 15.424030303955078, 9.022178649902344,\n",
      "          7.204612731933594, 4.052057266235352, 65.03560638427734, 4.282169342041016,\n",
      "          12.498147010803223, 3.273408889770508, 1.4955787658691406, 9.586872100830078,\n",
      "          2.4917984008789062, 1.8097000122070312, 1.1105918884277344, 3.814950942993164,\n",
      "          4.823383331298828, 6.6046142578125, 5.153428077697754, 6.166923522949219,\n",
      "          13.018896102905273, 12.740131378173828, 12.883920669555664, 13.707228660583496,\n",
      "          1.4706802368164062, 7.340120315551758, 4.579802513122559, 1.0403671264648438]\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 160032\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 160032\n",
      "    num_target_updates: 7\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 160032\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 160032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.767346938775507\n",
      "    ram_util_percent: 27.11020408163265\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07705017270522659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.48324024108933644\n",
      "    mean_inference_ms: 4.168545849975477\n",
      "    mean_raw_obs_processing_ms: 1.9920047944572854\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1187.943857996734\n",
      "    episode_reward_mean: 1161.9844411972008\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1169.5627617671107, 1164.702107785678, 1146.9509086617184, 1159.19356383167,\n",
      "        1169.0416564941315, 1153.1568119069077, 1131.492356484583, 1187.943857996734,\n",
      "        1167.6463231935545, 1165.4321160962988, 1160.5609910098847, 1167.472206065449,\n",
      "        1162.6347126792239, 1169.7491047711187, 1156.5952497742326, 1171.5012147456332,\n",
      "        1165.9608429618245, 1169.7868775127968, 1157.489281635276, 1160.1636269704593,\n",
      "        1135.4373108184816, 1167.6142987704247, 1151.727057625516, 1161.0540393492997,\n",
      "        1136.3167438281569, 1156.0183099922835, 1150.3416465863809, 1130.074408239591,\n",
      "        1160.4937820979442, 1179.2612343002804, 1155.4281371124441, 1165.9806350161869,\n",
      "        1147.8737374307136, 1143.0514583296742, 1170.3665325237748, 1160.4988449114476,\n",
      "        1172.11059608468, 1169.8408111515282, 1171.4851715641241, 1163.296681506112,\n",
      "        1159.3619455286369, 1156.8562159813478, 1171.8336049337597, 1144.4817151438883,\n",
      "        1149.6009432809383, 1145.6040996383565, 1157.8552999108706, 1164.8152411483745,\n",
      "        1173.517106826634, 1162.204572488136, 1186.25544641512, 1161.376347244283, 1165.530515390824,\n",
      "        1168.0472969980417, 1165.7151014737215, 1167.6263355156605, 1176.2459465573456,\n",
      "        1175.8502751253186, 1156.01727175097, 1152.9597062781856, 1163.908923946458,\n",
      "        1175.498594875335, 1172.7847671322634, 1169.6852294367152, 1160.610868142743,\n",
      "        1154.0478267751357, 1170.2748297107485, 1136.5200756954982, 1175.434487396968,\n",
      "        1156.4682247148828, 1176.6833525028326, 1143.8206771533423, 1166.202484870401,\n",
      "        1142.0570046736257, 1148.1839925697193, 1176.564785081392, 1160.7331769653413,\n",
      "        1186.1205696197187, 1165.3043900062098, 1175.9794958466293, 1171.380893554055,\n",
      "        1163.0653375768973, 1154.4095130857775, 1182.237449494122, 1152.7768695896,\n",
      "        1166.299055072828, 1169.5100833812503, 1170.4274430944804, 1164.0003070221608,\n",
      "        1161.2194825885572, 1144.50647313447, 1153.58192824728, 1149.0167222574578,\n",
      "        1173.6243121745892, 1171.878159907749, 1156.2820350551422, 1166.7368823655572,\n",
      "        1147.264368533752, 1167.3439061041381, 1168.9361431505079]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07705017270522659\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.48324024108933644\n",
      "      mean_inference_ms: 4.168545849975477\n",
      "      mean_raw_obs_processing_ms: 1.9920047944572854\n",
      "  time_since_restore: 843.7018086910248\n",
      "  time_this_iter_s: 34.31524682044983\n",
      "  time_total_s: 843.7018086910248\n",
      "  timers:\n",
      "    learn_throughput: 760.778\n",
      "    learn_time_ms: 42.062\n",
      "    load_throughput: 50195.493\n",
      "    load_time_ms: 0.638\n",
      "    synch_weights_time_ms: 3.789\n",
      "    training_iteration_time_ms: 138.703\n",
      "  timestamp: 1661054664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 21\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  counters:\n",
      "    last_target_update_ts: 39200\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 168032\n",
      "    num_env_steps_sampled: 41000\n",
      "    num_env_steps_trained: 168032\n",
      "    num_target_updates: 7\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-04-58\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1186.25544641512\n",
      "  episode_reward_mean: 1162.2168620438692\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 341\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 39200\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 14.083023071289062\n",
      "          choice_loss: 0.7177016735076904\n",
      "          grad_gnorm: 88.25996398925781\n",
      "          mean_actions: 9.875\n",
      "          next_q_target_max: 12.454675674438477\n",
      "          next_q_target_slate: 8.44243335723877\n",
      "          next_q_values: 15.243474006652832\n",
      "          q_clicked: 18.883630752563477\n",
      "          q_loss: 262.3804016113281\n",
      "          q_values: 17.03717613220215\n",
      "          replay_click_q: 18.883630752563477\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2470686435699463\n",
      "          slate_q_values: 17.41604995727539\n",
      "          target: 26.413150787353516\n",
      "          target_clicked: 26.413150787353516\n",
      "        mean_td_error: 8.955102920532227\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [2.1822891235351562, 3.4452037811279297, 9.712034225463867, 10.464273452758789,\n",
      "          4.540896415710449, 6.394588470458984, 3.210653305053711, 2.640260696411133,\n",
      "          6.613503456115723, 4.932582855224609, 5.535255432128906, 1.0865020751953125,\n",
      "          13.314687728881836, 3.6446590423583984, 3.1859378814697266, 3.6814517974853516,\n",
      "          10.156791687011719, 8.294130325317383, 12.601334571838379, 1.1768608093261719,\n",
      "          2.4439697265625, 9.843070983886719, 8.903890609741211, 56.7397575378418, 1.4503440856933594,\n",
      "          4.566570281982422, 4.785430908203125, 8.445810317993164, 2.8893489837646484,\n",
      "          2.3832130432128906, 62.401153564453125, 4.896854400634766]\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 168032\n",
      "    num_env_steps_sampled: 41000\n",
      "    num_env_steps_trained: 168032\n",
      "    num_target_updates: 7\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 41000\n",
      "  num_agent_steps_trained: 168032\n",
      "  num_env_steps_sampled: 41000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 168032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.233333333333334\n",
      "    ram_util_percent: 27.33125\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07719151509933977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4838419000530753\n",
      "    mean_inference_ms: 4.187490549340564\n",
      "    mean_raw_obs_processing_ms: 1.9928960093236427\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1186.25544641512\n",
      "    episode_reward_mean: 1162.2168620438692\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1167.6463231935545, 1165.4321160962988, 1160.5609910098847, 1167.472206065449,\n",
      "        1162.6347126792239, 1169.7491047711187, 1156.5952497742326, 1171.5012147456332,\n",
      "        1165.9608429618245, 1169.7868775127968, 1157.489281635276, 1160.1636269704593,\n",
      "        1135.4373108184816, 1167.6142987704247, 1151.727057625516, 1161.0540393492997,\n",
      "        1136.3167438281569, 1156.0183099922835, 1150.3416465863809, 1130.074408239591,\n",
      "        1160.4937820979442, 1179.2612343002804, 1155.4281371124441, 1165.9806350161869,\n",
      "        1147.8737374307136, 1143.0514583296742, 1170.3665325237748, 1160.4988449114476,\n",
      "        1172.11059608468, 1169.8408111515282, 1171.4851715641241, 1163.296681506112,\n",
      "        1159.3619455286369, 1156.8562159813478, 1171.8336049337597, 1144.4817151438883,\n",
      "        1149.6009432809383, 1145.6040996383565, 1157.8552999108706, 1164.8152411483745,\n",
      "        1173.517106826634, 1162.204572488136, 1186.25544641512, 1161.376347244283, 1165.530515390824,\n",
      "        1168.0472969980417, 1165.7151014737215, 1167.6263355156605, 1176.2459465573456,\n",
      "        1175.8502751253186, 1156.01727175097, 1152.9597062781856, 1163.908923946458,\n",
      "        1175.498594875335, 1172.7847671322634, 1169.6852294367152, 1160.610868142743,\n",
      "        1154.0478267751357, 1170.2748297107485, 1136.5200756954982, 1175.434487396968,\n",
      "        1156.4682247148828, 1176.6833525028326, 1143.8206771533423, 1166.202484870401,\n",
      "        1142.0570046736257, 1148.1839925697193, 1176.564785081392, 1160.7331769653413,\n",
      "        1186.1205696197187, 1165.3043900062098, 1175.9794958466293, 1171.380893554055,\n",
      "        1163.0653375768973, 1154.4095130857775, 1182.237449494122, 1152.7768695896,\n",
      "        1166.299055072828, 1169.5100833812503, 1170.4274430944804, 1164.0003070221608,\n",
      "        1161.2194825885572, 1144.50647313447, 1153.58192824728, 1149.0167222574578,\n",
      "        1173.6243121745892, 1171.878159907749, 1156.2820350551422, 1166.7368823655572,\n",
      "        1147.264368533752, 1167.3439061041381, 1168.9361431505079, 1162.3862616436402,\n",
      "        1175.919860172385, 1154.2032952838786, 1164.6716881678374, 1170.9239909793482,\n",
      "        1158.43211604694, 1150.561284485556, 1168.1876128158042]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07719151509933977\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4838419000530753\n",
      "      mean_inference_ms: 4.187490549340564\n",
      "      mean_raw_obs_processing_ms: 1.9928960093236427\n",
      "  time_since_restore: 877.8836562633514\n",
      "  time_this_iter_s: 34.18184757232666\n",
      "  time_total_s: 877.8836562633514\n",
      "  timers:\n",
      "    learn_throughput: 776.338\n",
      "    learn_time_ms: 41.219\n",
      "    load_throughput: 50818.874\n",
      "    load_time_ms: 0.63\n",
      "    synch_weights_time_ms: 3.775\n",
      "    training_iteration_time_ms: 136.2\n",
      "  timestamp: 1661054698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 22\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  counters:\n",
      "    last_target_update_ts: 39200\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 176032\n",
      "    num_env_steps_sampled: 42000\n",
      "    num_env_steps_trained: 176032\n",
      "    num_target_updates: 7\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-05-33\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1162.360868272323\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 350\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 39200\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 12.34411334991455\n",
      "          choice_loss: 0.7108034491539001\n",
      "          grad_gnorm: 67.19865417480469\n",
      "          mean_actions: 10.390625\n",
      "          next_q_target_max: 12.41563892364502\n",
      "          next_q_target_slate: 8.45901870727539\n",
      "          next_q_values: 15.226635932922363\n",
      "          q_clicked: 19.397140502929688\n",
      "          q_loss: 136.1149444580078\n",
      "          q_values: 17.36178207397461\n",
      "          replay_click_q: 19.397140502929688\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.25510835647583\n",
      "          slate_q_values: 18.647260665893555\n",
      "          target: 24.635595321655273\n",
      "          target_clicked: 24.635595321655273\n",
      "        mean_td_error: 5.860431671142578\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [1.5095939636230469, 2.673004150390625, 5.026035308837891, 0.1308269500732422,\n",
      "          5.261262893676758, 6.75715446472168, 1.1037254333496094, 6.258039474487305,\n",
      "          2.8747177124023438, 5.262361526489258, 2.3145980834960938, 3.6467151641845703,\n",
      "          0.09435272216796875, 6.828981399536133, 2.5893611907958984, 3.444154739379883,\n",
      "          59.74755096435547, 3.977815628051758, 2.944011688232422, 1.0059928894042969,\n",
      "          10.316557884216309, 1.013479232788086, 9.312952041625977, 4.766323089599609,\n",
      "          4.741420745849609, 3.636934280395508, 1.7285499572753906, 2.9662952423095703,\n",
      "          7.242902755737305, 4.370368957519531, 12.396133422851562, 1.5916366577148438]\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 176032\n",
      "    num_env_steps_sampled: 42000\n",
      "    num_env_steps_trained: 176032\n",
      "    num_target_updates: 7\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 42000\n",
      "  num_agent_steps_trained: 176032\n",
      "  num_env_steps_sampled: 42000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 176032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.416326530612245\n",
      "    ram_util_percent: 27.532653061224487\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07734435878650908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4844758320933325\n",
      "    mean_inference_ms: 4.207603638302491\n",
      "    mean_raw_obs_processing_ms: 1.993855438181773\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1162.360868272323\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1169.7868775127968, 1157.489281635276, 1160.1636269704593, 1135.4373108184816,\n",
      "        1167.6142987704247, 1151.727057625516, 1161.0540393492997, 1136.3167438281569,\n",
      "        1156.0183099922835, 1150.3416465863809, 1130.074408239591, 1160.4937820979442,\n",
      "        1179.2612343002804, 1155.4281371124441, 1165.9806350161869, 1147.8737374307136,\n",
      "        1143.0514583296742, 1170.3665325237748, 1160.4988449114476, 1172.11059608468,\n",
      "        1169.8408111515282, 1171.4851715641241, 1163.296681506112, 1159.3619455286369,\n",
      "        1156.8562159813478, 1171.8336049337597, 1144.4817151438883, 1149.6009432809383,\n",
      "        1145.6040996383565, 1157.8552999108706, 1164.8152411483745, 1173.517106826634,\n",
      "        1162.204572488136, 1186.25544641512, 1161.376347244283, 1165.530515390824, 1168.0472969980417,\n",
      "        1165.7151014737215, 1167.6263355156605, 1176.2459465573456, 1175.8502751253186,\n",
      "        1156.01727175097, 1152.9597062781856, 1163.908923946458, 1175.498594875335,\n",
      "        1172.7847671322634, 1169.6852294367152, 1160.610868142743, 1154.0478267751357,\n",
      "        1170.2748297107485, 1136.5200756954982, 1175.434487396968, 1156.4682247148828,\n",
      "        1176.6833525028326, 1143.8206771533423, 1166.202484870401, 1142.0570046736257,\n",
      "        1148.1839925697193, 1176.564785081392, 1160.7331769653413, 1186.1205696197187,\n",
      "        1165.3043900062098, 1175.9794958466293, 1171.380893554055, 1163.0653375768973,\n",
      "        1154.4095130857775, 1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503,\n",
      "        1170.4274430944804, 1164.0003070221608, 1161.2194825885572, 1144.50647313447,\n",
      "        1153.58192824728, 1149.0167222574578, 1173.6243121745892, 1171.878159907749,\n",
      "        1156.2820350551422, 1166.7368823655572, 1147.264368533752, 1167.3439061041381,\n",
      "        1168.9361431505079, 1162.3862616436402, 1175.919860172385, 1154.2032952838786,\n",
      "        1164.6716881678374, 1170.9239909793482, 1158.43211604694, 1150.561284485556,\n",
      "        1168.1876128158042, 1161.7689157195962, 1162.3009729610762, 1164.410891646221,\n",
      "        1156.1152905846955, 1183.4126362756163, 1171.7398370031865, 1191.3173453704933,\n",
      "        1151.9614074798817, 1158.9260871017896]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07734435878650908\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4844758320933325\n",
      "      mean_inference_ms: 4.207603638302491\n",
      "      mean_raw_obs_processing_ms: 1.993855438181773\n",
      "  time_since_restore: 912.2140345573425\n",
      "  time_this_iter_s: 34.33037829399109\n",
      "  time_total_s: 912.2140345573425\n",
      "  timers:\n",
      "    learn_throughput: 789.468\n",
      "    learn_time_ms: 40.534\n",
      "    load_throughput: 48490.815\n",
      "    load_time_ms: 0.66\n",
      "    synch_weights_time_ms: 3.594\n",
      "    training_iteration_time_ms: 137.587\n",
      "  timestamp: 1661054733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 23\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  counters:\n",
      "    last_target_update_ts: 42400\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 184032\n",
      "    num_env_steps_sampled: 43000\n",
      "    num_env_steps_trained: 184032\n",
      "    num_target_updates: 8\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1163.4088673084368\n",
      "  episode_reward_min: 1130.074408239591\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 358\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 42400\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.970878601074219\n",
      "          choice_loss: 0.7146838307380676\n",
      "          grad_gnorm: 28.542911529541016\n",
      "          mean_actions: 9.734375\n",
      "          next_q_target_max: 13.170137405395508\n",
      "          next_q_target_slate: 9.74021053314209\n",
      "          next_q_values: 17.499462127685547\n",
      "          q_clicked: 17.644432067871094\n",
      "          q_loss: 45.3245964050293\n",
      "          q_values: 18.09897232055664\n",
      "          replay_click_q: 17.644432067871094\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2600064277648926\n",
      "          slate_q_values: 17.575286865234375\n",
      "          target: 23.009313583374023\n",
      "          target_clicked: 23.009313583374023\n",
      "        mean_td_error: 5.778146743774414\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [5.126232147216797, 0.2774639129638672, 1.326406478881836, 5.096647262573242,\n",
      "          12.265830039978027, 2.1436023712158203, 3.0197982788085938, 7.342342376708984,\n",
      "          5.685600280761719, 4.04020881652832, 2.9976158142089844, 2.168872833251953,\n",
      "          4.284725189208984, 2.552082061767578, 9.635251998901367, 3.4158287048339844,\n",
      "          8.70394515991211, 9.4861478805542, 12.887632369995117, 11.759767532348633,\n",
      "          8.650192260742188, 1.9541091918945312, 3.082366943359375, 1.9294929504394531,\n",
      "          7.141530990600586, 10.22529411315918, 6.077402114868164, 9.52369213104248,\n",
      "          4.060157775878906, 9.308928489685059, 4.529132843017578, 4.202384948730469]\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 184032\n",
      "    num_env_steps_sampled: 43000\n",
      "    num_env_steps_trained: 184032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 43000\n",
      "  num_agent_steps_trained: 184032\n",
      "  num_env_steps_sampled: 43000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 184032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.242857142857144\n",
      "    ram_util_percent: 27.742857142857144\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07747473576959525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.48500307433811857\n",
      "    mean_inference_ms: 4.22439725108783\n",
      "    mean_raw_obs_processing_ms: 1.994687503678224\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1163.4088673084368\n",
      "    episode_reward_min: 1130.074408239591\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1156.0183099922835, 1150.3416465863809, 1130.074408239591, 1160.4937820979442,\n",
      "        1179.2612343002804, 1155.4281371124441, 1165.9806350161869, 1147.8737374307136,\n",
      "        1143.0514583296742, 1170.3665325237748, 1160.4988449114476, 1172.11059608468,\n",
      "        1169.8408111515282, 1171.4851715641241, 1163.296681506112, 1159.3619455286369,\n",
      "        1156.8562159813478, 1171.8336049337597, 1144.4817151438883, 1149.6009432809383,\n",
      "        1145.6040996383565, 1157.8552999108706, 1164.8152411483745, 1173.517106826634,\n",
      "        1162.204572488136, 1186.25544641512, 1161.376347244283, 1165.530515390824, 1168.0472969980417,\n",
      "        1165.7151014737215, 1167.6263355156605, 1176.2459465573456, 1175.8502751253186,\n",
      "        1156.01727175097, 1152.9597062781856, 1163.908923946458, 1175.498594875335,\n",
      "        1172.7847671322634, 1169.6852294367152, 1160.610868142743, 1154.0478267751357,\n",
      "        1170.2748297107485, 1136.5200756954982, 1175.434487396968, 1156.4682247148828,\n",
      "        1176.6833525028326, 1143.8206771533423, 1166.202484870401, 1142.0570046736257,\n",
      "        1148.1839925697193, 1176.564785081392, 1160.7331769653413, 1186.1205696197187,\n",
      "        1165.3043900062098, 1175.9794958466293, 1171.380893554055, 1163.0653375768973,\n",
      "        1154.4095130857775, 1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503,\n",
      "        1170.4274430944804, 1164.0003070221608, 1161.2194825885572, 1144.50647313447,\n",
      "        1153.58192824728, 1149.0167222574578, 1173.6243121745892, 1171.878159907749,\n",
      "        1156.2820350551422, 1166.7368823655572, 1147.264368533752, 1167.3439061041381,\n",
      "        1168.9361431505079, 1162.3862616436402, 1175.919860172385, 1154.2032952838786,\n",
      "        1164.6716881678374, 1170.9239909793482, 1158.43211604694, 1150.561284485556,\n",
      "        1168.1876128158042, 1161.7689157195962, 1162.3009729610762, 1164.410891646221,\n",
      "        1156.1152905846955, 1183.4126362756163, 1171.7398370031865, 1191.3173453704933,\n",
      "        1151.9614074798817, 1158.9260871017896, 1165.0765511574295, 1142.8448177240366,\n",
      "        1185.4930051823917, 1164.8544516402696, 1162.0267136423367, 1188.6413494142923,\n",
      "        1164.2148810984302, 1171.2373702626505]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07747473576959525\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.48500307433811857\n",
      "      mean_inference_ms: 4.22439725108783\n",
      "      mean_raw_obs_processing_ms: 1.994687503678224\n",
      "  time_since_restore: 946.6487641334534\n",
      "  time_this_iter_s: 34.43472957611084\n",
      "  time_total_s: 946.6487641334534\n",
      "  timers:\n",
      "    learn_throughput: 784.382\n",
      "    learn_time_ms: 40.796\n",
      "    load_throughput: 46532.287\n",
      "    load_time_ms: 0.688\n",
      "    synch_weights_time_ms: 3.735\n",
      "    training_iteration_time_ms: 138.261\n",
      "  timestamp: 1661054767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 24\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  counters:\n",
      "    last_target_update_ts: 42400\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 192032\n",
      "    num_env_steps_sampled: 44000\n",
      "    num_env_steps_trained: 192032\n",
      "    num_target_updates: 8\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-06-43\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1163.7586294898529\n",
      "  episode_reward_min: 1136.5200756954982\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 366\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 42400\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 16.044780731201172\n",
      "          choice_loss: 0.7053031921386719\n",
      "          grad_gnorm: 97.50860595703125\n",
      "          mean_actions: 9.375\n",
      "          next_q_target_max: 13.315954208374023\n",
      "          next_q_target_slate: 9.779866218566895\n",
      "          next_q_values: 17.60975456237793\n",
      "          q_clicked: 19.18303680419922\n",
      "          q_loss: 657.6499633789062\n",
      "          q_values: 18.533931732177734\n",
      "          replay_click_q: 19.18303680419922\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2553472518920898\n",
      "          slate_q_values: 18.731735229492188\n",
      "          target: 29.227577209472656\n",
      "          target_clicked: 29.227577209472656\n",
      "        mean_td_error: 10.745899200439453\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [5.714728355407715, 3.1136646270751953, 7.689032554626465, 112.23687744140625,\n",
      "          10.379972457885742, 7.140562057495117, 3.3430423736572266, 27.688465118408203,\n",
      "          3.18548583984375, 0.38491249084472656, 0.8712348937988281, 2.171825408935547,\n",
      "          3.331634521484375, 0.3293914794921875, 7.757627487182617, 83.2960205078125,\n",
      "          3.4810028076171875, 2.027132034301758, 1.197183609008789, 2.8226280212402344,\n",
      "          8.051299095153809, 9.277071952819824, 3.8787307739257812, 1.5106277465820312,\n",
      "          3.1141815185546875, 1.7131633758544922, 3.829202651977539, 7.434081077575684,\n",
      "          2.104034423828125, 1.2699661254882812, 4.484857559204102, 9.039140701293945]\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 192032\n",
      "    num_env_steps_sampled: 44000\n",
      "    num_env_steps_trained: 192032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 192032\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 192032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.646938775510204\n",
      "    ram_util_percent: 27.96938775510204\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07759838420447739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4855085467105221\n",
      "    mean_inference_ms: 4.240276479420282\n",
      "    mean_raw_obs_processing_ms: 1.9954799210474932\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1163.7586294898529\n",
      "    episode_reward_min: 1136.5200756954982\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1143.0514583296742, 1170.3665325237748, 1160.4988449114476, 1172.11059608468,\n",
      "        1169.8408111515282, 1171.4851715641241, 1163.296681506112, 1159.3619455286369,\n",
      "        1156.8562159813478, 1171.8336049337597, 1144.4817151438883, 1149.6009432809383,\n",
      "        1145.6040996383565, 1157.8552999108706, 1164.8152411483745, 1173.517106826634,\n",
      "        1162.204572488136, 1186.25544641512, 1161.376347244283, 1165.530515390824, 1168.0472969980417,\n",
      "        1165.7151014737215, 1167.6263355156605, 1176.2459465573456, 1175.8502751253186,\n",
      "        1156.01727175097, 1152.9597062781856, 1163.908923946458, 1175.498594875335,\n",
      "        1172.7847671322634, 1169.6852294367152, 1160.610868142743, 1154.0478267751357,\n",
      "        1170.2748297107485, 1136.5200756954982, 1175.434487396968, 1156.4682247148828,\n",
      "        1176.6833525028326, 1143.8206771533423, 1166.202484870401, 1142.0570046736257,\n",
      "        1148.1839925697193, 1176.564785081392, 1160.7331769653413, 1186.1205696197187,\n",
      "        1165.3043900062098, 1175.9794958466293, 1171.380893554055, 1163.0653375768973,\n",
      "        1154.4095130857775, 1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503,\n",
      "        1170.4274430944804, 1164.0003070221608, 1161.2194825885572, 1144.50647313447,\n",
      "        1153.58192824728, 1149.0167222574578, 1173.6243121745892, 1171.878159907749,\n",
      "        1156.2820350551422, 1166.7368823655572, 1147.264368533752, 1167.3439061041381,\n",
      "        1168.9361431505079, 1162.3862616436402, 1175.919860172385, 1154.2032952838786,\n",
      "        1164.6716881678374, 1170.9239909793482, 1158.43211604694, 1150.561284485556,\n",
      "        1168.1876128158042, 1161.7689157195962, 1162.3009729610762, 1164.410891646221,\n",
      "        1156.1152905846955, 1183.4126362756163, 1171.7398370031865, 1191.3173453704933,\n",
      "        1151.9614074798817, 1158.9260871017896, 1165.0765511574295, 1142.8448177240366,\n",
      "        1185.4930051823917, 1164.8544516402696, 1162.0267136423367, 1188.6413494142923,\n",
      "        1164.2148810984302, 1171.2373702626505, 1153.2186394314383, 1149.1851548234642,\n",
      "        1169.2810067432656, 1158.449168143026, 1148.130936342867, 1177.8516780623029,\n",
      "        1167.5898099070957, 1156.7417154639334]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07759838420447739\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4855085467105221\n",
      "      mean_inference_ms: 4.240276479420282\n",
      "      mean_raw_obs_processing_ms: 1.9954799210474932\n",
      "  time_since_restore: 982.7133376598358\n",
      "  time_this_iter_s: 36.064573526382446\n",
      "  time_total_s: 982.7133376598358\n",
      "  timers:\n",
      "    learn_throughput: 771.543\n",
      "    learn_time_ms: 41.475\n",
      "    load_throughput: 50709.433\n",
      "    load_time_ms: 0.631\n",
      "    synch_weights_time_ms: 3.723\n",
      "    training_iteration_time_ms: 136.856\n",
      "  timestamp: 1661054803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 25\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  counters:\n",
      "    last_target_update_ts: 42400\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 200032\n",
      "    num_env_steps_sampled: 45000\n",
      "    num_env_steps_trained: 200032\n",
      "    num_target_updates: 8\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-07-18\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1164.1670054499168\n",
      "  episode_reward_min: 1136.5200756954982\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 375\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 42400\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 10.070240020751953\n",
      "          choice_loss: 0.7173205018043518\n",
      "          grad_gnorm: 25.979848861694336\n",
      "          mean_actions: 10.65625\n",
      "          next_q_target_max: 13.338823318481445\n",
      "          next_q_target_slate: 9.671985626220703\n",
      "          next_q_values: 17.469310760498047\n",
      "          q_clicked: 18.79355239868164\n",
      "          q_loss: 39.565982818603516\n",
      "          q_values: 18.84624481201172\n",
      "          replay_click_q: 18.79355239868164\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.247866153717041\n",
      "          slate_q_values: 18.448387145996094\n",
      "          target: 22.86214828491211\n",
      "          target_clicked: 22.86214828491211\n",
      "        mean_td_error: 5.237482070922852\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [11.585899353027344, 2.2279129028320312, 12.460652351379395, 5.276329040527344,\n",
      "          0.2608795166015625, 3.360170364379883, 3.7121410369873047, 1.0391921997070312,\n",
      "          10.751518249511719, 3.8271045684814453, 7.128511428833008, 8.869879722595215,\n",
      "          6.745214462280273, 6.189390182495117, 7.681791305541992, 1.2922897338867188,\n",
      "          2.2089481353759766, 6.931415557861328, 2.936725616455078, 4.695711135864258,\n",
      "          3.7720603942871094, 5.314971923828125, 1.3444900512695312, 7.230974197387695,\n",
      "          4.653911590576172, 14.384905815124512, 7.361013412475586, 1.3805274963378906,\n",
      "          1.9439430236816406, 4.356637954711914, 2.0156936645507812, 4.658613204956055]\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 200032\n",
      "    num_env_steps_sampled: 45000\n",
      "    num_env_steps_trained: 200032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 45000\n",
      "  num_agent_steps_trained: 200032\n",
      "  num_env_steps_sampled: 45000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 200032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.318749999999998\n",
      "    ram_util_percent: 28.202083333333338\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07773077582200524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4860587700487304\n",
      "    mean_inference_ms: 4.257238681721952\n",
      "    mean_raw_obs_processing_ms: 1.996286687350652\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1164.1670054499168\n",
      "    episode_reward_min: 1136.5200756954982\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1171.8336049337597, 1144.4817151438883, 1149.6009432809383, 1145.6040996383565,\n",
      "        1157.8552999108706, 1164.8152411483745, 1173.517106826634, 1162.204572488136,\n",
      "        1186.25544641512, 1161.376347244283, 1165.530515390824, 1168.0472969980417,\n",
      "        1165.7151014737215, 1167.6263355156605, 1176.2459465573456, 1175.8502751253186,\n",
      "        1156.01727175097, 1152.9597062781856, 1163.908923946458, 1175.498594875335,\n",
      "        1172.7847671322634, 1169.6852294367152, 1160.610868142743, 1154.0478267751357,\n",
      "        1170.2748297107485, 1136.5200756954982, 1175.434487396968, 1156.4682247148828,\n",
      "        1176.6833525028326, 1143.8206771533423, 1166.202484870401, 1142.0570046736257,\n",
      "        1148.1839925697193, 1176.564785081392, 1160.7331769653413, 1186.1205696197187,\n",
      "        1165.3043900062098, 1175.9794958466293, 1171.380893554055, 1163.0653375768973,\n",
      "        1154.4095130857775, 1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503,\n",
      "        1170.4274430944804, 1164.0003070221608, 1161.2194825885572, 1144.50647313447,\n",
      "        1153.58192824728, 1149.0167222574578, 1173.6243121745892, 1171.878159907749,\n",
      "        1156.2820350551422, 1166.7368823655572, 1147.264368533752, 1167.3439061041381,\n",
      "        1168.9361431505079, 1162.3862616436402, 1175.919860172385, 1154.2032952838786,\n",
      "        1164.6716881678374, 1170.9239909793482, 1158.43211604694, 1150.561284485556,\n",
      "        1168.1876128158042, 1161.7689157195962, 1162.3009729610762, 1164.410891646221,\n",
      "        1156.1152905846955, 1183.4126362756163, 1171.7398370031865, 1191.3173453704933,\n",
      "        1151.9614074798817, 1158.9260871017896, 1165.0765511574295, 1142.8448177240366,\n",
      "        1185.4930051823917, 1164.8544516402696, 1162.0267136423367, 1188.6413494142923,\n",
      "        1164.2148810984302, 1171.2373702626505, 1153.2186394314383, 1149.1851548234642,\n",
      "        1169.2810067432656, 1158.449168143026, 1148.130936342867, 1177.8516780623029,\n",
      "        1167.5898099070957, 1156.7417154639334, 1175.4053994312524, 1170.5641283401237,\n",
      "        1177.675521874865, 1157.443173494445, 1162.6537669039933, 1167.7362328861245,\n",
      "        1157.5536696793044, 1166.3910114710234, 1172.2829495066287]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07773077582200524\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4860587700487304\n",
      "      mean_inference_ms: 4.257238681721952\n",
      "      mean_raw_obs_processing_ms: 1.996286687350652\n",
      "  time_since_restore: 1017.1358287334442\n",
      "  time_this_iter_s: 34.4224910736084\n",
      "  time_total_s: 1017.1358287334442\n",
      "  timers:\n",
      "    learn_throughput: 787.297\n",
      "    learn_time_ms: 40.645\n",
      "    load_throughput: 49306.685\n",
      "    load_time_ms: 0.649\n",
      "    synch_weights_time_ms: 3.774\n",
      "    training_iteration_time_ms: 136.343\n",
      "  timestamp: 1661054838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 26\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  counters:\n",
      "    last_target_update_ts: 45600\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 208032\n",
      "    num_env_steps_sampled: 46000\n",
      "    num_env_steps_trained: 208032\n",
      "    num_target_updates: 9\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-07-52\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1164.560822305794\n",
      "  episode_reward_min: 1136.5200756954982\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 383\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 45600\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 9.97463607788086\n",
      "          choice_loss: 0.7077578902244568\n",
      "          grad_gnorm: 24.425945281982422\n",
      "          mean_actions: 9.46875\n",
      "          next_q_target_max: 14.179255485534668\n",
      "          next_q_target_slate: 10.779714584350586\n",
      "          next_q_values: 19.33469581604004\n",
      "          q_clicked: 19.636280059814453\n",
      "          q_loss: 36.40436553955078\n",
      "          q_values: 19.532028198242188\n",
      "          replay_click_q: 19.636280059814453\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2604639530181885\n",
      "          slate_q_values: 19.59674644470215\n",
      "          target: 24.01209831237793\n",
      "          target_clicked: 24.01209831237793\n",
      "        mean_td_error: 5.0285444259643555\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [1.9683036804199219, 4.3608245849609375, 4.437793731689453, 2.5534629821777344,\n",
      "          5.259057998657227, 7.119359970092773, 0.7881374359130859, 0.9527778625488281,\n",
      "          2.0759506225585938, 2.807840347290039, 9.164467811584473, 3.3335018157958984,\n",
      "          9.603415489196777, 0.1462688446044922, 7.448539733886719, 1.1594123840332031,\n",
      "          7.425821304321289, 6.0574493408203125, 1.9394569396972656, 5.926088333129883,\n",
      "          11.767232894897461, 8.752585411071777, 1.6295051574707031, 11.592217445373535,\n",
      "          5.323905944824219, 1.4626903533935547, 8.139019012451172, 10.907066345214844,\n",
      "          1.7910842895507812, 4.108915328979492, 6.310398101806641, 4.600879669189453]\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 208032\n",
      "    num_env_steps_sampled: 46000\n",
      "    num_env_steps_trained: 208032\n",
      "    num_target_updates: 9\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 46000\n",
      "  num_agent_steps_trained: 208032\n",
      "  num_env_steps_sampled: 46000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 208032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.324489795918367\n",
      "    ram_util_percent: 28.410204081632653\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07784368753274486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4865258371076432\n",
      "    mean_inference_ms: 4.271630028418716\n",
      "    mean_raw_obs_processing_ms: 1.9970131600394418\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1164.560822305794\n",
      "    episode_reward_min: 1136.5200756954982\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1186.25544641512, 1161.376347244283, 1165.530515390824, 1168.0472969980417,\n",
      "        1165.7151014737215, 1167.6263355156605, 1176.2459465573456, 1175.8502751253186,\n",
      "        1156.01727175097, 1152.9597062781856, 1163.908923946458, 1175.498594875335,\n",
      "        1172.7847671322634, 1169.6852294367152, 1160.610868142743, 1154.0478267751357,\n",
      "        1170.2748297107485, 1136.5200756954982, 1175.434487396968, 1156.4682247148828,\n",
      "        1176.6833525028326, 1143.8206771533423, 1166.202484870401, 1142.0570046736257,\n",
      "        1148.1839925697193, 1176.564785081392, 1160.7331769653413, 1186.1205696197187,\n",
      "        1165.3043900062098, 1175.9794958466293, 1171.380893554055, 1163.0653375768973,\n",
      "        1154.4095130857775, 1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503,\n",
      "        1170.4274430944804, 1164.0003070221608, 1161.2194825885572, 1144.50647313447,\n",
      "        1153.58192824728, 1149.0167222574578, 1173.6243121745892, 1171.878159907749,\n",
      "        1156.2820350551422, 1166.7368823655572, 1147.264368533752, 1167.3439061041381,\n",
      "        1168.9361431505079, 1162.3862616436402, 1175.919860172385, 1154.2032952838786,\n",
      "        1164.6716881678374, 1170.9239909793482, 1158.43211604694, 1150.561284485556,\n",
      "        1168.1876128158042, 1161.7689157195962, 1162.3009729610762, 1164.410891646221,\n",
      "        1156.1152905846955, 1183.4126362756163, 1171.7398370031865, 1191.3173453704933,\n",
      "        1151.9614074798817, 1158.9260871017896, 1165.0765511574295, 1142.8448177240366,\n",
      "        1185.4930051823917, 1164.8544516402696, 1162.0267136423367, 1188.6413494142923,\n",
      "        1164.2148810984302, 1171.2373702626505, 1153.2186394314383, 1149.1851548234642,\n",
      "        1169.2810067432656, 1158.449168143026, 1148.130936342867, 1177.8516780623029,\n",
      "        1167.5898099070957, 1156.7417154639334, 1175.4053994312524, 1170.5641283401237,\n",
      "        1177.675521874865, 1157.443173494445, 1162.6537669039933, 1167.7362328861245,\n",
      "        1157.5536696793044, 1166.3910114710234, 1172.2829495066287, 1165.0370116139336,\n",
      "        1165.3971813701903, 1160.3373062045898, 1174.1417019538578, 1170.6694903812977,\n",
      "        1156.3882669556954, 1154.1066711014753, 1163.2166393776326]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07784368753274486\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4865258371076432\n",
      "      mean_inference_ms: 4.271630028418716\n",
      "      mean_raw_obs_processing_ms: 1.9970131600394418\n",
      "  time_since_restore: 1051.530369758606\n",
      "  time_this_iter_s: 34.39454102516174\n",
      "  time_total_s: 1051.530369758606\n",
      "  timers:\n",
      "    learn_throughput: 796.609\n",
      "    learn_time_ms: 40.17\n",
      "    load_throughput: 52157.824\n",
      "    load_time_ms: 0.614\n",
      "    synch_weights_time_ms: 3.697\n",
      "    training_iteration_time_ms: 136.886\n",
      "  timestamp: 1661054872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 27\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  counters:\n",
      "    last_target_update_ts: 45600\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 216032\n",
      "    num_env_steps_sampled: 47000\n",
      "    num_env_steps_trained: 216032\n",
      "    num_target_updates: 9\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-08-27\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1164.3203019874957\n",
      "  episode_reward_min: 1136.5200756954982\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 391\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 45600\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 13.24676513671875\n",
      "          choice_loss: 0.7040360569953918\n",
      "          grad_gnorm: 116.57005310058594\n",
      "          mean_actions: 9.78125\n",
      "          next_q_target_max: 13.86029052734375\n",
      "          next_q_target_slate: 10.741678237915039\n",
      "          next_q_values: 19.37899398803711\n",
      "          q_clicked: 20.63467025756836\n",
      "          q_loss: 283.83245849609375\n",
      "          q_values: 19.903194427490234\n",
      "          replay_click_q: 20.63467025756836\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2495949268341064\n",
      "          slate_q_values: 20.83226776123047\n",
      "          target: 26.968454360961914\n",
      "          target_clicked: 26.968454360961914\n",
      "        mean_td_error: 7.6562676429748535\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [2.5286693572998047, 5.6650238037109375, 7.941715240478516, 5.412588119506836,\n",
      "          0.8465023040771484, 5.166557312011719, 8.296433448791504, 1.1176338195800781,\n",
      "          6.758411407470703, 9.163524627685547, 3.2991409301757812, 3.2183914184570312,\n",
      "          3.783243179321289, 1.5479354858398438, 3.3700809478759766, 0.7081584930419922,\n",
      "          8.186540603637695, 86.57087707519531, 5.568315505981445, 0.4531898498535156,\n",
      "          1.4259490966796875, 5.536805152893066, 3.2837448120117188, 11.552556037902832,\n",
      "          3.7406387329101562, 2.9576454162597656, 3.8449554443359375, 3.574464797973633,\n",
      "          2.469207763671875, 0.6080913543701172, 8.56381607055664, 27.839744567871094]\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 216032\n",
      "    num_env_steps_sampled: 47000\n",
      "    num_env_steps_trained: 216032\n",
      "    num_target_updates: 9\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 47000\n",
      "  num_agent_steps_trained: 216032\n",
      "  num_env_steps_sampled: 47000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 216032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.687755102040814\n",
      "    ram_util_percent: 28.628571428571426\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07795395077752115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.48698187544710825\n",
      "    mean_inference_ms: 4.285387506056798\n",
      "    mean_raw_obs_processing_ms: 1.9977539923821888\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1164.3203019874957\n",
      "    episode_reward_min: 1136.5200756954982\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1156.01727175097, 1152.9597062781856, 1163.908923946458, 1175.498594875335,\n",
      "        1172.7847671322634, 1169.6852294367152, 1160.610868142743, 1154.0478267751357,\n",
      "        1170.2748297107485, 1136.5200756954982, 1175.434487396968, 1156.4682247148828,\n",
      "        1176.6833525028326, 1143.8206771533423, 1166.202484870401, 1142.0570046736257,\n",
      "        1148.1839925697193, 1176.564785081392, 1160.7331769653413, 1186.1205696197187,\n",
      "        1165.3043900062098, 1175.9794958466293, 1171.380893554055, 1163.0653375768973,\n",
      "        1154.4095130857775, 1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503,\n",
      "        1170.4274430944804, 1164.0003070221608, 1161.2194825885572, 1144.50647313447,\n",
      "        1153.58192824728, 1149.0167222574578, 1173.6243121745892, 1171.878159907749,\n",
      "        1156.2820350551422, 1166.7368823655572, 1147.264368533752, 1167.3439061041381,\n",
      "        1168.9361431505079, 1162.3862616436402, 1175.919860172385, 1154.2032952838786,\n",
      "        1164.6716881678374, 1170.9239909793482, 1158.43211604694, 1150.561284485556,\n",
      "        1168.1876128158042, 1161.7689157195962, 1162.3009729610762, 1164.410891646221,\n",
      "        1156.1152905846955, 1183.4126362756163, 1171.7398370031865, 1191.3173453704933,\n",
      "        1151.9614074798817, 1158.9260871017896, 1165.0765511574295, 1142.8448177240366,\n",
      "        1185.4930051823917, 1164.8544516402696, 1162.0267136423367, 1188.6413494142923,\n",
      "        1164.2148810984302, 1171.2373702626505, 1153.2186394314383, 1149.1851548234642,\n",
      "        1169.2810067432656, 1158.449168143026, 1148.130936342867, 1177.8516780623029,\n",
      "        1167.5898099070957, 1156.7417154639334, 1175.4053994312524, 1170.5641283401237,\n",
      "        1177.675521874865, 1157.443173494445, 1162.6537669039933, 1167.7362328861245,\n",
      "        1157.5536696793044, 1166.3910114710234, 1172.2829495066287, 1165.0370116139336,\n",
      "        1165.3971813701903, 1160.3373062045898, 1174.1417019538578, 1170.6694903812977,\n",
      "        1156.3882669556954, 1154.1066711014753, 1163.2166393776326, 1165.9956193333276,\n",
      "        1172.7586988082312, 1159.4752932623041, 1177.795537564808, 1182.6836883419655,\n",
      "        1150.1487105916292, 1165.6852896027224, 1168.0523953855045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07795395077752115\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.48698187544710825\n",
      "      mean_inference_ms: 4.285387506056798\n",
      "      mean_raw_obs_processing_ms: 1.9977539923821888\n",
      "  time_since_restore: 1085.9884932041168\n",
      "  time_this_iter_s: 34.458123445510864\n",
      "  time_total_s: 1085.9884932041168\n",
      "  timers:\n",
      "    learn_throughput: 768.81\n",
      "    learn_time_ms: 41.623\n",
      "    load_throughput: 48378.953\n",
      "    load_time_ms: 0.661\n",
      "    synch_weights_time_ms: 3.695\n",
      "    training_iteration_time_ms: 138.914\n",
      "  timestamp: 1661054907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 28\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    last_target_update_ts: 45600\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 224032\n",
      "    num_env_steps_sampled: 48000\n",
      "    num_env_steps_trained: 224032\n",
      "    num_target_updates: 9\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-09-01\n",
      "  done: false\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1165.1316680465463\n",
      "  episode_reward_min: 1136.5200756954982\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 400\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 45600\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 13.970139503479004\n",
      "          choice_loss: 0.7032091617584229\n",
      "          grad_gnorm: 135.12648010253906\n",
      "          mean_actions: 8.1875\n",
      "          next_q_target_max: 14.084527015686035\n",
      "          next_q_target_slate: 10.665739059448242\n",
      "          next_q_values: 19.2657527923584\n",
      "          q_clicked: 19.4808406829834\n",
      "          q_loss: 459.7463073730469\n",
      "          q_values: 19.99253273010254\n",
      "          replay_click_q: 19.4808406829834\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.2436401844024658\n",
      "          slate_q_values: 19.993967056274414\n",
      "          target: 27.465656280517578\n",
      "          target_clicked: 27.465656280517578\n",
      "        mean_td_error: 9.10523796081543\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [7.978142738342285, 0.945465087890625, 4.21452522277832, 9.803252220153809,\n",
      "          5.055334091186523, 3.006113052368164, 3.207489013671875, 8.823561668395996,\n",
      "          3.537626266479492, 7.940260887145996, 10.263792037963867, 8.533685684204102,\n",
      "          115.46429443359375, 11.530670166015625, 9.825041770935059, 2.152984619140625,\n",
      "          8.579792976379395, 10.643418312072754, 2.6411075592041016, 7.971982002258301,\n",
      "          3.669912338256836, 4.003902435302734, 8.883395195007324, 0.06816673278808594,\n",
      "          6.394488334655762, 0.42598533630371094, 6.475030899047852, 1.9670677185058594,\n",
      "          6.572395324707031, 0.3972206115722656, 9.713685035705566, 0.677825927734375]\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 224032\n",
      "    num_env_steps_sampled: 48000\n",
      "    num_env_steps_trained: 224032\n",
      "    num_target_updates: 9\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 224032\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 224032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.3265306122449\n",
      "    ram_util_percent: 28.84897959183674\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0780721161995936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.4874716543590098\n",
      "    mean_inference_ms: 4.300156806718068\n",
      "    mean_raw_obs_processing_ms: 1.9985519209593394\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1165.1316680465463\n",
      "    episode_reward_min: 1136.5200756954982\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1136.5200756954982, 1175.434487396968, 1156.4682247148828, 1176.6833525028326,\n",
      "        1143.8206771533423, 1166.202484870401, 1142.0570046736257, 1148.1839925697193,\n",
      "        1176.564785081392, 1160.7331769653413, 1186.1205696197187, 1165.3043900062098,\n",
      "        1175.9794958466293, 1171.380893554055, 1163.0653375768973, 1154.4095130857775,\n",
      "        1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503, 1170.4274430944804,\n",
      "        1164.0003070221608, 1161.2194825885572, 1144.50647313447, 1153.58192824728,\n",
      "        1149.0167222574578, 1173.6243121745892, 1171.878159907749, 1156.2820350551422,\n",
      "        1166.7368823655572, 1147.264368533752, 1167.3439061041381, 1168.9361431505079,\n",
      "        1162.3862616436402, 1175.919860172385, 1154.2032952838786, 1164.6716881678374,\n",
      "        1170.9239909793482, 1158.43211604694, 1150.561284485556, 1168.1876128158042,\n",
      "        1161.7689157195962, 1162.3009729610762, 1164.410891646221, 1156.1152905846955,\n",
      "        1183.4126362756163, 1171.7398370031865, 1191.3173453704933, 1151.9614074798817,\n",
      "        1158.9260871017896, 1165.0765511574295, 1142.8448177240366, 1185.4930051823917,\n",
      "        1164.8544516402696, 1162.0267136423367, 1188.6413494142923, 1164.2148810984302,\n",
      "        1171.2373702626505, 1153.2186394314383, 1149.1851548234642, 1169.2810067432656,\n",
      "        1158.449168143026, 1148.130936342867, 1177.8516780623029, 1167.5898099070957,\n",
      "        1156.7417154639334, 1175.4053994312524, 1170.5641283401237, 1177.675521874865,\n",
      "        1157.443173494445, 1162.6537669039933, 1167.7362328861245, 1157.5536696793044,\n",
      "        1166.3910114710234, 1172.2829495066287, 1165.0370116139336, 1165.3971813701903,\n",
      "        1160.3373062045898, 1174.1417019538578, 1170.6694903812977, 1156.3882669556954,\n",
      "        1154.1066711014753, 1163.2166393776326, 1165.9956193333276, 1172.7586988082312,\n",
      "        1159.4752932623041, 1177.795537564808, 1182.6836883419655, 1150.1487105916292,\n",
      "        1165.6852896027224, 1168.0523953855045, 1159.6263055547204, 1169.2238301363807,\n",
      "        1159.6154664983799, 1164.5847570890612, 1179.280397630546, 1171.1423522298344,\n",
      "        1178.6722634250832, 1189.6952835989805, 1185.0839677906176]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0780721161995936\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4874716543590098\n",
      "      mean_inference_ms: 4.300156806718068\n",
      "      mean_raw_obs_processing_ms: 1.9985519209593394\n",
      "  time_since_restore: 1120.2427244186401\n",
      "  time_this_iter_s: 34.254231214523315\n",
      "  time_total_s: 1120.2427244186401\n",
      "  timers:\n",
      "    learn_throughput: 793.245\n",
      "    learn_time_ms: 40.341\n",
      "    load_throughput: 51861.564\n",
      "    load_time_ms: 0.617\n",
      "    synch_weights_time_ms: 3.643\n",
      "    training_iteration_time_ms: 135.718\n",
      "  timestamp: 1661054941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 29\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Result for SlateQ_modified-lts_55930_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  counters:\n",
      "    last_target_update_ts: 48800\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 232032\n",
      "    num_env_steps_sampled: 49000\n",
      "    num_env_steps_trained: 232032\n",
      "    num_target_updates: 10\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-20_21-09-36\n",
      "  done: true\n",
      "  episode_len_mean: 120.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1191.3173453704933\n",
      "  episode_reward_mean: 1165.8204422369952\n",
      "  episode_reward_min: 1142.8448177240366\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 408\n",
      "  experiment_id: 430eda92fd764ad8b04ea650a6606255\n",
      "  hostname: ses-pyqcqaufuxprth5m8qn81ux9-head-node-type-5cc4cd6b98-pdrrd\n",
      "  info:\n",
      "    last_target_update_ts: 48800\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          bellman_reward: 11.931126594543457\n",
      "          choice_loss: 0.7022765278816223\n",
      "          grad_gnorm: 74.304931640625\n",
      "          mean_actions: 10.5625\n",
      "          next_q_target_max: 14.054429054260254\n",
      "          next_q_target_slate: 11.273810386657715\n",
      "          next_q_values: 20.34206771850586\n",
      "          q_clicked: 21.221372604370117\n",
      "          q_loss: 116.10602569580078\n",
      "          q_values: 20.402524948120117\n",
      "          replay_click_q: 21.221372604370117\n",
      "          score_no_click: 2.0\n",
      "          scores: 1.250104308128357\n",
      "          slate_q_values: 20.635398864746094\n",
      "          target: 25.001102447509766\n",
      "          target_clicked: 25.001102447509766\n",
      "        mean_td_error: 6.188254356384277\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32.0\n",
      "        td_error: [4.264045715332031, 7.95512580871582, 6.279092788696289, 2.299001693725586,\n",
      "          2.4125423431396484, 2.8188705444335938, 2.6503067016601562, 50.802223205566406,\n",
      "          9.078336715698242, 1.224578857421875, 9.693401336669922, 0.1864910125732422,\n",
      "          18.169023513793945, 0.33960533142089844, 5.801746368408203, 9.516217231750488,\n",
      "          6.524663925170898, 2.9506282806396484, 3.6168174743652344, 4.94590950012207,\n",
      "          1.4508953094482422, 7.08128547668457, 1.8747920989990234, 5.9392852783203125,\n",
      "          5.692089080810547, 2.8647079467773438, 4.3004608154296875, 8.595163345336914,\n",
      "          0.2830333709716797, 1.6464710235595703, 6.270774841308594, 0.49655914306640625]\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 232032\n",
      "    num_env_steps_sampled: 49000\n",
      "    num_env_steps_trained: 232032\n",
      "    num_target_updates: 10\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.18.12.66\n",
      "  num_agent_steps_sampled: 49000\n",
      "  num_agent_steps_trained: 232032\n",
      "  num_env_steps_sampled: 49000\n",
      "  num_env_steps_sampled_this_iter: 1000\n",
      "  num_env_steps_trained: 232032\n",
      "  num_env_steps_trained_this_iter: 8000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 8000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.445833333333336\n",
      "    ram_util_percent: 29.054166666666664\n",
      "  pid: 249606\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07817183963490545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.48788265115543344\n",
      "    mean_inference_ms: 4.312557601865269\n",
      "    mean_raw_obs_processing_ms: 1.9991924021719052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 120.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1191.3173453704933\n",
      "    episode_reward_mean: 1165.8204422369952\n",
      "    episode_reward_min: 1142.8448177240366\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
      "        120, 120, 120, 120, 120, 120, 120, 120]\n",
      "      episode_reward: [1176.564785081392, 1160.7331769653413, 1186.1205696197187, 1165.3043900062098,\n",
      "        1175.9794958466293, 1171.380893554055, 1163.0653375768973, 1154.4095130857775,\n",
      "        1182.237449494122, 1152.7768695896, 1166.299055072828, 1169.5100833812503, 1170.4274430944804,\n",
      "        1164.0003070221608, 1161.2194825885572, 1144.50647313447, 1153.58192824728,\n",
      "        1149.0167222574578, 1173.6243121745892, 1171.878159907749, 1156.2820350551422,\n",
      "        1166.7368823655572, 1147.264368533752, 1167.3439061041381, 1168.9361431505079,\n",
      "        1162.3862616436402, 1175.919860172385, 1154.2032952838786, 1164.6716881678374,\n",
      "        1170.9239909793482, 1158.43211604694, 1150.561284485556, 1168.1876128158042,\n",
      "        1161.7689157195962, 1162.3009729610762, 1164.410891646221, 1156.1152905846955,\n",
      "        1183.4126362756163, 1171.7398370031865, 1191.3173453704933, 1151.9614074798817,\n",
      "        1158.9260871017896, 1165.0765511574295, 1142.8448177240366, 1185.4930051823917,\n",
      "        1164.8544516402696, 1162.0267136423367, 1188.6413494142923, 1164.2148810984302,\n",
      "        1171.2373702626505, 1153.2186394314383, 1149.1851548234642, 1169.2810067432656,\n",
      "        1158.449168143026, 1148.130936342867, 1177.8516780623029, 1167.5898099070957,\n",
      "        1156.7417154639334, 1175.4053994312524, 1170.5641283401237, 1177.675521874865,\n",
      "        1157.443173494445, 1162.6537669039933, 1167.7362328861245, 1157.5536696793044,\n",
      "        1166.3910114710234, 1172.2829495066287, 1165.0370116139336, 1165.3971813701903,\n",
      "        1160.3373062045898, 1174.1417019538578, 1170.6694903812977, 1156.3882669556954,\n",
      "        1154.1066711014753, 1163.2166393776326, 1165.9956193333276, 1172.7586988082312,\n",
      "        1159.4752932623041, 1177.795537564808, 1182.6836883419655, 1150.1487105916292,\n",
      "        1165.6852896027224, 1168.0523953855045, 1159.6263055547204, 1169.2238301363807,\n",
      "        1159.6154664983799, 1164.5847570890612, 1179.280397630546, 1171.1423522298344,\n",
      "        1178.6722634250832, 1189.6952835989805, 1185.0839677906176, 1166.4529056307615,\n",
      "        1160.7507628761985, 1172.3832434171954, 1160.9106348509297, 1164.7519546770566,\n",
      "        1156.6346717030833, 1174.7747655682724, 1157.5887798986419]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07817183963490545\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.48788265115543344\n",
      "      mean_inference_ms: 4.312557601865269\n",
      "      mean_raw_obs_processing_ms: 1.9991924021719052\n",
      "  time_since_restore: 1154.611480474472\n",
      "  time_this_iter_s: 34.36875605583191\n",
      "  time_total_s: 1154.611480474472\n",
      "  timers:\n",
      "    learn_throughput: 754.351\n",
      "    learn_time_ms: 42.421\n",
      "    load_throughput: 47921.211\n",
      "    load_time_ms: 0.668\n",
      "    synch_weights_time_ms: 8.088\n",
      "    training_iteration_time_ms: 142.512\n",
      "  timestamp: 1661054976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 30\n",
      "  trial_id: '55930_00000'\n",
      "  warmup_time: 7.564483165740967\n",
      "  \n",
      "Training took 1175.20 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train using Ray Tune\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# You can change timesteps_total here to see more tuning\n",
    "experiment_results = tune.run(\"SlateQ\", \n",
    "                              \n",
    "        # training config params (translated into a python dict!)\n",
    "        config=slateq_config.to_dict(), \n",
    "         \n",
    "        # Stopping criteria whichever occurs first:\n",
    "        stop={\"timesteps_total\":600000, \n",
    "              \"training_iteration\":30},\n",
    "         \n",
    "        # redirect logs instead of default ~/ray_results/\n",
    "        local_dir=\"results/slateq_recsim\",\n",
    "\n",
    "        # Always save last checkpoint (no matter the frequency).\n",
    "        checkpoint_at_end=True,\n",
    "\n",
    "        ###############\n",
    "        # Note about Ray Tune verbosity.\n",
    "        # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "        # 0 = silent\n",
    "        # 1 = only status updates, no logging messages\n",
    "        # 2 = status and brief trial results, includes logging messages\n",
    "        # 3 = status and detailed trial results, includes logging messages\n",
    "        # Defaults to 3.\n",
    "        ###############\n",
    "        verbose=3,\n",
    "\n",
    "        # Define what we are comparing for, when we search for the\n",
    "        # \"best\" checkpoint at the end.\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        )\n",
    "\n",
    "# train time\n",
    "print(f\"Training took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Training 10 iterations took 486.55 seconds; approx 8 minutes\n",
    "# Training 30 iterations took 1174.30 seconds; approx 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train using RLlib API .train()\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# # See reward progress with time\n",
    "# # Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# for _ in range(60):\n",
    "#     results = slateq_algo.train()\n",
    "#     print(f\"Iteration={slateq.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
    "    \n",
    "# # train time\n",
    "# print(f\"Training took {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (1, 458)\n",
      "['num_recreated_workers', 'episode_reward_max', 'episode_reward_min', 'episode_reward_mean', 'episode_len_mean', 'episodes_this_iter', 'num_faulty_episodes', 'num_healthy_workers', 'num_agent_steps_sampled', 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'timesteps_total', 'num_steps_trained_this_iter', 'agent_timesteps_total', 'done', 'episodes_total', 'training_iteration', 'experiment_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'timesteps_since_restore', 'iterations_since_restore', 'warmup_time', 'experiment_tag', 'info/num_env_steps_sampled', 'info/num_env_steps_trained', 'info/num_agent_steps_sampled', 'info/num_agent_steps_trained', 'info/last_target_update_ts', 'info/num_target_updates', 'sampler_results/episode_reward_max', 'sampler_results/episode_reward_min', 'sampler_results/episode_reward_mean', 'sampler_results/episode_len_mean', 'sampler_results/episodes_this_iter', 'sampler_results/num_faulty_episodes', 'hist_stats/episode_reward', 'hist_stats/episode_lengths', 'sampler_perf/mean_raw_obs_processing_ms', 'sampler_perf/mean_inference_ms', 'sampler_perf/mean_action_processing_ms', 'sampler_perf/mean_env_wait_ms', 'sampler_perf/mean_env_render_ms', 'timers/training_iteration_time_ms', 'timers/load_time_ms', 'timers/load_throughput', 'timers/learn_time_ms', 'timers/learn_throughput', 'timers/synch_weights_time_ms', 'counters/num_env_steps_sampled', 'counters/num_env_steps_trained', 'counters/num_agent_steps_sampled', 'counters/num_agent_steps_trained', 'counters/last_target_update_ts', 'counters/num_target_updates', 'config/num_gpus', 'config/num_cpus_per_worker', 'config/num_gpus_per_worker', 'config/_fake_gpus', 'config/placement_strategy', 'config/eager_tracing', 'config/eager_max_retraces', 'config/env', 'config/observation_space', 'config/action_space', 'config/env_task_fn', 'config/render_env', 'config/clip_rewards', 'config/normalize_actions', 'config/clip_actions', 'config/disable_env_checking', 'config/num_workers', 'config/num_envs_per_worker', 'config/sample_collector', 'config/sample_async', 'config/enable_connectors', 'config/rollout_fragment_length', 'config/batch_mode', 'config/remote_worker_envs', 'config/remote_env_batch_wait_ms', 'config/validate_workers_after_construction', 'config/ignore_worker_failures', 'config/recreate_failed_workers', 'config/restart_failed_sub_environments', 'config/num_consecutive_worker_failures_tolerance', 'config/horizon', 'config/soft_horizon', 'config/no_done_at_end', 'config/preprocessor_pref', 'config/observation_filter', 'config/synchronize_filters', 'config/compress_observations', 'config/enable_tf1_exec_eagerly', 'config/sampler_perf_stats_ema_coef', 'config/gamma', 'config/lr', 'config/train_batch_size', 'config/explore', 'config/actions_in_input_normalized', 'config/postprocess_inputs', 'config/shuffle_buffer_size', 'config/output', 'config/output_compress_columns', 'config/output_max_file_size', 'config/evaluation_interval', 'config/evaluation_duration', 'config/evaluation_duration_unit', 'config/evaluation_sample_timeout_s', 'config/evaluation_parallel_to_training', 'config/evaluation_num_workers', 'config/always_attach_evaluation_results', 'config/in_evaluation', 'config/sync_filters_on_rollout_workers_timeout_s', 'config/keep_per_episode_custom_metrics', 'config/metrics_episode_collection_timeout_s', 'config/metrics_num_episodes_for_smoothing', 'config/min_time_s_per_iteration', 'config/min_train_timesteps_per_iteration', 'config/min_sample_timesteps_per_iteration', 'config/logger_creator', 'config/logger_config', 'config/log_level', 'config/log_sys_usage', 'config/fake_sampler', 'config/seed', 'config/_tf_policy_handles_more_than_one_loss', 'config/_disable_preprocessor_api', 'config/_disable_action_flattening', 'config/_disable_execution_plan_api', 'config/simple_optimizer', 'config/monitor', 'config/evaluation_num_episodes', 'config/metrics_smoothing_episodes', 'config/timesteps_per_iteration', 'config/min_iter_time_s', 'config/collect_metrics_timeout', 'config/buffer_size', 'config/prioritized_replay', 'config/learning_starts', 'config/replay_batch_size', 'config/replay_sequence_length', 'config/prioritized_replay_alpha', 'config/prioritized_replay_beta', 'config/prioritized_replay_eps', 'config/min_time_s_per_reporting', 'config/min_train_timesteps_per_reporting', 'config/min_sample_timesteps_per_reporting', 'config/input_evaluation', 'config/fcnet_hiddens_per_candidate', 'config/target_network_update_freq', 'config/tau', 'config/use_huber', 'config/huber_threshold', 'config/training_intensity', 'config/lr_schedule', 'config/lr_choice_model', 'config/rmsprop_epsilon', 'config/grad_clip', 'config/n_step', 'config/input', 'config/callbacks', 'config/create_env_on_driver', 'config/custom_eval_function', 'config/framework', 'config/num_cpus_for_driver', 'perf/cpu_util_percent', 'perf/ram_util_percent', 'sampler_results/hist_stats/episode_reward', 'sampler_results/hist_stats/episode_lengths', 'sampler_results/sampler_perf/mean_raw_obs_processing_ms', 'sampler_results/sampler_perf/mean_inference_ms', 'sampler_results/sampler_perf/mean_action_processing_ms', 'sampler_results/sampler_perf/mean_env_wait_ms', 'sampler_results/sampler_perf/mean_env_render_ms', 'config/tf_session_args/intra_op_parallelism_threads', 'config/tf_session_args/inter_op_parallelism_threads', 'config/tf_session_args/log_device_placement', 'config/tf_session_args/allow_soft_placement', 'config/local_tf_session_args/intra_op_parallelism_threads', 'config/local_tf_session_args/inter_op_parallelism_threads', 'config/env_config/num_candidates', 'config/env_config/slate_size', 'config/env_config/resample_documents', 'config/env_config/convert_to_discrete_action_space', 'config/env_config/wrap_for_bandits', 'config/env_config/seed', 'config/model/_use_default_native_models', 'config/model/_disable_preprocessor_api', 'config/model/_disable_action_flattening', 'config/model/fcnet_hiddens', 'config/model/fcnet_activation', 'config/model/conv_filters', 'config/model/conv_activation', 'config/model/post_fcnet_hiddens', 'config/model/post_fcnet_activation', 'config/model/free_log_std', 'config/model/no_final_linear', 'config/model/vf_share_layers', 'config/model/use_lstm', 'config/model/max_seq_len', 'config/model/lstm_cell_size', 'config/model/lstm_use_prev_action', 'config/model/lstm_use_prev_reward', 'config/model/_time_major', 'config/model/use_attention', 'config/model/attention_num_transformer_units', 'config/model/attention_dim', 'config/model/attention_num_heads', 'config/model/attention_head_dim', 'config/model/attention_memory_inference', 'config/model/attention_memory_training', 'config/model/attention_position_wise_mlp_dim', 'config/model/attention_init_gru_gate_bias', 'config/model/attention_use_n_prev_actions', 'config/model/attention_use_n_prev_rewards', 'config/model/framestack', 'config/model/dim', 'config/model/grayscale', 'config/model/zero_mean', 'config/model/custom_model', 'config/model/custom_action_dist', 'config/model/custom_preprocessor', 'config/model/lstm_use_prev_action_reward', 'config/exploration_config/type', 'config/exploration_config/warmup_timesteps', 'config/exploration_config/epsilon_timesteps', 'config/exploration_config/final_epsilon', 'config/evaluation_config/num_gpus', 'config/evaluation_config/num_cpus_per_worker', 'config/evaluation_config/num_gpus_per_worker', 'config/evaluation_config/_fake_gpus', 'config/evaluation_config/placement_strategy', 'config/evaluation_config/eager_tracing', 'config/evaluation_config/eager_max_retraces', 'config/evaluation_config/env', 'config/evaluation_config/observation_space', 'config/evaluation_config/action_space', 'config/evaluation_config/env_task_fn', 'config/evaluation_config/render_env', 'config/evaluation_config/clip_rewards', 'config/evaluation_config/normalize_actions', 'config/evaluation_config/clip_actions', 'config/evaluation_config/disable_env_checking', 'config/evaluation_config/num_workers', 'config/evaluation_config/num_envs_per_worker', 'config/evaluation_config/sample_collector', 'config/evaluation_config/sample_async', 'config/evaluation_config/enable_connectors', 'config/evaluation_config/rollout_fragment_length', 'config/evaluation_config/batch_mode', 'config/evaluation_config/remote_worker_envs', 'config/evaluation_config/remote_env_batch_wait_ms', 'config/evaluation_config/validate_workers_after_construction', 'config/evaluation_config/ignore_worker_failures', 'config/evaluation_config/recreate_failed_workers', 'config/evaluation_config/restart_failed_sub_environments', 'config/evaluation_config/num_consecutive_worker_failures_tolerance', 'config/evaluation_config/horizon', 'config/evaluation_config/soft_horizon', 'config/evaluation_config/no_done_at_end', 'config/evaluation_config/preprocessor_pref', 'config/evaluation_config/observation_filter', 'config/evaluation_config/synchronize_filters', 'config/evaluation_config/compress_observations', 'config/evaluation_config/enable_tf1_exec_eagerly', 'config/evaluation_config/sampler_perf_stats_ema_coef', 'config/evaluation_config/gamma', 'config/evaluation_config/lr', 'config/evaluation_config/train_batch_size', 'config/evaluation_config/explore', 'config/evaluation_config/actions_in_input_normalized', 'config/evaluation_config/postprocess_inputs', 'config/evaluation_config/shuffle_buffer_size', 'config/evaluation_config/output', 'config/evaluation_config/output_compress_columns', 'config/evaluation_config/output_max_file_size', 'config/evaluation_config/evaluation_interval', 'config/evaluation_config/evaluation_duration', 'config/evaluation_config/evaluation_duration_unit', 'config/evaluation_config/evaluation_sample_timeout_s', 'config/evaluation_config/evaluation_parallel_to_training', 'config/evaluation_config/evaluation_num_workers', 'config/evaluation_config/always_attach_evaluation_results', 'config/evaluation_config/in_evaluation', 'config/evaluation_config/sync_filters_on_rollout_workers_timeout_s', 'config/evaluation_config/keep_per_episode_custom_metrics', 'config/evaluation_config/metrics_episode_collection_timeout_s', 'config/evaluation_config/metrics_num_episodes_for_smoothing', 'config/evaluation_config/min_time_s_per_iteration', 'config/evaluation_config/min_train_timesteps_per_iteration', 'config/evaluation_config/min_sample_timesteps_per_iteration', 'config/evaluation_config/logger_creator', 'config/evaluation_config/logger_config', 'config/evaluation_config/log_level', 'config/evaluation_config/log_sys_usage', 'config/evaluation_config/fake_sampler', 'config/evaluation_config/seed', 'config/evaluation_config/_tf_policy_handles_more_than_one_loss', 'config/evaluation_config/_disable_preprocessor_api', 'config/evaluation_config/_disable_action_flattening', 'config/evaluation_config/_disable_execution_plan_api', 'config/evaluation_config/simple_optimizer', 'config/evaluation_config/monitor', 'config/evaluation_config/evaluation_num_episodes', 'config/evaluation_config/metrics_smoothing_episodes', 'config/evaluation_config/timesteps_per_iteration', 'config/evaluation_config/min_iter_time_s', 'config/evaluation_config/collect_metrics_timeout', 'config/evaluation_config/buffer_size', 'config/evaluation_config/prioritized_replay', 'config/evaluation_config/learning_starts', 'config/evaluation_config/replay_batch_size', 'config/evaluation_config/replay_sequence_length', 'config/evaluation_config/prioritized_replay_alpha', 'config/evaluation_config/prioritized_replay_beta', 'config/evaluation_config/prioritized_replay_eps', 'config/evaluation_config/min_time_s_per_reporting', 'config/evaluation_config/min_train_timesteps_per_reporting', 'config/evaluation_config/min_sample_timesteps_per_reporting', 'config/evaluation_config/input_evaluation', 'config/evaluation_config/fcnet_hiddens_per_candidate', 'config/evaluation_config/target_network_update_freq', 'config/evaluation_config/tau', 'config/evaluation_config/use_huber', 'config/evaluation_config/huber_threshold', 'config/evaluation_config/training_intensity', 'config/evaluation_config/lr_schedule', 'config/evaluation_config/lr_choice_model', 'config/evaluation_config/rmsprop_epsilon', 'config/evaluation_config/grad_clip', 'config/evaluation_config/n_step', 'config/evaluation_config/input', 'config/evaluation_config/callbacks', 'config/evaluation_config/create_env_on_driver', 'config/evaluation_config/custom_eval_function', 'config/evaluation_config/framework', 'config/evaluation_config/num_cpus_for_driver', 'config/replay_buffer_config/type', 'config/replay_buffer_config/capacity', 'config/replay_buffer_config/prioritized_replay_alpha', 'config/replay_buffer_config/prioritized_replay_beta', 'config/replay_buffer_config/prioritized_replay_eps', 'config/replay_buffer_config/replay_sequence_length', 'config/replay_buffer_config/worker_side_prioritization', 'config/replay_buffer_config/learning_starts', 'config/replay_buffer_config/replay_mode', 'config/multiagent/policy_map_capacity', 'config/multiagent/policy_map_cache', 'config/multiagent/policy_mapping_fn', 'config/multiagent/policies_to_train', 'config/multiagent/observation_fn', 'config/multiagent/replay_mode', 'config/multiagent/count_steps_by', 'info/learner/default_policy/td_error', 'info/learner/default_policy/mean_td_error', 'info/learner/default_policy/num_agent_steps_trained', 'config/tf_session_args/gpu_options/allow_growth', 'config/tf_session_args/device_count/CPU', 'config/evaluation_config/tf_session_args/intra_op_parallelism_threads', 'config/evaluation_config/tf_session_args/inter_op_parallelism_threads', 'config/evaluation_config/tf_session_args/log_device_placement', 'config/evaluation_config/tf_session_args/allow_soft_placement', 'config/evaluation_config/local_tf_session_args/intra_op_parallelism_threads', 'config/evaluation_config/local_tf_session_args/inter_op_parallelism_threads', 'config/evaluation_config/env_config/num_candidates', 'config/evaluation_config/env_config/slate_size', 'config/evaluation_config/env_config/resample_documents', 'config/evaluation_config/env_config/convert_to_discrete_action_space', 'config/evaluation_config/env_config/wrap_for_bandits', 'config/evaluation_config/env_config/seed', 'config/evaluation_config/model/_use_default_native_models', 'config/evaluation_config/model/_disable_preprocessor_api', 'config/evaluation_config/model/_disable_action_flattening', 'config/evaluation_config/model/fcnet_hiddens', 'config/evaluation_config/model/fcnet_activation', 'config/evaluation_config/model/conv_filters', 'config/evaluation_config/model/conv_activation', 'config/evaluation_config/model/post_fcnet_hiddens', 'config/evaluation_config/model/post_fcnet_activation', 'config/evaluation_config/model/free_log_std', 'config/evaluation_config/model/no_final_linear', 'config/evaluation_config/model/vf_share_layers', 'config/evaluation_config/model/use_lstm', 'config/evaluation_config/model/max_seq_len', 'config/evaluation_config/model/lstm_cell_size', 'config/evaluation_config/model/lstm_use_prev_action', 'config/evaluation_config/model/lstm_use_prev_reward', 'config/evaluation_config/model/_time_major', 'config/evaluation_config/model/use_attention', 'config/evaluation_config/model/attention_num_transformer_units', 'config/evaluation_config/model/attention_dim', 'config/evaluation_config/model/attention_num_heads', 'config/evaluation_config/model/attention_head_dim', 'config/evaluation_config/model/attention_memory_inference', 'config/evaluation_config/model/attention_memory_training', 'config/evaluation_config/model/attention_position_wise_mlp_dim', 'config/evaluation_config/model/attention_init_gru_gate_bias', 'config/evaluation_config/model/attention_use_n_prev_actions', 'config/evaluation_config/model/attention_use_n_prev_rewards', 'config/evaluation_config/model/framestack', 'config/evaluation_config/model/dim', 'config/evaluation_config/model/grayscale', 'config/evaluation_config/model/zero_mean', 'config/evaluation_config/model/custom_model', 'config/evaluation_config/model/custom_action_dist', 'config/evaluation_config/model/custom_preprocessor', 'config/evaluation_config/model/lstm_use_prev_action_reward', 'config/evaluation_config/exploration_config/type', 'config/evaluation_config/exploration_config/warmup_timesteps', 'config/evaluation_config/exploration_config/epsilon_timesteps', 'config/evaluation_config/exploration_config/final_epsilon', 'config/evaluation_config/evaluation_config/explore', 'config/evaluation_config/replay_buffer_config/type', 'config/evaluation_config/replay_buffer_config/capacity', 'config/evaluation_config/replay_buffer_config/prioritized_replay_alpha', 'config/evaluation_config/replay_buffer_config/prioritized_replay_beta', 'config/evaluation_config/replay_buffer_config/prioritized_replay_eps', 'config/evaluation_config/replay_buffer_config/replay_sequence_length', 'config/evaluation_config/replay_buffer_config/worker_side_prioritization', 'config/evaluation_config/replay_buffer_config/learning_starts', 'config/evaluation_config/replay_buffer_config/replay_mode', 'config/evaluation_config/multiagent/policy_map_capacity', 'config/evaluation_config/multiagent/policy_map_cache', 'config/evaluation_config/multiagent/policy_mapping_fn', 'config/evaluation_config/multiagent/policies_to_train', 'config/evaluation_config/multiagent/observation_fn', 'config/evaluation_config/multiagent/replay_mode', 'config/evaluation_config/multiagent/count_steps_by', 'config/multiagent/policies/default_policy', 'info/learner/default_policy/learner_stats/allreduce_latency', 'info/learner/default_policy/learner_stats/grad_gnorm', 'info/learner/default_policy/learner_stats/q_values', 'info/learner/default_policy/learner_stats/q_clicked', 'info/learner/default_policy/learner_stats/scores', 'info/learner/default_policy/learner_stats/score_no_click', 'info/learner/default_policy/learner_stats/slate_q_values', 'info/learner/default_policy/learner_stats/replay_click_q', 'info/learner/default_policy/learner_stats/bellman_reward', 'info/learner/default_policy/learner_stats/next_q_values', 'info/learner/default_policy/learner_stats/target', 'info/learner/default_policy/learner_stats/next_q_target_slate', 'info/learner/default_policy/learner_stats/next_q_target_max', 'info/learner/default_policy/learner_stats/target_clicked', 'info/learner/default_policy/learner_stats/q_loss', 'info/learner/default_policy/learner_stats/mean_actions', 'info/learner/default_policy/learner_stats/choice_loss', 'config/evaluation_config/tf_session_args/gpu_options/allow_growth', 'config/evaluation_config/tf_session_args/device_count/CPU', 'config/evaluation_config/multiagent/policies/default_policy']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_recreated_workers</th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_faulty_episodes</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55930_00000</th>\n",
       "      <td>0</td>\n",
       "      <td>1191.317345</td>\n",
       "      <td>1142.844818</td>\n",
       "      <td>1165.820442</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             num_recreated_workers  episode_reward_max  episode_reward_min  \\\n",
       "trial_id                                                                     \n",
       "55930_00000                      0         1191.317345         1142.844818   \n",
       "\n",
       "             episode_reward_mean  episode_len_mean  episodes_this_iter  \\\n",
       "trial_id                                                                 \n",
       "55930_00000          1165.820442             120.0                   8   \n",
       "\n",
       "             num_faulty_episodes  num_healthy_workers  \n",
       "trial_id                                               \n",
       "55930_00000                    0                    1  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read trainer results in a pandas dataframe\n",
    "df = experiment_results.results_df\n",
    "print(f\"df.shape: {df.shape}\")  #Only 1 trial\n",
    "\n",
    "print(df.columns.tolist())\n",
    "df.iloc[:,0:8].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/train_batch_size</th>\n",
       "      <th>config/explore</th>\n",
       "      <th>config/actions_in_input_normalized</th>\n",
       "      <th>config/postprocess_inputs</th>\n",
       "      <th>config/shuffle_buffer_size</th>\n",
       "      <th>config/output</th>\n",
       "      <th>config/output_compress_columns</th>\n",
       "      <th>config/output_max_file_size</th>\n",
       "      <th>config/evaluation_interval</th>\n",
       "      <th>config/evaluation_duration</th>\n",
       "      <th>config/evaluation_duration_unit</th>\n",
       "      <th>config/evaluation_sample_timeout_s</th>\n",
       "      <th>config/evaluation_parallel_to_training</th>\n",
       "      <th>config/evaluation_num_workers</th>\n",
       "      <th>config/always_attach_evaluation_results</th>\n",
       "      <th>config/in_evaluation</th>\n",
       "      <th>config/sync_filters_on_rollout_workers_timeout_s</th>\n",
       "      <th>config/keep_per_episode_custom_metrics</th>\n",
       "      <th>config/metrics_episode_collection_timeout_s</th>\n",
       "      <th>config/metrics_num_episodes_for_smoothing</th>\n",
       "      <th>config/min_time_s_per_iteration</th>\n",
       "      <th>config/min_train_timesteps_per_iteration</th>\n",
       "      <th>config/min_sample_timesteps_per_iteration</th>\n",
       "      <th>config/logger_creator</th>\n",
       "      <th>config/logger_config</th>\n",
       "      <th>config/log_level</th>\n",
       "      <th>config/log_sys_usage</th>\n",
       "      <th>config/fake_sampler</th>\n",
       "      <th>config/seed</th>\n",
       "      <th>config/_tf_policy_handles_more_than_one_loss</th>\n",
       "      <th>config/_disable_preprocessor_api</th>\n",
       "      <th>config/_disable_action_flattening</th>\n",
       "      <th>config/_disable_execution_plan_api</th>\n",
       "      <th>config/simple_optimizer</th>\n",
       "      <th>config/monitor</th>\n",
       "      <th>config/evaluation_num_episodes</th>\n",
       "      <th>config/metrics_smoothing_episodes</th>\n",
       "      <th>config/timesteps_per_iteration</th>\n",
       "      <th>config/min_iter_time_s</th>\n",
       "      <th>config/collect_metrics_timeout</th>\n",
       "      <th>config/buffer_size</th>\n",
       "      <th>config/prioritized_replay</th>\n",
       "      <th>config/learning_starts</th>\n",
       "      <th>config/replay_batch_size</th>\n",
       "      <th>config/replay_sequence_length</th>\n",
       "      <th>config/prioritized_replay_alpha</th>\n",
       "      <th>config/prioritized_replay_beta</th>\n",
       "      <th>config/prioritized_replay_eps</th>\n",
       "      <th>config/min_time_s_per_reporting</th>\n",
       "      <th>config/min_train_timesteps_per_reporting</th>\n",
       "      <th>config/min_sample_timesteps_per_reporting</th>\n",
       "      <th>config/input_evaluation</th>\n",
       "      <th>config/fcnet_hiddens_per_candidate</th>\n",
       "      <th>config/target_network_update_freq</th>\n",
       "      <th>config/tau</th>\n",
       "      <th>config/use_huber</th>\n",
       "      <th>config/huber_threshold</th>\n",
       "      <th>config/training_intensity</th>\n",
       "      <th>config/lr_schedule</th>\n",
       "      <th>config/lr_choice_model</th>\n",
       "      <th>config/rmsprop_epsilon</th>\n",
       "      <th>config/grad_clip</th>\n",
       "      <th>config/n_step</th>\n",
       "      <th>config/input</th>\n",
       "      <th>config/callbacks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55930_00000</th>\n",
       "      <td>0.00025</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[obs, new_obs]</td>\n",
       "      <td>67108864</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>episodes</td>\n",
       "      <td>180.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>INFO</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>415</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>3200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>sampler</td>\n",
       "      <td>&lt;class 'ray.rllib.algorithms.callbacks.Default...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             config/lr  config/train_batch_size  config/explore  \\\n",
       "trial_id                                                          \n",
       "55930_00000    0.00025                       32            True   \n",
       "\n",
       "             config/actions_in_input_normalized  config/postprocess_inputs  \\\n",
       "trial_id                                                                     \n",
       "55930_00000                               False                      False   \n",
       "\n",
       "             config/shuffle_buffer_size config/output  \\\n",
       "trial_id                                                \n",
       "55930_00000                           0          None   \n",
       "\n",
       "            config/output_compress_columns  config/output_max_file_size  \\\n",
       "trial_id                                                                  \n",
       "55930_00000                 [obs, new_obs]                     67108864   \n",
       "\n",
       "            config/evaluation_interval  config/evaluation_duration  \\\n",
       "trial_id                                                             \n",
       "55930_00000                       None                          10   \n",
       "\n",
       "            config/evaluation_duration_unit  \\\n",
       "trial_id                                      \n",
       "55930_00000                        episodes   \n",
       "\n",
       "             config/evaluation_sample_timeout_s  \\\n",
       "trial_id                                          \n",
       "55930_00000                               180.0   \n",
       "\n",
       "             config/evaluation_parallel_to_training  \\\n",
       "trial_id                                              \n",
       "55930_00000                                   False   \n",
       "\n",
       "             config/evaluation_num_workers  \\\n",
       "trial_id                                     \n",
       "55930_00000                              0   \n",
       "\n",
       "             config/always_attach_evaluation_results  config/in_evaluation  \\\n",
       "trial_id                                                                     \n",
       "55930_00000                                    False                 False   \n",
       "\n",
       "             config/sync_filters_on_rollout_workers_timeout_s  \\\n",
       "trial_id                                                        \n",
       "55930_00000                                              60.0   \n",
       "\n",
       "             config/keep_per_episode_custom_metrics  \\\n",
       "trial_id                                              \n",
       "55930_00000                                   False   \n",
       "\n",
       "             config/metrics_episode_collection_timeout_s  \\\n",
       "trial_id                                                   \n",
       "55930_00000                                         60.0   \n",
       "\n",
       "             config/metrics_num_episodes_for_smoothing  \\\n",
       "trial_id                                                 \n",
       "55930_00000                                        100   \n",
       "\n",
       "             config/min_time_s_per_iteration  \\\n",
       "trial_id                                       \n",
       "55930_00000                                1   \n",
       "\n",
       "             config/min_train_timesteps_per_iteration  \\\n",
       "trial_id                                                \n",
       "55930_00000                                         1   \n",
       "\n",
       "             config/min_sample_timesteps_per_iteration config/logger_creator  \\\n",
       "trial_id                                                                       \n",
       "55930_00000                                       1000                  None   \n",
       "\n",
       "            config/logger_config config/log_level  config/log_sys_usage  \\\n",
       "trial_id                                                                  \n",
       "55930_00000                 None             INFO                  True   \n",
       "\n",
       "             config/fake_sampler  config/seed  \\\n",
       "trial_id                                        \n",
       "55930_00000                False          415   \n",
       "\n",
       "             config/_tf_policy_handles_more_than_one_loss  \\\n",
       "trial_id                                                    \n",
       "55930_00000                                         False   \n",
       "\n",
       "             config/_disable_preprocessor_api  \\\n",
       "trial_id                                        \n",
       "55930_00000                              True   \n",
       "\n",
       "             config/_disable_action_flattening  \\\n",
       "trial_id                                         \n",
       "55930_00000                              False   \n",
       "\n",
       "             config/_disable_execution_plan_api  config/simple_optimizer  \\\n",
       "trial_id                                                                   \n",
       "55930_00000                                True                    False   \n",
       "\n",
       "             config/monitor  config/evaluation_num_episodes  \\\n",
       "trial_id                                                      \n",
       "55930_00000              -1                              -1   \n",
       "\n",
       "             config/metrics_smoothing_episodes  \\\n",
       "trial_id                                         \n",
       "55930_00000                                 -1   \n",
       "\n",
       "             config/timesteps_per_iteration  config/min_iter_time_s  \\\n",
       "trial_id                                                              \n",
       "55930_00000                              -1                      -1   \n",
       "\n",
       "             config/collect_metrics_timeout  config/buffer_size  \\\n",
       "trial_id                                                          \n",
       "55930_00000                              -1                  -1   \n",
       "\n",
       "             config/prioritized_replay  config/learning_starts  \\\n",
       "trial_id                                                         \n",
       "55930_00000                         -1                      -1   \n",
       "\n",
       "             config/replay_batch_size config/replay_sequence_length  \\\n",
       "trial_id                                                              \n",
       "55930_00000                        -1                          None   \n",
       "\n",
       "             config/prioritized_replay_alpha  config/prioritized_replay_beta  \\\n",
       "trial_id                                                                       \n",
       "55930_00000                               -1                              -1   \n",
       "\n",
       "             config/prioritized_replay_eps  config/min_time_s_per_reporting  \\\n",
       "trial_id                                                                      \n",
       "55930_00000                             -1                               -1   \n",
       "\n",
       "             config/min_train_timesteps_per_reporting  \\\n",
       "trial_id                                                \n",
       "55930_00000                                        -1   \n",
       "\n",
       "             config/min_sample_timesteps_per_reporting  \\\n",
       "trial_id                                                 \n",
       "55930_00000                                         -1   \n",
       "\n",
       "             config/input_evaluation config/fcnet_hiddens_per_candidate  \\\n",
       "trial_id                                                                  \n",
       "55930_00000                       -1                          [256, 32]   \n",
       "\n",
       "             config/target_network_update_freq  config/tau  config/use_huber  \\\n",
       "trial_id                                                                       \n",
       "55930_00000                               3200         1.0             False   \n",
       "\n",
       "             config/huber_threshold config/training_intensity  \\\n",
       "trial_id                                                        \n",
       "55930_00000                     1.0                      None   \n",
       "\n",
       "            config/lr_schedule  config/lr_choice_model  \\\n",
       "trial_id                                                 \n",
       "55930_00000               None                   0.001   \n",
       "\n",
       "             config/rmsprop_epsilon config/grad_clip  config/n_step  \\\n",
       "trial_id                                                              \n",
       "55930_00000                 0.00001             None              1   \n",
       "\n",
       "            config/input                                   config/callbacks  \n",
       "trial_id                                                                     \n",
       "55930_00000      sampler  <class 'ray.rllib.algorithms.callbacks.Default...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect training params\n",
    "pd.set_option('display.max_columns', 200)\n",
    "df.iloc[:, 104:170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1176.564785081392,\n",
       " 1160.7331769653413,\n",
       " 1186.1205696197187,\n",
       " 1165.3043900062098,\n",
       " 1175.9794958466293,\n",
       " 1171.380893554055,\n",
       " 1163.0653375768973,\n",
       " 1154.4095130857775,\n",
       " 1182.237449494122,\n",
       " 1152.7768695896]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# found the rewards?\n",
    "print(len(df['sampler_results/hist_stats/episode_reward'][0]))\n",
    "df['sampler_results/hist_stats/episode_reward'][0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot per-timestep (episode) rewards.\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,4))\n",
    "\n",
    "# collect plot data from bandit rewards\n",
    "start_at = 18  #bandit rewards are nan for 1st 3 iterations\n",
    "smoothing_win = 100\n",
    "\n",
    "x = list(range(start_at, len(rewards)-1))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) \n",
    "     for i in range(start_at, len(rewards)-1)]\n",
    "# plot bandit rewards\n",
    "ax.plot(x, y, label=\"LinUCBBandit\")\n",
    "\n",
    "rewards_slateq = df['sampler_results/hist_stats/episode_reward'][0]\n",
    "x = list(range(start_at, len(rewards_slateq)-1))\n",
    "y = [np.nanmean(rewards_slateq[max(i - smoothing_win, 0):i + 1]) \n",
    "     for i in range(start_at, len(rewards_slateq)-1)]\n",
    "# plot bandit rewards\n",
    "ax.plot(x, y, label=\"SlateQ\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_mean_sweetest_reward, \n",
    "            color=\"r\", \n",
    "            linestyle=\"-\",\n",
    "            label=\"Sweetest baseline\",)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='center right', frameon=True)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"Compare Training Mean Reward\")\n",
    "plt.xlabel(\"Training iterations\")\n",
    "\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline RL in RecSys\n",
    "\n",
    "To be continued in a few weeks at the [ACM RecSys Conference](https://recsys.acm.org/recsys22/tutorials/) in Seattle ...!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_07_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray if you are done\n",
    "import ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
