{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07. Introduction to RL applied to Recommender Systems\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn how to:\n",
    "\n",
    " * [Intro RecSys with RL](#recsys_rl)\n",
    " * [Create a RecSys RL environment](#recsys_env)\n",
    " * [Train a Contextual Bandit on the environment](#cb)\n",
    " * [Train using a RL Offline algorithm on the environment](#offline)\n",
    " \n",
    " \n",
    " find RLlib algos to train policy models on environments.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro RecSys with RL <a class=\"anchor\" id=\"recsys_rl\"></a>\n",
    "\n",
    "A Recommender System <b>(RecSys)</b> suggests items that are most pertinent to a particular user.  Examples of recommender systems include:\n",
    "<ul>\n",
    "    <li>Movie/video/music recommendations (Netflix/YouTube/Spotify)</li>\n",
    "    <li>Online shopping recommendations (Amazon/Shopify)</li>\n",
    "    <li>Filtering your feed as you scroll (Twitter/Instagram)</li>\n",
    "</ul>\n",
    "\n",
    "<b>Two main approaches to training algorithms</b> for RecSys are: \n",
    "<ol>\n",
    "    <li>Traditional Machine Learning <b>(ML)</b></li>\n",
    "    <li>Reinforcement Learning <b>(RL)</b></li>\n",
    "    </ol>\n",
    "\n",
    "<b>In the traditional ML method</b>, data is gathered about users interactions with items.  Matrix factorization is often used to turn this data into trainable representations of users and items (\"embeddings\" or features or X's), and the views or actions by users of those items (dependent variable or y's). A ranking algorithm (e.g. collaborative filtering) is trained on all the data at once as if all the actions happend during the same time step.  Such a <b><i>static</i> </b>, fixed model is useful when there are millions of items and users, since learning from all data at once in <b>batch learning</b> is efficient.\n",
    "\n",
    "<b>In RL</b>, users interact with offers repeatedly over time.  This <b><i>dynamic</i></b> model is iteratively trained based on \n",
    "the last observation, action, and reward.  One caveat with RL, since a recommendation needs to be calculated at every time step, only a pre-selected handful of top candidate items per user (from the traditional ML ranking model) is presented in the simulation environment.  \n",
    "\n",
    "<b>Online RL deployed to production in RecSys</b> takes as input the top-K ranked and ordered recommendations per user, keeps the user and item feature embeddings as context, but optimizes a sequence of user in-session interactions <b>real-time</b> using live results.  \n",
    "\n",
    "<img src=\"./images/recsys_overview.png\" width=\"100%\"/>\n",
    "\n",
    "<b>Offline RL in RecSys</b> uses offline data to explore a previous training run (or production) recommender model policy, implicitly through the data.  ‚ÄúSerendipitous‚Äù aspects of user experience can be explored through offline RL, since random actions the user did not historically take can be tried in the simulation.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "    <b>üí° Online vs Offline RL, when the algorithm learning from an environment is: </b> <br><br>\n",
    "    ‚úîÔ∏è live (typically gaming platforms or complex systems simulations), this is called <b>online RL</b> and evaluation during training is <b>on-policy</b>. The live environment could also be in production. <br><br>\n",
    "    ‚úîÔ∏è trained using data (could be log files of users interactions with items) converted into trajectory sequences of tuples, this is called <b>offline RL</b> and evaluation during training is <b>off-policy</b>, because the policy (RL word for model) used to log the data is different from the policy used to explore the data. Off-policy evaluation is also called <b>counterfactual evaluation</b>.</b> \n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a RecSys RL Environment <a class=\"anchor\" id=\"recsys_env\"></a>\n",
    "\n",
    "As we learned in the first 2 lessons, the first step to training a RL RecSys policy model is to create an <b>environment</b> that can interact with a RL Algorithm to train a recommender agent. \n",
    "\n",
    "In this notebook, we will use <b><a href=\"https://github.com/google-research/recsim\">Google's RecSim environment</a></b>, which was developed for the YouTube recommendation problem.  The environment is <i>Timelimit-based</i>, meaning the termination condition for an episode will be after a fixed number (60) of videos are watched. The RecSim environment consists of:\n",
    "\n",
    "<img src=\"./images/recsim_environment.png\" width=\"90%\" />\n",
    "\n",
    "* <b>Document Model</b>, in the range [0, 1].  \n",
    "<ul>\n",
    "    <li>On the 0-end of the scale, <b>\"sweet\"</b> documents lead to large amounts of <b>\"click bait\"</b> or immediate engagement. Sweetness values are drawn from ln Normal(Œºsweet, œÉsweet).</li>\n",
    "    <li>On the 1-end of the scale, documents termed <b>kale</b>, are less click-bait, but tend to <b>increase user long-term satisfaction</b>. Kale values are drawn from ln Normal(Œºkale, œÉkale)</li>\n",
    "    <li>Mixed doc values are drawn from linear interpolation between parameters of the two distributions in proportion to their kaleness.</li>\n",
    "    </ul>\n",
    "* <b>User Model</b>, empty vectors.  <i>Since user features are so business-dependent, RecSim out-of-the-box does not include any user features.</i>  User features, typically imported from real data, could contain: \n",
    "<ul>\n",
    "    <li>evolving, unknown contexts (interests, preferences, satisfaction, activity, mood)</li>\n",
    "    <li>unobservable events that could impact user behavior (personalized promotions, interuptions that cause turning off a video such as because someone rang their doorbell).</li>\n",
    "    </ul>\n",
    "* <b>Rewards</b>, or user satisfaction after their choice, modeled in the range [0, 1] that stochastically (and slowly) increases or decreases with the consumption of different types of content; kale or sweetness.  \n",
    "\n",
    "\n",
    "<b>RLlib comes with 3 RecSim environments</b>  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "üëâ <b>Long Term Satisfaction</b> (used in this tutorial) <br>\n",
    "- Interest Evolution <br>\n",
    "- Interest Exploration <br>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.stats import linregress, sem\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "from pprint import pprint\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# silence the many tensorflow warnings\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import recsim\n",
    "\n",
    "print(f\"tensorflow: {tf.__version__}\")\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = \\\n",
    "                gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array(\n",
    "                [self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified-lts\", \n",
    "                  lambda env_config: LTSWithStrongerDissatisfactionEffect(\n",
    "                      LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified-lts' to be used in RLlib configs (see below)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to make RecSim environments?\n",
    "\n",
    "# Env with 20 candidate items and a slate-size of 2.\n",
    "# Try to strengthen the dissatisfaction effect on the engagement\n",
    "\n",
    "# Step 1. define a config dictionary\n",
    "env_config_20 = \\\n",
    "    {\n",
    "        # The number of possible documents/videos/candidates that we can recommend\n",
    "        \"num_candidates\": 20,  \n",
    "        # The number of recommendations that we will be making\n",
    "        \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "        # Set to False for re-using the same candidate doecuments each timestep.\n",
    "        \"resample_documents\": True,\n",
    "        # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "        \"wrap_for_bandits\": True,}\n",
    "\n",
    "# Step 2. create a RecSim environment\n",
    "lts_20_2_env = LongTermSatisfactionRecSimEnv(env_config_20)\n",
    "\n",
    "# step 3. create a modified RecSim environment\n",
    "# LTSWithStrongerDissatisfactionEffect(recsim_env)\n",
    "modified_lts_20_2_env = \\\n",
    "    LTSWithStrongerDissatisfactionEffect(lts_20_2_env)\n",
    "\n",
    "print(type(modified_lts_20_2_env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {modified_lts_20_2_env}\")\n",
    "\n",
    "# print gym Spaces\n",
    "if isinstance(modified_lts_20_2_env.action_space, gym.spaces.Space):\n",
    "    print()\n",
    "    print(f\"action space: {modified_lts_20_2_env.action_space}\")\n",
    "if isinstance(modified_lts_20_2_env.observation_space, gym.spaces.Space):\n",
    "    print()\n",
    "    print(f\"observation space: \", end=\"\")\n",
    "    print(pretty_print(dict(modified_lts_20_2_env.observation_space)))\n",
    "    # pprint(modified_lts_20_2_env.observation_space)\n",
    "\n",
    "# reset env\n",
    "obs = modified_lts_20_2_env.reset()\n",
    "\n",
    "# get a random action\n",
    "range_actions = modified_lts_20_2_env.action_space.n\n",
    "action = random.randint(0,range_actions-1)\n",
    "print(f\"action: {action}, \", end=\"\")\n",
    "\n",
    "# get the next obs, next reward, done based on the action\n",
    "obs, reward, done, _ = modified_lts_20_2_env.step(action)\n",
    "# print(f\"obs: \", end=\"\")\n",
    "# pprint(obs)\n",
    "print(f\"reward: {reward:.2f}, \", end=\"\")\n",
    "print(f\"done: {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the RecSim documentation, we can see that:\n",
    "\n",
    "<ul>\n",
    "    <li><b>action</b>, is a number between 0 and num_products - 1 that represents index of products clicked, between 0 and 399 in this case. </li>\n",
    "    <li><b>observation</b> will be the next session of 20 candidate products user sees, based on the user's action.\n",
    "    </li>\n",
    "    <li><b>reward</b> is the engagement value of the product user clicked.  Notice RecSim has a simplified assumption that the user always clicks the 1st-position product recommended to the user.</li>\n",
    "    <li><b>done</b> is a True/False flag indicating if the episode or user's timeline (fixed 60 sessions) is over.</li>\n",
    "    <li><b>info</b> currently not used, so it is always an empty dictionary.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Get an environment baseline</b>\n",
    "\n",
    "It is always best practice, before training an algorithm, to run through the environment and record the mean reward as a baseline.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° For this environment, we will calculate 3 baselines: <br>\n",
    "    <ul>\n",
    "        <li>Random baseline: pick randomly every time</li>\n",
    "        <li>Greedy sweet baseline: always pick the sweetest item </li>\n",
    "        <li>Greedy kale baseline: always pick the kaleist item </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def calc_baseline(env, \n",
    "                  baseline_type=\"random\",\n",
    "                  episodes=100, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce an action\n",
    "        action_random = env.action_space.sample()\n",
    "        action_sweetest = np.argmax(obs['item'])\n",
    "        action_kaleiest = np.argmin(obs['item'])\n",
    "        if baseline_type == \"random\":\n",
    "            action = action_random\n",
    "        elif baseline_type == \"sweetest\":\n",
    "            action = action_sweetest\n",
    "        elif baseline_type == \"kaleist\":\n",
    "            action = action_kaleiest\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 99 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 9 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "    env_sd_reward = sem(episode_rewards)\n",
    "\n",
    "    print(f\"\\nMean {baseline_type} baseline reward: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a baseline type: random, sweetest, or kaleist\n",
    "lts_20_2_mean_kaleist_reward, _ = \\\n",
    "    calc_baseline(lts_20_2_env, \n",
    "                  baseline_type=\"kaleist\",\n",
    "                  episodes=100)\n",
    "lts_20_2_mean_random_reward, _ = \\\n",
    "    calc_baseline(lts_20_2_env, \n",
    "                  baseline_type=\"random\",\n",
    "                  episodes=100)\n",
    "lts_20_2_mean_sweetest_reward, _ = \\\n",
    "    calc_baseline(lts_20_2_env, \n",
    "                  baseline_type=\"sweetest\",\n",
    "                  episodes=100)\n",
    "\n",
    "# Kaleist baseline: 1085.60+/-1.01   # smallest\n",
    "# Random baseline: 1156.79+/-1.12    # next biggest\n",
    "# Sweetest baseline: 1162.94+/-1.08  # biggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Contextual Bandit on the environment <a class=\"anchor\" id=\"cb\"></a>\n",
    "\n",
    "A Bandit session is one where we have an opportunity to recommend the user an item and observe their behaviour. We receive a reward if they click.\n",
    "\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>Bandits</b></i></li>\n",
    "    <li>On the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#bandits\">algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select RLlib Bandit algorithm w/Upper Confidence Bound (UCB) exploration\n",
    "# and find that algorithm's config class\n",
    "\n",
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.bandit import BanditLinUCBConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment below to see the long list of default config values\n",
    "# print(f\"Bandit's default config is:\")\n",
    "# print(pretty_print(BanditLinUCBConfig().to_dict()))\n",
    "\n",
    "# Create a defaut Algorithmconfig:\n",
    "bandit_config = BanditLinUCBConfig()\n",
    "\n",
    "# Create the Env config\n",
    "env_config_20 = \\\n",
    "    {\n",
    "        # The number of possible documents/videos/candidates that we can recommend\n",
    "        \"num_candidates\": 20,  \n",
    "        # The number of recommendations that we will be making\n",
    "        \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "        # Set to False for re-using the same candidate doecuments each timestep.\n",
    "        \"resample_documents\": True,\n",
    "        # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "        \"wrap_for_bandits\": True,}\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "bandit_config.environment(env=\"modified-lts\", env_config=env_config_20)\n",
    "\n",
    "# Decide if you want torch or tensorflow DL framework\n",
    "bandit_config.framework(\"torch\")\n",
    "\n",
    "# Set up evaluation:\n",
    "bandit_config.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=10,\n",
    "    # Use separate resources (RLlib rollout workers).\n",
    "    evaluation_num_workers=2,\n",
    "    # Run 20 episodes per evaluation (per iteration) \n",
    "    # -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=20,\n",
    "    evaluation_duration_unit=\"timesteps\",\n",
    "    # # Run evaluation alternatingly with training (not in parallel).\n",
    "    # evaluation_parallel_to_training=False,\n",
    ")\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "bandit_config.rollouts(\n",
    "    num_rollout_workers=2, \n",
    "    num_envs_per_worker=4)\n",
    "\n",
    "bandit_config.resources(num_gpus=0)\n",
    "\n",
    "bandit_config.debugging(\n",
    "    seed=415, \n",
    "    # Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "    log_level=\"ERROR\")\n",
    "\n",
    "print(type(bandit_config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    \n",
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "linucb_algo = bandit_config.build()\n",
    "print(f\"Algorithm type: {type(linucb_algo)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# train the Bandit Algorithm instance for n iterations (timesteps)\n",
    "rewards = []\n",
    "n_timesteps = 100\n",
    "for i in range(n_timesteps):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = linucb_algo.train()\n",
    "    \n",
    "    # Extract reward from results.\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "    if i % 99 == 0:\n",
    "        print(f\"\\nIteration={i}, \", end=\"\")\n",
    "        print(f\"Mean Bandit Reward={result['episode_reward_mean']:.2f}\",end=\"\")\n",
    "        try:\n",
    "            print(f\"+/-{np.std(rewards[9:]):.2f}\")\n",
    "        except:\n",
    "            print()\n",
    "    elif i % 9 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")\n",
    "\n",
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "linucb_algo.stop()\n",
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-timestep (episode) rewards.\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,4))\n",
    "\n",
    "# collect plot data from bandit rewards\n",
    "start_at = 4  #bandit rewards are nan for 1st 3 iterations\n",
    "smoothing_win = 100\n",
    "x = list(range(start_at, len(rewards)-1))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) \n",
    "     for i in range(start_at, len(rewards)-1)]\n",
    "\n",
    "# plot bandit rewards\n",
    "ax.plot(x, y, label=\"LinUCBBandit\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_mean_sweetest_reward, \n",
    "            color=\"r\", \n",
    "            linestyle=\"-\",\n",
    "            label=\"Sweetest baseline\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='center right', frameon=True)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"Training Mean Reward\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "\n",
    "plt.plot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our algorithms over 100 timesteps:\n",
    "<ul>\n",
    "    <li>Kaleist baseline: 1085.60+/-1.01 </li>\n",
    "    <li>Random baseline: 1156.79+/-1.12 </li>\n",
    "    <b><li>Sweetest baseline: 1162.94+/-1.08 </li>\n",
    "        <li>Bandit mean reward = 1163.89+/-1.45</li></b>\n",
    "    </ul>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "ü§î Notice that the bandit mean reward is suspiciously the same as the sweetest baseline! <br>\n",
    "</div>\n",
    "\n",
    "In this dummy Recsim environment, we did not have any user features.  This makes the contextual bandit without any user context, i.e. without any state.  A stateless bandit cannot remember things between timesteps, so it will be exactly the most greedy policy.  \n",
    "\n",
    "So, it makes sense that the Bandit reward = Sweetest baseline!  The context-less bandit was unable to learn anything!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using a RL Offline algorithm on the environment <a class=\"anchor\" id=\"offline\"></a>\n",
    "\n",
    "CRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first take a look at some of this (JSON) data using pandas:\n",
    "json_file = \"offline_rl_data/recsys_expert.json\"\n",
    "df = pd.read_json(json_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the config class of the algorithm, we would like to train with: CRR.\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a defaut CRR config:\n",
    "config = CRRConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "# NOTE: We said above that we wouldn't really have an environment available (so how can\n",
    "# we set one up here??).\n",
    "# The following is only to tell the algorithm, which environment our offline data was actually taken from.\n",
    "config.environment(env=\"modified-lts\")\n",
    "# If you really really don't have an environment, set `env=None` here and additionally define your action- and\n",
    "# observation spaces.\n",
    "# config.environment(env=None, action_space=..., observation_space=...)\n",
    "\n",
    "#################################################\n",
    "# This is the most important piece of code \n",
    "# in this notebook:\n",
    "# It explains how to point your \n",
    "# algorithm to the correct offline data file\n",
    "# (instead of a live-environment).\n",
    "#################################################\n",
    "config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        # If you feel daring here, use the `pendulum_beginner.json` file instead of the expert one here.\n",
    "        # You may need to train a little longer, then, in order to get a decent policy.\n",
    "        # But since you have the actual Pendulum environment available for evaluation, you should be able\n",
    "        # to perfectly stop learning once a good episode reward (> -300.0) has been reached.\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/pendulum_expert.json\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    # The (continuous) actions in our input files are already normalized\n",
    "    # (meaning between -1.0 and 1.0) -> We don't have to do anything with them prior to\n",
    "    # computing losses.\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "# RLlib's CRR is a very new algorithm (since 1.13) and only supports\n",
    "# the PyTorch framework thus far. We'll provide a tf version in the near future.\n",
    "config.framework(\"torch\")\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "config.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=1,\n",
    "    # Use a separate resource (\"RLlib rollout worker\")\n",
    "    evaluation_num_workers=1,\n",
    "\n",
    "    # # Use separate resources (RLlib rollout workers).\n",
    "    # evaluation_num_workers=2,\n",
    "\n",
    "    # Run 20 episodes per evaluation (per iteration) -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=20,\n",
    "    evaluation_duration_unit=\"episodes\",\n",
    "\n",
    "    # Use a slightly different config for the evaluation:\n",
    "    evaluation_config={\n",
    "        # - Use a real environment (so we can fully trust the evaluation results, rewards, etc..)\n",
    "        \"input\": \"sampler\",\n",
    "        # - Switch off exploration for better (less stochastic) action computations.\n",
    "        \"explore\": False,\n",
    "    },\n",
    "\n",
    "    # Run evaluation alternatingly with training (not in parallel).\n",
    "    evaluation_parallel_to_training=False,\n",
    ")\n",
    "\n",
    "print(type(config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_interval = 100   #100, num training episodes to run between eval steps\n",
    "verbosity = 2 # Tune screen verbosity\n",
    "\n",
    "experiment_results = tune.run(\"CRR\", \n",
    "                    \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={#\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\n",
    "          \"training_iteration\": 10,  # stop if achieved 100 episodes\n",
    "          # \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\n",
    "          },  \n",
    "              \n",
    "    # training config params\n",
    "    config = config.to_dict(),\n",
    "                    \n",
    "    #redirect logs instead of default ~/ray_results/\n",
    "    local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq = checkpoint_freq,\n",
    "    checkpoint_at_end=True,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    verbose = verbosity,\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
