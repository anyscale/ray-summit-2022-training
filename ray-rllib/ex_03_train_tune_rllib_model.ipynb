{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03. Introduction to Ray Tune and hyperparameter optimization (HPO)\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this notebook, you will learn:\n",
    " * [How to configure Ray Tune to find solid hyperparameters more easily](#configure_ray_tune)\n",
    " * [The details behind Ray RLlib resource allocation](#resource_allocation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "\n",
    "# Importing the very same environment class that we have coded together in\n",
    "# the previous notebook.\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena, play_one_episode\n",
    "\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to configure Ray Tune to find solid hyperparameters more easily <a class=\"anchor\" id=\"configure_ray_tune\"></a>\n",
    "\n",
    "In the previous experiments, we used a single algorithm's (PPO) configuration to create\n",
    "exactly one Algorithm object and call its `train()` method manually a couple of times.\n",
    "\n",
    "A common thing to try when doing ML or RL is to look for better choices of hyperparameters, neural network architectures, or algorithm settings. This hyperparameter optimization\n",
    "problem can be tackled in a scalable fashion using Ray Tune (in combination with RLlib!).\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates, how you can setup a simple grid-search for one very important hyperparameter (the learning rate), using our already existing PPO config object and Ray Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x1cc35874130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOConfig object (same as we did in the previous notebook):\n",
    "config = PPOConfig()\n",
    "\n",
    "# Setup our config object the exact same way as before:\n",
    "# Point to our MultiAgentArena env:\n",
    "config.environment(env=MultiAgentArena)\n",
    "\n",
    "# Setup multi-agent mapping:\n",
    "\n",
    "# Environment provides M agent IDs.\n",
    "# RLlib has N policies (neural networks).\n",
    "# The `policy_mapping_fn` maps M agent IDs to N policies (M <= N).\n",
    "\n",
    "# If you don't provide a policy_mapping_fn, all agent IDs will map to \"default_policy\".\n",
    "config.multi_agent(\n",
    "    # Tell RLlib to create 2 policies with these IDs here:\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    # Tell RLlib to map agent1 to policy1 and agent2 to policy2.\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "\n",
    "# Reduce the number of workers from 2 (default) to 1 to save some resources on the expensive hyperparameter sweep.\n",
    "# IMPORTANT: More information on resource requirements for tune hyperparameter sweeps and different RLlib algorithm setups\n",
    "# below.\n",
    "config.rollouts(num_rollout_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore how a very simple hyperparameter search should be configured with RLlib and Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default learning rate for PPO is: 5e-05\n",
      "Default train batch size for PPO is: 4000\n"
     ]
    }
   ],
   "source": [
    "# Before setting up the learning rate hyperparam sweep,\n",
    "# let's see what the default learning rate and train batch size is for PPO:\n",
    "print(f\"Default learning rate for PPO is: {config.lr}\")\n",
    "print(f\"Default train batch size for PPO is: {config.train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x1cc35874130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's change our existing config object and add a simple\n",
    "# grid-search over two different learning rates to it:\n",
    "config.training(\n",
    "    lr=tune.grid_search([5e-5, 1e-4]),\n",
    "    train_batch_size=tune.grid_search([3000, 4000]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 19:55:15,856\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m.\n",
      "\u001b[2m\u001b[36m(PPO pid=13368)\u001b[0m 2022-08-14 19:55:24,122\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=13368)\u001b[0m 2022-08-14 19:55:24,123\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18428)\u001b[0m 2022-08-14 19:55:29,648\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:55:18 (running for 00:00:00.27)\n",
      "Memory usage on this node: 12.7/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (3 PENDING, 1 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | PENDING  |                 | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | PENDING  |                 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | PENDING  |                 | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=13368)\u001b[0m 2022-08-14 19:55:33,523\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=14916)\u001b[0m 2022-08-14 19:55:39,070\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=14916)\u001b[0m 2022-08-14 19:55:39,070\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=13368)\u001b[0m 2022-08-14 19:55:42,710\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21632)\u001b[0m 2022-08-14 19:55:44,606\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:55:33 (running for 00:00:15.34)\n",
      "Memory usage on this node: 15.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (2 PENDING, 2 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | PENDING  |                 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | PENDING  |                 | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=14916)\u001b[0m 2022-08-14 19:55:48,793\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=14084)\u001b[0m 2022-08-14 19:55:54,399\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=14084)\u001b[0m 2022-08-14 19:55:54,400\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=14916)\u001b[0m 2022-08-14 19:55:59,368\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21368)\u001b[0m 2022-08-14 19:55:59,958\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:55:48 (running for 00:00:30.61)\n",
      "Memory usage on this node: 18.0/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (1 PENDING, 3 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | PENDING  |                 | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=14084)\u001b[0m 2022-08-14 19:56:04,189\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=13928)\u001b[0m 2022-08-14 19:56:09,810\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=13928)\u001b[0m 2022-08-14 19:56:09,811\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3440)\u001b[0m 2022-08-14 19:56:15,427\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n",
      "\u001b[2m\u001b[36m(PPO pid=14084)\u001b[0m 2022-08-14 19:56:18,043\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:04 (running for 00:00:46.01)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-56-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999904\n",
      "  episode_reward_mean: -10.609999999999998\n",
      "  episode_reward_min: -33.00000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3682465553283691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01840326003730297\n",
      "          model: {}\n",
      "          policy_loss: -0.04391688108444214\n",
      "          total_loss: 7.386646270751953\n",
      "          vf_explained_var: 0.006299562752246857\n",
      "          vf_loss: 7.426882743835449\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3705053329467773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015813326463103294\n",
      "          model: {}\n",
      "          policy_loss: -0.04492653161287308\n",
      "          total_loss: 3.1492743492126465\n",
      "          vf_explained_var: 0.26977676153182983\n",
      "          vf_loss: 3.191037893295288\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 20.132142857142856\n",
      "    ram_util_percent: 60.06785714285714\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 0.9999999999999934\n",
      "  policy_reward_mean:\n",
      "    policy1: -1.6\n",
      "    policy2: -9.009999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07177010968064355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.019603155327415283\n",
      "    mean_inference_ms: 2.9760017986100573\n",
      "    mean_raw_obs_processing_ms: 0.4294722606960197\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999904\n",
      "    episode_reward_mean: -10.609999999999998\n",
      "    episode_reward_min: -33.00000000000003\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [19.499999999999904, 1.6181500583911657e-14, -15.899999999999988,\n",
      "        -10.499999999999979, -21.0, -16.499999999999993, -11.399999999999988, -22.500000000000007,\n",
      "        -14.999999999999979, -10.49999999999998, -33.00000000000003, -28.50000000000003,\n",
      "        -10.499999999999975, 8.10000000000002, -31.500000000000043, -30.000000000000025,\n",
      "        -13.500000000000023, -10.499999999999979, 1.5000000000000253, -22.500000000000043,\n",
      "        1.5000000000000204, -24.000000000000036, -14.399999999999993, -10.499999999999984,\n",
      "        -10.49999999999998, -0.8999999999999897, -2.999999999999971, 14.700000000000026,\n",
      "        3.0, -6.938893903907228e-16]\n",
      "      policy_policy1_reward: [29.5, 10.0, -7.0, -0.5, -11.0, -17.5, -2.5, -12.5, -5.0,\n",
      "        -0.5, -23.0, -18.5, -0.5, 11.5, -21.5, -20.0, -3.5, -0.5, 11.5, -12.5, 11.5,\n",
      "        -19.5, -5.5, -0.5, -0.5, 8.0, 7.0, 22.5, 13.0, 10.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, 0.9999999999999934, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.400000000000007, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -4.499999999999984, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 0.9999999999999934\n",
      "    policy_reward_mean:\n",
      "      policy1: -1.6\n",
      "      policy2: -9.009999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -23.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07177010968064355\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.019603155327415283\n",
      "      mean_inference_ms: 2.9760017986100573\n",
      "      mean_raw_obs_processing_ms: 0.4294722606960197\n",
      "  time_since_restore: 19.39815878868103\n",
      "  time_this_iter_s: 19.39815878868103\n",
      "  time_total_s: 19.39815878868103\n",
      "  timers:\n",
      "    learn_throughput: 340.194\n",
      "    learn_time_ms: 8818.486\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 19392.176\n",
      "  timestamp: 1660499768\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:19 (running for 00:01:01.48)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00001 with episode_reward_mean=-10.609999999999998 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B3F3D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B0A940>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E460>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |        |                  |      |          |                        |                        |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |                   -1.6 |                  -9.01 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |        |                  |      |          |                        |                        |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.300000000000022\n",
      "  episode_reward_mean: -11.460000000000004\n",
      "  episode_reward_min: -36.90000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3758571147918701\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010535988956689835\n",
      "          model: {}\n",
      "          policy_loss: -0.02646591141819954\n",
      "          total_loss: 7.688065528869629\n",
      "          vf_explained_var: 0.006029615644365549\n",
      "          vf_loss: 7.712424278259277\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3820884227752686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00422206986695528\n",
      "          model: {}\n",
      "          policy_loss: -0.019792290404438972\n",
      "          total_loss: 3.888664722442627\n",
      "          vf_explained_var: 0.16779157519340515\n",
      "          vf_loss: 3.9076123237609863\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.292307692307695\n",
      "    ram_util_percent: 51.63461538461539\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 16.0\n",
      "    policy2: -1.200000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.816666666666667\n",
      "    policy2: -8.643333333333315\n",
      "  policy_reward_min:\n",
      "    policy1: -28.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06778642996991728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.019627624811708277\n",
      "    mean_inference_ms: 2.5305799625985257\n",
      "    mean_raw_obs_processing_ms: 0.4161216941764855\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 9.300000000000022\n",
      "    episode_reward_mean: -11.460000000000004\n",
      "    episode_reward_min: -36.90000000000005\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [3.6000000000000023, -36.90000000000005, -16.79999999999999, 4.500000000000018,\n",
      "        -22.200000000000017, -22.500000000000018, -5.399999999999979, -28.20000000000003,\n",
      "        -16.499999999999993, -1.7999999999999763, -26.100000000000026, -2.9999999999999876,\n",
      "        9.300000000000022, 4.8000000000000025, -2.699999999999988, -17.999999999999993,\n",
      "        -24.900000000000002, -1.7999999999999905, -36.00000000000005, 9.520162436160717e-15,\n",
      "        -8.999999999999986, 9.076073226310655e-15, -8.399999999999975, -10.499999999999977,\n",
      "        6.000000000000001, -15.000000000000039, -18.299999999999997, -4.4999999999999805,\n",
      "        -10.499999999999982, -33.00000000000004]\n",
      "      policy_policy1_reward: [12.5, -28.0, -9.0, 14.5, -15.5, -12.5, 3.5, -21.5, -6.5,\n",
      "        6.0, -20.5, 7.0, 16.0, 6.0, 4.0, -8.0, -16.0, 6.0, -26.0, 10.0, 1.0, 10.0, 0.5,\n",
      "        -0.5, 16.0, -5.0, -10.5, 5.5, -0.5, -23.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -8.899999999999986,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999982, -5.599999999999986,\n",
      "        -9.99999999999998, -6.69999999999999, -1.200000000000004, -6.699999999999995,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 16.0\n",
      "      policy2: -1.200000000000004\n",
      "    policy_reward_mean:\n",
      "      policy1: -2.816666666666667\n",
      "      policy2: -8.643333333333315\n",
      "    policy_reward_min:\n",
      "      policy1: -28.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06778642996991728\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.019627624811708277\n",
      "      mean_inference_ms: 2.5305799625985257\n",
      "      mean_raw_obs_processing_ms: 0.4161216941764855\n",
      "  time_since_restore: 17.916444301605225\n",
      "  time_this_iter_s: 17.916444301605225\n",
      "  time_total_s: 17.916444301605225\n",
      "  timers:\n",
      "    learn_throughput: 343.935\n",
      "    learn_time_ms: 8722.585\n",
      "    synch_weights_time_ms: 2.991\n",
      "    training_iteration_time_ms: 17914.449\n",
      "  timestamp: 1660499751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=13928)\u001b[0m 2022-08-14 19:56:19,614\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:24 (running for 00:01:06.62)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00001 with episode_reward_mean=-10.609999999999998 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B3F3D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B0A940>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E460>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |        |                  |      |          |                        |                        |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:29 (running for 00:01:11.70)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00001 with episode_reward_mean=-10.609999999999998 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B3F3D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B0A940>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E460>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |        |                  |      |          |                        |                        |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:34 (running for 00:01:16.76)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00001 with episode_reward_mean=-10.609999999999998 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B3F3D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B0A940>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E460>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |        |                  |      |          |                        |                        |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 19:56:40,021\tWARNING trial_runner.py:879 -- Trial Runner checkpointing failed: [WinError 5] Access is denied: 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\.tmp_checkpoint' -> 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\experiment_state-2022-08-14_19-55-18.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:40 (running for 00:01:21.82)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00001 with episode_reward_mean=-10.609999999999998 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B3F3D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B0A940>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E460>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |        |                  |      |          |                        |                        |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 19:56:45,068\tWARNING trial_runner.py:879 -- Trial Runner checkpointing failed: [WinError 5] Access is denied: 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\.tmp_checkpoint' -> 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\experiment_state-2022-08-14_19-55-18.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:45 (running for 00:01:26.87)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00001 with episode_reward_mean=-10.609999999999998 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B3F3D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B0A940>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B4E460>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75A91EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |        |                  |      |          |                        |                        |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 19:56:45,656\tWARNING trial_runner.py:879 -- Trial Runner checkpointing failed: [WinError 5] Access is denied: 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\.tmp_checkpoint' -> 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\experiment_state-2022-08-14_19-55-18.json'\n",
      "2022-08-14 19:56:45,714\tWARNING trial_runner.py:879 -- Trial Runner checkpointing failed: [WinError 5] Access is denied: 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\.tmp_checkpoint' -> 'C:\\\\Dropbox\\\\Projects\\\\ray-summit-2022-training\\\\ray-rllib\\\\results\\\\PPO\\\\experiment_state-2022-08-14_19-55-18.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_4205e_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.299999999999986\n",
      "  episode_reward_mean: -10.11\n",
      "  episode_reward_min: -34.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 847f41dbc3e943d8b61110c9b40c9844\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3723297119140625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01424507424235344\n",
      "          model: {}\n",
      "          policy_loss: -0.032470036298036575\n",
      "          total_loss: 7.071531295776367\n",
      "          vf_explained_var: 0.010149246081709862\n",
      "          vf_loss: 7.101151466369629\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.379096508026123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007166510447859764\n",
      "          model: {}\n",
      "          policy_loss: -0.021334825083613396\n",
      "          total_loss: 3.5342931747436523\n",
      "          vf_explained_var: 0.24294492602348328\n",
      "          vf_loss: 3.554194688796997\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 27.27457627118644\n",
      "    ram_util_percent: 70.43389830508475\n",
      "  pid: 14084\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -1.2000000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: -1.2375\n",
      "    policy2: -8.872499999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07974627732455924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021955008150428213\n",
      "    mean_inference_ms: 2.9168686131422774\n",
      "    mean_raw_obs_processing_ms: 0.4206118479993039\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.299999999999986\n",
      "    episode_reward_mean: -10.11\n",
      "    episode_reward_min: -34.500000000000064\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [12.000000000000012, -14.699999999999992, -6.899999999999988,\n",
      "        -4.499999999999991, -10.500000000000039, -17.999999999999996, 3.6000000000000116,\n",
      "        -5.399999999999975, -1.499999999999997, -5.999999999999983, -19.500000000000032,\n",
      "        -10.79999999999999, -5.999999999999988, 0.5999999999999598, -18.00000000000002,\n",
      "        -9.299999999999985, -34.500000000000064, -10.199999999999974, -4.199999999999989,\n",
      "        -13.49999999999999, -2.9999999999999947, 1.5000000000000138, -20.700000000000014,\n",
      "        -27.000000000000007, -8.999999999999984, 7.500000000000005, -10.499999999999977,\n",
      "        -12.899999999999977, -29.400000000000027, -16.799999999999976, 18.299999999999986,\n",
      "        5.100000000000012, -5.699999999999985, 3.6000000000000156, -34.50000000000006,\n",
      "        -25.500000000000014, -19.5, -11.999999999999973, -15.600000000000001, -21.000000000000007]\n",
      "      policy_policy1_reward: [22.0, -8.0, 2.0, 5.5, -0.5, -8.0, 12.5, 3.5, 8.5, 4.0,\n",
      "        -9.5, -3.0, 4.0, 9.5, -8.0, -1.5, -24.5, -3.5, -3.0, -3.5, 7.0, 11.5, -14.0,\n",
      "        -17.0, 1.0, 17.5, -0.5, -4.0, -20.5, -9.0, 25.0, 14.0, 1.0, 12.5, -24.5, -15.5,\n",
      "        -9.5, -2.0, -10.0, -11.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -6.699999999999982, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -6.6999999999999815, -1.2000000000000015,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.69999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999984, -8.89999999999998, -7.799999999999981, -6.699999999999986,\n",
      "        -8.899999999999986, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -1.2000000000000015\n",
      "    policy_reward_mean:\n",
      "      policy1: -1.2375\n",
      "      policy2: -8.872499999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -24.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07974627732455924\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021955008150428213\n",
      "      mean_inference_ms: 2.9168686131422774\n",
      "      mean_raw_obs_processing_ms: 0.4206118479993039\n",
      "  time_since_restore: 41.3987672328949\n",
      "  time_this_iter_s: 41.3987672328949\n",
      "  time_total_s: 41.3987672328949\n",
      "  timers:\n",
      "    learn_throughput: 145.284\n",
      "    learn_time_ms: 27532.187\n",
      "    synch_weights_time_ms: 2.991\n",
      "    training_iteration_time_ms: 41383.807\n",
      "  timestamp: 1660499805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 4205e_00002\n",
      "  warmup_time: 9.795740365982056\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=13928)\u001b[0m 2022-08-14 19:56:48,737\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:50 (running for 00:01:32.57)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=-10.11 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2A90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5160>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC7554EC10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5490>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC7554EC10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |   -10.11 |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:56:55 (running for 00:01:37.63)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=-10.11 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2A90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5160>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC7554EC10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5490>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC7554EC10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |   -10.11 |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:00 (running for 00:01:42.69)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=-10.11 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2A90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5160>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC7554EC10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5490>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC7554EC10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |   -11.46 |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      1 |          19.3982 | 3000 |   -10.61 |               -1.6     |               -9.01    |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |   -10.11 |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.1\n",
      "  episode_reward_mean: -7.154999999999993\n",
      "  episode_reward_min: -33.00000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3203816413879395\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021236134693026543\n",
      "          model: {}\n",
      "          policy_loss: -0.05920632556080818\n",
      "          total_loss: 6.600078582763672\n",
      "          vf_explained_var: -0.02729797177016735\n",
      "          vf_loss: 6.6550374031066895\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3221055269241333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020929209887981415\n",
      "          model: {}\n",
      "          policy_loss: -0.0640004575252533\n",
      "          total_loss: 1.9499651193618774\n",
      "          vf_explained_var: 0.392106294631958\n",
      "          vf_loss: 2.009779930114746\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 28.587654320987653\n",
      "    ram_util_percent: 71.64691358024693\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 0.9999999999999934\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.7083333333333333\n",
      "    policy2: -8.863333333333316\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07169706693793525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0209350419550493\n",
      "    mean_inference_ms: 3.9295532159449333\n",
      "    mean_raw_obs_processing_ms: 0.4325450700867927\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 20.1\n",
      "    episode_reward_mean: -7.154999999999993\n",
      "    episode_reward_min: -33.00000000000003\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [19.499999999999904, 1.6181500583911657e-14, -15.899999999999988,\n",
      "        -10.499999999999979, -21.0, -16.499999999999993, -11.399999999999988, -22.500000000000007,\n",
      "        -14.999999999999979, -10.49999999999998, -33.00000000000003, -28.50000000000003,\n",
      "        -10.499999999999975, 8.10000000000002, -31.500000000000043, -30.000000000000025,\n",
      "        -13.500000000000023, -10.499999999999979, 1.5000000000000253, -22.500000000000043,\n",
      "        1.5000000000000204, -24.000000000000036, -14.399999999999993, -10.499999999999984,\n",
      "        -10.49999999999998, -0.8999999999999897, -2.999999999999971, 14.700000000000026,\n",
      "        3.0, -6.938893903907228e-16, -11.999999999999984, 1.0852430065710905e-14, -5.099999999999987,\n",
      "        -24.00000000000001, -5.999999999999975, -6.899999999999984, 13.800000000000026,\n",
      "        7.500000000000007, 8.400000000000023, -25.5, -22.500000000000007, -9.899999999999972,\n",
      "        -8.999999999999993, -12.299999999999985, 9.600000000000009, 7.200000000000012,\n",
      "        3.6000000000000285, 6.300000000000011, 20.1, -8.999999999999984, -2.999999999999987,\n",
      "        -9.89999999999998, -10.49999999999999, -5.999999999999977, 1.662558979376172e-14,\n",
      "        -10.499999999999991, -5.699999999999992, -13.499999999999979, 10.80000000000003,\n",
      "        2.999999999999999]\n",
      "      policy_policy1_reward: [29.5, 10.0, -7.0, -0.5, -11.0, -17.5, -2.5, -12.5, -5.0,\n",
      "        -0.5, -23.0, -18.5, -0.5, 11.5, -21.5, -20.0, -3.5, -0.5, 11.5, -12.5, 11.5,\n",
      "        -19.5, -5.5, -0.5, -0.5, 8.0, 7.0, 22.5, 13.0, 10.0, -2.0, 10.0, 0.5, -14.0,\n",
      "        4.0, 2.0, 20.5, 17.5, 14.0, -15.5, -12.5, -1.0, 1.0, -4.5, 18.5, 15.0, 12.5,\n",
      "        7.5, 29.0, 1.0, 7.0, -1.0, -0.5, 4.0, 10.0, -0.5, 1.0, -3.5, 17.5, 13.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, 0.9999999999999934, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.400000000000007, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -4.499999999999984, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999994, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999895, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -7.799999999999981, -8.899999999999986,\n",
      "        -1.200000000000005, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 0.9999999999999934\n",
      "    policy_reward_mean:\n",
      "      policy1: 1.7083333333333333\n",
      "      policy2: -8.863333333333316\n",
      "    policy_reward_min:\n",
      "      policy1: -23.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07169706693793525\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0209350419550493\n",
      "      mean_inference_ms: 3.9295532159449333\n",
      "      mean_raw_obs_processing_ms: 0.4325450700867927\n",
      "  time_since_restore: 65.54696917533875\n",
      "  time_this_iter_s: 46.148810386657715\n",
      "  time_total_s: 65.54696917533875\n",
      "  timers:\n",
      "    learn_throughput: 182.263\n",
      "    learn_time_ms: 16459.705\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 32769.496\n",
      "  timestamp: 1660499825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:05 (running for 00:01:47.73)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00001 with episode_reward_mean=-7.154999999999993 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C01B50>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C01D30>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C01F70>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C09130>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      1 |          17.9164 | 3000 |  -11.46  |               -2.81667 |               -8.64333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 |   -7.155 |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |  -10.11  |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-57-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.60000000000003\n",
      "  episode_reward_mean: -6.504999999999994\n",
      "  episode_reward_min: -36.90000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3544955253601074\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007244151085615158\n",
      "          model: {}\n",
      "          policy_loss: -0.026293378323316574\n",
      "          total_loss: 7.021142482757568\n",
      "          vf_explained_var: -0.02595503069460392\n",
      "          vf_loss: 7.045988082885742\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3540202379226685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009059828706085682\n",
      "          model: {}\n",
      "          policy_loss: -0.03320569545030594\n",
      "          total_loss: 1.9618971347808838\n",
      "          vf_explained_var: 0.3614092469215393\n",
      "          vf_loss: 1.9941967725753784\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 26.483809523809526\n",
      "    ram_util_percent: 69.17714285714287\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 24.5\n",
      "    policy2: -1.200000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.3583333333333334\n",
      "    policy2: -8.863333333333316\n",
      "  policy_reward_min:\n",
      "    policy1: -28.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06945246604270472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020114321403052516\n",
      "    mean_inference_ms: 3.6084279078468757\n",
      "    mean_raw_obs_processing_ms: 0.42038675545481285\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.60000000000003\n",
      "    episode_reward_mean: -6.504999999999994\n",
      "    episode_reward_min: -36.90000000000005\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.6000000000000023, -36.90000000000005, -16.79999999999999, 4.500000000000018,\n",
      "        -22.200000000000017, -22.500000000000018, -5.399999999999979, -28.20000000000003,\n",
      "        -16.499999999999993, -1.7999999999999763, -26.100000000000026, -2.9999999999999876,\n",
      "        9.300000000000022, 4.8000000000000025, -2.699999999999988, -17.999999999999993,\n",
      "        -24.900000000000002, -1.7999999999999905, -36.00000000000005, 9.520162436160717e-15,\n",
      "        -8.999999999999986, 9.076073226310655e-15, -8.399999999999975, -10.499999999999977,\n",
      "        6.000000000000001, -15.000000000000039, -18.299999999999997, -4.4999999999999805,\n",
      "        -10.499999999999982, -33.00000000000004, 6.000000000000034, -4.499999999999979,\n",
      "        3.0000000000000173, -5.999999999999975, 4.500000000000009, 1.500000000000017,\n",
      "        -13.199999999999983, 8.399999999999984, -18.899999999999984, 10.500000000000028,\n",
      "        -7.499999999999984, 3.5999999999999654, -4.799999999999985, 4.500000000000015,\n",
      "        -15.899999999999986, 6.9000000000000234, 3.000000000000018, -14.999999999999982,\n",
      "        8.100000000000026, -4.7999999999999865, 15.60000000000003, -5.099999999999988,\n",
      "        6.000000000000012, 3.60000000000001, 4.500000000000023, -7.500000000000002,\n",
      "        -1.499999999999988, 6.411537967210279e-15, -19.499999999999993, -11.999999999999979]\n",
      "      policy_policy1_reward: [12.5, -28.0, -9.0, 14.5, -15.5, -12.5, 3.5, -21.5, -6.5,\n",
      "        6.0, -20.5, 7.0, 16.0, 6.0, 4.0, -8.0, -16.0, 6.0, -26.0, 10.0, 1.0, 10.0, 0.5,\n",
      "        -0.5, 16.0, -5.0, -10.5, 5.5, -0.5, -23.0, 16.0, 5.5, 13.0, 4.0, 14.5, 11.5,\n",
      "        -6.5, 14.0, -10.0, 20.5, 2.5, 12.5, 3.0, 14.5, -7.0, 12.5, 13.0, -5.0, 17.0,\n",
      "        3.0, 24.5, 0.5, 16.0, 12.5, 14.5, 2.5, 8.5, 10.0, -9.5, -2.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -8.899999999999986,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999982, -5.599999999999986,\n",
      "        -9.99999999999998, -6.69999999999999, -1.200000000000004, -6.699999999999995,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -5.599999999999993, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -8.899999999999983,\n",
      "        -5.5999999999999845, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 24.5\n",
      "      policy2: -1.200000000000004\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.3583333333333334\n",
      "      policy2: -8.863333333333316\n",
      "    policy_reward_min:\n",
      "      policy1: -28.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06945246604270472\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020114321403052516\n",
      "      mean_inference_ms: 3.6084279078468757\n",
      "      mean_raw_obs_processing_ms: 0.42038675545481285\n",
      "  time_since_restore: 64.21385526657104\n",
      "  time_this_iter_s: 46.29741096496582\n",
      "  time_total_s: 64.21385526657104\n",
      "  timers:\n",
      "    learn_throughput: 182.652\n",
      "    learn_time_ms: 16424.72\n",
      "    synch_weights_time_ms: 2.991\n",
      "    training_iteration_time_ms: 32102.938\n",
      "  timestamp: 1660499826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:11 (running for 00:01:53.04)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-6.504999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5C40>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B55610>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B557C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2D90>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      2 |          64.2139 | 6000 |   -6.505 |                2.35833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 |   -7.155 |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |  -10.11  |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:16 (running for 00:01:58.10)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-6.504999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5C40>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B55610>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B557C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2D90>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      2 |          64.2139 | 6000 |   -6.505 |                2.35833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 |   -7.155 |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |  -10.11  |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |        |                  |      |          |                        |                        |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-57-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.599999999999987\n",
      "  episode_reward_mean: -9.239999999999995\n",
      "  episode_reward_min: -35.40000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: d9b180bb1fb4403ba3d13246b5f1de5e\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3678340911865234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018925732001662254\n",
      "          model: {}\n",
      "          policy_loss: -0.04015488177537918\n",
      "          total_loss: 6.8557209968566895\n",
      "          vf_explained_var: 0.018373841419816017\n",
      "          vf_loss: 6.892090797424316\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3684223890304565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017923779785633087\n",
      "          model: {}\n",
      "          policy_loss: -0.04283098503947258\n",
      "          total_loss: 2.781219720840454\n",
      "          vf_explained_var: 0.3167298436164856\n",
      "          vf_loss: 2.8204658031463623\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.477011494252874\n",
      "    ram_util_percent: 72.63908045977011\n",
      "  pid: 13928\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -2.300000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: -0.3125\n",
      "    policy2: -8.92749999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08000668452042874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02416823095632952\n",
      "    mean_inference_ms: 6.6981245892788595\n",
      "    mean_raw_obs_processing_ms: 0.4524096522561255\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.599999999999987\n",
      "    episode_reward_mean: -9.239999999999995\n",
      "    episode_reward_min: -35.40000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [7.5000000000000195, -35.40000000000006, -11.999999999999986,\n",
      "        -11.399999999999972, -2.0999999999999948, -16.500000000000007, -4.199999999999992,\n",
      "        -27.000000000000078, -28.20000000000001, -9.299999999999974, -8.999999999999977,\n",
      "        -2.3999999999999866, 9.000000000000014, -2.9999999999999867, -22.50000000000003,\n",
      "        -8.999999999999975, 3.0000000000000178, -13.499999999999979, -5.999999999999975,\n",
      "        -4.199999999999989, 3.0000000000000187, 2.4000000000000274, -14.999999999999984,\n",
      "        -24.000000000000007, -33.000000000000036, -0.2999999999999906, -28.50000000000002,\n",
      "        3.0000000000000098, -13.499999999999979, -15.299999999999988, 18.599999999999987,\n",
      "        -9.899999999999977, 6.300515664747763e-15, -28.80000000000002, 3.0000000000000044,\n",
      "        -13.500000000000016, -14.99999999999998, 4.500000000000025, -10.199999999999989,\n",
      "        -0.8999999999999808]\n",
      "      policy_policy1_reward: [17.5, -26.5, -2.0, -2.5, 3.5, -6.5, 2.5, -17.0, -21.5,\n",
      "        -1.5, 1.0, 6.5, 19.0, 7.0, -12.5, 1.0, 13.0, -3.5, 4.0, 2.5, 13.0, 8.0, -5.0,\n",
      "        -14.0, -23.0, 2.0, -18.5, 13.0, -3.5, -7.5, 27.5, -1.0, 10.0, -21.0, 13.0, -3.5,\n",
      "        -5.0, 14.5, -3.5, 8.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998, -6.6999999999999815, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999982, -9.99999999999998, -5.599999999999993, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -2.300000000000002, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -8.89999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999985,\n",
      "        -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: -2.300000000000002\n",
      "    policy_reward_mean:\n",
      "      policy1: -0.3125\n",
      "      policy2: -8.92749999999998\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08000668452042874\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02416823095632952\n",
      "      mean_inference_ms: 6.6981245892788595\n",
      "      mean_raw_obs_processing_ms: 0.4524096522561255\n",
      "  time_since_restore: 61.49566197395325\n",
      "  time_this_iter_s: 61.49566197395325\n",
      "  time_total_s: 61.49566197395325\n",
      "  timers:\n",
      "    learn_throughput: 123.584\n",
      "    learn_time_ms: 32366.761\n",
      "    synch_weights_time_ms: 3.989\n",
      "    training_iteration_time_ms: 61489.677\n",
      "  timestamp: 1660499841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 4205e_00003\n",
      "  warmup_time: 9.807717561721802\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:26 (running for 00:02:08.08)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-6.504999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5C40>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B55610>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B557C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2D90>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      2 |          64.2139 | 6000 |   -6.505 |                2.35833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 |   -7.155 |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |  -10.11  |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 |   -9.24  |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:31 (running for 00:02:13.14)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-6.504999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5C40>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B55610>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B557C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2D90>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      2 |          64.2139 | 6000 |   -6.505 |                2.35833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 |   -7.155 |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |  -10.11  |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 |   -9.24  |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:36 (running for 00:02:18.20)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-6.504999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5C40>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B55610>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B557C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2D90>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      2 |          64.2139 | 6000 |   -6.505 |                2.35833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 |   -7.155 |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |  -10.11  |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 |   -9.24  |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:41 (running for 00:02:23.25)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-6.504999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BB5C40>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B55610>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75B557C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75BA2D90>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75AE2D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      2 |          64.2139 | 6000 |   -6.505 |                2.35833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 |   -7.155 |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      1 |          41.3988 | 4000 |  -10.11  |               -1.2375  |               -8.8725  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 |   -9.24  |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.800000000000015\n",
      "  episode_reward_mean: -5.583749999999994\n",
      "  episode_reward_min: -34.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 847f41dbc3e943d8b61110c9b40c9844\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3382643461227417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011424290016293526\n",
      "          model: {}\n",
      "          policy_loss: -0.03439690172672272\n",
      "          total_loss: 6.822493553161621\n",
      "          vf_explained_var: 0.0025441711768507957\n",
      "          vf_loss: 6.854605197906494\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.341667890548706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011322738602757454\n",
      "          model: {}\n",
      "          policy_loss: -0.036314528435468674\n",
      "          total_loss: 2.2905023097991943\n",
      "          vf_explained_var: 0.34968698024749756\n",
      "          vf_loss: 2.324552297592163\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.204705882352943\n",
      "    ram_util_percent: 72.70235294117647\n",
      "  pid: 14084\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -1.2000000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.06875\n",
      "    policy2: -8.652499999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07827086651702228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022948598969289545\n",
      "    mean_inference_ms: 3.851101508518834\n",
      "    mean_raw_obs_processing_ms: 0.4229191113025407\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.800000000000015\n",
      "    episode_reward_mean: -5.583749999999994\n",
      "    episode_reward_min: -34.500000000000064\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [12.000000000000012, -14.699999999999992, -6.899999999999988,\n",
      "        -4.499999999999991, -10.500000000000039, -17.999999999999996, 3.6000000000000116,\n",
      "        -5.399999999999975, -1.499999999999997, -5.999999999999983, -19.500000000000032,\n",
      "        -10.79999999999999, -5.999999999999988, 0.5999999999999598, -18.00000000000002,\n",
      "        -9.299999999999985, -34.500000000000064, -10.199999999999974, -4.199999999999989,\n",
      "        -13.49999999999999, -2.9999999999999947, 1.5000000000000138, -20.700000000000014,\n",
      "        -27.000000000000007, -8.999999999999984, 7.500000000000005, -10.499999999999977,\n",
      "        -12.899999999999977, -29.400000000000027, -16.799999999999976, 18.299999999999986,\n",
      "        5.100000000000012, -5.699999999999985, 3.6000000000000156, -34.50000000000006,\n",
      "        -25.500000000000014, -19.5, -11.999999999999973, -15.600000000000001, -21.000000000000007,\n",
      "        3.600000000000031, -3.900000000000004, 2.5951463200613034e-14, -10.49999999999998,\n",
      "        3.0000000000000258, 6.900000000000015, 1.662558979376172e-14, -4.499999999999993,\n",
      "        17.99999999999999, 0.6000000000000184, -14.699999999999992, -7.499999999999977,\n",
      "        4.5000000000000036, 17.99999999999994, -6.899999999999993, -1.199999999999994,\n",
      "        3.0000000000000115, 8.70000000000002, 13.499999999999982, -10.79999999999999,\n",
      "        -7.499999999999998, -30.000000000000018, -5.399999999999986, -17.999999999999996,\n",
      "        5.700000000000031, -1.1999999999999793, 22.800000000000015, 5.100000000000026,\n",
      "        -4.799999999999992, 7.500000000000011, -16.49999999999998, 12.300000000000013,\n",
      "        -16.799999999999997, 9.600000000000026, -1.4999999999999845, 3.3000000000000074,\n",
      "        -2.699999999999974, 4.191091917959966e-15, -2.9999999999999765, -21.0]\n",
      "      policy_policy1_reward: [22.0, -8.0, 2.0, 5.5, -0.5, -8.0, 12.5, 3.5, 8.5, 4.0,\n",
      "        -9.5, -3.0, 4.0, 9.5, -8.0, -1.5, -24.5, -3.5, -3.0, -3.5, 7.0, 11.5, -14.0,\n",
      "        -17.0, 1.0, 17.5, -0.5, -4.0, -20.5, -9.0, 25.0, 14.0, 1.0, 12.5, -24.5, -15.5,\n",
      "        -9.5, -2.0, -10.0, -11.0, 7.0, 5.0, 10.0, -0.5, 13.0, 12.5, 10.0, 5.5, 28.0,\n",
      "        4.0, -8.0, 2.5, 14.5, 28.0, 2.0, 5.5, 13.0, 16.5, 23.5, -3.0, 2.5, -20.0, 3.5,\n",
      "        -8.0, 13.5, 5.5, 29.5, 14.0, 3.0, 17.5, -6.5, 19.0, -9.0, 18.5, 8.5, 10.0, 4.0,\n",
      "        10.0, 1.5, -11.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -6.699999999999982, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -6.6999999999999815, -1.2000000000000015,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.69999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999984, -8.89999999999998, -7.799999999999981, -6.699999999999986,\n",
      "        -8.899999999999986, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -3.3999999999999826, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.4000000000000044, -6.699999999999991,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999984, -6.6999999999999815, -6.69999999999999,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -7.799999999999981, -8.899999999999984, -9.99999999999998,\n",
      "        -6.6999999999999815, -6.6999999999999815, -9.99999999999998, -4.5, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -1.2000000000000015\n",
      "    policy_reward_mean:\n",
      "      policy1: 3.06875\n",
      "      policy2: -8.652499999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -24.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07827086651702228\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022948598969289545\n",
      "      mean_inference_ms: 3.851101508518834\n",
      "      mean_raw_obs_processing_ms: 0.4229191113025407\n",
      "  time_since_restore: 101.89960289001465\n",
      "  time_this_iter_s: 60.50083565711975\n",
      "  time_total_s: 101.89960289001465\n",
      "  timers:\n",
      "    learn_throughput: 135.063\n",
      "    learn_time_ms: 29615.851\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 50939.33\n",
      "  timestamp: 1660499866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 4205e_00002\n",
      "  warmup_time: 9.795740365982056\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:51 (running for 00:02:33.13)\n",
      "Memory usage on this node: 22.9/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=-5.583749999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C603D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C60580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06280>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C607C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C60A00>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06280>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      2 |          64.2139 | 6000 | -6.505   |                2.35833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      2 |          65.547  | 6000 | -7.155   |                1.70833 |               -8.86333 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |         101.9    | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 | -9.24    |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.1\n",
      "  episode_reward_mean: -5.783333333333324\n",
      "  episode_reward_min: -33.00000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2691441774368286\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020075712352991104\n",
      "          model: {}\n",
      "          policy_loss: -0.0610005222260952\n",
      "          total_loss: 6.727275371551514\n",
      "          vf_explained_var: -0.040345557034015656\n",
      "          vf_loss: 6.782252788543701\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2591838836669922\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020948896184563637\n",
      "          model: {}\n",
      "          policy_loss: -0.06210772320628166\n",
      "          total_loss: 2.443936586380005\n",
      "          vf_explained_var: 0.1548536717891693\n",
      "          vf_loss: 2.4997596740722656\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.80153846153846\n",
      "    ram_util_percent: 72.72307692307693\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 0.9999999999999943\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.811111111111111\n",
      "    policy2: -8.594444444444427\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07213579340226343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021564271876946314\n",
      "    mean_inference_ms: 4.446718584221577\n",
      "    mean_raw_obs_processing_ms: 0.43369977176267543\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 20.1\n",
      "    episode_reward_mean: -5.783333333333324\n",
      "    episode_reward_min: -33.00000000000003\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [19.499999999999904, 1.6181500583911657e-14, -15.899999999999988,\n",
      "        -10.499999999999979, -21.0, -16.499999999999993, -11.399999999999988, -22.500000000000007,\n",
      "        -14.999999999999979, -10.49999999999998, -33.00000000000003, -28.50000000000003,\n",
      "        -10.499999999999975, 8.10000000000002, -31.500000000000043, -30.000000000000025,\n",
      "        -13.500000000000023, -10.499999999999979, 1.5000000000000253, -22.500000000000043,\n",
      "        1.5000000000000204, -24.000000000000036, -14.399999999999993, -10.499999999999984,\n",
      "        -10.49999999999998, -0.8999999999999897, -2.999999999999971, 14.700000000000026,\n",
      "        3.0, -6.938893903907228e-16, -11.999999999999984, 1.0852430065710905e-14, -5.099999999999987,\n",
      "        -24.00000000000001, -5.999999999999975, -6.899999999999984, 13.800000000000026,\n",
      "        7.500000000000007, 8.400000000000023, -25.5, -22.500000000000007, -9.899999999999972,\n",
      "        -8.999999999999993, -12.299999999999985, 9.600000000000009, 7.200000000000012,\n",
      "        3.6000000000000285, 6.300000000000011, 20.1, -8.999999999999984, -2.999999999999987,\n",
      "        -9.89999999999998, -10.49999999999999, -5.999999999999977, 1.662558979376172e-14,\n",
      "        -10.499999999999991, -5.699999999999992, -13.499999999999979, 10.80000000000003,\n",
      "        2.999999999999999, -16.49999999999998, -1.4999999999999734, -7.499999999999989,\n",
      "        -8.699999999999996, 13.500000000000016, 5.099999999999952, -14.399999999999975,\n",
      "        4.500000000000024, -18.000000000000007, -14.099999999999985, -3.299999999999986,\n",
      "        -3.2999999999999767, -6.899999999999974, 6.000000000000025, 9.600000000000025,\n",
      "        9.90000000000001, 12.000000000000028, -17.999999999999996, -0.2999999999999742,\n",
      "        -7.19999999999998, 3.2999999999999785, -6.299999999999992, -2.6999999999999997,\n",
      "        -13.199999999999983, 9.520162436160717e-15, -7.499999999999984, -14.399999999999979,\n",
      "        8.700000000000026, 8.999999999999998, -8.999999999999986]\n",
      "      policy_policy1_reward: [29.5, 10.0, -7.0, -0.5, -11.0, -17.5, -2.5, -12.5, -5.0,\n",
      "        -0.5, -23.0, -18.5, -0.5, 11.5, -21.5, -20.0, -3.5, -0.5, 11.5, -12.5, 11.5,\n",
      "        -19.5, -5.5, -0.5, -0.5, 8.0, 7.0, 22.5, 13.0, 10.0, -2.0, 10.0, 0.5, -14.0,\n",
      "        4.0, 2.0, 20.5, 17.5, 14.0, -15.5, -12.5, -1.0, 1.0, -4.5, 18.5, 15.0, 12.5,\n",
      "        7.5, 29.0, 1.0, 7.0, -1.0, -0.5, 4.0, 10.0, -0.5, 1.0, -3.5, 17.5, 13.0, -6.5,\n",
      "        8.5, 2.5, -2.0, 23.5, 14.0, -5.5, 14.5, -8.0, -8.5, 4.5, 4.5, 2.0, 16.0, 18.5,\n",
      "        15.5, 22.0, -8.0, 7.5, -0.5, 4.5, 1.5, 4.0, -6.5, 10.0, 2.5, -5.5, 16.5, 19.0,\n",
      "        -10.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, 0.9999999999999934, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.400000000000007, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -4.499999999999984, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999994, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999895, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -7.799999999999981, -8.899999999999986,\n",
      "        -1.200000000000005, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999895, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999982, -7.7999999999999865,\n",
      "        -7.799999999999989, -8.89999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -5.599999999999989, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -6.699999999999994, -1.1999999999999975, -7.799999999999981, -6.699999999999995,\n",
      "        -6.699999999999991, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -7.799999999999984, -9.99999999999998, 0.9999999999999943]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 0.9999999999999943\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.811111111111111\n",
      "      policy2: -8.594444444444427\n",
      "    policy_reward_min:\n",
      "      policy1: -23.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07213579340226343\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021564271876946314\n",
      "      mean_inference_ms: 4.446718584221577\n",
      "      mean_raw_obs_processing_ms: 0.43369977176267543\n",
      "  time_since_restore: 111.10931515693665\n",
      "  time_this_iter_s: 45.5623459815979\n",
      "  time_total_s: 111.10931515693665\n",
      "  timers:\n",
      "    learn_throughput: 158.479\n",
      "    learn_time_ms: 18929.972\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 37031.784\n",
      "  timestamp: 1660499871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.60000000000003\n",
      "  episode_reward_mean: -4.586666666666657\n",
      "  episode_reward_min: -36.90000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3381433486938477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009494197554886341\n",
      "          model: {}\n",
      "          policy_loss: -0.03601009026169777\n",
      "          total_loss: 6.7824788093566895\n",
      "          vf_explained_var: 0.0051774135790765285\n",
      "          vf_loss: 6.816590785980225\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3152194023132324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014098013751208782\n",
      "          model: {}\n",
      "          policy_loss: -0.04806354269385338\n",
      "          total_loss: 2.5538299083709717\n",
      "          vf_explained_var: 0.29897838830947876\n",
      "          vf_loss: 2.6004838943481445\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.895384615384618\n",
      "    ram_util_percent: 72.71384615384615\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 24.5\n",
      "    policy2: -0.10000000000000331\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.8\n",
      "    policy2: -8.386666666666649\n",
      "  policy_reward_min:\n",
      "    policy1: -28.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07041667278700672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02009309060920125\n",
      "    mean_inference_ms: 4.18804135029528\n",
      "    mean_raw_obs_processing_ms: 0.4234913367218244\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.60000000000003\n",
      "    episode_reward_mean: -4.586666666666657\n",
      "    episode_reward_min: -36.90000000000005\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.6000000000000023, -36.90000000000005, -16.79999999999999, 4.500000000000018,\n",
      "        -22.200000000000017, -22.500000000000018, -5.399999999999979, -28.20000000000003,\n",
      "        -16.499999999999993, -1.7999999999999763, -26.100000000000026, -2.9999999999999876,\n",
      "        9.300000000000022, 4.8000000000000025, -2.699999999999988, -17.999999999999993,\n",
      "        -24.900000000000002, -1.7999999999999905, -36.00000000000005, 9.520162436160717e-15,\n",
      "        -8.999999999999986, 9.076073226310655e-15, -8.399999999999975, -10.499999999999977,\n",
      "        6.000000000000001, -15.000000000000039, -18.299999999999997, -4.4999999999999805,\n",
      "        -10.499999999999982, -33.00000000000004, 6.000000000000034, -4.499999999999979,\n",
      "        3.0000000000000173, -5.999999999999975, 4.500000000000009, 1.500000000000017,\n",
      "        -13.199999999999983, 8.399999999999984, -18.899999999999984, 10.500000000000028,\n",
      "        -7.499999999999984, 3.5999999999999654, -4.799999999999985, 4.500000000000015,\n",
      "        -15.899999999999986, 6.9000000000000234, 3.000000000000018, -14.999999999999982,\n",
      "        8.100000000000026, -4.7999999999999865, 15.60000000000003, -5.099999999999988,\n",
      "        6.000000000000012, 3.60000000000001, 4.500000000000023, -7.500000000000002,\n",
      "        -1.499999999999988, 6.411537967210279e-15, -19.499999999999993, -11.999999999999979,\n",
      "        -7.499999999999982, 3.000000000000022, -3.899999999999976, -13.499999999999986,\n",
      "        -5.999999999999982, 8.699999999999969, -18.899999999999984, 0.9000000000000156,\n",
      "        -0.2999999999999917, -10.49999999999998, -3.8999999999999893, -9.299999999999976,\n",
      "        -4.799999999999978, -1.799999999999979, -3.29999999999998, 3.0000000000000213,\n",
      "        -3.8999999999999866, 3.900000000000014, 13.200000000000012, 5.9674487573602164e-15,\n",
      "        2.100000000000023, 5.100000000000026, 10.499999999999947, 1.4999999999999782,\n",
      "        9.600000000000033, 1.500000000000027, -4.4999999999999805, -4.799999999999974,\n",
      "        14.699999999999998, -3.299999999999973]\n",
      "      policy_policy1_reward: [12.5, -28.0, -9.0, 14.5, -15.5, -12.5, 3.5, -21.5, -6.5,\n",
      "        6.0, -20.5, 7.0, 16.0, 6.0, 4.0, -8.0, -16.0, 6.0, -26.0, 10.0, 1.0, 10.0, 0.5,\n",
      "        -0.5, 16.0, -5.0, -10.5, 5.5, -0.5, -23.0, 16.0, 5.5, 13.0, 4.0, 14.5, 11.5,\n",
      "        -6.5, 14.0, -10.0, 20.5, 2.5, 12.5, 3.0, 14.5, -7.0, 12.5, 13.0, -5.0, 17.0,\n",
      "        3.0, 24.5, 0.5, 16.0, 12.5, 14.5, 2.5, 8.5, 10.0, -9.5, -2.0, 2.5, 13.0, 5.0,\n",
      "        -3.5, -1.5, 11.0, -10.0, 1.0, 7.5, -0.5, 5.0, -1.5, 3.0, 0.5, 4.5, 7.5, 5.0,\n",
      "        9.5, 21.0, 10.0, 5.5, 14.0, 15.0, 11.5, 18.5, 11.5, 5.5, 3.0, 22.5, 4.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -8.899999999999986,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999982, -5.599999999999986,\n",
      "        -9.99999999999998, -6.69999999999999, -1.200000000000004, -6.699999999999995,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -5.599999999999993, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -8.899999999999983,\n",
      "        -5.5999999999999845, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -4.499999999999987, -2.2999999999999963, -8.89999999999998,\n",
      "        -0.10000000000000331, -7.799999999999981, -9.99999999999998, -8.899999999999986,\n",
      "        -7.79999999999999, -7.799999999999981, -2.3000000000000003, -7.79999999999999,\n",
      "        -4.499999999999995, -8.89999999999998, -5.6, -7.799999999999985, -9.99999999999998,\n",
      "        -3.4, -8.89999999999998, -4.49999999999999, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -7.799999999999983]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 24.5\n",
      "      policy2: -0.10000000000000331\n",
      "    policy_reward_mean:\n",
      "      policy1: 3.8\n",
      "      policy2: -8.386666666666649\n",
      "    policy_reward_min:\n",
      "      policy1: -28.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07041667278700672\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02009309060920125\n",
      "      mean_inference_ms: 4.18804135029528\n",
      "      mean_raw_obs_processing_ms: 0.4234913367218244\n",
      "  time_since_restore: 110.3726019859314\n",
      "  time_this_iter_s: 46.15874671936035\n",
      "  time_total_s: 110.3726019859314\n",
      "  timers:\n",
      "    learn_throughput: 156.933\n",
      "    learn_time_ms: 19116.419\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 36785.881\n",
      "  timestamp: 1660499872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:57:57 (running for 00:02:39.19)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-4.586666666666657 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C78D90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C39700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F190>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      3 |         110.373  | 9000 | -4.58667 |                3.8     |               -8.38667 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      3 |         111.109  | 9000 | -5.78333 |                2.81111 |               -8.59444 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |         101.9    | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 | -9.24    |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:02 (running for 00:02:44.30)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-4.586666666666657 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C78D90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C39700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F190>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      3 |         110.373  | 9000 | -4.58667 |                3.8     |               -8.38667 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      3 |         111.109  | 9000 | -5.78333 |                2.81111 |               -8.59444 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |         101.9    | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 | -9.24    |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:07 (running for 00:02:49.35)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-4.586666666666657 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C78D90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C39700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F190>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      3 |         110.373  | 9000 | -4.58667 |                3.8     |               -8.38667 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      3 |         111.109  | 9000 | -5.78333 |                2.81111 |               -8.59444 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |         101.9    | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 | -9.24    |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:12 (running for 00:02:54.42)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-4.586666666666657 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C78D90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C39700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F190>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      3 |         110.373  | 9000 | -4.58667 |                3.8     |               -8.38667 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      3 |         111.109  | 9000 | -5.78333 |                2.81111 |               -8.59444 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |         101.9    | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 | -9.24    |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:17 (running for 00:02:59.49)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-4.586666666666657 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C78D90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C39700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F190>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      3 |         110.373  | 9000 | -4.58667 |                3.8     |               -8.38667 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      3 |         111.109  | 9000 | -5.78333 |                2.81111 |               -8.59444 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |         101.9    | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      1 |          61.4957 | 4000 | -9.24    |               -0.3125  |               -8.9275  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-58-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.599999999999987\n",
      "  episode_reward_mean: -7.683749999999992\n",
      "  episode_reward_min: -35.40000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: d9b180bb1fb4403ba3d13246b5f1de5e\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.323320984840393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02075326070189476\n",
      "          model: {}\n",
      "          policy_loss: -0.0528404675424099\n",
      "          total_loss: 7.1624298095703125\n",
      "          vf_explained_var: 0.04859062656760216\n",
      "          vf_loss: 7.211119651794434\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.318500280380249\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02040194347500801\n",
      "          model: {}\n",
      "          policy_loss: -0.053014155477285385\n",
      "          total_loss: 2.097438335418701\n",
      "          vf_explained_var: 0.3317217528820038\n",
      "          vf_loss: 2.146372079849243\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.781609195402293\n",
      "    ram_util_percent: 72.03218390804594\n",
      "  pid: 13928\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -2.300000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 0.96875\n",
      "    policy2: -8.652499999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07937681029285867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023994272240223862\n",
      "    mean_inference_ms: 6.711537697470507\n",
      "    mean_raw_obs_processing_ms: 0.4492603955721487\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.599999999999987\n",
      "    episode_reward_mean: -7.683749999999992\n",
      "    episode_reward_min: -35.40000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [7.5000000000000195, -35.40000000000006, -11.999999999999986,\n",
      "        -11.399999999999972, -2.0999999999999948, -16.500000000000007, -4.199999999999992,\n",
      "        -27.000000000000078, -28.20000000000001, -9.299999999999974, -8.999999999999977,\n",
      "        -2.3999999999999866, 9.000000000000014, -2.9999999999999867, -22.50000000000003,\n",
      "        -8.999999999999975, 3.0000000000000178, -13.499999999999979, -5.999999999999975,\n",
      "        -4.199999999999989, 3.0000000000000187, 2.4000000000000274, -14.999999999999984,\n",
      "        -24.000000000000007, -33.000000000000036, -0.2999999999999906, -28.50000000000002,\n",
      "        3.0000000000000098, -13.499999999999979, -15.299999999999988, 18.599999999999987,\n",
      "        -9.899999999999977, 6.300515664747763e-15, -28.80000000000002, 3.0000000000000044,\n",
      "        -13.500000000000016, -14.99999999999998, 4.500000000000025, -10.199999999999989,\n",
      "        -0.8999999999999808, -5.999999999999989, -8.999999999999973, -13.199999999999974,\n",
      "        -13.799999999999976, -14.399999999999983, -25.500000000000004, 10.500000000000028,\n",
      "        -5.399999999999984, -7.5, -5.999999999999988, -16.79999999999998, -12.59999999999998,\n",
      "        -27.000000000000064, 6.600000000000027, 5.400000000000029, -16.499999999999986,\n",
      "        0.6000000000000166, -2.999999999999989, -10.499999999999975, 12.000000000000027,\n",
      "        -2.9999999999999822, -19.200000000000003, -10.800000000000006, 2.6999999999999997,\n",
      "        -21.60000000000001, -3.2999999999999763, -1.1999999999999933, -2.999999999999991,\n",
      "        8.100000000000025, -14.099999999999982, -4.499999999999991, 2.1000000000000214,\n",
      "        -16.799999999999976, -4.499999999999975, 3.0000000000000044, -19.499999999999993,\n",
      "        9.600000000000017, 6.000000000000022, -4.19999999999999, 1.2000000000000224]\n",
      "      policy_policy1_reward: [17.5, -26.5, -2.0, -2.5, 3.5, -6.5, 2.5, -17.0, -21.5,\n",
      "        -1.5, 1.0, 6.5, 19.0, 7.0, -12.5, 1.0, 13.0, -3.5, 4.0, 2.5, 13.0, 8.0, -5.0,\n",
      "        -14.0, -23.0, 2.0, -18.5, 13.0, -3.5, -7.5, 27.5, -1.0, 10.0, -21.0, 13.0, -3.5,\n",
      "        -5.0, 14.5, -3.5, 8.0, 4.0, 1.0, -6.5, -6.0, -5.5, -21.0, 20.5, 3.5, 2.5, 4.0,\n",
      "        -9.0, -7.0, -17.0, 15.5, 11.0, -6.5, 9.5, 1.5, -0.5, 22.0, 7.0, -12.5, -3.0,\n",
      "        10.5, -16.0, 4.5, 5.5, 7.0, 17.0, -8.5, 5.5, 11.0, -9.0, 5.5, 13.0, -9.5, 18.5,\n",
      "        16.0, 2.5, 9.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998, -6.6999999999999815, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999982, -9.99999999999998, -5.599999999999993, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -2.300000000000002, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -8.89999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999985,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999983,\n",
      "        -7.799999999999981, -8.89999999999998, -4.500000000000002, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999983,\n",
      "        -5.599999999999982, -9.99999999999998, -8.89999999999998, -5.5999999999999925,\n",
      "        -9.99999999999998, -8.89999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999986, -7.799999999999984,\n",
      "        -7.799999999999987, -5.599999999999995, -7.799999999999986, -6.699999999999992,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -7.799999999999989]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: -2.300000000000002\n",
      "    policy_reward_mean:\n",
      "      policy1: 0.96875\n",
      "      policy2: -8.652499999999982\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07937681029285867\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023994272240223862\n",
      "      mean_inference_ms: 6.711537697470507\n",
      "      mean_raw_obs_processing_ms: 0.4492603955721487\n",
      "  time_since_restore: 122.65319561958313\n",
      "  time_this_iter_s: 61.15753364562988\n",
      "  time_total_s: 122.65319561958313\n",
      "  timers:\n",
      "    learn_throughput: 124.459\n",
      "    learn_time_ms: 32139.101\n",
      "    synch_weights_time_ms: 3.49\n",
      "    training_iteration_time_ms: 61322.608\n",
      "  timestamp: 1660499902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 4205e_00003\n",
      "  warmup_time: 9.807717561721802\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:27 (running for 00:03:09.30)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-4.586666666666657 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C78D90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C39700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F190>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      3 |          110.373 | 9000 | -4.58667 |                3.8     |               -8.38667 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      3 |          111.109 | 9000 | -5.78333 |                2.81111 |               -8.59444 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |          101.9   | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 | 8000 | -7.68375 |                0.96875 |               -8.6525  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:32 (running for 00:03:14.34)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-4.586666666666657 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C78D90>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C39700>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75C1F190>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      3 |          110.373 | 9000 | -4.58667 |                3.8     |               -8.38667 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      3 |          111.109 | 9000 | -5.78333 |                2.81111 |               -8.59444 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |          101.9   | 8000 | -5.58375 |                3.06875 |               -8.6525  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 | 8000 | -7.68375 |                0.96875 |               -8.6525  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-58-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.1\n",
      "  episode_reward_mean: -2.8169999999999877\n",
      "  episode_reward_min: -25.5\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.235213041305542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017162475734949112\n",
      "          model: {}\n",
      "          policy_loss: -0.05415964499115944\n",
      "          total_loss: 6.747828960418701\n",
      "          vf_explained_var: -0.0015011589275673032\n",
      "          vf_loss: 6.794265270233154\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2485491037368774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016110360622406006\n",
      "          model: {}\n",
      "          policy_loss: -0.056987423449754715\n",
      "          total_loss: 2.1917121410369873\n",
      "          vf_explained_var: 0.29893386363983154\n",
      "          vf_loss: 2.2414498329162598\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.55625\n",
      "    ram_util_percent: 71.3578125\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 29.0\n",
      "    policy2: 0.9999999999999943\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.555\n",
      "    policy2: -8.371999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -19.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07238029161098546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022791529657252035\n",
      "    mean_inference_ms: 5.142229760924433\n",
      "    mean_raw_obs_processing_ms: 0.4354992126047067\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 20.1\n",
      "    episode_reward_mean: -2.8169999999999877\n",
      "    episode_reward_min: -25.5\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [1.5000000000000204, -24.000000000000036, -14.399999999999993,\n",
      "        -10.499999999999984, -10.49999999999998, -0.8999999999999897, -2.999999999999971,\n",
      "        14.700000000000026, 3.0, -6.938893903907228e-16, -11.999999999999984, 1.0852430065710905e-14,\n",
      "        -5.099999999999987, -24.00000000000001, -5.999999999999975, -6.899999999999984,\n",
      "        13.800000000000026, 7.500000000000007, 8.400000000000023, -25.5, -22.500000000000007,\n",
      "        -9.899999999999972, -8.999999999999993, -12.299999999999985, 9.600000000000009,\n",
      "        7.200000000000012, 3.6000000000000285, 6.300000000000011, 20.1, -8.999999999999984,\n",
      "        -2.999999999999987, -9.89999999999998, -10.49999999999999, -5.999999999999977,\n",
      "        1.662558979376172e-14, -10.499999999999991, -5.699999999999992, -13.499999999999979,\n",
      "        10.80000000000003, 2.999999999999999, -16.49999999999998, -1.4999999999999734,\n",
      "        -7.499999999999989, -8.699999999999996, 13.500000000000016, 5.099999999999952,\n",
      "        -14.399999999999975, 4.500000000000024, -18.000000000000007, -14.099999999999985,\n",
      "        -3.299999999999986, -3.2999999999999767, -6.899999999999974, 6.000000000000025,\n",
      "        9.600000000000025, 9.90000000000001, 12.000000000000028, -17.999999999999996,\n",
      "        -0.2999999999999742, -7.19999999999998, 3.2999999999999785, -6.299999999999992,\n",
      "        -2.6999999999999997, -13.199999999999983, 9.520162436160717e-15, -7.499999999999984,\n",
      "        -14.399999999999979, 8.700000000000026, 8.999999999999998, -8.999999999999986,\n",
      "        -3.899999999999986, 17.999999999999922, 3.300000000000023, -19.499999999999996,\n",
      "        -2.9999999999999805, -2.999999999999981, -8.999999999999991, 6.600000000000017,\n",
      "        -0.899999999999998, -5.700000000000005, 16.199999999999967, -8.999999999999973,\n",
      "        12.000000000000028, -2.999999999999978, 4.50000000000003, -14.099999999999989,\n",
      "        10.800000000000002, -9.899999999999991, -1.1999999999999755, 12.00000000000003,\n",
      "        -4.799999999999978, 0.3000000000000076, 8.100000000000033, -3.5999999999999943,\n",
      "        1.2000000000000142, -8.999999999999988, -4.499999999999984, -3.899999999999981,\n",
      "        2.4000000000000177, -22.80000000000001]\n",
      "      policy_policy1_reward: [11.5, -19.5, -5.5, -0.5, -0.5, 8.0, 7.0, 22.5, 13.0, 10.0,\n",
      "        -2.0, 10.0, 0.5, -14.0, 4.0, 2.0, 20.5, 17.5, 14.0, -15.5, -12.5, -1.0, 1.0,\n",
      "        -4.5, 18.5, 15.0, 12.5, 7.5, 29.0, 1.0, 7.0, -1.0, -0.5, 4.0, 10.0, -0.5, 1.0,\n",
      "        -3.5, 17.5, 13.0, -6.5, 8.5, 2.5, -2.0, 23.5, 14.0, -5.5, 14.5, -8.0, -8.5,\n",
      "        4.5, 4.5, 2.0, 16.0, 18.5, 15.5, 22.0, -8.0, 7.5, -0.5, 4.5, 1.5, 4.0, -6.5,\n",
      "        10.0, 2.5, -5.5, 16.5, 19.0, -10.0, -0.5, 28.0, 10.0, -9.5, 7.0, 7.0, 1.0, 15.5,\n",
      "        8.0, 1.0, 24.0, 1.0, 22.0, 7.0, 14.5, -8.5, 17.5, -1.0, 0.0, 22.0, 3.0, 7.0,\n",
      "        17.0, 2.0, 9.0, 1.0, 5.5, 5.0, 8.0, -15.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -4.499999999999984, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999994, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999895, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -7.799999999999981, -8.899999999999986,\n",
      "        -1.200000000000005, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999895, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999982, -7.7999999999999865,\n",
      "        -7.799999999999989, -8.89999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -5.599999999999989, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -6.699999999999994, -1.1999999999999975, -7.799999999999981, -6.699999999999995,\n",
      "        -6.699999999999991, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -7.799999999999984, -9.99999999999998, 0.9999999999999943, -3.399999999999996,\n",
      "        -9.99999999999998, -6.699999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -6.699999999999995, -7.799999999999989, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999999, -6.699999999999992,\n",
      "        -8.89999999999998, -1.2000000000000046, -9.99999999999998, -7.799999999999981,\n",
      "        -6.699999999999995, -8.899999999999986, -5.599999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -5.599999999999992,\n",
      "        -7.799999999999988]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.0\n",
      "      policy2: 0.9999999999999943\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.555\n",
      "      policy2: -8.371999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -19.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07238029161098546\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022791529657252035\n",
      "      mean_inference_ms: 5.142229760924433\n",
      "      mean_raw_obs_processing_ms: 0.4354992126047067\n",
      "  time_since_restore: 156.8107852935791\n",
      "  time_this_iter_s: 45.701470136642456\n",
      "  time_total_s: 156.8107852935791\n",
      "  timers:\n",
      "    learn_throughput: 148.64\n",
      "    learn_time_ms: 20182.932\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 39197.71\n",
      "  timestamp: 1660499917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.49999999999995\n",
      "  episode_reward_mean: -1.2449999999999906\n",
      "  episode_reward_min: -33.00000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3017381429672241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009086606092751026\n",
      "          model: {}\n",
      "          policy_loss: -0.03280848264694214\n",
      "          total_loss: 7.1983642578125\n",
      "          vf_explained_var: 0.018750224262475967\n",
      "          vf_loss: 7.229355335235596\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2777421474456787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010046780109405518\n",
      "          model: {}\n",
      "          policy_loss: -0.03897435963153839\n",
      "          total_loss: 2.272948980331421\n",
      "          vf_explained_var: 0.259204238653183\n",
      "          vf_loss: 2.3109185695648193\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.487878787878785\n",
      "    ram_util_percent: 71.3590909090909\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 24.5\n",
      "    policy2: 4.300000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.05\n",
      "    policy2: -8.294999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07195477432449814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02056436375014551\n",
      "    mean_inference_ms: 4.971416641360047\n",
      "    mean_raw_obs_processing_ms: 0.42796275271479645\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 16.49999999999995\n",
      "    episode_reward_mean: -1.2449999999999906\n",
      "    episode_reward_min: -33.00000000000004\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.999999999999986, 9.076073226310655e-15, -8.399999999999975,\n",
      "        -10.499999999999977, 6.000000000000001, -15.000000000000039, -18.299999999999997,\n",
      "        -4.4999999999999805, -10.499999999999982, -33.00000000000004, 6.000000000000034,\n",
      "        -4.499999999999979, 3.0000000000000173, -5.999999999999975, 4.500000000000009,\n",
      "        1.500000000000017, -13.199999999999983, 8.399999999999984, -18.899999999999984,\n",
      "        10.500000000000028, -7.499999999999984, 3.5999999999999654, -4.799999999999985,\n",
      "        4.500000000000015, -15.899999999999986, 6.9000000000000234, 3.000000000000018,\n",
      "        -14.999999999999982, 8.100000000000026, -4.7999999999999865, 15.60000000000003,\n",
      "        -5.099999999999988, 6.000000000000012, 3.60000000000001, 4.500000000000023,\n",
      "        -7.500000000000002, -1.499999999999988, 6.411537967210279e-15, -19.499999999999993,\n",
      "        -11.999999999999979, -7.499999999999982, 3.000000000000022, -3.899999999999976,\n",
      "        -13.499999999999986, -5.999999999999982, 8.699999999999969, -18.899999999999984,\n",
      "        0.9000000000000156, -0.2999999999999917, -10.49999999999998, -3.8999999999999893,\n",
      "        -9.299999999999976, -4.799999999999978, -1.799999999999979, -3.29999999999998,\n",
      "        3.0000000000000213, -3.8999999999999866, 3.900000000000014, 13.200000000000012,\n",
      "        5.9674487573602164e-15, 2.100000000000023, 5.100000000000026, 10.499999999999947,\n",
      "        1.4999999999999782, 9.600000000000033, 1.500000000000027, -4.4999999999999805,\n",
      "        -4.799999999999974, 14.699999999999998, -3.299999999999973, 1.5000000000000266,\n",
      "        -2.9999999999999867, 0.5999999999999641, -2.999999999999984, -7.499999999999977,\n",
      "        10.800000000000033, 4.800000000000031, 2.7000000000000015, 6.600000000000014,\n",
      "        4.499999999999982, 11.100000000000007, -5.999999999999986, -16.5, -8.399999999999993,\n",
      "        1.5000000000000209, -5.699999999999985, 12.599999999999955, 1.1296519275560968e-14,\n",
      "        16.199999999999907, -6.000000000000017, 7.200000000000006, 7.500000000000018,\n",
      "        1.2000000000000113, 1.2000000000000224, -7.499999999999982, 0.600000000000015,\n",
      "        -8.400000000000004, 16.49999999999995, 10.500000000000002, 2.1000000000000223]\n",
      "      policy_policy1_reward: [1.0, 10.0, 0.5, -0.5, 16.0, -5.0, -10.5, 5.5, -0.5, -23.0,\n",
      "        16.0, 5.5, 13.0, 4.0, 14.5, 11.5, -6.5, 14.0, -10.0, 20.5, 2.5, 12.5, 3.0, 14.5,\n",
      "        -7.0, 12.5, 13.0, -5.0, 17.0, 3.0, 24.5, 0.5, 16.0, 12.5, 14.5, 2.5, 8.5, 10.0,\n",
      "        -9.5, -2.0, 2.5, 13.0, 5.0, -3.5, -1.5, 11.0, -10.0, 1.0, 7.5, -0.5, 5.0, -1.5,\n",
      "        3.0, 0.5, 4.5, 7.5, 5.0, 9.5, 21.0, 10.0, 5.5, 14.0, 15.0, 11.5, 18.5, 11.5,\n",
      "        5.5, 3.0, 22.5, 4.5, 11.5, 7.0, 9.5, 7.0, 2.5, 17.5, 11.5, 10.5, 15.5, 14.5,\n",
      "        20.0, 4.0, -6.5, -5.0, 6.0, -10.0, 21.5, 10.0, 24.0, 4.0, 15.0, 12.0, 9.0, 9.0,\n",
      "        2.5, 9.5, 0.5, 21.0, 20.5, 11.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -5.599999999999993, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -8.899999999999983,\n",
      "        -5.5999999999999845, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -4.499999999999987, -2.2999999999999963, -8.89999999999998,\n",
      "        -0.10000000000000331, -7.799999999999981, -9.99999999999998, -8.899999999999986,\n",
      "        -7.79999999999999, -7.799999999999981, -2.3000000000000003, -7.79999999999999,\n",
      "        -4.499999999999995, -8.89999999999998, -5.6, -7.799999999999985, -9.99999999999998,\n",
      "        -3.4, -8.89999999999998, -4.49999999999999, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -7.799999999999983, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999992, -6.6999999999999815,\n",
      "        -7.799999999999981, -8.89999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.399999999999985, -4.500000000000003,\n",
      "        4.300000000000005, -8.89999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -7.799999999999981, -4.5000000000000036, -7.799999999999981,\n",
      "        -7.799999999999981, -9.99999999999998, -8.89999999999998, -8.899999999999983,\n",
      "        -4.500000000000003, -9.99999999999998, -8.899999999999984]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 24.5\n",
      "      policy2: 4.300000000000005\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.05\n",
      "      policy2: -8.294999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -23.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07195477432449814\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02056436375014551\n",
      "      mean_inference_ms: 4.971416641360047\n",
      "      mean_raw_obs_processing_ms: 0.42796275271479645\n",
      "  time_since_restore: 156.8888909816742\n",
      "  time_this_iter_s: 46.5162889957428\n",
      "  time_total_s: 156.8888909816742\n",
      "  timers:\n",
      "    learn_throughput: 146.425\n",
      "    learn_time_ms: 20488.324\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 39216.737\n",
      "  timestamp: 1660499918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:38 (running for 00:03:20.79)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |          101.9   |  8000 | -5.58375 |                3.06875 |                -8.6525 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:44 (running for 00:03:25.84)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      2 |          101.9   |  8000 | -5.58375 |                3.06875 |                -8.6525 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.800000000000015\n",
      "  episode_reward_mean: -2.1959999999999904\n",
      "  episode_reward_min: -34.50000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 847f41dbc3e943d8b61110c9b40c9844\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3080828189849854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01120022777467966\n",
      "          model: {}\n",
      "          policy_loss: -0.03414170444011688\n",
      "          total_loss: 6.8309431076049805\n",
      "          vf_explained_var: 0.0850016325712204\n",
      "          vf_loss: 6.862844944000244\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3054643869400024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010946891270577908\n",
      "          model: {}\n",
      "          policy_loss: -0.03621159493923187\n",
      "          total_loss: 2.0605363845825195\n",
      "          vf_explained_var: 0.24534931778907776\n",
      "          vf_loss: 2.0945587158203125\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.922093023255815\n",
      "    ram_util_percent: 71.47209302325582\n",
      "  pid: 14084\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -3.3999999999999826\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.11\n",
      "    policy2: -8.305999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07695640740162553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023680384133542093\n",
      "    mean_inference_ms: 4.66920791466323\n",
      "    mean_raw_obs_processing_ms: 0.42614233601443163\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.800000000000015\n",
      "    episode_reward_mean: -2.1959999999999904\n",
      "    episode_reward_min: -34.50000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-2.9999999999999947, 1.5000000000000138, -20.700000000000014,\n",
      "        -27.000000000000007, -8.999999999999984, 7.500000000000005, -10.499999999999977,\n",
      "        -12.899999999999977, -29.400000000000027, -16.799999999999976, 18.299999999999986,\n",
      "        5.100000000000012, -5.699999999999985, 3.6000000000000156, -34.50000000000006,\n",
      "        -25.500000000000014, -19.5, -11.999999999999973, -15.600000000000001, -21.000000000000007,\n",
      "        3.600000000000031, -3.900000000000004, 2.5951463200613034e-14, -10.49999999999998,\n",
      "        3.0000000000000258, 6.900000000000015, 1.662558979376172e-14, -4.499999999999993,\n",
      "        17.99999999999999, 0.6000000000000184, -14.699999999999992, -7.499999999999977,\n",
      "        4.5000000000000036, 17.99999999999994, -6.899999999999993, -1.199999999999994,\n",
      "        3.0000000000000115, 8.70000000000002, 13.499999999999982, -10.79999999999999,\n",
      "        -7.499999999999998, -30.000000000000018, -5.399999999999986, -17.999999999999996,\n",
      "        5.700000000000031, -1.1999999999999793, 22.800000000000015, 5.100000000000026,\n",
      "        -4.799999999999992, 7.500000000000011, -16.49999999999998, 12.300000000000013,\n",
      "        -16.799999999999997, 9.600000000000026, -1.4999999999999845, 3.3000000000000074,\n",
      "        -2.699999999999974, 4.191091917959966e-15, -2.9999999999999765, -21.0, 8.40000000000001,\n",
      "        8.400000000000027, -9.899999999999977, -3.9000000000000035, -14.399999999999977,\n",
      "        11.400000000000029, -7.199999999999976, 2.7000000000000184, -4.499999999999989,\n",
      "        3.300000000000006, -10.199999999999976, 0.5999999999999864, -14.699999999999978,\n",
      "        12.599999999999937, -2.0999999999999908, -2.099999999999981, -8.399999999999988,\n",
      "        14.700000000000008, -8.999999999999984, 6.899999999999952, -5.999999999999975,\n",
      "        3.000000000000019, -17.399999999999977, -2.999999999999983, 6.000000000000009,\n",
      "        -1.1999999999999797, 5.100000000000011, 4.500000000000028, 6.000000000000028,\n",
      "        9.000000000000016, 0.600000000000022, 0.6000000000000215, 17.99999999999998,\n",
      "        -1.4999999999999853, 8.100000000000023, 4.800000000000006, 13.799999999999997,\n",
      "        8.700000000000031, 6.00000000000003, 2.1000000000000174]\n",
      "      policy_policy1_reward: [7.0, 11.5, -14.0, -17.0, 1.0, 17.5, -0.5, -4.0, -20.5,\n",
      "        -9.0, 25.0, 14.0, 1.0, 12.5, -24.5, -15.5, -9.5, -2.0, -10.0, -11.0, 7.0, 5.0,\n",
      "        10.0, -0.5, 13.0, 12.5, 10.0, 5.5, 28.0, 4.0, -8.0, 2.5, 14.5, 28.0, 2.0, 5.5,\n",
      "        13.0, 16.5, 23.5, -3.0, 2.5, -20.0, 3.5, -8.0, 13.5, 5.5, 29.5, 14.0, 3.0, 17.5,\n",
      "        -6.5, 19.0, -9.0, 18.5, 8.5, 10.0, 4.0, 10.0, 1.5, -11.0, 14.0, 14.0, -1.0,\n",
      "        5.0, -5.5, 17.0, -0.5, 10.5, 5.5, 10.0, -3.5, 4.0, -8.0, 21.5, 3.5, 3.5, 0.5,\n",
      "        22.5, 1.0, 12.5, 4.0, 7.5, -8.5, 1.5, 16.0, 5.5, 14.0, 14.5, 16.0, 19.0, 9.5,\n",
      "        9.5, 28.0, 8.5, 17.0, 11.5, 20.5, 16.5, 16.0, 11.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -6.69999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999984, -8.89999999999998, -7.799999999999981, -6.699999999999986,\n",
      "        -8.899999999999986, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -3.3999999999999826, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.4000000000000044, -6.699999999999991,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999984, -6.6999999999999815, -6.69999999999999,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -7.799999999999981, -8.899999999999984, -9.99999999999998,\n",
      "        -6.6999999999999815, -6.6999999999999815, -9.99999999999998, -4.5, -9.99999999999998,\n",
      "        -5.599999999999999, -5.599999999999992, -8.899999999999986, -8.89999999999998,\n",
      "        -8.89999999999998, -5.599999999999991, -6.6999999999999895, -7.799999999999988,\n",
      "        -9.99999999999998, -6.6999999999999815, -6.699999999999995, -3.400000000000005,\n",
      "        -6.699999999999988, -8.89999999999998, -5.599999999999982, -5.6, -8.89999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -5.6, -9.99999999999998, -4.500000000000003,\n",
      "        -8.899999999999986, -4.499999999999984, -9.99999999999998, -6.699999999999992,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999993, -6.699999999999995, -7.799999999999986,\n",
      "        -9.99999999999998, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -3.3999999999999826\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.11\n",
      "      policy2: -8.305999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -24.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07695640740162553\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023680384133542093\n",
      "      mean_inference_ms: 4.66920791466323\n",
      "      mean_raw_obs_processing_ms: 0.42614233601443163\n",
      "  time_since_restore: 162.47120642662048\n",
      "  time_this_iter_s: 60.571603536605835\n",
      "  time_total_s: 162.47120642662048\n",
      "  timers:\n",
      "    learn_throughput: 132.273\n",
      "    learn_time_ms: 30240.426\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 54147.761\n",
      "  timestamp: 1660499926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 4205e_00002\n",
      "  warmup_time: 9.795740365982056\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:52 (running for 00:03:33.80)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 | -2.196   |                6.11    |                -8.306  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:58:57 (running for 00:03:38.86)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 | -2.196   |                6.11    |                -8.306  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:02 (running for 00:03:43.93)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 | -2.196   |                6.11    |                -8.306  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:07 (running for 00:03:48.99)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 | -2.196   |                6.11    |                -8.306  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:12 (running for 00:03:54.06)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 | -2.196   |                6.11    |                -8.306  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:17 (running for 00:03:59.12)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 | -2.196   |                6.11    |                -8.306  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:22 (running for 00:04:04.20)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-1.2449999999999906 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB77F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7FA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CB7490>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75C06C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      4 |          156.889 | 12000 | -1.245   |                7.05    |                -8.295  |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      4 |          156.811 | 12000 | -2.817   |                5.555   |                -8.372  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 | -2.196   |                6.11    |                -8.306  |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      2 |          122.653 |  8000 | -7.68375 |                0.96875 |                -8.6525 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.999999999999922\n",
      "  episode_reward_mean: -1.09799999999999\n",
      "  episode_reward_min: -22.80000000000001\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2059850692749023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0170621145516634\n",
      "          model: {}\n",
      "          policy_loss: -0.05437203124165535\n",
      "          total_loss: 6.704473972320557\n",
      "          vf_explained_var: 0.14132992923259735\n",
      "          vf_loss: 6.7511677742004395\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2060801982879639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017018182203173637\n",
      "          model: {}\n",
      "          policy_loss: -0.05528168007731438\n",
      "          total_loss: 2.8252460956573486\n",
      "          vf_explained_var: 0.17558445036411285\n",
      "          vf_loss: 2.8728694915771484\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.558461538461533\n",
      "    ram_util_percent: 71.34461538461538\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: 7.599999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.625\n",
      "    policy2: -7.722999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07291229569327974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023718990913644055\n",
      "    mean_inference_ms: 5.655471825668153\n",
      "    mean_raw_obs_processing_ms: 0.4362083138818413\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.999999999999922\n",
      "    episode_reward_mean: -1.09799999999999\n",
      "    episode_reward_min: -22.80000000000001\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-2.999999999999987, -9.89999999999998, -10.49999999999999, -5.999999999999977,\n",
      "        1.662558979376172e-14, -10.499999999999991, -5.699999999999992, -13.499999999999979,\n",
      "        10.80000000000003, 2.999999999999999, -16.49999999999998, -1.4999999999999734,\n",
      "        -7.499999999999989, -8.699999999999996, 13.500000000000016, 5.099999999999952,\n",
      "        -14.399999999999975, 4.500000000000024, -18.000000000000007, -14.099999999999985,\n",
      "        -3.299999999999986, -3.2999999999999767, -6.899999999999974, 6.000000000000025,\n",
      "        9.600000000000025, 9.90000000000001, 12.000000000000028, -17.999999999999996,\n",
      "        -0.2999999999999742, -7.19999999999998, 3.2999999999999785, -6.299999999999992,\n",
      "        -2.6999999999999997, -13.199999999999983, 9.520162436160717e-15, -7.499999999999984,\n",
      "        -14.399999999999979, 8.700000000000026, 8.999999999999998, -8.999999999999986,\n",
      "        -3.899999999999986, 17.999999999999922, 3.300000000000023, -19.499999999999996,\n",
      "        -2.9999999999999805, -2.999999999999981, -8.999999999999991, 6.600000000000017,\n",
      "        -0.899999999999998, -5.700000000000005, 16.199999999999967, -8.999999999999973,\n",
      "        12.000000000000028, -2.999999999999978, 4.50000000000003, -14.099999999999989,\n",
      "        10.800000000000002, -9.899999999999991, -1.1999999999999755, 12.00000000000003,\n",
      "        -4.799999999999978, 0.3000000000000076, 8.100000000000033, -3.5999999999999943,\n",
      "        1.2000000000000142, -8.999999999999988, -4.499999999999984, -3.899999999999981,\n",
      "        2.4000000000000177, -22.80000000000001, -4.799999999999977, -0.8999999999999847,\n",
      "        -14.099999999999989, 14.099999999999971, 11.400000000000029, 3.000000000000024,\n",
      "        11.999999999999973, -2.999999999999985, -5.399999999999979, 4.200000000000022,\n",
      "        1.1999999999999686, -4.799999999999976, -2.9999999999999765, 8.099999999999955,\n",
      "        -2.6999999999999815, -2.9999999999999964, 8.699999999999962, 12.300000000000026,\n",
      "        2.099999999999972, -4.499999999999977, -1.5000000000000164, 6.900000000000016,\n",
      "        8.400000000000002, 16.49999999999995, -6.899999999999979, 1.2000000000000162,\n",
      "        -5.09999999999998, 5.700000000000033, 4.500000000000034, 1.5000000000000149]\n",
      "      policy_policy1_reward: [7.0, -1.0, -0.5, 4.0, 10.0, -0.5, 1.0, -3.5, 17.5, 13.0,\n",
      "        -6.5, 8.5, 2.5, -2.0, 23.5, 14.0, -5.5, 14.5, -8.0, -8.5, 4.5, 4.5, 2.0, 16.0,\n",
      "        18.5, 15.5, 22.0, -8.0, 7.5, -0.5, 4.5, 1.5, 4.0, -6.5, 10.0, 2.5, -5.5, 16.5,\n",
      "        19.0, -10.0, -0.5, 28.0, 10.0, -9.5, 7.0, 7.0, 1.0, 15.5, 8.0, 1.0, 24.0, 1.0,\n",
      "        22.0, 7.0, 14.5, -8.5, 17.5, -1.0, 0.0, 22.0, 3.0, 7.0, 17.0, 2.0, 9.0, 1.0,\n",
      "        5.5, 5.0, 8.0, -15.0, 3.0, 8.0, -8.5, 23.0, 17.0, 13.0, 16.5, 1.5, -2.0, 6.5,\n",
      "        9.0, 3.0, 7.0, 0.5, -1.5, 7.0, 16.5, 19.0, 11.0, 5.5, 8.5, 12.5, 8.5, 26.5,\n",
      "        -3.5, 9.0, 0.5, 13.5, 14.5, 11.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999895, -9.99999999999998,\n",
      "        -8.899999999999986, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -7.7999999999999865, -7.799999999999989, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -6.699999999999994, -1.1999999999999975,\n",
      "        -7.799999999999981, -6.699999999999995, -6.699999999999991, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -7.799999999999984, -9.99999999999998,\n",
      "        0.9999999999999943, -3.399999999999996, -9.99999999999998, -6.699999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -6.699999999999995, -7.799999999999989,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999999, -6.699999999999992, -8.89999999999998, -1.2000000000000046,\n",
      "        -9.99999999999998, -7.799999999999981, -6.699999999999995, -8.899999999999986,\n",
      "        -5.599999999999999, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -5.599999999999992, -7.799999999999988, -7.799999999999989,\n",
      "        -8.89999999999998, -5.599999999999982, -8.89999999999998, -5.6, -9.99999999999998,\n",
      "        -4.500000000000001, -4.499999999999997, -3.400000000000003, -2.2999999999999954,\n",
      "        -7.799999999999987, -7.799999999999981, -9.99999999999998, 7.599999999999996,\n",
      "        -1.1999999999999984, -9.99999999999998, -7.799999999999986, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999992,\n",
      "        -0.10000000000000275, -9.99999999999998, -3.4000000000000052, -7.79999999999999,\n",
      "        -5.599999999999991, -7.799999999999981, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: 7.599999999999996\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.625\n",
      "      policy2: -7.722999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -15.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07291229569327974\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023718990913644055\n",
      "      mean_inference_ms: 5.655471825668153\n",
      "      mean_raw_obs_processing_ms: 0.4362083138818413\n",
      "  time_since_restore: 202.2931730747223\n",
      "  time_this_iter_s: 45.48238778114319\n",
      "  time_total_s: 202.2931730747223\n",
      "  timers:\n",
      "    learn_throughput: 143.409\n",
      "    learn_time_ms: 20919.149\n",
      "    synch_weights_time_ms: 2.792\n",
      "    training_iteration_time_ms: 40452.252\n",
      "  timestamp: 1660499962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-59-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999986\n",
      "  episode_reward_mean: -3.7979999999999894\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: d9b180bb1fb4403ba3d13246b5f1de5e\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.283525824546814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019501209259033203\n",
      "          model: {}\n",
      "          policy_loss: -0.04887094348669052\n",
      "          total_loss: 6.5406951904296875\n",
      "          vf_explained_var: 0.08471183478832245\n",
      "          vf_loss: 6.58371639251709\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2848762273788452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015969347208738327\n",
      "          model: {}\n",
      "          policy_loss: -0.044649068266153336\n",
      "          total_loss: 2.4290919303894043\n",
      "          vf_explained_var: 0.31905606389045715\n",
      "          vf_loss: 2.468950033187866\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.34137931034483\n",
      "    ram_util_percent: 71.35862068965514\n",
      "  pid: 13928\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -1.2000000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.42\n",
      "    policy2: -8.217999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0775420601838083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023872044561259756\n",
      "    mean_inference_ms: 6.722727758688828\n",
      "    mean_raw_obs_processing_ms: 0.4465946317465541\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999986\n",
      "    episode_reward_mean: -3.7979999999999894\n",
      "    episode_reward_min: -33.000000000000036\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.0000000000000187, 2.4000000000000274, -14.999999999999984,\n",
      "        -24.000000000000007, -33.000000000000036, -0.2999999999999906, -28.50000000000002,\n",
      "        3.0000000000000098, -13.499999999999979, -15.299999999999988, 18.599999999999987,\n",
      "        -9.899999999999977, 6.300515664747763e-15, -28.80000000000002, 3.0000000000000044,\n",
      "        -13.500000000000016, -14.99999999999998, 4.500000000000025, -10.199999999999989,\n",
      "        -0.8999999999999808, -5.999999999999989, -8.999999999999973, -13.199999999999974,\n",
      "        -13.799999999999976, -14.399999999999983, -25.500000000000004, 10.500000000000028,\n",
      "        -5.399999999999984, -7.5, -5.999999999999988, -16.79999999999998, -12.59999999999998,\n",
      "        -27.000000000000064, 6.600000000000027, 5.400000000000029, -16.499999999999986,\n",
      "        0.6000000000000166, -2.999999999999989, -10.499999999999975, 12.000000000000027,\n",
      "        -2.9999999999999822, -19.200000000000003, -10.800000000000006, 2.6999999999999997,\n",
      "        -21.60000000000001, -3.2999999999999763, -1.1999999999999933, -2.999999999999991,\n",
      "        8.100000000000025, -14.099999999999982, -4.499999999999991, 2.1000000000000214,\n",
      "        -16.799999999999976, -4.499999999999975, 3.0000000000000044, -19.499999999999993,\n",
      "        9.600000000000017, 6.000000000000022, -4.19999999999999, 1.2000000000000224,\n",
      "        19.499999999999986, -13.799999999999992, -1.4999999999999862, 3.299999999999968,\n",
      "        5.100000000000028, 10.50000000000002, 13.799999999999969, -0.29999999999999305,\n",
      "        1.7069679003611782e-14, 6.000000000000028, -2.999999999999977, 9.520162436160717e-15,\n",
      "        7.799999999999974, -1.4999999999999853, -3.299999999999978, 7.500000000000028,\n",
      "        2.400000000000016, -8.399999999999995, 9.300000000000033, 0.30000000000002003,\n",
      "        -13.499999999999982, 2.700000000000021, -5.999999999999994, 9.600000000000021,\n",
      "        -4.499999999999993, 1.1629586182948515e-14, 0.6000000000000291, 14.09999999999998,\n",
      "        3.0000000000000044, -7.799999999999978, -6.299999999999979, -1.8457457784393227e-14,\n",
      "        2.683964162031316e-14, 14.400000000000023, -4.499999999999987, -2.9999999999999774,\n",
      "        -11.399999999999983, -2.9999999999999973, -8.399999999999983, 9.000000000000004]\n",
      "      policy_policy1_reward: [13.0, 8.0, -5.0, -14.0, -23.0, 2.0, -18.5, 13.0, -3.5,\n",
      "        -7.5, 27.5, -1.0, 10.0, -21.0, 13.0, -3.5, -5.0, 14.5, -3.5, 8.0, 4.0, 1.0,\n",
      "        -6.5, -6.0, -5.5, -21.0, 20.5, 3.5, 2.5, 4.0, -9.0, -7.0, -17.0, 15.5, 11.0,\n",
      "        -6.5, 9.5, 1.5, -0.5, 22.0, 7.0, -12.5, -3.0, 10.5, -16.0, 4.5, 5.5, 7.0, 17.0,\n",
      "        -8.5, 5.5, 11.0, -9.0, 5.5, 13.0, -9.5, 18.5, 16.0, 2.5, 9.0, 29.5, -6.0, 8.5,\n",
      "        10.0, 14.0, 20.5, 20.5, 7.5, 10.0, 10.5, 7.0, 10.0, 9.0, 3.0, 4.5, 17.5, 8.0,\n",
      "        0.5, 16.0, 7.0, -3.5, 10.5, 4.0, 18.5, 5.5, 10.0, 9.5, 17.5, 13.0, -5.5, -4.0,\n",
      "        10.0, 10.0, 20.0, 5.5, 7.0, -2.5, 1.5, -5.0, 19.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -5.599999999999993, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -2.300000000000002, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -8.89999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999985,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999983,\n",
      "        -7.799999999999981, -8.89999999999998, -4.500000000000002, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999983,\n",
      "        -5.599999999999982, -9.99999999999998, -8.89999999999998, -5.5999999999999925,\n",
      "        -9.99999999999998, -8.89999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999986, -7.799999999999984,\n",
      "        -7.799999999999987, -5.599999999999995, -7.799999999999986, -6.699999999999992,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -7.799999999999989, -9.99999999999998, -7.799999999999988, -9.99999999999998,\n",
      "        -6.699999999999995, -8.89999999999998, -9.99999999999998, -6.699999999999988,\n",
      "        -7.799999999999981, -9.99999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -1.2000000000000015, -4.499999999999986, -7.799999999999981,\n",
      "        -9.99999999999998, -5.599999999999996, -8.89999999999998, -6.699999999999989,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -3.3999999999999964, -9.99999999999998, -2.2999999999999914, -2.300000000000003,\n",
      "        -9.99999999999998, -9.99999999999998, -5.59999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -4.500000000000002, -3.39999999999999,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -1.2000000000000015\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.42\n",
      "      policy2: -8.217999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -23.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0775420601838083\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023872044561259756\n",
      "      mean_inference_ms: 6.722727758688828\n",
      "      mean_raw_obs_processing_ms: 0.4465946317465541\n",
      "  time_since_restore: 184.31221437454224\n",
      "  time_this_iter_s: 61.659018754959106\n",
      "  time_total_s: 184.31221437454224\n",
      "  timers:\n",
      "    learn_throughput: 124.028\n",
      "    learn_time_ms: 32250.711\n",
      "    synch_weights_time_ms: 2.991\n",
      "    training_iteration_time_ms: 61432.751\n",
      "  timestamp: 1660499964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 4205e_00003\n",
      "  warmup_time: 9.807717561721802\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-59-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.39999999999994\n",
      "  episode_reward_mean: -0.3599999999999899\n",
      "  episode_reward_min: -19.499999999999993\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2735021114349365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011132312007248402\n",
      "          model: {}\n",
      "          policy_loss: -0.04063936695456505\n",
      "          total_loss: 6.7801899909973145\n",
      "          vf_explained_var: 0.055506378412246704\n",
      "          vf_loss: 6.818603038787842\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2395001649856567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013386035338044167\n",
      "          model: {}\n",
      "          policy_loss: -0.04584344103932381\n",
      "          total_loss: 2.6867411136627197\n",
      "          vf_explained_var: 0.23101764917373657\n",
      "          vf_loss: 2.73124623298645\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.571212121212124\n",
      "    ram_util_percent: 71.34545454545453\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: 4.300000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.495\n",
      "    policy2: -7.854999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07309273096930355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020902840759192128\n",
      "    mean_inference_ms: 5.55109663044737\n",
      "    mean_raw_obs_processing_ms: 0.4315483016420463\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.39999999999994\n",
      "    episode_reward_mean: -0.3599999999999899\n",
      "    episode_reward_min: -19.499999999999993\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [15.60000000000003, -5.099999999999988, 6.000000000000012, 3.60000000000001,\n",
      "        4.500000000000023, -7.500000000000002, -1.499999999999988, 6.411537967210279e-15,\n",
      "        -19.499999999999993, -11.999999999999979, -7.499999999999982, 3.000000000000022,\n",
      "        -3.899999999999976, -13.499999999999986, -5.999999999999982, 8.699999999999969,\n",
      "        -18.899999999999984, 0.9000000000000156, -0.2999999999999917, -10.49999999999998,\n",
      "        -3.8999999999999893, -9.299999999999976, -4.799999999999978, -1.799999999999979,\n",
      "        -3.29999999999998, 3.0000000000000213, -3.8999999999999866, 3.900000000000014,\n",
      "        13.200000000000012, 5.9674487573602164e-15, 2.100000000000023, 5.100000000000026,\n",
      "        10.499999999999947, 1.4999999999999782, 9.600000000000033, 1.500000000000027,\n",
      "        -4.4999999999999805, -4.799999999999974, 14.699999999999998, -3.299999999999973,\n",
      "        1.5000000000000266, -2.9999999999999867, 0.5999999999999641, -2.999999999999984,\n",
      "        -7.499999999999977, 10.800000000000033, 4.800000000000031, 2.7000000000000015,\n",
      "        6.600000000000014, 4.499999999999982, 11.100000000000007, -5.999999999999986,\n",
      "        -16.5, -8.399999999999993, 1.5000000000000209, -5.699999999999985, 12.599999999999955,\n",
      "        1.1296519275560968e-14, 16.199999999999907, -6.000000000000017, 7.200000000000006,\n",
      "        7.500000000000018, 1.2000000000000113, 1.2000000000000224, -7.499999999999982,\n",
      "        0.600000000000015, -8.400000000000004, 16.49999999999995, 10.500000000000002,\n",
      "        2.1000000000000223, -1.4999999999999787, 3.0000000000000107, -4.799999999999979,\n",
      "        -4.1999999999999815, -0.2999999999999855, -8.999999999999973, -7.799999999999994,\n",
      "        -8.100000000000001, -10.799999999999986, -5.999999999999976, -4.799999999999981,\n",
      "        7.500000000000027, -14.999999999999973, -0.8999999999999826, 7.743805596760467e-15,\n",
      "        2.450817326860033e-14, 17.39999999999994, -0.29999999999999905, -11.099999999999994,\n",
      "        -2.999999999999978, -9.89999999999998, 3.000000000000033, 16.499999999999964,\n",
      "        14.400000000000025, 3.000000000000014, -2.999999999999989, -0.29999999999997196,\n",
      "        -5.399999999999983, -5.099999999999977, 1.2000000000000077]\n",
      "      policy_policy1_reward: [24.5, 0.5, 16.0, 12.5, 14.5, 2.5, 8.5, 10.0, -9.5, -2.0,\n",
      "        2.5, 13.0, 5.0, -3.5, -1.5, 11.0, -10.0, 1.0, 7.5, -0.5, 5.0, -1.5, 3.0, 0.5,\n",
      "        4.5, 7.5, 5.0, 9.5, 21.0, 10.0, 5.5, 14.0, 15.0, 11.5, 18.5, 11.5, 5.5, 3.0,\n",
      "        22.5, 4.5, 11.5, 7.0, 9.5, 7.0, 2.5, 17.5, 11.5, 10.5, 15.5, 14.5, 20.0, 4.0,\n",
      "        -6.5, -5.0, 6.0, -10.0, 21.5, 10.0, 24.0, 4.0, 15.0, 12.0, 9.0, 9.0, 2.5, 9.5,\n",
      "        0.5, 21.0, 20.5, 11.0, 8.5, 13.0, 3.0, 2.5, 2.0, 1.0, 0.0, -2.5, -3.0, 4.0,\n",
      "        -2.5, 17.5, -10.5, 8.0, 10.0, 10.0, 23.0, 7.5, -11.0, 7.0, -1.0, 13.0, 26.5,\n",
      "        20.0, 13.0, 7.0, 7.5, 3.5, 0.5, 9.0]\n",
      "      policy_policy2_reward: [-8.899999999999983, -5.5999999999999845, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -9.99999999999998, -4.499999999999987,\n",
      "        -2.2999999999999963, -8.89999999999998, -0.10000000000000331, -7.799999999999981,\n",
      "        -9.99999999999998, -8.899999999999986, -7.79999999999999, -7.799999999999981,\n",
      "        -2.3000000000000003, -7.79999999999999, -4.499999999999995, -8.89999999999998,\n",
      "        -5.6, -7.799999999999985, -9.99999999999998, -3.4, -8.89999999999998, -4.49999999999999,\n",
      "        -9.99999999999998, -8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999981, -7.799999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999992, -6.6999999999999815, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -3.399999999999985, -4.500000000000003, 4.300000000000005, -8.89999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -9.99999999999998, -7.799999999999981,\n",
      "        -4.5000000000000036, -7.799999999999981, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999983, -4.500000000000003, -9.99999999999998,\n",
      "        -8.899999999999984, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -6.699999999999986, -2.299999999999987, -9.99999999999998, -7.799999999999981,\n",
      "        -5.599999999999993, -7.799999999999981, -9.99999999999998, -2.3000000000000043,\n",
      "        -9.99999999999998, -4.499999999999987, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -7.799999999999989, -0.09999999999999254,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999987, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -8.89999999999998, -5.6, -7.799999999999984]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: 4.300000000000005\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.495\n",
      "      policy2: -7.854999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -11.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07309273096930355\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020902840759192128\n",
      "      mean_inference_ms: 5.55109663044737\n",
      "      mean_raw_obs_processing_ms: 0.4315483016420463\n",
      "  time_since_restore: 203.37558269500732\n",
      "  time_this_iter_s: 46.48669171333313\n",
      "  time_total_s: 203.37558269500732\n",
      "  timers:\n",
      "    learn_throughput: 140.664\n",
      "    learn_time_ms: 21327.423\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 40669.531\n",
      "  timestamp: 1660499965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:30 (running for 00:04:12.39)\n",
      "Memory usage on this node: 22.9/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-0.3599999999999899 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01C70>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CF6820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD670>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD9D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 |   -2.196 |                  6.11  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:35 (running for 00:04:17.51)\n",
      "Memory usage on this node: 22.9/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-0.3599999999999899 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01C70>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CF6820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD670>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD9D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 |   -2.196 |                  6.11  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:40 (running for 00:04:22.57)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-0.3599999999999899 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01C70>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CF6820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD670>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD9D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 |   -2.196 |                  6.11  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:45 (running for 00:04:27.63)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=-0.3599999999999899 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01C70>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CF6820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD670>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75CCD9D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DE50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      3 |          162.471 | 12000 |   -2.196 |                  6.11  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_19-59-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.800000000000015\n",
      "  episode_reward_mean: 0.2670000000000108\n",
      "  episode_reward_min: -30.000000000000018\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 847f41dbc3e943d8b61110c9b40c9844\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2762508392333984\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010753197595477104\n",
      "          model: {}\n",
      "          policy_loss: -0.03569629043340683\n",
      "          total_loss: 7.2173566818237305\n",
      "          vf_explained_var: 0.07034215331077576\n",
      "          vf_loss: 7.25090217590332\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2556859254837036\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01250769104808569\n",
      "          model: {}\n",
      "          policy_loss: -0.03790929168462753\n",
      "          total_loss: 2.3340108394622803\n",
      "          vf_explained_var: 0.25323811173439026\n",
      "          vf_loss: 2.3694188594818115\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.758823529411764\n",
      "    ram_util_percent: 71.44470588235295\n",
      "  pid: 14084\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -2.3000000000000025\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.32\n",
      "    policy2: -8.052999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -20.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0757440816146518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024282051480732206\n",
      "    mean_inference_ms: 5.42173449065226\n",
      "    mean_raw_obs_processing_ms: 0.43032882992800053\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.800000000000015\n",
      "    episode_reward_mean: 0.2670000000000108\n",
      "    episode_reward_min: -30.000000000000018\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-7.499999999999998, -30.000000000000018, -5.399999999999986,\n",
      "        -17.999999999999996, 5.700000000000031, -1.1999999999999793, 22.800000000000015,\n",
      "        5.100000000000026, -4.799999999999992, 7.500000000000011, -16.49999999999998,\n",
      "        12.300000000000013, -16.799999999999997, 9.600000000000026, -1.4999999999999845,\n",
      "        3.3000000000000074, -2.699999999999974, 4.191091917959966e-15, -2.9999999999999765,\n",
      "        -21.0, 8.40000000000001, 8.400000000000027, -9.899999999999977, -3.9000000000000035,\n",
      "        -14.399999999999977, 11.400000000000029, -7.199999999999976, 2.7000000000000184,\n",
      "        -4.499999999999989, 3.300000000000006, -10.199999999999976, 0.5999999999999864,\n",
      "        -14.699999999999978, 12.599999999999937, -2.0999999999999908, -2.099999999999981,\n",
      "        -8.399999999999988, 14.700000000000008, -8.999999999999984, 6.899999999999952,\n",
      "        -5.999999999999975, 3.000000000000019, -17.399999999999977, -2.999999999999983,\n",
      "        6.000000000000009, -1.1999999999999797, 5.100000000000011, 4.500000000000028,\n",
      "        6.000000000000028, 9.000000000000016, 0.600000000000022, 0.6000000000000215,\n",
      "        17.99999999999998, -1.4999999999999853, 8.100000000000023, 4.800000000000006,\n",
      "        13.799999999999997, 8.700000000000031, 6.00000000000003, 2.1000000000000174,\n",
      "        -0.29999999999997995, 1.8000000000000105, -2.999999999999986, -4.499999999999986,\n",
      "        -9.299999999999985, 3.8999999999999857, 18.29999999999995, -6.29999999999998,\n",
      "        3.60000000000001, 1.5000000000000167, 1.1999999999999829, -1.1999999999999922,\n",
      "        -4.799999999999994, 5.700000000000001, 18.000000000000025, -2.6999999999999846,\n",
      "        9.000000000000034, -2.700000000000022, 3.0000000000000275, -10.499999999999977,\n",
      "        17.099999999999923, 14.699999999999994, -10.499999999999984, 6.000000000000034,\n",
      "        4.200000000000033, -13.499999999999982, 19.199999999999974, -2.3999999999999835,\n",
      "        -1.799999999999974, -1.7999999999999758, -3.8999999999999906, 4.500000000000021,\n",
      "        1.5000000000000147, 3.000000000000005, -3.2999999999999767, -4.199999999999987,\n",
      "        -7.4999999999999805, 1.4999999999999676, -5.6999999999999815, 1.2000000000000215]\n",
      "      policy_policy1_reward: [2.5, -20.0, 3.5, -8.0, 13.5, 5.5, 29.5, 14.0, 3.0, 17.5,\n",
      "        -6.5, 19.0, -9.0, 18.5, 8.5, 10.0, 4.0, 10.0, 1.5, -11.0, 14.0, 14.0, -1.0,\n",
      "        5.0, -5.5, 17.0, -0.5, 10.5, 5.5, 10.0, -3.5, 4.0, -8.0, 21.5, 3.5, 3.5, 0.5,\n",
      "        22.5, 1.0, 12.5, 4.0, 7.5, -8.5, 1.5, 16.0, 5.5, 14.0, 14.5, 16.0, 19.0, 9.5,\n",
      "        9.5, 28.0, 8.5, 17.0, 11.5, 20.5, 16.5, 16.0, 11.0, 7.5, 8.5, 7.0, 5.5, -7.0,\n",
      "        9.5, 25.0, 1.5, 12.5, 11.5, 9.0, 5.5, 3.0, 13.5, 28.0, 4.0, 19.0, 4.0, 13.0,\n",
      "        -0.5, 26.0, 22.5, -0.5, 16.0, 12.0, -3.5, 27.0, 6.5, 6.0, 6.0, 5.0, 9.0, 11.5,\n",
      "        13.0, -1.0, 2.5, 2.5, 11.5, 1.0, 9.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999984, -6.6999999999999815, -6.69999999999999,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -7.799999999999981, -8.899999999999984, -9.99999999999998,\n",
      "        -6.6999999999999815, -6.6999999999999815, -9.99999999999998, -4.5, -9.99999999999998,\n",
      "        -5.599999999999999, -5.599999999999992, -8.899999999999986, -8.89999999999998,\n",
      "        -8.89999999999998, -5.599999999999991, -6.6999999999999895, -7.799999999999988,\n",
      "        -9.99999999999998, -6.6999999999999815, -6.699999999999995, -3.400000000000005,\n",
      "        -6.699999999999988, -8.89999999999998, -5.599999999999982, -5.6, -8.89999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -5.6, -9.99999999999998, -4.500000000000003,\n",
      "        -8.899999999999986, -4.499999999999984, -9.99999999999998, -6.699999999999992,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999993, -6.699999999999995, -7.799999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -2.300000000000004, -5.599999999999992,\n",
      "        -6.6999999999999815, -7.79999999999999, -8.899999999999986, -9.99999999999998,\n",
      "        -7.79999999999999, -6.699999999999985, -7.799999999999988, -7.7999999999999865,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -6.699999999999995,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -8.89999999999998, -4.4999999999999964, -9.99999999999998, -9.99999999999998,\n",
      "        -2.3000000000000025, -6.699999999999994, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999989, -7.79999999999999]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -2.3000000000000025\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.32\n",
      "      policy2: -8.052999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -20.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0757440816146518\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024282051480732206\n",
      "      mean_inference_ms: 5.42173449065226\n",
      "      mean_raw_obs_processing_ms: 0.43032882992800053\n",
      "  time_since_restore: 222.69407296180725\n",
      "  time_this_iter_s: 60.22286653518677\n",
      "  time_total_s: 222.69407296180725\n",
      "  timers:\n",
      "    learn_throughput: 131.003\n",
      "    learn_time_ms: 30533.764\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 55665.042\n",
      "  timestamp: 1660499987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 4205e_00002\n",
      "  warmup_time: 9.795740365982056\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:52 (running for 00:04:34.03)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=0.2670000000000108 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01E80>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33B20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 19:59:57 (running for 00:04:39.13)\n",
      "Memory usage on this node: 22.9/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=0.2670000000000108 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01E80>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33B20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:02 (running for 00:04:44.18)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=0.2670000000000108 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01E80>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33B20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:07 (running for 00:04:49.23)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=0.2670000000000108 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D01E80>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D33B20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4D8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      5 |          203.376 | 15000 |   -0.36  |                  7.495 |                 -7.855 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      5 |          202.293 | 15000 |   -1.098 |                  6.625 |                 -7.723 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-00-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999982\n",
      "  episode_reward_mean: 0.42000000000000876\n",
      "  episode_reward_min: -22.80000000000001\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1718103885650635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01622701808810234\n",
      "          model: {}\n",
      "          policy_loss: -0.05190224200487137\n",
      "          total_loss: 6.512868881225586\n",
      "          vf_explained_var: 0.12585222721099854\n",
      "          vf_loss: 6.557469367980957\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1772067546844482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615634188055992\n",
      "          model: {}\n",
      "          policy_loss: -0.05265524238348007\n",
      "          total_loss: 2.3095571994781494\n",
      "          vf_explained_var: 0.2726321816444397\n",
      "          vf_loss: 2.3549418449401855\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.772307692307688\n",
      "    ram_util_percent: 71.59384615384616\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: 7.599999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.835\n",
      "    policy2: -7.414999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07318236960733566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024042672828823424\n",
      "    mean_inference_ms: 5.898904929627391\n",
      "    mean_raw_obs_processing_ms: 0.43644183818178633\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999982\n",
      "    episode_reward_mean: 0.42000000000000876\n",
      "    episode_reward_min: -22.80000000000001\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.2999999999999785, -6.299999999999992, -2.6999999999999997,\n",
      "        -13.199999999999983, 9.520162436160717e-15, -7.499999999999984, -14.399999999999979,\n",
      "        8.700000000000026, 8.999999999999998, -8.999999999999986, -3.899999999999986,\n",
      "        17.999999999999922, 3.300000000000023, -19.499999999999996, -2.9999999999999805,\n",
      "        -2.999999999999981, -8.999999999999991, 6.600000000000017, -0.899999999999998,\n",
      "        -5.700000000000005, 16.199999999999967, -8.999999999999973, 12.000000000000028,\n",
      "        -2.999999999999978, 4.50000000000003, -14.099999999999989, 10.800000000000002,\n",
      "        -9.899999999999991, -1.1999999999999755, 12.00000000000003, -4.799999999999978,\n",
      "        0.3000000000000076, 8.100000000000033, -3.5999999999999943, 1.2000000000000142,\n",
      "        -8.999999999999988, -4.499999999999984, -3.899999999999981, 2.4000000000000177,\n",
      "        -22.80000000000001, -4.799999999999977, -0.8999999999999847, -14.099999999999989,\n",
      "        14.099999999999971, 11.400000000000029, 3.000000000000024, 11.999999999999973,\n",
      "        -2.999999999999985, -5.399999999999979, 4.200000000000022, 1.1999999999999686,\n",
      "        -4.799999999999976, -2.9999999999999765, 8.099999999999955, -2.6999999999999815,\n",
      "        -2.9999999999999964, 8.699999999999962, 12.300000000000026, 2.099999999999972,\n",
      "        -4.499999999999977, -1.5000000000000164, 6.900000000000016, 8.400000000000002,\n",
      "        16.49999999999995, -6.899999999999979, 1.2000000000000162, -5.09999999999998,\n",
      "        5.700000000000033, 4.500000000000034, 1.5000000000000149, -3.8999999999999857,\n",
      "        1.5000000000000138, 10.500000000000009, -4.5000000000000275, -1.5000000000000009,\n",
      "        -5.399999999999972, 3.000000000000024, 4.500000000000023, 12.300000000000013,\n",
      "        -0.8999999999999826, 22.499999999999982, -8.999999999999993, 6.599999999999989,\n",
      "        -0.9000000000000019, 7.5000000000000195, -3.2999999999999843, 4.800000000000022,\n",
      "        -8.399999999999977, 7.799999999999995, -5.39999999999999, -2.400000000000012,\n",
      "        6.000000000000032, -5.399999999999981, 0.30000000000001115, -19.499999999999986,\n",
      "        3.6000000000000116, 4.500000000000021, 5.6999999999999975, 10.200000000000015,\n",
      "        6.6]\n",
      "      policy_policy1_reward: [4.5, 1.5, 4.0, -6.5, 10.0, 2.5, -5.5, 16.5, 19.0, -10.0,\n",
      "        -0.5, 28.0, 10.0, -9.5, 7.0, 7.0, 1.0, 15.5, 8.0, 1.0, 24.0, 1.0, 22.0, 7.0,\n",
      "        14.5, -8.5, 17.5, -1.0, 0.0, 22.0, 3.0, 7.0, 17.0, 2.0, 9.0, 1.0, 5.5, 5.0,\n",
      "        8.0, -15.0, 3.0, 8.0, -8.5, 23.0, 17.0, 13.0, 16.5, 1.5, -2.0, 6.5, 9.0, 3.0,\n",
      "        7.0, 0.5, -1.5, 7.0, 16.5, 19.0, 11.0, 5.5, 8.5, 12.5, 8.5, 26.5, -3.5, 9.0,\n",
      "        0.5, 13.5, 14.5, 11.5, -6.0, 11.5, 20.5, 5.5, 8.5, 3.5, 13.0, 3.5, 13.5, 8.0,\n",
      "        32.5, 1.0, 15.5, 8.0, 17.5, 4.5, 11.5, 0.5, 14.5, 3.5, 6.5, 16.0, 3.5, 1.5,\n",
      "        -9.5, 12.5, 14.5, 13.5, 18.0, 15.5]\n",
      "      policy_policy2_reward: [-1.1999999999999975, -7.799999999999981, -6.699999999999995,\n",
      "        -6.699999999999991, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -7.799999999999984, -9.99999999999998, 0.9999999999999943, -3.399999999999996,\n",
      "        -9.99999999999998, -6.699999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -6.699999999999995, -7.799999999999989, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999999, -6.699999999999992,\n",
      "        -8.89999999999998, -1.2000000000000046, -9.99999999999998, -7.799999999999981,\n",
      "        -6.699999999999995, -8.899999999999986, -5.599999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -5.599999999999992,\n",
      "        -7.799999999999988, -7.799999999999989, -8.89999999999998, -5.599999999999982,\n",
      "        -8.89999999999998, -5.6, -9.99999999999998, -4.500000000000001, -4.499999999999997,\n",
      "        -3.400000000000003, -2.2999999999999954, -7.799999999999987, -7.799999999999981,\n",
      "        -9.99999999999998, 7.599999999999996, -1.1999999999999984, -9.99999999999998,\n",
      "        -7.799999999999986, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999992, -0.10000000000000275, -9.99999999999998,\n",
      "        -3.4000000000000052, -7.79999999999999, -5.599999999999991, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, 2.0999999999999943, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, 0.9999999999999961, -1.199999999999996, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999989, -6.6999999999999895, -8.89999999999998,\n",
      "        -6.699999999999995, -8.899999999999986, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -1.200000000000003, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -7.799999999999981, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: 7.599999999999996\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.835\n",
      "      policy2: -7.414999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -15.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07318236960733566\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024042672828823424\n",
      "      mean_inference_ms: 5.898904929627391\n",
      "      mean_raw_obs_processing_ms: 0.43644183818178633\n",
      "  time_since_restore: 248.2818741798401\n",
      "  time_this_iter_s: 45.9887011051178\n",
      "  time_total_s: 248.2818741798401\n",
      "  timers:\n",
      "    learn_throughput: 139.939\n",
      "    learn_time_ms: 21437.885\n",
      "    synch_weights_time_ms: 2.826\n",
      "    training_iteration_time_ms: 41373.83\n",
      "  timestamp: 1660500008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-00-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.39999999999994\n",
      "  episode_reward_mean: 0.48900000000000815\n",
      "  episode_reward_min: -21.000000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2420217990875244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012230738066136837\n",
      "          model: {}\n",
      "          policy_loss: -0.041222333908081055\n",
      "          total_loss: 6.844018459320068\n",
      "          vf_explained_var: 0.09545443952083588\n",
      "          vf_loss: 6.8827948570251465\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1859933137893677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014067724347114563\n",
      "          model: {}\n",
      "          policy_loss: -0.04256080463528633\n",
      "          total_loss: 3.3665194511413574\n",
      "          vf_explained_var: 0.15897367894649506\n",
      "          vf_loss: 3.4076733589172363\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.76923076923077\n",
      "    ram_util_percent: 71.59384615384616\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: 4.300000000000005\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.695\n",
      "    policy2: -7.205999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -19.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07390150578143366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02104817796312819\n",
      "    mean_inference_ms: 5.824755450951122\n",
      "    mean_raw_obs_processing_ms: 0.4331909327088618\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.39999999999994\n",
      "    episode_reward_mean: 0.48900000000000815\n",
      "    episode_reward_min: -21.000000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [2.100000000000023, 5.100000000000026, 10.499999999999947, 1.4999999999999782,\n",
      "        9.600000000000033, 1.500000000000027, -4.4999999999999805, -4.799999999999974,\n",
      "        14.699999999999998, -3.299999999999973, 1.5000000000000266, -2.9999999999999867,\n",
      "        0.5999999999999641, -2.999999999999984, -7.499999999999977, 10.800000000000033,\n",
      "        4.800000000000031, 2.7000000000000015, 6.600000000000014, 4.499999999999982,\n",
      "        11.100000000000007, -5.999999999999986, -16.5, -8.399999999999993, 1.5000000000000209,\n",
      "        -5.699999999999985, 12.599999999999955, 1.1296519275560968e-14, 16.199999999999907,\n",
      "        -6.000000000000017, 7.200000000000006, 7.500000000000018, 1.2000000000000113,\n",
      "        1.2000000000000224, -7.499999999999982, 0.600000000000015, -8.400000000000004,\n",
      "        16.49999999999995, 10.500000000000002, 2.1000000000000223, -1.4999999999999787,\n",
      "        3.0000000000000107, -4.799999999999979, -4.1999999999999815, -0.2999999999999855,\n",
      "        -8.999999999999973, -7.799999999999994, -8.100000000000001, -10.799999999999986,\n",
      "        -5.999999999999976, -4.799999999999981, 7.500000000000027, -14.999999999999973,\n",
      "        -0.8999999999999826, 7.743805596760467e-15, 2.450817326860033e-14, 17.39999999999994,\n",
      "        -0.29999999999999905, -11.099999999999994, -2.999999999999978, -9.89999999999998,\n",
      "        3.000000000000033, 16.499999999999964, 14.400000000000025, 3.000000000000014,\n",
      "        -2.999999999999989, -0.29999999999997196, -5.399999999999983, -5.099999999999977,\n",
      "        1.2000000000000077, -14.69999999999998, 4.200000000000028, -10.499999999999975,\n",
      "        -14.999999999999993, 0.900000000000028, 6.5999999999999925, 10.19999999999995,\n",
      "        17.099999999999916, 10.499999999999991, 9.90000000000002, -1.7999999999999963,\n",
      "        -4.199999999999985, 2.40000000000001, -1.799999999999991, -4.799999999999978,\n",
      "        -9.899999999999988, 11.100000000000032, 4.799999999999969, -17.999999999999993,\n",
      "        -1.799999999999986, 4.80000000000002, -21.000000000000007, 7.500000000000032,\n",
      "        -0.899999999999982, -2.0999999999999774, 3.0000000000000293, 5.400000000000025,\n",
      "        10.799999999999983, 3.6000000000000316, 7.800000000000026]\n",
      "      policy_policy1_reward: [5.5, 14.0, 15.0, 11.5, 18.5, 11.5, 5.5, 3.0, 22.5, 4.5,\n",
      "        11.5, 7.0, 9.5, 7.0, 2.5, 17.5, 11.5, 10.5, 15.5, 14.5, 20.0, 4.0, -6.5, -5.0,\n",
      "        6.0, -10.0, 21.5, 10.0, 24.0, 4.0, 15.0, 12.0, 9.0, 9.0, 2.5, 9.5, 0.5, 21.0,\n",
      "        20.5, 11.0, 8.5, 13.0, 3.0, 2.5, 2.0, 1.0, 0.0, -2.5, -3.0, 4.0, -2.5, 17.5,\n",
      "        -10.5, 8.0, 10.0, 10.0, 23.0, 7.5, -11.0, 7.0, -1.0, 13.0, 26.5, 20.0, 13.0,\n",
      "        7.0, 7.5, 3.5, 0.5, 9.0, -19.0, 12.0, -0.5, -5.0, 1.0, 10.0, 7.0, 26.0, 15.0,\n",
      "        10.0, 6.0, 2.5, 8.0, 6.0, 3.0, -1.0, 20.0, 11.5, -8.0, 6.0, 6.0, -11.0, 17.5,\n",
      "        8.0, -2.0, 13.0, 11.0, 6.5, 12.5, 14.5]\n",
      "      policy_policy2_reward: [-3.4, -8.89999999999998, -4.49999999999999, -9.99999999999998,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999981, -7.799999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -6.699999999999992,\n",
      "        -6.6999999999999815, -7.799999999999981, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -3.399999999999985,\n",
      "        -4.500000000000003, 4.300000000000005, -8.89999999999998, -9.99999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -7.799999999999981, -4.5000000000000036,\n",
      "        -7.799999999999981, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -8.899999999999983, -4.500000000000003, -9.99999999999998, -8.899999999999984,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -6.699999999999986,\n",
      "        -2.299999999999987, -9.99999999999998, -7.799999999999981, -5.599999999999993,\n",
      "        -7.799999999999981, -9.99999999999998, -2.3000000000000043, -9.99999999999998,\n",
      "        -4.499999999999987, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -7.799999999999989, -0.09999999999999254, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999987,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -8.89999999999998,\n",
      "        -5.6, -7.799999999999984, 4.3000000000000025, -7.799999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -0.10000000000000331, -3.4000000000000044, 3.200000000000001,\n",
      "        -8.89999999999998, -4.5000000000000036, -0.10000000000000508, -7.799999999999981,\n",
      "        -6.6999999999999815, -5.599999999999982, -7.799999999999981, -7.799999999999981,\n",
      "        -8.89999999999998, -8.89999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -7.799999999999981, -1.1999999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -0.10000000000000153, -9.99999999999998, -5.599999999999982,\n",
      "        4.299999999999998, -8.89999999999998, -6.699999999999994]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: 4.300000000000005\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.695\n",
      "      policy2: -7.205999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -19.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07390150578143366\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02104817796312819\n",
      "      mean_inference_ms: 5.824755450951122\n",
      "      mean_raw_obs_processing_ms: 0.4331909327088618\n",
      "  time_since_restore: 249.52086329460144\n",
      "  time_this_iter_s: 46.145280599594116\n",
      "  time_total_s: 249.52086329460144\n",
      "  timers:\n",
      "    learn_throughput: 137.461\n",
      "    learn_time_ms: 21824.323\n",
      "    synch_weights_time_ms: 2.825\n",
      "    training_iteration_time_ms: 41580.993\n",
      "  timestamp: 1660500011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:16 (running for 00:04:58.62)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=0.48900000000000815 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D335B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      6 |          249.521 | 18000 |    0.489 |                  7.695 |                 -7.206 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      6 |          248.282 | 18000 |    0.42  |                  7.835 |                 -7.415 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:21 (running for 00:05:03.74)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=0.48900000000000815 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D335B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      6 |          249.521 | 18000 |    0.489 |                  7.695 |                 -7.206 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      6 |          248.282 | 18000 |    0.42  |                  7.835 |                 -7.415 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      3 |          184.312 | 12000 |   -3.798 |                  4.42  |                 -8.218 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-00-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999986\n",
      "  episode_reward_mean: -0.43499999999998834\n",
      "  episode_reward_min: -21.60000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: d9b180bb1fb4403ba3d13246b5f1de5e\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2477213144302368\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02047351747751236\n",
      "          model: {}\n",
      "          policy_loss: -0.05338127911090851\n",
      "          total_loss: 6.594135761260986\n",
      "          vf_explained_var: 0.10253830254077911\n",
      "          vf_loss: 6.641374111175537\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2527275085449219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017177434638142586\n",
      "          model: {}\n",
      "          policy_loss: -0.04989699274301529\n",
      "          total_loss: 1.850469946861267\n",
      "          vf_explained_var: 0.37728819251060486\n",
      "          vf_loss: 1.8952137231826782\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.921839080459772\n",
      "    ram_util_percent: 71.57586206896555\n",
      "  pid: 13928\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -1.2000000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.805\n",
      "    policy2: -8.239999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0759517854807151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02404898782047068\n",
      "    mean_inference_ms: 6.722058149508912\n",
      "    mean_raw_obs_processing_ms: 0.44426602904785445\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999986\n",
      "    episode_reward_mean: -0.43499999999998834\n",
      "    episode_reward_min: -21.60000000000001\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-2.9999999999999822, -19.200000000000003, -10.800000000000006,\n",
      "        2.6999999999999997, -21.60000000000001, -3.2999999999999763, -1.1999999999999933,\n",
      "        -2.999999999999991, 8.100000000000025, -14.099999999999982, -4.499999999999991,\n",
      "        2.1000000000000214, -16.799999999999976, -4.499999999999975, 3.0000000000000044,\n",
      "        -19.499999999999993, 9.600000000000017, 6.000000000000022, -4.19999999999999,\n",
      "        1.2000000000000224, 19.499999999999986, -13.799999999999992, -1.4999999999999862,\n",
      "        3.299999999999968, 5.100000000000028, 10.50000000000002, 13.799999999999969,\n",
      "        -0.29999999999999305, 1.7069679003611782e-14, 6.000000000000028, -2.999999999999977,\n",
      "        9.520162436160717e-15, 7.799999999999974, -1.4999999999999853, -3.299999999999978,\n",
      "        7.500000000000028, 2.400000000000016, -8.399999999999995, 9.300000000000033,\n",
      "        0.30000000000002003, -13.499999999999982, 2.700000000000021, -5.999999999999994,\n",
      "        9.600000000000021, -4.499999999999993, 1.1629586182948515e-14, 0.6000000000000291,\n",
      "        14.09999999999998, 3.0000000000000044, -7.799999999999978, -6.299999999999979,\n",
      "        -1.8457457784393227e-14, 2.683964162031316e-14, 14.400000000000023, -4.499999999999987,\n",
      "        -2.9999999999999774, -11.399999999999983, -2.9999999999999973, -8.399999999999983,\n",
      "        9.000000000000004, 2.100000000000028, -7.799999999999981, 7.200000000000026,\n",
      "        17.399999999999956, -12.299999999999978, 5.7000000000000135, -10.199999999999973,\n",
      "        -3.8999999999999813, 7.199999999999989, -8.999999999999995, -3.8999999999999955,\n",
      "        -3.5999999999999748, 3.30000000000003, -7.499999999999977, -8.999999999999982,\n",
      "        6.000000000000009, 3.6000000000000134, -2.6999999999999824, -1.1685097334179773e-14,\n",
      "        -3.0, -0.899999999999991, 11.999999999999984, 5.700000000000028, -5.69999999999999,\n",
      "        18.000000000000007, 7.500000000000011, -2.399999999999996, 8.10000000000003,\n",
      "        -0.2999999999999764, 5.100000000000021, -1.499999999999977, 1.7069679003611782e-14,\n",
      "        9.00000000000003, -4.499999999999984, -7.500000000000021, 9.000000000000016,\n",
      "        1.5000000000000115, -11.999999999999995, -10.499999999999973, 0.5999999999999833]\n",
      "      policy_policy1_reward: [7.0, -12.5, -3.0, 10.5, -16.0, 4.5, 5.5, 7.0, 17.0, -8.5,\n",
      "        5.5, 11.0, -9.0, 5.5, 13.0, -9.5, 18.5, 16.0, 2.5, 9.0, 29.5, -6.0, 8.5, 10.0,\n",
      "        14.0, 20.5, 20.5, 7.5, 10.0, 10.5, 7.0, 10.0, 9.0, 3.0, 4.5, 17.5, 8.0, 0.5,\n",
      "        16.0, 7.0, -3.5, 10.5, 4.0, 18.5, 5.5, 10.0, 9.5, 17.5, 13.0, -5.5, -4.0, 10.0,\n",
      "        10.0, 20.0, 5.5, 7.0, -2.5, 1.5, -5.0, 19.0, 11.0, 0.0, 15.0, 23.0, -4.5, 13.5,\n",
      "        -3.5, 5.0, 15.0, 1.0, 5.0, 2.0, 10.0, 2.5, 1.0, 16.0, 12.5, 4.0, 10.0, 7.0,\n",
      "        8.0, 22.0, 13.5, 1.0, 28.0, 17.5, 6.5, 17.0, 7.5, 14.0, 3.0, 10.0, 19.0, 5.5,\n",
      "        2.5, 19.0, 11.5, -2.0, -0.5, 9.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -6.699999999999986, -7.799999999999984,\n",
      "        -7.799999999999987, -5.599999999999995, -7.799999999999986, -6.699999999999992,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -9.99999999999998,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -7.799999999999989, -9.99999999999998, -7.799999999999988, -9.99999999999998,\n",
      "        -6.699999999999995, -8.89999999999998, -9.99999999999998, -6.699999999999988,\n",
      "        -7.799999999999981, -9.99999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -1.2000000000000015, -4.499999999999986, -7.799999999999981,\n",
      "        -9.99999999999998, -5.599999999999996, -8.89999999999998, -6.699999999999989,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -3.3999999999999964, -9.99999999999998, -2.2999999999999914, -2.300000000000003,\n",
      "        -9.99999999999998, -9.99999999999998, -5.59999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -4.500000000000002, -3.39999999999999,\n",
      "        -9.99999999999998, -8.899999999999986, -7.799999999999981, -7.7999999999999865,\n",
      "        -5.599999999999991, -7.799999999999981, -7.799999999999986, -6.6999999999999815,\n",
      "        -8.89999999999998, -7.79999999999999, -9.99999999999998, -8.89999999999998,\n",
      "        -5.599999999999984, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -6.699999999999982, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -8.89999999999998, -4.499999999999997,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -1.2000000000000015\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.805\n",
      "      policy2: -8.239999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -16.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0759517854807151\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02404898782047068\n",
      "      mean_inference_ms: 6.722058149508912\n",
      "      mean_raw_obs_processing_ms: 0.44426602904785445\n",
      "  time_since_restore: 245.6776864528656\n",
      "  time_this_iter_s: 61.365472078323364\n",
      "  time_total_s: 245.6776864528656\n",
      "  timers:\n",
      "    learn_throughput: 123.735\n",
      "    learn_time_ms: 32327.211\n",
      "    synch_weights_time_ms: 2.991\n",
      "    training_iteration_time_ms: 61414.435\n",
      "  timestamp: 1660500025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 4205e_00003\n",
      "  warmup_time: 9.807717561721802\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:30 (running for 00:05:12.41)\n",
      "Memory usage on this node: 22.9/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=0.48900000000000815 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D335B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      6 |          249.521 | 18000 |    0.489 |                  7.695 |                 -7.206 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      6 |          248.282 | 18000 |    0.42  |                  7.835 |                 -7.415 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:35 (running for 00:05:17.50)\n",
      "Memory usage on this node: 23.0/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=0.48900000000000815 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D335B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      6 |          249.521 | 18000 |    0.489 |                  7.695 |                 -7.206 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      6 |          248.282 | 18000 |    0.42  |                  7.835 |                 -7.415 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:40 (running for 00:05:22.54)\n",
      "Memory usage on this node: 23.0/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=0.48900000000000815 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D335B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      6 |          249.521 | 18000 |    0.489 |                  7.695 |                 -7.206 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      6 |          248.282 | 18000 |    0.42  |                  7.835 |                 -7.415 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:45 (running for 00:05:27.60)\n",
      "Memory usage on this node: 23.1/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=0.48900000000000815 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D335B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D45310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DB3EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      6 |          249.521 | 18000 |    0.489 |                  7.695 |                 -7.206 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      6 |          248.282 | 18000 |    0.42  |                  7.835 |                 -7.415 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      4 |          222.694 | 16000 |    0.267 |                  8.32  |                 -8.053 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999937\n",
      "  episode_reward_mean: 2.310000000000011\n",
      "  episode_reward_min: -17.399999999999977\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 847f41dbc3e943d8b61110c9b40c9844\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2434227466583252\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012362649664282799\n",
      "          model: {}\n",
      "          policy_loss: -0.038350071758031845\n",
      "          total_loss: 6.492027759552002\n",
      "          vf_explained_var: 0.1948428452014923\n",
      "          vf_loss: 6.527905464172363\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2310221195220947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01098576094955206\n",
      "          model: {}\n",
      "          policy_loss: -0.037296246737241745\n",
      "          total_loss: 2.50433611869812\n",
      "          vf_explained_var: 0.25575339794158936\n",
      "          vf_loss: 2.5394349098205566\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.50116279069767\n",
      "    ram_util_percent: 71.73488372093024\n",
      "  pid: 14084\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: -2.2999999999999905\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.495\n",
      "    policy2: -8.184999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -8.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07498356252268704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024237438506034943\n",
      "    mean_inference_ms: 5.746716959251367\n",
      "    mean_raw_obs_processing_ms: 0.4328274795242436\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.099999999999937\n",
      "    episode_reward_mean: 2.310000000000011\n",
      "    episode_reward_min: -17.399999999999977\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-5.999999999999975, 3.000000000000019, -17.399999999999977, -2.999999999999983,\n",
      "        6.000000000000009, -1.1999999999999797, 5.100000000000011, 4.500000000000028,\n",
      "        6.000000000000028, 9.000000000000016, 0.600000000000022, 0.6000000000000215,\n",
      "        17.99999999999998, -1.4999999999999853, 8.100000000000023, 4.800000000000006,\n",
      "        13.799999999999997, 8.700000000000031, 6.00000000000003, 2.1000000000000174,\n",
      "        -0.29999999999997995, 1.8000000000000105, -2.999999999999986, -4.499999999999986,\n",
      "        -9.299999999999985, 3.8999999999999857, 18.29999999999995, -6.29999999999998,\n",
      "        3.60000000000001, 1.5000000000000167, 1.1999999999999829, -1.1999999999999922,\n",
      "        -4.799999999999994, 5.700000000000001, 18.000000000000025, -2.6999999999999846,\n",
      "        9.000000000000034, -2.700000000000022, 3.0000000000000275, -10.499999999999977,\n",
      "        17.099999999999923, 14.699999999999994, -10.499999999999984, 6.000000000000034,\n",
      "        4.200000000000033, -13.499999999999982, 19.199999999999974, -2.3999999999999835,\n",
      "        -1.799999999999974, -1.7999999999999758, -3.8999999999999906, 4.500000000000021,\n",
      "        1.5000000000000147, 3.000000000000005, -3.2999999999999767, -4.199999999999987,\n",
      "        -7.4999999999999805, 1.4999999999999676, -5.6999999999999815, 1.2000000000000215,\n",
      "        9.000000000000004, 3.000000000000013, 4.200000000000015, 16.79999999999997,\n",
      "        9.000000000000032, 12.599999999999994, 14.699999999999987, 0.6000000000000245,\n",
      "        -14.999999999999984, -7.4999999999999805, 8.69999999999999, -7.499999999999991,\n",
      "        4.200000000000024, -4.499999999999984, 3.600000000000015, -2.6999999999999824,\n",
      "        14.999999999999996, -11.999999999999977, 9.600000000000016, 1.5000000000000084,\n",
      "        3.000000000000008, 4.800000000000013, 5.700000000000026, 9.60000000000003, 6.000000000000032,\n",
      "        2.700000000000028, 4.5000000000000195, 4.5000000000000195, -8.399999999999975,\n",
      "        2.100000000000029, 2.1000000000000085, 3.0000000000000107, -2.999999999999982,\n",
      "        23.099999999999937, 1.4999999999999765, -6.899999999999974, -0.2999999999999856,\n",
      "        3.9000000000000212, -4.499999999999981, 8.10000000000003]\n",
      "      policy_policy1_reward: [4.0, 7.5, -8.5, 1.5, 16.0, 5.5, 14.0, 14.5, 16.0, 19.0,\n",
      "        9.5, 9.5, 28.0, 8.5, 17.0, 11.5, 20.5, 16.5, 16.0, 11.0, 7.5, 8.5, 7.0, 5.5,\n",
      "        -7.0, 9.5, 25.0, 1.5, 12.5, 11.5, 9.0, 5.5, 3.0, 13.5, 28.0, 4.0, 19.0, 4.0,\n",
      "        13.0, -0.5, 26.0, 22.5, -0.5, 16.0, 12.0, -3.5, 27.0, 6.5, 6.0, 6.0, 5.0, 9.0,\n",
      "        11.5, 13.0, -1.0, 2.5, 2.5, 11.5, 1.0, 9.0, 19.0, 13.0, 12.0, 23.5, 19.0, 21.5,\n",
      "        22.5, 9.5, -5.0, 2.5, 16.5, 2.5, 12.0, 5.5, 12.5, 4.0, 25.0, -2.0, 13.0, 11.5,\n",
      "        13.0, 11.5, 8.0, 13.0, 10.5, 5.0, 14.5, 14.5, 0.5, 5.5, 11.0, 13.0, 7.0, 32.0,\n",
      "        11.5, 2.0, 7.5, 9.5, 5.5, 17.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -4.500000000000003, -8.899999999999986,\n",
      "        -4.499999999999984, -9.99999999999998, -6.699999999999992, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -6.699999999999993, -6.699999999999995, -7.799999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -2.300000000000004, -5.599999999999992, -6.6999999999999815,\n",
      "        -7.79999999999999, -8.899999999999986, -9.99999999999998, -7.79999999999999,\n",
      "        -6.699999999999985, -7.799999999999988, -7.7999999999999865, -9.99999999999998,\n",
      "        -6.699999999999993, -9.99999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -7.799999999999981, -7.799999999999981, -8.89999999999998,\n",
      "        -4.4999999999999964, -9.99999999999998, -9.99999999999998, -2.3000000000000025,\n",
      "        -6.699999999999994, -9.99999999999998, -9.99999999999998, -6.699999999999989,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -7.799999999999987,\n",
      "        -6.6999999999999815, -9.99999999999998, -8.89999999999998, -7.799999999999986,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -6.69999999999999, -9.99999999999998, -9.99999999999998, -3.4000000000000052,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999995, -2.2999999999999905,\n",
      "        -3.3999999999999955, -4.499999999999999, -2.3000000000000034, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -3.3999999999999932, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -5.6, -9.99999999999998, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.0\n",
      "      policy2: -2.2999999999999905\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.495\n",
      "      policy2: -8.184999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -8.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07498356252268704\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024237438506034943\n",
      "      mean_inference_ms: 5.746716959251367\n",
      "      mean_raw_obs_processing_ms: 0.4328274795242436\n",
      "  time_since_restore: 283.782089471817\n",
      "  time_this_iter_s: 61.088016510009766\n",
      "  time_total_s: 283.782089471817\n",
      "  timers:\n",
      "    learn_throughput: 129.565\n",
      "    learn_time_ms: 30872.491\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 56747.642\n",
      "  timestamp: 1660500048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 4205e_00002\n",
      "  warmup_time: 9.795740365982056\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:53 (running for 00:05:35.16)\n",
      "Memory usage on this node: 23.1/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      6 |          249.521 | 18000 |    0.489 |                  7.695 |                 -7.206 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      6 |          248.282 | 18000 |    0.42  |                  7.835 |                 -7.415 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 42000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999982\n",
      "  episode_reward_mean: 1.2540000000000107\n",
      "  episode_reward_min: -22.80000000000001\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 210\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1533989906311035\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01607499271631241\n",
      "          model: {}\n",
      "          policy_loss: -0.05217404291033745\n",
      "          total_loss: 6.11376428604126\n",
      "          vf_explained_var: 0.19382721185684204\n",
      "          vf_loss: 6.1587042808532715\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1384400129318237\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01773151010274887\n",
      "          model: {}\n",
      "          policy_loss: -0.05636453628540039\n",
      "          total_loss: 2.728196144104004\n",
      "          vf_explained_var: 0.2058570683002472\n",
      "          vf_loss: 2.7765815258026123\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 42000\n",
      "  num_agent_steps_trained: 42000\n",
      "  num_env_steps_sampled: 21000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 21000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 33.32575757575757\n",
      "    ram_util_percent: 71.8969696969697\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: 7.599999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.185\n",
      "    policy2: -6.930999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07384581538918246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023972463016904356\n",
      "    mean_inference_ms: 6.05492565163095\n",
      "    mean_raw_obs_processing_ms: 0.43691568819047144\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999982\n",
      "    episode_reward_mean: 1.2540000000000107\n",
      "    episode_reward_min: -22.80000000000001\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-4.799999999999978, 0.3000000000000076, 8.100000000000033, -3.5999999999999943,\n",
      "        1.2000000000000142, -8.999999999999988, -4.499999999999984, -3.899999999999981,\n",
      "        2.4000000000000177, -22.80000000000001, -4.799999999999977, -0.8999999999999847,\n",
      "        -14.099999999999989, 14.099999999999971, 11.400000000000029, 3.000000000000024,\n",
      "        11.999999999999973, -2.999999999999985, -5.399999999999979, 4.200000000000022,\n",
      "        1.1999999999999686, -4.799999999999976, -2.9999999999999765, 8.099999999999955,\n",
      "        -2.6999999999999815, -2.9999999999999964, 8.699999999999962, 12.300000000000026,\n",
      "        2.099999999999972, -4.499999999999977, -1.5000000000000164, 6.900000000000016,\n",
      "        8.400000000000002, 16.49999999999995, -6.899999999999979, 1.2000000000000162,\n",
      "        -5.09999999999998, 5.700000000000033, 4.500000000000034, 1.5000000000000149,\n",
      "        -3.8999999999999857, 1.5000000000000138, 10.500000000000009, -4.5000000000000275,\n",
      "        -1.5000000000000009, -5.399999999999972, 3.000000000000024, 4.500000000000023,\n",
      "        12.300000000000013, -0.8999999999999826, 22.499999999999982, -8.999999999999993,\n",
      "        6.599999999999989, -0.9000000000000019, 7.5000000000000195, -3.2999999999999843,\n",
      "        4.800000000000022, -8.399999999999977, 7.799999999999995, -5.39999999999999,\n",
      "        -2.400000000000012, 6.000000000000032, -5.399999999999981, 0.30000000000001115,\n",
      "        -19.499999999999986, 3.6000000000000116, 4.500000000000021, 5.6999999999999975,\n",
      "        10.200000000000015, 6.6, -5.699999999999976, 4.200000000000032, 9.300000000000026,\n",
      "        -8.399999999999983, 1.7999999999999865, -3.300000000000003, -1.4999999999999774,\n",
      "        -2.9999999999999782, 1.2000000000000073, 5.523359547510154e-15, -4.799999999999994,\n",
      "        4.500000000000027, -0.899999999999993, -0.29999999999998284, 2.100000000000021,\n",
      "        6.600000000000019, 7.500000000000027, 12.300000000000024, 0.6000000000000134,\n",
      "        13.500000000000002, 9.300000000000018, 3.0000000000000258, -7.199999999999992,\n",
      "        -3.8999999999999853, 2.4000000000000283, 3.600000000000015, 7.80000000000002,\n",
      "        6.3000000000000185, -2.100000000000004, -2.399999999999987]\n",
      "      policy_policy1_reward: [3.0, 7.0, 17.0, 2.0, 9.0, 1.0, 5.5, 5.0, 8.0, -15.0, 3.0,\n",
      "        8.0, -8.5, 23.0, 17.0, 13.0, 16.5, 1.5, -2.0, 6.5, 9.0, 3.0, 7.0, 0.5, -1.5,\n",
      "        7.0, 16.5, 19.0, 11.0, 5.5, 8.5, 12.5, 8.5, 26.5, -3.5, 9.0, 0.5, 13.5, 14.5,\n",
      "        11.5, -6.0, 11.5, 20.5, 5.5, 8.5, 3.5, 13.0, 3.5, 13.5, 8.0, 32.5, 1.0, 15.5,\n",
      "        8.0, 17.5, 4.5, 11.5, 0.5, 14.5, 3.5, 6.5, 16.0, 3.5, 1.5, -9.5, 12.5, 14.5,\n",
      "        13.5, 18.0, 15.5, 1.0, 12.0, 16.0, -5.0, 8.5, 4.5, 8.5, 1.5, 9.0, 4.5, -2.5,\n",
      "        14.5, 2.5, 7.5, 11.0, 10.0, 6.5, 19.0, 9.5, 23.5, 16.0, 7.5, -0.5, 5.0, 8.0,\n",
      "        12.5, 9.0, 13.0, -2.0, 6.5]\n",
      "      policy_policy2_reward: [-7.799999999999981, -6.699999999999995, -8.899999999999986,\n",
      "        -5.599999999999999, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -5.599999999999992, -7.799999999999988, -7.799999999999989,\n",
      "        -8.89999999999998, -5.599999999999982, -8.89999999999998, -5.6, -9.99999999999998,\n",
      "        -4.500000000000001, -4.499999999999997, -3.400000000000003, -2.2999999999999954,\n",
      "        -7.799999999999987, -7.799999999999981, -9.99999999999998, 7.599999999999996,\n",
      "        -1.1999999999999984, -9.99999999999998, -7.799999999999986, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999992,\n",
      "        -0.10000000000000275, -9.99999999999998, -3.4000000000000052, -7.79999999999999,\n",
      "        -5.599999999999991, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        2.0999999999999943, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -9.99999999999998, 0.9999999999999961,\n",
      "        -1.199999999999996, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.89999999999998, -9.99999999999998, -7.799999999999989,\n",
      "        -6.6999999999999895, -8.89999999999998, -6.699999999999995, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -1.200000000000003,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999981, -8.89999999999998, -6.6999999999999815, -7.799999999999989,\n",
      "        -6.699999999999992, -3.4000000000000057, -6.699999999999982, -7.799999999999985,\n",
      "        -9.99999999999998, -4.5000000000000036, -7.799999999999988, -4.499999999999982,\n",
      "        -2.2999999999999847, -9.99999999999998, -3.399999999999988, -7.799999999999981,\n",
      "        -8.899999999999986, -3.4000000000000017, 0.9999999999999943, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -6.699999999999995, -4.499999999999992,\n",
      "        -6.699999999999983, -8.89999999999998, -5.6, -8.89999999999998, -1.2, -6.699999999999989,\n",
      "        -0.10000000000000464, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: 7.599999999999996\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.185\n",
      "      policy2: -6.930999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -15.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07384581538918246\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023972463016904356\n",
      "      mean_inference_ms: 6.05492565163095\n",
      "      mean_raw_obs_processing_ms: 0.43691568819047144\n",
      "  time_since_restore: 295.01338386535645\n",
      "  time_this_iter_s: 46.73150968551636\n",
      "  time_total_s: 295.01338386535645\n",
      "  timers:\n",
      "    learn_throughput: 137.22\n",
      "    learn_time_ms: 21862.694\n",
      "    synch_weights_time_ms: 2.849\n",
      "    training_iteration_time_ms: 42138.358\n",
      "  timestamp: 1660500055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 7\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.999999999999947\n",
      "  episode_reward_mean: 1.2930000000000057\n",
      "  episode_reward_min: -21.000000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 210\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.198833703994751\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010744818486273289\n",
      "          model: {}\n",
      "          policy_loss: -0.03824921324849129\n",
      "          total_loss: 6.523116111755371\n",
      "          vf_explained_var: 0.09412984549999237\n",
      "          vf_loss: 6.559216499328613\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1132817268371582\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013670981861650944\n",
      "          model: {}\n",
      "          policy_loss: -0.04550008103251457\n",
      "          total_loss: 2.6673977375030518\n",
      "          vf_explained_var: 0.32340362668037415\n",
      "          vf_loss: 2.711530923843384\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 42000\n",
      "  num_agent_steps_trained: 42000\n",
      "  num_env_steps_sampled: 21000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 21000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 33.48358208955224\n",
      "    ram_util_percent: 71.95522388059699\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: 4.3000000000000025\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.345\n",
      "    policy2: -7.051999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -19.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07469184995306762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02121122823422125\n",
      "    mean_inference_ms: 5.993561102109602\n",
      "    mean_raw_obs_processing_ms: 0.43489694753454927\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.999999999999947\n",
      "    episode_reward_mean: 1.2930000000000057\n",
      "    episode_reward_min: -21.000000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [7.200000000000006, 7.500000000000018, 1.2000000000000113, 1.2000000000000224,\n",
      "        -7.499999999999982, 0.600000000000015, -8.400000000000004, 16.49999999999995,\n",
      "        10.500000000000002, 2.1000000000000223, -1.4999999999999787, 3.0000000000000107,\n",
      "        -4.799999999999979, -4.1999999999999815, -0.2999999999999855, -8.999999999999973,\n",
      "        -7.799999999999994, -8.100000000000001, -10.799999999999986, -5.999999999999976,\n",
      "        -4.799999999999981, 7.500000000000027, -14.999999999999973, -0.8999999999999826,\n",
      "        7.743805596760467e-15, 2.450817326860033e-14, 17.39999999999994, -0.29999999999999905,\n",
      "        -11.099999999999994, -2.999999999999978, -9.89999999999998, 3.000000000000033,\n",
      "        16.499999999999964, 14.400000000000025, 3.000000000000014, -2.999999999999989,\n",
      "        -0.29999999999997196, -5.399999999999983, -5.099999999999977, 1.2000000000000077,\n",
      "        -14.69999999999998, 4.200000000000028, -10.499999999999975, -14.999999999999993,\n",
      "        0.900000000000028, 6.5999999999999925, 10.19999999999995, 17.099999999999916,\n",
      "        10.499999999999991, 9.90000000000002, -1.7999999999999963, -4.199999999999985,\n",
      "        2.40000000000001, -1.799999999999991, -4.799999999999978, -9.899999999999988,\n",
      "        11.100000000000032, 4.799999999999969, -17.999999999999993, -1.799999999999986,\n",
      "        4.80000000000002, -21.000000000000007, 7.500000000000032, -0.899999999999982,\n",
      "        -2.0999999999999774, 3.0000000000000293, 5.400000000000025, 10.799999999999983,\n",
      "        3.6000000000000316, 7.800000000000026, 16.199999999999903, 6.000000000000017,\n",
      "        11.99999999999994, 10.499999999999991, 6.9000000000000306, 17.999999999999947,\n",
      "        1.8000000000000256, -3.2999999999999847, -0.3000000000000016, 17.99999999999994,\n",
      "        6.000000000000023, -5.999999999999976, -16.499999999999986, 14.99999999999997,\n",
      "        10.199999999999942, -4.499999999999988, 17.999999999999947, -4.799999999999976,\n",
      "        3.0000000000000293, -3.0000000000000013, -2.699999999999972, 1.2000000000000297,\n",
      "        2.7000000000000277, -6.899999999999988, 10.800000000000031, 0.600000000000006,\n",
      "        -1.4999999999999867, 7.500000000000014, 16.199999999999932, -1.5000000000000029]\n",
      "      policy_policy1_reward: [15.0, 12.0, 9.0, 9.0, 2.5, 9.5, 0.5, 21.0, 20.5, 11.0,\n",
      "        8.5, 13.0, 3.0, 2.5, 2.0, 1.0, 0.0, -2.5, -3.0, 4.0, -2.5, 17.5, -10.5, 8.0,\n",
      "        10.0, 10.0, 23.0, 7.5, -11.0, 7.0, -1.0, 13.0, 26.5, 20.0, 13.0, 7.0, 7.5, 3.5,\n",
      "        0.5, 9.0, -19.0, 12.0, -0.5, -5.0, 1.0, 10.0, 7.0, 26.0, 15.0, 10.0, 6.0, 2.5,\n",
      "        8.0, 6.0, 3.0, -1.0, 20.0, 11.5, -8.0, 6.0, 6.0, -11.0, 17.5, 8.0, -2.0, 13.0,\n",
      "        11.0, 6.5, 12.5, 14.5, 24.0, 16.0, 22.0, 20.5, 12.5, 28.0, 8.5, -1.0, 7.5, 28.0,\n",
      "        16.0, -1.5, -6.5, 25.0, 12.5, 5.5, 28.0, 3.0, 13.0, 7.0, 4.0, 9.0, 10.5, -3.5,\n",
      "        12.0, 4.0, 8.5, 17.5, 18.5, 3.0]\n",
      "      policy_policy2_reward: [-7.799999999999981, -4.5000000000000036, -7.799999999999981,\n",
      "        -7.799999999999981, -9.99999999999998, -8.89999999999998, -8.899999999999983,\n",
      "        -4.500000000000003, -9.99999999999998, -8.899999999999984, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -6.699999999999986, -2.299999999999987,\n",
      "        -9.99999999999998, -7.799999999999981, -5.599999999999993, -7.799999999999981,\n",
      "        -9.99999999999998, -2.3000000000000043, -9.99999999999998, -4.499999999999987,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -7.799999999999989, -0.09999999999999254, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999987, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -8.89999999999998, -5.6, -7.799999999999984,\n",
      "        4.3000000000000025, -7.799999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -0.10000000000000331, -3.4000000000000044, 3.200000000000001, -8.89999999999998,\n",
      "        -4.5000000000000036, -0.10000000000000508, -7.799999999999981, -6.6999999999999815,\n",
      "        -5.599999999999982, -7.799999999999981, -7.799999999999981, -8.89999999999998,\n",
      "        -8.89999999999998, -6.699999999999995, -9.99999999999998, -7.799999999999981,\n",
      "        -1.1999999999999982, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -0.10000000000000153, -9.99999999999998, -5.599999999999982, 4.299999999999998,\n",
      "        -8.89999999999998, -6.699999999999994, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999995, -9.99999999999998,\n",
      "        -6.699999999999982, -2.3000000000000043, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -4.499999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -2.300000000000001, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -7.799999999999986,\n",
      "        -7.799999999999981, -3.399999999999988, -1.1999999999999984, -3.3999999999999972,\n",
      "        -9.99999999999998, -9.99999999999998, -2.3000000000000043, -4.499999999999994]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: 4.3000000000000025\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.345\n",
      "      policy2: -7.051999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -19.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07469184995306762\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02121122823422125\n",
      "      mean_inference_ms: 5.993561102109602\n",
      "      mean_raw_obs_processing_ms: 0.43489694753454927\n",
      "  time_since_restore: 296.6084179878235\n",
      "  time_this_iter_s: 47.087554693222046\n",
      "  time_total_s: 296.6084179878235\n",
      "  timers:\n",
      "    learn_throughput: 134.625\n",
      "    learn_time_ms: 22284.114\n",
      "    synch_weights_time_ms: 2.849\n",
      "    training_iteration_time_ms: 42366.647\n",
      "  timestamp: 1660500058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 7\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:00:59 (running for 00:05:40.82)\n",
      "Memory usage on this node: 23.1/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:04 (running for 00:05:45.87)\n",
      "Memory usage on this node: 23.1/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:09 (running for 00:05:50.97)\n",
      "Memory usage on this node: 23.1/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:14 (running for 00:05:56.01)\n",
      "Memory usage on this node: 23.0/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:19 (running for 00:06:01.08)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:24 (running for 00:06:06.13)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      4 |          245.678 | 16000 |   -0.435 |                  7.805 |                 -8.24  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999968\n",
      "  episode_reward_mean: 1.6320000000000092\n",
      "  episode_reward_min: -13.499999999999982\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: d9b180bb1fb4403ba3d13246b5f1de5e\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2100677490234375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0172614436596632\n",
      "          model: {}\n",
      "          policy_loss: -0.05178972706198692\n",
      "          total_loss: 6.666609287261963\n",
      "          vf_explained_var: 0.16326303780078888\n",
      "          vf_loss: 6.710630893707275\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2371690273284912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01634678617119789\n",
      "          model: {}\n",
      "          policy_loss: -0.046135056763887405\n",
      "          total_loss: 1.988045334815979\n",
      "          vf_explained_var: 0.33641114830970764\n",
      "          vf_loss: 2.0292766094207764\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 34.076404494382025\n",
      "    ram_util_percent: 72.26179775280899\n",
      "  pid: 13928\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: -2.2999999999999914\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.96\n",
      "    policy2: -8.327999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -5.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07551119473634225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02410555896957668\n",
      "    mean_inference_ms: 6.746316659268482\n",
      "    mean_raw_obs_processing_ms: 0.44409278253599865\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999968\n",
      "    episode_reward_mean: 1.6320000000000092\n",
      "    episode_reward_min: -13.499999999999982\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-13.499999999999982, 2.700000000000021, -5.999999999999994, 9.600000000000021,\n",
      "        -4.499999999999993, 1.1629586182948515e-14, 0.6000000000000291, 14.09999999999998,\n",
      "        3.0000000000000044, -7.799999999999978, -6.299999999999979, -1.8457457784393227e-14,\n",
      "        2.683964162031316e-14, 14.400000000000023, -4.499999999999987, -2.9999999999999774,\n",
      "        -11.399999999999983, -2.9999999999999973, -8.399999999999983, 9.000000000000004,\n",
      "        2.100000000000028, -7.799999999999981, 7.200000000000026, 17.399999999999956,\n",
      "        -12.299999999999978, 5.7000000000000135, -10.199999999999973, -3.8999999999999813,\n",
      "        7.199999999999989, -8.999999999999995, -3.8999999999999955, -3.5999999999999748,\n",
      "        3.30000000000003, -7.499999999999977, -8.999999999999982, 6.000000000000009,\n",
      "        3.6000000000000134, -2.6999999999999824, -1.1685097334179773e-14, -3.0, -0.899999999999991,\n",
      "        11.999999999999984, 5.700000000000028, -5.69999999999999, 18.000000000000007,\n",
      "        7.500000000000011, -2.399999999999996, 8.10000000000003, -0.2999999999999764,\n",
      "        5.100000000000021, -1.499999999999977, 1.7069679003611782e-14, 9.00000000000003,\n",
      "        -4.499999999999984, -7.500000000000021, 9.000000000000016, 1.5000000000000115,\n",
      "        -11.999999999999995, -10.499999999999973, 0.5999999999999833, 0.6000000000000202,\n",
      "        11.399999999999975, -2.9999999999999827, 11.099999999999943, 6.600000000000033,\n",
      "        8.69999999999999, -6.299999999999988, -9.599999999999984, 10.500000000000018,\n",
      "        12.000000000000007, -4.19999999999999, 10.499999999999956, 0.30000000000001736,\n",
      "        4.200000000000031, -2.39999999999999, -4.499999999999975, -6.299999999999979,\n",
      "        2.3037127760972e-15, 12.600000000000023, 10.499999999999982, -8.999999999999991,\n",
      "        4.500000000000007, 2.7000000000000246, 4.500000000000027, -5.399999999999984,\n",
      "        4.500000000000014, 3.9000000000000177, 8.700000000000031, 4.4999999999999964,\n",
      "        15.899999999999972, 8.700000000000026, 7.2000000000000135, 21.299999999999933,\n",
      "        22.499999999999968, 8.099999999999985, -2.9999999999999796, 9.000000000000032,\n",
      "        4.500000000000005, -3.6914915568786455e-15, -8.39999999999999]\n",
      "      policy_policy1_reward: [-3.5, 10.5, 4.0, 18.5, 5.5, 10.0, 9.5, 17.5, 13.0, -5.5,\n",
      "        -4.0, 10.0, 10.0, 20.0, 5.5, 7.0, -2.5, 1.5, -5.0, 19.0, 11.0, 0.0, 15.0, 23.0,\n",
      "        -4.5, 13.5, -3.5, 5.0, 15.0, 1.0, 5.0, 2.0, 10.0, 2.5, 1.0, 16.0, 12.5, 4.0,\n",
      "        10.0, 7.0, 8.0, 22.0, 13.5, 1.0, 28.0, 17.5, 6.5, 17.0, 7.5, 14.0, 3.0, 10.0,\n",
      "        19.0, 5.5, 2.5, 19.0, 11.5, -2.0, -0.5, 9.5, 9.5, 17.0, 7.0, 20.0, 15.5, 16.5,\n",
      "        1.5, -4.0, 20.5, 16.5, 2.5, 20.5, 7.0, 12.0, 6.5, 5.5, -4.0, 10.0, 21.5, 20.5,\n",
      "        1.0, 14.5, 10.5, 14.5, -2.0, 14.5, 9.5, 16.5, 14.5, 21.5, 16.5, 15.0, 28.0,\n",
      "        32.5, 17.0, 7.0, 19.0, 14.5, 10.0, 0.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -3.3999999999999964, -9.99999999999998, -2.2999999999999914, -2.300000000000003,\n",
      "        -9.99999999999998, -9.99999999999998, -5.59999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -4.500000000000002, -3.39999999999999,\n",
      "        -9.99999999999998, -8.899999999999986, -7.799999999999981, -7.7999999999999865,\n",
      "        -5.599999999999991, -7.799999999999981, -7.799999999999986, -6.6999999999999815,\n",
      "        -8.89999999999998, -7.79999999999999, -9.99999999999998, -8.89999999999998,\n",
      "        -5.599999999999984, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -6.699999999999982, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -8.89999999999998, -4.499999999999997,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.899999999999986, -5.599999999999995, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -5.5999999999999845, -9.99999999999998, -4.500000000000003, -6.699999999999984,\n",
      "        -9.99999999999998, -6.699999999999992, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -2.300000000000004, -9.99999999999998, -8.899999999999984,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999984,\n",
      "        -9.99999999999998, -3.400000000000004, -9.99999999999998, -5.599999999999998,\n",
      "        -7.7999999999999865, -9.99999999999998, -5.599999999999994, -7.799999999999981,\n",
      "        -7.799999999999986, -6.699999999999983, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: -2.2999999999999914\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.96\n",
      "      policy2: -8.327999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -5.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07551119473634225\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02410555896957668\n",
      "      mean_inference_ms: 6.746316659268482\n",
      "      mean_raw_obs_processing_ms: 0.44409278253599865\n",
      "  time_since_restore: 308.980349779129\n",
      "  time_this_iter_s: 63.30266332626343\n",
      "  time_total_s: 308.980349779129\n",
      "  timers:\n",
      "    learn_throughput: 123.531\n",
      "    learn_time_ms: 32380.607\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 61790.684\n",
      "  timestamp: 1660500088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 4205e_00003\n",
      "  warmup_time: 9.807717561721802\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:34 (running for 00:06:15.84)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:39 (running for 00:06:20.88)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00002 with episode_reward_mean=2.310000000000011 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82760>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D826A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D74100>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75D4DF70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      7 |          296.608 | 21000 |    1.293 |                  8.345 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      7 |          295.013 | 21000 |    1.254 |                  8.185 |                 -6.931 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999982\n",
      "  episode_reward_mean: 1.983000000000011\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 240\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1343733072280884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01682240143418312\n",
      "          model: {}\n",
      "          policy_loss: -0.05517331138253212\n",
      "          total_loss: 6.887158393859863\n",
      "          vf_explained_var: 0.28391993045806885\n",
      "          vf_loss: 6.9347615242004395\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1255648136138916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015208554454147816\n",
      "          model: {}\n",
      "          policy_loss: -0.0463765449821949\n",
      "          total_loss: 2.469325542449951\n",
      "          vf_explained_var: 0.28520551323890686\n",
      "          vf_loss: 2.5088579654693604\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 34.01515151515152\n",
      "    ram_util_percent: 72.51060606060607\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: 2.0999999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.035\n",
      "    policy2: -7.051999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07464432304938325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023787479847632183\n",
      "    mean_inference_ms: 6.174064499050231\n",
      "    mean_raw_obs_processing_ms: 0.43830940241158145\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999982\n",
      "    episode_reward_mean: 1.983000000000011\n",
      "    episode_reward_min: -19.499999999999986\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-1.5000000000000164, 6.900000000000016, 8.400000000000002, 16.49999999999995,\n",
      "        -6.899999999999979, 1.2000000000000162, -5.09999999999998, 5.700000000000033,\n",
      "        4.500000000000034, 1.5000000000000149, -3.8999999999999857, 1.5000000000000138,\n",
      "        10.500000000000009, -4.5000000000000275, -1.5000000000000009, -5.399999999999972,\n",
      "        3.000000000000024, 4.500000000000023, 12.300000000000013, -0.8999999999999826,\n",
      "        22.499999999999982, -8.999999999999993, 6.599999999999989, -0.9000000000000019,\n",
      "        7.5000000000000195, -3.2999999999999843, 4.800000000000022, -8.399999999999977,\n",
      "        7.799999999999995, -5.39999999999999, -2.400000000000012, 6.000000000000032,\n",
      "        -5.399999999999981, 0.30000000000001115, -19.499999999999986, 3.6000000000000116,\n",
      "        4.500000000000021, 5.6999999999999975, 10.200000000000015, 6.6, -5.699999999999976,\n",
      "        4.200000000000032, 9.300000000000026, -8.399999999999983, 1.7999999999999865,\n",
      "        -3.300000000000003, -1.4999999999999774, -2.9999999999999782, 1.2000000000000073,\n",
      "        5.523359547510154e-15, -4.799999999999994, 4.500000000000027, -0.899999999999993,\n",
      "        -0.29999999999998284, 2.100000000000021, 6.600000000000019, 7.500000000000027,\n",
      "        12.300000000000024, 0.6000000000000134, 13.500000000000002, 9.300000000000018,\n",
      "        3.0000000000000258, -7.199999999999992, -3.8999999999999853, 2.4000000000000283,\n",
      "        3.600000000000015, 7.80000000000002, 6.3000000000000185, -2.100000000000004,\n",
      "        -2.399999999999987, 12.599999999999998, 4.499999999999973, 1.5000000000000147,\n",
      "        -13.499999999999975, 6.600000000000033, -4.200000000000014, 14.999999999999982,\n",
      "        -1.800000000000006, 1.1999999999999957, 3.000000000000015, -3.899999999999983,\n",
      "        -2.099999999999988, -0.8999999999999816, 3.5999999999999974, -2.399999999999985,\n",
      "        18.59999999999996, 11.700000000000022, 8.700000000000022, 6.599999999999989,\n",
      "        0.8999999999999996, -6.299999999999975, 11.100000000000033, 3.599999999999978,\n",
      "        -11.999999999999975, 9.000000000000027, 0.6000000000000154, 6.00000000000003,\n",
      "        -4.500000000000021, -1.4999999999999827, -4.499999999999982]\n",
      "      policy_policy1_reward: [8.5, 12.5, 8.5, 26.5, -3.5, 9.0, 0.5, 13.5, 14.5, 11.5,\n",
      "        -6.0, 11.5, 20.5, 5.5, 8.5, 3.5, 13.0, 3.5, 13.5, 8.0, 32.5, 1.0, 15.5, 8.0,\n",
      "        17.5, 4.5, 11.5, 0.5, 14.5, 3.5, 6.5, 16.0, 3.5, 1.5, -9.5, 12.5, 14.5, 13.5,\n",
      "        18.0, 15.5, 1.0, 12.0, 16.0, -5.0, 8.5, 4.5, 8.5, 1.5, 9.0, 4.5, -2.5, 14.5,\n",
      "        2.5, 7.5, 11.0, 10.0, 6.5, 19.0, 9.5, 23.5, 16.0, 7.5, -0.5, 5.0, 8.0, 12.5,\n",
      "        9.0, 13.0, -2.0, 6.5, 21.5, 9.0, 11.5, -3.5, 15.5, 2.5, 25.0, 6.0, 9.0, 13.0,\n",
      "        5.0, 3.5, 8.0, 12.5, 1.0, 27.5, 19.5, 16.5, 15.5, 1.0, 1.5, 20.0, 1.5, -7.5,\n",
      "        19.0, 4.0, 10.5, 0.0, 8.5, 5.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -5.599999999999992, -0.10000000000000275,\n",
      "        -9.99999999999998, -3.4000000000000052, -7.79999999999999, -5.599999999999991,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, 2.0999999999999943,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, 0.9999999999999961, -1.199999999999996,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999989, -6.6999999999999895,\n",
      "        -8.89999999999998, -6.699999999999995, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -1.200000000000003, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -8.89999999999998, -6.6999999999999815, -7.799999999999989, -6.699999999999992,\n",
      "        -3.4000000000000057, -6.699999999999982, -7.799999999999985, -9.99999999999998,\n",
      "        -4.5000000000000036, -7.799999999999988, -4.499999999999982, -2.2999999999999847,\n",
      "        -9.99999999999998, -3.399999999999988, -7.799999999999981, -8.899999999999986,\n",
      "        -3.4000000000000017, 0.9999999999999943, -6.6999999999999815, -8.89999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -4.499999999999992, -6.699999999999983,\n",
      "        -8.89999999999998, -5.6, -8.89999999999998, -1.2, -6.699999999999989, -0.10000000000000464,\n",
      "        -8.899999999999986, -8.89999999999998, -4.499999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999983, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999986, -9.99999999999998, -8.89999999999998,\n",
      "        -5.599999999999985, -8.89999999999998, -8.89999999999998, -3.4000000000000044,\n",
      "        -8.89999999999998, -7.799999999999986, -7.799999999999981, -8.899999999999986,\n",
      "        -0.10000000000000464, -7.799999999999981, -8.899999999999986, 2.0999999999999956,\n",
      "        -4.500000000000001, -9.99999999999998, -3.3999999999999826, -4.500000000000003,\n",
      "        -4.499999999999991, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: 2.0999999999999956\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.035\n",
      "      policy2: -7.051999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -9.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07464432304938325\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023787479847632183\n",
      "      mean_inference_ms: 6.174064499050231\n",
      "      mean_raw_obs_processing_ms: 0.43830940241158145\n",
      "  time_since_restore: 342.12866950035095\n",
      "  time_this_iter_s: 47.11528563499451\n",
      "  time_total_s: 342.12866950035095\n",
      "  timers:\n",
      "    learn_throughput: 135.183\n",
      "    learn_time_ms: 22192.151\n",
      "    synch_weights_time_ms: 2.867\n",
      "    training_iteration_time_ms: 42759.726\n",
      "  timestamp: 1660500102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 8\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-01-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.89999999999992\n",
      "  episode_reward_mean: 2.4690000000000034\n",
      "  episode_reward_min: -21.000000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 240\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1369765996932983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014879899099469185\n",
      "          model: {}\n",
      "          policy_loss: -0.045475222170352936\n",
      "          total_loss: 7.192043781280518\n",
      "          vf_explained_var: 0.04983694106340408\n",
      "          vf_loss: 7.234543323516846\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0845798254013062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015288368798792362\n",
      "          model: {}\n",
      "          policy_loss: -0.04778193309903145\n",
      "          total_loss: 2.3530287742614746\n",
      "          vf_explained_var: 0.2448733001947403\n",
      "          vf_loss: 2.3992817401885986\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 33.950746268656715\n",
      "    ram_util_percent: 72.54626865671644\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: 4.3000000000000025\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.455\n",
      "    policy2: -6.985999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -19.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07560463177397136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021501093906769776\n",
      "    mean_inference_ms: 6.119814898518659\n",
      "    mean_raw_obs_processing_ms: 0.43667891323254865\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 21.89999999999992\n",
      "    episode_reward_mean: 2.4690000000000034\n",
      "    episode_reward_min: -21.000000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-9.89999999999998, 3.000000000000033, 16.499999999999964, 14.400000000000025,\n",
      "        3.000000000000014, -2.999999999999989, -0.29999999999997196, -5.399999999999983,\n",
      "        -5.099999999999977, 1.2000000000000077, -14.69999999999998, 4.200000000000028,\n",
      "        -10.499999999999975, -14.999999999999993, 0.900000000000028, 6.5999999999999925,\n",
      "        10.19999999999995, 17.099999999999916, 10.499999999999991, 9.90000000000002,\n",
      "        -1.7999999999999963, -4.199999999999985, 2.40000000000001, -1.799999999999991,\n",
      "        -4.799999999999978, -9.899999999999988, 11.100000000000032, 4.799999999999969,\n",
      "        -17.999999999999993, -1.799999999999986, 4.80000000000002, -21.000000000000007,\n",
      "        7.500000000000032, -0.899999999999982, -2.0999999999999774, 3.0000000000000293,\n",
      "        5.400000000000025, 10.799999999999983, 3.6000000000000316, 7.800000000000026,\n",
      "        16.199999999999903, 6.000000000000017, 11.99999999999994, 10.499999999999991,\n",
      "        6.9000000000000306, 17.999999999999947, 1.8000000000000256, -3.2999999999999847,\n",
      "        -0.3000000000000016, 17.99999999999994, 6.000000000000023, -5.999999999999976,\n",
      "        -16.499999999999986, 14.99999999999997, 10.199999999999942, -4.499999999999988,\n",
      "        17.999999999999947, -4.799999999999976, 3.0000000000000293, -3.0000000000000013,\n",
      "        -2.699999999999972, 1.2000000000000297, 2.7000000000000277, -6.899999999999988,\n",
      "        10.800000000000031, 0.600000000000006, -1.4999999999999867, 7.500000000000014,\n",
      "        16.199999999999932, -1.5000000000000029, -2.3999999999999764, -2.6999999999999775,\n",
      "        8.69999999999998, 17.09999999999995, -4.499999999999975, 11.999999999999972,\n",
      "        -1.7999999999999745, 13.799999999999995, 7.500000000000014, 2.700000000000012,\n",
      "        -13.799999999999981, -5.999999999999972, 4.500000000000012, 4.200000000000015,\n",
      "        0.6000000000000149, 8.700000000000006, 8.699999999999951, -5.099999999999983,\n",
      "        10.79999999999992, 17.70000000000003, 2.806088694740083e-14, 5.100000000000026,\n",
      "        -1.799999999999971, -10.49999999999999, 0.9000000000000251, 21.89999999999992,\n",
      "        -2.6999999999999806, 3.2999999999999767, -1.1999999999999833, -6.899999999999975]\n",
      "      policy_policy1_reward: [-1.0, 13.0, 26.5, 20.0, 13.0, 7.0, 7.5, 3.5, 0.5, 9.0,\n",
      "        -19.0, 12.0, -0.5, -5.0, 1.0, 10.0, 7.0, 26.0, 15.0, 10.0, 6.0, 2.5, 8.0, 6.0,\n",
      "        3.0, -1.0, 20.0, 11.5, -8.0, 6.0, 6.0, -11.0, 17.5, 8.0, -2.0, 13.0, 11.0, 6.5,\n",
      "        12.5, 14.5, 24.0, 16.0, 22.0, 20.5, 12.5, 28.0, 8.5, -1.0, 7.5, 28.0, 16.0,\n",
      "        -1.5, -6.5, 25.0, 12.5, 5.5, 28.0, 3.0, 13.0, 7.0, 4.0, 9.0, 10.5, -3.5, 12.0,\n",
      "        4.0, 8.5, 17.5, 18.5, 3.0, 1.0, -1.5, 16.5, 26.0, 5.5, 22.0, 6.0, 20.5, 17.5,\n",
      "        10.5, -6.0, 4.0, 14.5, 12.0, 4.0, 16.5, 16.5, 0.5, 17.5, 20.0, 10.0, 14.0, 6.0,\n",
      "        -0.5, 6.5, 27.5, 4.0, 10.0, 5.5, 2.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999987, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -8.89999999999998, -5.6, -7.799999999999984, 4.3000000000000025, -7.799999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -0.10000000000000331, -3.4000000000000044,\n",
      "        3.200000000000001, -8.89999999999998, -4.5000000000000036, -0.10000000000000508,\n",
      "        -7.799999999999981, -6.6999999999999815, -5.599999999999982, -7.799999999999981,\n",
      "        -7.799999999999981, -8.89999999999998, -8.89999999999998, -6.699999999999995,\n",
      "        -9.99999999999998, -7.799999999999981, -1.1999999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -0.10000000000000153, -9.99999999999998,\n",
      "        -5.599999999999982, 4.299999999999998, -8.89999999999998, -6.699999999999994,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999995, -9.99999999999998, -6.699999999999982, -2.3000000000000043,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -4.499999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -2.300000000000001, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -7.799999999999986, -7.799999999999981, -3.399999999999988,\n",
      "        -1.1999999999999984, -3.3999999999999972, -9.99999999999998, -9.99999999999998,\n",
      "        -2.3000000000000043, -4.499999999999994, -3.4000000000000044, -1.200000000000003,\n",
      "        -7.799999999999986, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -7.79999999999999, -6.6999999999999895, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -3.399999999999997, -7.79999999999999, -7.799999999999981, -5.599999999999982,\n",
      "        -6.6999999999999815, -2.299999999999999, -9.99999999999998, -8.89999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -5.599999999999993, -5.599999999999998,\n",
      "        -6.699999999999993, -6.699999999999994, -6.699999999999986, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: 4.3000000000000025\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.455\n",
      "      policy2: -6.985999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -19.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07560463177397136\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021501093906769776\n",
      "      mean_inference_ms: 6.119814898518659\n",
      "      mean_raw_obs_processing_ms: 0.43667891323254865\n",
      "  time_since_restore: 344.17805886268616\n",
      "  time_this_iter_s: 47.56964087486267\n",
      "  time_total_s: 344.17805886268616\n",
      "  timers:\n",
      "    learn_throughput: 132.794\n",
      "    learn_time_ms: 22591.383\n",
      "    synch_weights_time_ms: 2.867\n",
      "    training_iteration_time_ms: 43016.149\n",
      "  timestamp: 1660500106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 8\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:46 (running for 00:06:28.54)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      5 |          283.782 | 20000 |    2.31  |                 10.495 |                 -8.185 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999937\n",
      "  episode_reward_mean: 1.7640000000000122\n",
      "  episode_reward_min: -24.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 847f41dbc3e943d8b61110c9b40c9844\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1863924264907837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014398935250937939\n",
      "          model: {}\n",
      "          policy_loss: -0.042547330260276794\n",
      "          total_loss: 6.687630653381348\n",
      "          vf_explained_var: 0.1916634887456894\n",
      "          vf_loss: 6.727297782897949\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1870559453964233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010300814174115658\n",
      "          model: {}\n",
      "          policy_loss: -0.035611797124147415\n",
      "          total_loss: 1.8641315698623657\n",
      "          vf_explained_var: 0.4007021486759186\n",
      "          vf_loss: 1.8976832628250122\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 33.88636363636364\n",
      "    ram_util_percent: 72.51704545454545\n",
      "  pid: 14084\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: -2.2999999999999905\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.07\n",
      "    policy2: -8.305999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07474648179891498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02441901673731301\n",
      "    mean_inference_ms: 5.946792203756139\n",
      "    mean_raw_obs_processing_ms: 0.4349779566689021\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.099999999999937\n",
      "    episode_reward_mean: 1.7640000000000122\n",
      "    episode_reward_min: -24.0\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [17.099999999999923, 14.699999999999994, -10.499999999999984,\n",
      "        6.000000000000034, 4.200000000000033, -13.499999999999982, 19.199999999999974,\n",
      "        -2.3999999999999835, -1.799999999999974, -1.7999999999999758, -3.8999999999999906,\n",
      "        4.500000000000021, 1.5000000000000147, 3.000000000000005, -3.2999999999999767,\n",
      "        -4.199999999999987, -7.4999999999999805, 1.4999999999999676, -5.6999999999999815,\n",
      "        1.2000000000000215, 9.000000000000004, 3.000000000000013, 4.200000000000015,\n",
      "        16.79999999999997, 9.000000000000032, 12.599999999999994, 14.699999999999987,\n",
      "        0.6000000000000245, -14.999999999999984, -7.4999999999999805, 8.69999999999999,\n",
      "        -7.499999999999991, 4.200000000000024, -4.499999999999984, 3.600000000000015,\n",
      "        -2.6999999999999824, 14.999999999999996, -11.999999999999977, 9.600000000000016,\n",
      "        1.5000000000000084, 3.000000000000008, 4.800000000000013, 5.700000000000026,\n",
      "        9.60000000000003, 6.000000000000032, 2.700000000000028, 4.5000000000000195,\n",
      "        4.5000000000000195, -8.399999999999975, 2.100000000000029, 2.1000000000000085,\n",
      "        3.0000000000000107, -2.999999999999982, 23.099999999999937, 1.4999999999999765,\n",
      "        -6.899999999999974, -0.2999999999999856, 3.9000000000000212, -4.499999999999981,\n",
      "        8.10000000000003, 11.100000000000025, 3.000000000000017, 12.600000000000025,\n",
      "        -0.8999999999999773, -6.899999999999979, 2.7000000000000255, -2.999999999999987,\n",
      "        10.500000000000034, 1.5000000000000298, 4.500000000000031, 8.699999999999926,\n",
      "        -8.09999999999998, 15.000000000000018, 8.100000000000009, -5.999999999999974,\n",
      "        -1.499999999999988, 2.100000000000028, 3.0000000000000298, -9.299999999999978,\n",
      "        -24.0, 2.1000000000000307, 2.6999999999999935, -3.2999999999999745, -8.399999999999972,\n",
      "        2.1000000000000103, 7.500000000000018, 9.599999999999968, -1.8000000000000047,\n",
      "        5.999999999999938, 1.2000000000000257, 15.000000000000014, 0.6000000000000038,\n",
      "        -7.799999999999979, -2.999999999999989, -10.499999999999984, -5.399999999999979,\n",
      "        -2.9999999999999747, 12.300000000000017, -10.49999999999998, 4.800000000000027]\n",
      "      policy_policy1_reward: [26.0, 22.5, -0.5, 16.0, 12.0, -3.5, 27.0, 6.5, 6.0, 6.0,\n",
      "        5.0, 9.0, 11.5, 13.0, -1.0, 2.5, 2.5, 11.5, 1.0, 9.0, 19.0, 13.0, 12.0, 23.5,\n",
      "        19.0, 21.5, 22.5, 9.5, -5.0, 2.5, 16.5, 2.5, 12.0, 5.5, 12.5, 4.0, 25.0, -2.0,\n",
      "        13.0, 11.5, 13.0, 11.5, 8.0, 13.0, 10.5, 5.0, 14.5, 14.5, 0.5, 5.5, 11.0, 13.0,\n",
      "        7.0, 32.0, 11.5, 2.0, 7.5, 9.5, 5.5, 17.0, 20.0, 7.5, 21.5, 8.0, 2.0, 10.5,\n",
      "        7.0, 20.5, 11.5, 14.5, 16.5, -2.5, 25.0, 11.5, 4.0, 8.5, 11.0, 13.0, -1.5, -14.0,\n",
      "        11.0, 10.5, -1.0, 0.5, 11.0, 17.5, 18.5, 6.0, 16.0, 9.0, 25.0, 9.5, 0.0, 7.0,\n",
      "        -0.5, 3.5, 7.0, 19.0, -0.5, 11.5]\n",
      "      policy_policy2_reward: [-8.899999999999986, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -7.799999999999981, -7.799999999999981, -8.89999999999998,\n",
      "        -4.4999999999999964, -9.99999999999998, -9.99999999999998, -2.3000000000000025,\n",
      "        -6.699999999999994, -9.99999999999998, -9.99999999999998, -6.699999999999989,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -7.799999999999987,\n",
      "        -6.6999999999999815, -9.99999999999998, -8.89999999999998, -7.799999999999986,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -6.69999999999999, -9.99999999999998, -9.99999999999998, -3.4000000000000052,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999995, -2.2999999999999905,\n",
      "        -3.3999999999999955, -4.499999999999999, -2.3000000000000034, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -3.3999999999999932, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -5.6, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -4.499999999999987, -8.89999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -5.599999999999982,\n",
      "        -9.99999999999998, -3.3999999999999932, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999983, -2.300000000000003, -8.89999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -7.799999999999985, -9.99999999999998, -8.89999999999998,\n",
      "        -7.799999999999988, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -6.699999999999983, -9.99999999999998, -6.699999999999995]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.0\n",
      "      policy2: -2.2999999999999905\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.07\n",
      "      policy2: -8.305999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -14.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07474648179891498\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02441901673731301\n",
      "      mean_inference_ms: 5.946792203756139\n",
      "      mean_raw_obs_processing_ms: 0.4349779566689021\n",
      "  time_since_restore: 346.0174148082733\n",
      "  time_this_iter_s: 62.2353253364563\n",
      "  time_total_s: 346.0174148082733\n",
      "  timers:\n",
      "    learn_throughput: 128.516\n",
      "    learn_time_ms: 31124.54\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 57660.926\n",
      "  timestamp: 1660500110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 4205e_00002\n",
      "  warmup_time: 9.795740365982056\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:01:55 (running for 00:06:37.48)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:00 (running for 00:06:42.61)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:05 (running for 00:06:47.66)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:10 (running for 00:06:52.73)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:15 (running for 00:06:57.78)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:21 (running for 00:07:02.85)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:26 (running for 00:07:07.91)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00000 with episode_reward_mean=2.4690000000000034 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D82A30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D6E760>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75D9DBE0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75E11D30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      8 |          342.129 | 24000 |    1.983 |                  9.035 |                 -7.052 |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      5 |          308.98  | 20000 |    1.632 |                  9.96  |                 -8.328 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00001:\n",
      "  agent_timesteps_total: 54000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-02-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999966\n",
      "  episode_reward_mean: 1.9950000000000108\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 270\n",
      "  experiment_id: 14a111a27d2b4b48ae1a1c248c64001a\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1118505001068115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01675289124250412\n",
      "          model: {}\n",
      "          policy_loss: -0.052838899195194244\n",
      "          total_loss: 6.678053855895996\n",
      "          vf_explained_var: 0.18376269936561584\n",
      "          vf_loss: 6.723353385925293\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0813242197036743\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016348516568541527\n",
      "          model: {}\n",
      "          policy_loss: -0.04812008887529373\n",
      "          total_loss: 2.481159210205078\n",
      "          vf_explained_var: 0.2928955852985382\n",
      "          vf_loss: 2.5219225883483887\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 54000\n",
      "  num_agent_steps_trained: 54000\n",
      "  num_env_steps_sampled: 27000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 27000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.76060606060605\n",
      "    ram_util_percent: 72.89242424242424\n",
      "  pid: 14916\n",
      "  policy_reward_max:\n",
      "    policy1: 33.5\n",
      "    policy2: 2.0999999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.915\n",
      "    policy2: -6.919999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07554295519265518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023855932393267835\n",
      "    mean_inference_ms: 6.2650413754076935\n",
      "    mean_raw_obs_processing_ms: 0.4402083436758666\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.599999999999966\n",
      "    episode_reward_mean: 1.9950000000000108\n",
      "    episode_reward_min: -19.499999999999986\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-2.400000000000012, 6.000000000000032, -5.399999999999981, 0.30000000000001115,\n",
      "        -19.499999999999986, 3.6000000000000116, 4.500000000000021, 5.6999999999999975,\n",
      "        10.200000000000015, 6.6, -5.699999999999976, 4.200000000000032, 9.300000000000026,\n",
      "        -8.399999999999983, 1.7999999999999865, -3.300000000000003, -1.4999999999999774,\n",
      "        -2.9999999999999782, 1.2000000000000073, 5.523359547510154e-15, -4.799999999999994,\n",
      "        4.500000000000027, -0.899999999999993, -0.29999999999998284, 2.100000000000021,\n",
      "        6.600000000000019, 7.500000000000027, 12.300000000000024, 0.6000000000000134,\n",
      "        13.500000000000002, 9.300000000000018, 3.0000000000000258, -7.199999999999992,\n",
      "        -3.8999999999999853, 2.4000000000000283, 3.600000000000015, 7.80000000000002,\n",
      "        6.3000000000000185, -2.100000000000004, -2.399999999999987, 12.599999999999998,\n",
      "        4.499999999999973, 1.5000000000000147, -13.499999999999975, 6.600000000000033,\n",
      "        -4.200000000000014, 14.999999999999982, -1.800000000000006, 1.1999999999999957,\n",
      "        3.000000000000015, -3.899999999999983, -2.099999999999988, -0.8999999999999816,\n",
      "        3.5999999999999974, -2.399999999999985, 18.59999999999996, 11.700000000000022,\n",
      "        8.700000000000022, 6.599999999999989, 0.8999999999999996, -6.299999999999975,\n",
      "        11.100000000000033, 3.599999999999978, -11.999999999999975, 9.000000000000027,\n",
      "        0.6000000000000154, 6.00000000000003, -4.500000000000021, -1.4999999999999827,\n",
      "        -4.499999999999982, 0.600000000000006, 3.900000000000032, 0.6000000000000117,\n",
      "        24.599999999999966, 3.0000000000000115, -14.999999999999979, -5.699999999999988,\n",
      "        -2.3999999999999977, 1.5000000000000187, 10.500000000000032, -6.29999999999998,\n",
      "        7.800000000000029, -7.799999999999979, -3.899999999999993, 1.5000000000000182,\n",
      "        0.2999999999999978, 17.70000000000001, 11.100000000000017, 16.499999999999957,\n",
      "        0.5999999999999776, 0.8999999999999951, -5.699999999999985, -8.69999999999998,\n",
      "        -0.2999999999999835, 15.000000000000025, 12.60000000000003, -8.999999999999995,\n",
      "        -8.99999999999998, 3.300000000000025, 11.999999999999982]\n",
      "      policy_policy1_reward: [6.5, 16.0, 3.5, 1.5, -9.5, 12.5, 14.5, 13.5, 18.0, 15.5,\n",
      "        1.0, 12.0, 16.0, -5.0, 8.5, 4.5, 8.5, 1.5, 9.0, 4.5, -2.5, 14.5, 2.5, 7.5, 11.0,\n",
      "        10.0, 6.5, 19.0, 9.5, 23.5, 16.0, 7.5, -0.5, 5.0, 8.0, 12.5, 9.0, 13.0, -2.0,\n",
      "        6.5, 21.5, 9.0, 11.5, -3.5, 15.5, 2.5, 25.0, 6.0, 9.0, 13.0, 5.0, 3.5, 8.0,\n",
      "        12.5, 1.0, 27.5, 19.5, 16.5, 15.5, 1.0, 1.5, 20.0, 1.5, -7.5, 19.0, 4.0, 10.5,\n",
      "        0.0, 8.5, 5.5, 9.5, 9.5, 9.5, 33.5, 7.5, -5.0, 1.0, 6.5, 11.5, 20.5, 1.5, 9.0,\n",
      "        0.0, -0.5, 0.5, 1.5, 25.5, 20.0, 21.0, 9.5, 6.5, 1.0, -2.0, 7.5, 25.0, 21.5,\n",
      "        -4.5, 1.0, 10.0, 22.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -1.200000000000003, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999981, -8.89999999999998, -6.6999999999999815,\n",
      "        -7.799999999999989, -6.699999999999992, -3.4000000000000057, -6.699999999999982,\n",
      "        -7.799999999999985, -9.99999999999998, -4.5000000000000036, -7.799999999999988,\n",
      "        -4.499999999999982, -2.2999999999999847, -9.99999999999998, -3.399999999999988,\n",
      "        -7.799999999999981, -8.899999999999986, -3.4000000000000017, 0.9999999999999943,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -6.699999999999995,\n",
      "        -4.499999999999992, -6.699999999999983, -8.89999999999998, -5.6, -8.89999999999998,\n",
      "        -1.2, -6.699999999999989, -0.10000000000000464, -8.899999999999986, -8.89999999999998,\n",
      "        -4.499999999999999, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -6.699999999999983, -9.99999999999998, -7.799999999999981, -7.799999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999985, -8.89999999999998,\n",
      "        -8.89999999999998, -3.4000000000000044, -8.89999999999998, -7.799999999999986,\n",
      "        -7.799999999999981, -8.899999999999986, -0.10000000000000464, -7.799999999999981,\n",
      "        -8.899999999999986, 2.0999999999999956, -4.500000000000001, -9.99999999999998,\n",
      "        -3.3999999999999826, -4.500000000000003, -4.499999999999991, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -5.59999999999999, -8.89999999999998,\n",
      "        -8.89999999999998, -4.499999999999987, -9.99999999999998, -6.699999999999983,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -1.1999999999999968, -7.799999999999981, -3.39999999999999, 0.9999999999999988,\n",
      "        -1.200000000000002, -7.799999999999981, -8.899999999999986, -4.500000000000001,\n",
      "        -8.89999999999998, -5.599999999999998, -6.6999999999999895, -6.6999999999999815,\n",
      "        -7.799999999999983, -9.99999999999998, -8.89999999999998, -4.500000000000002,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.5\n",
      "      policy2: 2.0999999999999956\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.915\n",
      "      policy2: -6.919999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -9.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07554295519265518\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023855932393267835\n",
      "      mean_inference_ms: 6.2650413754076935\n",
      "      mean_raw_obs_processing_ms: 0.4402083436758666\n",
      "  time_since_restore: 388.49499773979187\n",
      "  time_this_iter_s: 46.36632823944092\n",
      "  time_total_s: 388.49499773979187\n",
      "  timers:\n",
      "    learn_throughput: 133.794\n",
      "    learn_time_ms: 22422.577\n",
      "    synch_weights_time_ms: 2.881\n",
      "    training_iteration_time_ms: 43159.794\n",
      "  timestamp: 1660500149\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 9\n",
      "  trial_id: 4205e_00001\n",
      "  warmup_time: 9.727915525436401\n",
      "  \n",
      "Result for PPO_MultiAgentArena_4205e_00003:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-02-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999968\n",
      "  episode_reward_mean: 3.9780000000000033\n",
      "  episode_reward_min: -14.999999999999982\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: d9b180bb1fb4403ba3d13246b5f1de5e\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.175122618675232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01721898838877678\n",
      "          model: {}\n",
      "          policy_loss: -0.05186411365866661\n",
      "          total_loss: 6.937846660614014\n",
      "          vf_explained_var: 0.1860046088695526\n",
      "          vf_loss: 6.98196268081665\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.20522940158844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01744834892451763\n",
      "          model: {}\n",
      "          policy_loss: -0.04886160045862198\n",
      "          total_loss: 2.2755467891693115\n",
      "          vf_explained_var: 0.29955801367759705\n",
      "          vf_loss: 2.31917405128479\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.88863636363636\n",
      "    ram_util_percent: 72.86022727272729\n",
      "  pid: 13928\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: -1.200000000000003\n",
      "  policy_reward_mean:\n",
      "    policy1: 12.185\n",
      "    policy2: -8.206999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07609054987751449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024287773643829745\n",
      "    mean_inference_ms: 6.776507786502335\n",
      "    mean_raw_obs_processing_ms: 0.4450095530722335\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999968\n",
      "    episode_reward_mean: 3.9780000000000033\n",
      "    episode_reward_min: -14.999999999999982\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-0.899999999999991, 11.999999999999984, 5.700000000000028, -5.69999999999999,\n",
      "        18.000000000000007, 7.500000000000011, -2.399999999999996, 8.10000000000003,\n",
      "        -0.2999999999999764, 5.100000000000021, -1.499999999999977, 1.7069679003611782e-14,\n",
      "        9.00000000000003, -4.499999999999984, -7.500000000000021, 9.000000000000016,\n",
      "        1.5000000000000115, -11.999999999999995, -10.499999999999973, 0.5999999999999833,\n",
      "        0.6000000000000202, 11.399999999999975, -2.9999999999999827, 11.099999999999943,\n",
      "        6.600000000000033, 8.69999999999999, -6.299999999999988, -9.599999999999984,\n",
      "        10.500000000000018, 12.000000000000007, -4.19999999999999, 10.499999999999956,\n",
      "        0.30000000000001736, 4.200000000000031, -2.39999999999999, -4.499999999999975,\n",
      "        -6.299999999999979, 2.3037127760972e-15, 12.600000000000023, 10.499999999999982,\n",
      "        -8.999999999999991, 4.500000000000007, 2.7000000000000246, 4.500000000000027,\n",
      "        -5.399999999999984, 4.500000000000014, 3.9000000000000177, 8.700000000000031,\n",
      "        4.4999999999999964, 15.899999999999972, 8.700000000000026, 7.2000000000000135,\n",
      "        21.299999999999933, 22.499999999999968, 8.099999999999985, -2.9999999999999796,\n",
      "        9.000000000000032, 4.500000000000005, -3.6914915568786455e-15, -8.39999999999999,\n",
      "        4.500000000000034, 11.69999999999997, 1.8000000000000131, 16.49999999999998,\n",
      "        -4.799999999999978, -4.499999999999975, 12.899999999999942, 17.9999999999999,\n",
      "        17.99999999999995, 19.199999999999974, 1.499999999999965, -0.5999999999999731,\n",
      "        -14.999999999999982, 4.500000000000016, 16.500000000000014, 10.499999999999972,\n",
      "        8.631984016460592e-15, -1.7999999999999852, 7.200000000000022, -1.8000000000000325,\n",
      "        12.900000000000022, 1.50000000000002, 20.99999999999997, 4.200000000000001,\n",
      "        5.100000000000016, 1.5000000000000107, 5.999999999999952, 10.200000000000017,\n",
      "        7.50000000000003, 7.500000000000011, -1.499999999999981, 6.299999999999993,\n",
      "        -3.8999999999999906, -12.299999999999992, -3.8999999999999986, 1.5000000000000058,\n",
      "        13.500000000000016, 2.1066481892262345e-14, 13.49999999999996, -5.699999999999993]\n",
      "      policy_policy1_reward: [8.0, 22.0, 13.5, 1.0, 28.0, 17.5, 6.5, 17.0, 7.5, 14.0,\n",
      "        3.0, 10.0, 19.0, 5.5, 2.5, 19.0, 11.5, -2.0, -0.5, 9.5, 9.5, 17.0, 7.0, 20.0,\n",
      "        15.5, 16.5, 1.5, -4.0, 20.5, 16.5, 2.5, 20.5, 7.0, 12.0, 6.5, 5.5, -4.0, 10.0,\n",
      "        21.5, 20.5, 1.0, 14.5, 10.5, 14.5, -2.0, 14.5, 9.5, 16.5, 14.5, 21.5, 16.5,\n",
      "        15.0, 28.0, 32.5, 17.0, 7.0, 19.0, 14.5, 10.0, 0.5, 14.5, 19.5, 8.5, 26.5, 3.0,\n",
      "        5.5, 18.5, 28.0, 28.0, 27.0, 6.0, 5.0, -5.0, 14.5, 26.5, 15.0, 10.0, 6.0, 15.0,\n",
      "        6.0, 18.5, 6.0, 31.0, 12.0, 14.0, 11.5, 16.0, 18.0, 12.0, 17.5, 3.0, 13.0, -0.5,\n",
      "        -4.5, 5.0, 11.5, 23.5, 4.5, 23.5, -4.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -6.699999999999982, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -8.89999999999998, -4.499999999999997,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.899999999999986, -5.599999999999995, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -5.5999999999999845, -9.99999999999998, -4.500000000000003, -6.699999999999984,\n",
      "        -9.99999999999998, -6.699999999999992, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -2.300000000000004, -9.99999999999998, -8.899999999999984,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999984,\n",
      "        -9.99999999999998, -3.400000000000004, -9.99999999999998, -5.599999999999998,\n",
      "        -7.7999999999999865, -9.99999999999998, -5.599999999999994, -7.799999999999981,\n",
      "        -7.799999999999986, -6.699999999999983, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -6.699999999999983,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -5.599999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -4.499999999999995,\n",
      "        -5.599999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -4.499999999999989, -9.99999999999998, -7.799999999999981, -7.79999999999999,\n",
      "        -7.799999999999981, -5.599999999999997, -4.499999999999999, -9.99999999999998,\n",
      "        -7.799999999999981, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -4.500000000000001, -9.99999999999998, -4.499999999999998,\n",
      "        -6.699999999999994, -3.3999999999999946, -7.799999999999981, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -4.500000000000003, -9.99999999999998,\n",
      "        -1.200000000000003]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: -1.200000000000003\n",
      "    policy_reward_mean:\n",
      "      policy1: 12.185\n",
      "      policy2: -8.206999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -5.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07609054987751449\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024287773643829745\n",
      "      mean_inference_ms: 6.776507786502335\n",
      "      mean_raw_obs_processing_ms: 0.4450095530722335\n",
      "  time_since_restore: 371.43258142471313\n",
      "  time_this_iter_s: 62.452231645584106\n",
      "  time_total_s: 371.43258142471313\n",
      "  timers:\n",
      "    learn_throughput: 123.264\n",
      "    learn_time_ms: 32450.691\n",
      "    synch_weights_time_ms: 3.158\n",
      "    training_iteration_time_ms: 61899.778\n",
      "  timestamp: 1660500151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 4205e_00003\n",
      "  warmup_time: 9.807717561721802\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:31 (running for 00:07:13.35)\n",
      "Memory usage on this node: 23.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      8 |          344.178 | 24000 |    2.469 |                  9.455 |                 -6.986 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-02-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.89999999999992\n",
      "  episode_reward_mean: 3.177000000000005\n",
      "  episode_reward_min: -21.000000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 270\n",
      "  experiment_id: 2025501066c147ff885d224cd5fdc214\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1034225225448608\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012665127404034138\n",
      "          model: {}\n",
      "          policy_loss: -0.04007060080766678\n",
      "          total_loss: 6.8707194328308105\n",
      "          vf_explained_var: 0.10970541834831238\n",
      "          vf_loss: 6.908257484436035\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0459295511245728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014736874960362911\n",
      "          model: {}\n",
      "          policy_loss: -0.04668828099966049\n",
      "          total_loss: 2.7686963081359863\n",
      "          vf_explained_var: 0.1992572396993637\n",
      "          vf_loss: 2.813910722732544\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 54000\n",
      "  num_agent_steps_trained: 54000\n",
      "  num_env_steps_sampled: 27000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 27000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.67164179104478\n",
      "    ram_util_percent: 72.88507462686569\n",
      "  pid: 13368\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: 4.299999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.185\n",
      "    policy2: -7.0079999999999885\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07664637824104764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022020070891012117\n",
      "    mean_inference_ms: 6.216755111327325\n",
      "    mean_raw_obs_processing_ms: 0.4382817674587822\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 21.89999999999992\n",
      "    episode_reward_mean: 3.177000000000005\n",
      "    episode_reward_min: -21.000000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [4.80000000000002, -21.000000000000007, 7.500000000000032, -0.899999999999982,\n",
      "        -2.0999999999999774, 3.0000000000000293, 5.400000000000025, 10.799999999999983,\n",
      "        3.6000000000000316, 7.800000000000026, 16.199999999999903, 6.000000000000017,\n",
      "        11.99999999999994, 10.499999999999991, 6.9000000000000306, 17.999999999999947,\n",
      "        1.8000000000000256, -3.2999999999999847, -0.3000000000000016, 17.99999999999994,\n",
      "        6.000000000000023, -5.999999999999976, -16.499999999999986, 14.99999999999997,\n",
      "        10.199999999999942, -4.499999999999988, 17.999999999999947, -4.799999999999976,\n",
      "        3.0000000000000293, -3.0000000000000013, -2.699999999999972, 1.2000000000000297,\n",
      "        2.7000000000000277, -6.899999999999988, 10.800000000000031, 0.600000000000006,\n",
      "        -1.4999999999999867, 7.500000000000014, 16.199999999999932, -1.5000000000000029,\n",
      "        -2.3999999999999764, -2.6999999999999775, 8.69999999999998, 17.09999999999995,\n",
      "        -4.499999999999975, 11.999999999999972, -1.7999999999999745, 13.799999999999995,\n",
      "        7.500000000000014, 2.700000000000012, -13.799999999999981, -5.999999999999972,\n",
      "        4.500000000000012, 4.200000000000015, 0.6000000000000149, 8.700000000000006,\n",
      "        8.699999999999951, -5.099999999999983, 10.79999999999992, 17.70000000000003,\n",
      "        2.806088694740083e-14, 5.100000000000026, -1.799999999999971, -10.49999999999999,\n",
      "        0.9000000000000251, 21.89999999999992, -2.6999999999999806, 3.2999999999999767,\n",
      "        -1.1999999999999833, -6.899999999999975, -2.9999999999999942, -1.4999999999999774,\n",
      "        13.49999999999994, 10.200000000000006, -4.199999999999987, -6.299999999999978,\n",
      "        7.800000000000011, -1.499999999999972, -16.499999999999986, 10.500000000000018,\n",
      "        11.700000000000028, -8.999999999999986, 1.499999999999996, -2.999999999999976,\n",
      "        -3.5999999999999823, -6.299999999999983, 2.6999999999999766, -8.699999999999983,\n",
      "        0.30000000000002625, 4.500000000000023, 6.300000000000026, 3.899999999999995,\n",
      "        14.099999999999971, 18.89999999999997, 1.2000000000000277, -1.8000000000000012,\n",
      "        8.99999999999999, 10.500000000000034, 11.699999999999989, 7.500000000000025]\n",
      "      policy_policy1_reward: [6.0, -11.0, 17.5, 8.0, -2.0, 13.0, 11.0, 6.5, 12.5, 14.5,\n",
      "        24.0, 16.0, 22.0, 20.5, 12.5, 28.0, 8.5, -1.0, 7.5, 28.0, 16.0, -1.5, -6.5,\n",
      "        25.0, 12.5, 5.5, 28.0, 3.0, 13.0, 7.0, 4.0, 9.0, 10.5, -3.5, 12.0, 4.0, 8.5,\n",
      "        17.5, 18.5, 3.0, 1.0, -1.5, 16.5, 26.0, 5.5, 22.0, 6.0, 20.5, 17.5, 10.5, -6.0,\n",
      "        4.0, 14.5, 12.0, 4.0, 16.5, 16.5, 0.5, 17.5, 20.0, 10.0, 14.0, 6.0, -0.5, 6.5,\n",
      "        27.5, 4.0, 10.0, 5.5, 2.0, 1.5, 8.5, 18.0, 18.0, 2.5, -9.5, 14.5, 3.0, -6.5,\n",
      "        20.5, 19.5, 1.0, 11.5, 7.0, -3.5, -4.0, 10.5, -2.0, 7.0, 9.0, 13.0, 9.5, 23.0,\n",
      "        24.5, 9.0, 0.5, 19.0, 20.5, 19.5, 17.5]\n",
      "      policy_policy2_reward: [-1.1999999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -0.10000000000000153, -9.99999999999998, -5.599999999999982,\n",
      "        4.299999999999998, -8.89999999999998, -6.699999999999994, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999995,\n",
      "        -9.99999999999998, -6.699999999999982, -2.3000000000000043, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -2.300000000000001, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -7.799999999999986, -7.799999999999981, -3.399999999999988, -1.1999999999999984,\n",
      "        -3.3999999999999972, -9.99999999999998, -9.99999999999998, -2.3000000000000043,\n",
      "        -4.499999999999994, -3.4000000000000044, -1.200000000000003, -7.799999999999986,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -6.6999999999999895, -9.99999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -3.399999999999997,\n",
      "        -7.79999999999999, -7.799999999999981, -5.599999999999982, -6.6999999999999815,\n",
      "        -2.299999999999999, -9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -5.599999999999993, -5.599999999999998, -6.699999999999993,\n",
      "        -6.699999999999994, -6.699999999999986, -8.899999999999986, -4.499999999999982,\n",
      "        -9.99999999999998, -4.5000000000000036, -7.799999999999981, -6.699999999999987,\n",
      "        3.200000000000002, -6.6999999999999815, -4.4999999999999964, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -0.10000000000000242, -2.3000000000000025, -7.79999999999999,\n",
      "        -6.699999999999988, -6.6999999999999815, -4.500000000000004, -6.699999999999991,\n",
      "        -5.599999999999998, -8.899999999999986, -5.599999999999986, -7.799999999999986,\n",
      "        -2.299999999999997, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: 4.299999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.185\n",
      "      policy2: -7.0079999999999885\n",
      "    policy_reward_min:\n",
      "      policy1: -11.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07664637824104764\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022020070891012117\n",
      "      mean_inference_ms: 6.216755111327325\n",
      "      mean_raw_obs_processing_ms: 0.4382817674587822\n",
      "  time_since_restore: 391.2748644351959\n",
      "  time_this_iter_s: 47.096805572509766\n",
      "  time_total_s: 391.2748644351959\n",
      "  timers:\n",
      "    learn_throughput: 131.276\n",
      "    learn_time_ms: 22852.582\n",
      "    synch_weights_time_ms: 2.881\n",
      "    training_iteration_time_ms: 43468.89\n",
      "  timestamp: 1660500153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 9\n",
      "  trial_id: 4205e_00000\n",
      "  warmup_time: 9.404762029647827\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:38 (running for 00:07:20.75)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      9 |          391.275 | 27000 |    3.177 |                 10.185 |                 -7.008 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:44 (running for 00:07:25.90)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      9 |          391.275 | 27000 |    3.177 |                 10.185 |                 -7.008 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:49 (running for 00:07:30.95)\n",
      "Memory usage on this node: 23.3/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      9 |          391.275 | 27000 |    3.177 |                 10.185 |                 -7.008 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      6 |          346.017 | 24000 |    1.764 |                 10.07  |                 -8.306 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_4205e_00002:\n",
      "  agent_timesteps_total: 56000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-14_20-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.299999999999933\n",
      "  episode_reward_mean: 1.4640000000000115\n",
      "  episode_reward_min: -24.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 847f41dbc3e943d8b61110c9b40c9844\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.145195484161377\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013210048899054527\n",
      "          model: {}\n",
      "          policy_loss: -0.038462795317173004\n",
      "          total_loss: 6.925600528717041\n",
      "          vf_explained_var: 0.20876331627368927\n",
      "          vf_loss: 6.961421489715576\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1664758920669556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01085571851581335\n",
      "          model: {}\n",
      "          policy_loss: -0.03188011795282364\n",
      "          total_loss: 2.5724644660949707\n",
      "          vf_explained_var: 0.2514621317386627\n",
      "          vf_loss: 2.602173328399658\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 28000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 33.03522727272727\n",
      "    ram_util_percent: 72.8715909090909\n",
      "  pid: 14084\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 0.9999999999999939\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.55\n",
      "    policy2: -8.085999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07491916346223046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02476295048070171\n",
      "    mean_inference_ms: 6.091976094969737\n",
      "    mean_raw_obs_processing_ms: 0.4366862214207828\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 30.299999999999933\n",
      "    episode_reward_mean: 1.4640000000000115\n",
      "    episode_reward_min: -24.0\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.000000000000008, 4.800000000000013, 5.700000000000026, 9.60000000000003,\n",
      "        6.000000000000032, 2.700000000000028, 4.5000000000000195, 4.5000000000000195,\n",
      "        -8.399999999999975, 2.100000000000029, 2.1000000000000085, 3.0000000000000107,\n",
      "        -2.999999999999982, 23.099999999999937, 1.4999999999999765, -6.899999999999974,\n",
      "        -0.2999999999999856, 3.9000000000000212, -4.499999999999981, 8.10000000000003,\n",
      "        11.100000000000025, 3.000000000000017, 12.600000000000025, -0.8999999999999773,\n",
      "        -6.899999999999979, 2.7000000000000255, -2.999999999999987, 10.500000000000034,\n",
      "        1.5000000000000298, 4.500000000000031, 8.699999999999926, -8.09999999999998,\n",
      "        15.000000000000018, 8.100000000000009, -5.999999999999974, -1.499999999999988,\n",
      "        2.100000000000028, 3.0000000000000298, -9.299999999999978, -24.0, 2.1000000000000307,\n",
      "        2.6999999999999935, -3.2999999999999745, -8.399999999999972, 2.1000000000000103,\n",
      "        7.500000000000018, 9.599999999999968, -1.8000000000000047, 5.999999999999938,\n",
      "        1.2000000000000257, 15.000000000000014, 0.6000000000000038, -7.799999999999979,\n",
      "        -2.999999999999989, -10.499999999999984, -5.399999999999979, -2.9999999999999747,\n",
      "        12.300000000000017, -10.49999999999998, 4.800000000000027, 12.900000000000018,\n",
      "        12.299999999999956, 1.2000000000000237, 12.00000000000003, 3.30000000000003,\n",
      "        30.299999999999933, -9.899999999999974, -4.800000000000027, -11.699999999999976,\n",
      "        6.600000000000012, -13.499999999999993, 2.017830347256222e-14, 1.5000000000000244,\n",
      "        7.500000000000031, 3.299999999999994, -1.7999999999999785, -5.399999999999981,\n",
      "        1.8846035843012032e-14, 11.700000000000008, 8.400000000000023, 14.999999999999977,\n",
      "        -13.499999999999993, -1.799999999999979, 2.9999999999999725, -1.7999999999999763,\n",
      "        4.500000000000023, -9.599999999999978, 1.5000000000000306, -16.499999999999975,\n",
      "        -5.099999999999975, -10.199999999999974, 7.500000000000021, 10.50000000000002,\n",
      "        16.199999999999946, -6.900000000000009, -11.399999999999974, -8.999999999999972,\n",
      "        -5.399999999999972, 20.999999999999915, -0.29999999999997384]\n",
      "      policy_policy1_reward: [13.0, 11.5, 8.0, 13.0, 10.5, 5.0, 14.5, 14.5, 0.5, 5.5,\n",
      "        11.0, 13.0, 7.0, 32.0, 11.5, 2.0, 7.5, 9.5, 5.5, 17.0, 20.0, 7.5, 21.5, 8.0,\n",
      "        2.0, 10.5, 7.0, 20.5, 11.5, 14.5, 16.5, -2.5, 25.0, 11.5, 4.0, 8.5, 11.0, 13.0,\n",
      "        -1.5, -14.0, 11.0, 10.5, -1.0, 0.5, 11.0, 17.5, 18.5, 6.0, 16.0, 9.0, 25.0,\n",
      "        9.5, 0.0, 7.0, -0.5, 3.5, 7.0, 19.0, -0.5, 11.5, 18.5, 13.5, 9.0, 11.0, 10.0,\n",
      "        37.0, -1.0, 3.0, -5.0, 15.5, -3.5, 10.0, 11.5, 17.5, 10.0, 6.0, 3.5, 10.0, 19.5,\n",
      "        14.0, 25.0, -3.5, 6.0, 7.5, 6.0, 14.5, -4.0, 11.5, -6.5, 0.5, -3.5, 17.5, 20.5,\n",
      "        24.0, 2.0, -2.5, 1.0, 3.5, 31.0, 7.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -6.699999999999995, -2.2999999999999905,\n",
      "        -3.3999999999999955, -4.499999999999999, -2.3000000000000034, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -3.3999999999999932, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -5.6, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -4.499999999999987, -8.89999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -5.599999999999982,\n",
      "        -9.99999999999998, -3.3999999999999932, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999983, -2.300000000000003, -8.89999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -7.799999999999985, -9.99999999999998, -8.89999999999998,\n",
      "        -7.799999999999988, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -6.699999999999983, -9.99999999999998, -6.699999999999995,\n",
      "        -5.599999999999983, -1.1999999999999922, -7.7999999999999865, 0.9999999999999939,\n",
      "        -6.699999999999995, -6.6999999999999815, -8.899999999999984, -7.79999999999999,\n",
      "        -6.699999999999995, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999985, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -4.499999999999982,\n",
      "        -7.799999999999981, -9.99999999999998, -5.599999999999988, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999995, -6.6999999999999895, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 37.0\n",
      "      policy2: 0.9999999999999939\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.55\n",
      "      policy2: -8.085999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -14.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07491916346223046\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02476295048070171\n",
      "      mean_inference_ms: 6.091976094969737\n",
      "      mean_raw_obs_processing_ms: 0.4366862214207828\n",
      "  time_since_restore: 407.98332691192627\n",
      "  time_this_iter_s: 61.965912103652954\n",
      "  time_total_s: 407.98332691192627\n",
      "  timers:\n",
      "    learn_throughput: 127.791\n",
      "    learn_time_ms: 31301.211\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 58274.927\n",
      "  timestamp: 1660500172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 4205e_00002\n",
      "  warmup_time: 9.795740365982056\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:02:57 (running for 00:07:39.57)\n",
      "Memory usage on this node: 22.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      9 |          391.275 | 27000 |    3.177 |                 10.185 |                 -7.008 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      7 |          407.983 | 28000 |    1.464 |                  9.55  |                 -8.086 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:03:02 (running for 00:07:44.63)\n",
      "Memory usage on this node: 22.7/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      9 |          391.275 | 27000 |    3.177 |                 10.185 |                 -7.008 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      7 |          407.983 | 28000 |    1.464 |                  9.55  |                 -8.086 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:03:07 (running for 00:07:49.70)\n",
      "Memory usage on this node: 22.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      9 |          391.275 | 27000 |    3.177 |                 10.185 |                 -7.008 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      7 |          407.983 | 28000 |    1.464 |                  9.55  |                 -8.086 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-14 20:03:12 (running for 00:07:54.74)\n",
      "Memory usage on this node: 22.9/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/12.73 GiB heap, 0.0/6.37 GiB objects\n",
      "Current best trial: 4205e_00003 with episode_reward_mean=3.9780000000000033 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB95B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9910>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB9940>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001CC75DB91F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001CC75DF1160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   sampler_results/p... |   sampler_results/p... |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------|\n",
      "| PPO_MultiAgentArena_4205e_00000 | RUNNING  | 127.0.0.1:13368 | 5e-05  |               3000 |      9 |          391.275 | 27000 |    3.177 |                 10.185 |                 -7.008 |\n",
      "| PPO_MultiAgentArena_4205e_00001 | RUNNING  | 127.0.0.1:14916 | 0.0001 |               3000 |      9 |          388.495 | 27000 |    1.995 |                  8.915 |                 -6.92  |\n",
      "| PPO_MultiAgentArena_4205e_00002 | RUNNING  | 127.0.0.1:14084 | 5e-05  |               4000 |      7 |          407.983 | 28000 |    1.464 |                  9.55  |                 -8.086 |\n",
      "| PPO_MultiAgentArena_4205e_00003 | RUNNING  | 127.0.0.1:13928 | 0.0001 |               4000 |      6 |          371.433 | 24000 |    3.978 |                 12.185 |                 -8.207 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+------------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example using Ray tune API (`tune.run()`) until some stopping condition is met.\n",
    "# This will create one (or more) Algorithms under the hood automatically w/o us having to\n",
    "# build these algos from the config.\n",
    "\n",
    "# Use a custom \"reporter\" that adds the individual policies' rewards to the output.\n",
    "reporter = CLIReporter()\n",
    "reporter.add_metric_column(\"sampler_results/policy_reward_mean/policy1\")\n",
    "reporter.add_metric_column(\"sampler_results/policy_reward_mean/policy2\")\n",
    "\n",
    "\n",
    "experiment_results = tune.run(\n",
    "    \"PPO\",\n",
    "\n",
    "    # training config params (translated into a python dict!)\n",
    "    config=config.to_dict(),\n",
    "\n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\n",
    "        \"training_iteration\": 10,     # stop after n training iterations (calls to `Algorithm.train()`)\n",
    "        #\"episode_reward_mean\": 400, # stop if average (sum of) rewards in an episode is 400 or more\n",
    "        #\"timesteps_total\": 100000,  # stop if reached 100,000 sampling timesteps\n",
    "    },  \n",
    "    progress_reporter=reporter,\n",
    "\n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=\"results\",\n",
    "         \n",
    "    # Every how many train() calls do we create a checkpoint?\n",
    "    checkpoint_freq=1,\n",
    "    # Always save last checkpoint (no matter the frequency).\n",
    "    checkpoint_at_end=True,\n",
    "\n",
    "    ###############\n",
    "    # Note about Ray Tune verbosity.\n",
    "    # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "    # 0 = silent\n",
    "    # 1 = only status updates, no logging messages\n",
    "    # 2 = status and brief trial results, includes logging messages\n",
    "    # 3 = status and detailed trial results, includes logging messages\n",
    "    # Defaults to 3.\n",
    "    ###############\n",
    "    verbose=3,\n",
    "                   \n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the returned `experiment_results` object,\n",
    "# we can extract from it the best checkpoint according to some criterium, e.g. `episode_reward_mean`.\n",
    "\n",
    "# We had 4 single trials (4 Algorithm instance); return the one that performed best here.\n",
    "best_trial = experiment_results.get_best_trial()\n",
    "print(\"Best trial: \", best_trial)\n",
    "\n",
    "# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\n",
    "best_checkpoint = experiment_results.get_best_checkpoint(trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# We would expect this to be either the very last checkpoint or one close to it:\n",
    "print(f\"Best checkpoint from training: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details behind Ray RLlib resource allocation <a class=\"anchor\" id=\"resource_allocation\"></a>\n",
    "\n",
    "#### Why did we use 8 CPUs in the tune run above (2 CPUs per trial)?\n",
    "\n",
    "```\n",
    "== Status ==\n",
    "Current time: 2022-07-24 18:18:28 (running for 00:02:09.35)\n",
    "Memory usage on this node: 9.9/16.0 GiB\n",
    "Using FIFO scheduling algorithm.\n",
    "Resources requested: 8/16 CPUs, 0/0 GPUs, 0.5/3.97 GiB heap, 0.5/1.98 GiB objects\n",
    "```\n",
    "\n",
    "<img src=\"images/closer_look_at_rllib.png\" width=700 />\n",
    "\n",
    "By default, the PPO Algorithm uses 2 so called `RolloutWorkers` (you can change this via `config.rollouts(num_rollout_workers=2)`) for collecting samples from\n",
    "environments in parallel.\n",
    "We changed this setting to only 1 worker via the `config.rollouts(num_rollout_workers=1)` call in the cell above.\n",
    "\n",
    "`RolloutWorkers` are Ray Actors that have their own copies of the environment and step through episodes in parallel. Each Actor in Ray normally uses a single CPU, but besides `RolloutWorker`s, an Algorithm in RLlib also always has one local process (aka. the \"driver\" process or the \"local worker\"), which - in case of PPO -\n",
    "handles the model/policy learning updates.\n",
    "\n",
    "For our experiment above, this gives us 2 CPUs (1 rollout worker + 1 local learner) per Algorithm instance.\n",
    "\n",
    "Since our config specifies two `grid_search` with 2 different learning rates AND 2 different batch sizes, we were running 4 Algorithms in parallel above (2 learning rates x 2 batch sizes = 4 trials), hence 8 CPUs were required (4 algos x 2 CPUs each = 8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt, how to:\n",
    "\n",
    "* Use Ray Tune in combination with RLlib for hyperparameter tuning\n",
    "* How an RLlib Algorithm configuration and the Tune hyperparameter search setup determine the required computational resources for a given `tune.run()` experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 03<a ></a>\n",
    "\n",
    "#### Using the `config` that we have built so far, let's run another `tune.run()`.\n",
    "\n",
    "But this time, apply the following changes to our setup:\n",
    "\n",
    "- Setup only 1 learning rate using the `config.training(lr=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size using the `config.training(train_batch_size=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set the number of RolloutWorkers to 5 using the `config.rollouts(num_rollout_workers=5)` method call, which will allow us to collect more environment samples in parallel.\n",
    "- Set the `num_envs_per_worker` config parameter to 5 using the `config.rollouts(num_envs_per_worker=...)` method call. This will batch our environment on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "- Set the stop criterium to \"training_iteration=180\".\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo our tune hyperparameter search:\n",
    "config.training(\n",
    "    # ...\n",
    ")\n",
    "\n",
    "# Change the config as stated in the exercise task:\n",
    "config.rollouts(\n",
    "    # ...\n",
    ")\n",
    "\n",
    "# Run the experiment for 180 iterations:\n",
    "experiment_results = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config.to_dict(),\n",
    "    stop={\n",
    "        # ...\n",
    "    },\n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=\"results\",\n",
    "    checkpoint_freq=10,\n",
    "    checkpoint_at_end=True,\n",
    "    verbose=1,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only had a single trial (one Algorithm instance), so this should be returned here.\n",
    "best_trial = experiment_results.get_best_trial()\n",
    "print(\"Best trial: \", best_trial)\n",
    "\n",
    "# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\n",
    "best_checkpoint = experiment_results.get_best_checkpoint(trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# We would expect this to be either the very last checkpoint or one close to it:\n",
    "print(f\"Best checkpoint from training: {best_checkpoint}\")\n",
    "\n",
    "# Create a fresh algorithm and retstore its state, using our best checkploint from the experiment above.\n",
    "new_ppo = config.build()\n",
    "new_ppo.restore(best_checkpoint)\n",
    "\n",
    "# Let's see how we are doing now.\n",
    "play_one_episode(env=None, algo=new_ppo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    " * [Tune, Scalable Hyperparameter Tuning](https://docs.ray.io/en/latest/tune/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
