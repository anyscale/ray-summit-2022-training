{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03. Introduction to Ray Tune and hyperparameter optimization (HPO)\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this notebook, you will learn:\n",
    " * [How to configure Ray Tune to find solid hyperparameters more easily](#configure_ray_tune)\n",
    " * [The details behind Ray RLlib resource allocation](#resource_allocation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "# Importing the very same environment class that we have coded together in\n",
    "# the previous notebook.\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena\n",
    "\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to configure Ray Tune to find solid hyperparameters more easily <a class=\"anchor\" id=\"configure_ray_tune\"></a>\n",
    "\n",
    "In the previous experiments, we used a single algorithm's (PPO) configuration to create\n",
    "exactly one Algorithm object and call its `train()` method manually a couple of times.\n",
    "\n",
    "A common thing to try when doing ML or RL is to look for better choices of hyperparameters, neural network architectures, or algorithm settings. This hyperparameter optimization\n",
    "problem can be tackled in a scalable fashion using Ray Tune (in combination with RLlib!).\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates, how you can setup a simple grid-search for one very important hyperparameter (the learning rate), using our already existing PPO config object and Ray Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO default learning rate is 5e-05 and the train batch size is 4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x1390f880a60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOConfig object (same as we did in the previous notebook):\n",
    "config = PPOConfig()\n",
    "print(f\"PPO default learning rate is {config.lr} and the train batch size is {config.train_batch_size}\")\n",
    "\n",
    "# Setup our config object the exact same way as before:\n",
    "# Point to our MultiAgentArena env:\n",
    "config.environment(env=MultiAgentArena)\n",
    "\n",
    "# Setup multi-agent mapping:\n",
    "\n",
    "# Environment provides M agent IDs.\n",
    "# RLlib has N policies (neural networks).\n",
    "# The `policy_mapping_fn` maps M agent IDs to N policies (M <= N).\n",
    "\n",
    "# If you don't provide a policy_mapping_fn, all agent IDs will map to \"default_policy\".\n",
    "config.multi_agent(\n",
    "    # Tell RLlib to create 2 policies with these IDs here:\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    # Tell RLlib to map agent1 to policy1 and agent2 to policy2.\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "\n",
    "# Reduce the number of workers from 2 (default) to 1 to save some resources on the expensive hyperparameter sweep.\n",
    "# IMPORTANT: More information on resource requirements for tune hyperparameter sweeps and different RLlib algorithm setups\n",
    "# below.\n",
    "config.rollouts(num_rollout_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore how a very simple hyperparameter search should be configured with RLlib and Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default learning rate for PPO is: 5e-05\n",
      "Default train batch size for PPO is: 4000\n"
     ]
    }
   ],
   "source": [
    "# Before setting up the learning rate hyperparam sweep,\n",
    "# let's see what the default learning rate and train batch size is for PPO:\n",
    "print(f\"Default learning rate for PPO is: {config.lr}\")\n",
    "print(f\"Default train batch size for PPO is: {config.train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x13947653b80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's change our existing config object and add a simple\n",
    "# grid-search over two different learning rates to it:\n",
    "config.training(\n",
    "    lr=tune.grid_search([5e-5, 1e-4]),\n",
    "    train_batch_size=tune.grid_search([3000, 4000]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 11:15:54,749\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-09 11:22:14 (running for 00:06:17.63)<br>Memory usage on this node: 13.0/31.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/13.83 GiB heap, 0.0/6.92 GiB objects<br>Current best trial: e0758_00003 with episode_reward_mean=3.1530000000000094 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001390C28F6D0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001390C28F460>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001390C509430>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000001390C28F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000001390C28F310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000001390C509430>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}<br>Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_wor...</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e0758_00000</td><td>TERMINATED</td><td>127.0.0.1:10368</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         268.406</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">   1.665</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                25.2</td><td style=\"text-align: right;\">               -13.5</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0758_00001</td><td>TERMINATED</td><td>127.0.0.1:11124</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         268.993</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">   1.83 </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                30.3</td><td style=\"text-align: right;\">               -21.3</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0758_00002</td><td>TERMINATED</td><td>127.0.0.1:17352</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         320.524</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   2.292</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               -24.3</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_e0758_00003</td><td>TERMINATED</td><td>127.0.0.1:11556</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         316.389</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   3.153</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               -18.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=10368)\u001b[0m 2022-08-09 11:16:02,693\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=10368)\u001b[0m 2022-08-09 11:16:02,694\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15000)\u001b[0m 2022-08-09 11:16:08,050\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=10368)\u001b[0m 2022-08-09 11:16:12,913\tINFO trainable.py:160 -- Trainable.setup took 10.221 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=10368)\u001b[0m 2022-08-09 11:16:12,913\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=11124)\u001b[0m 2022-08-09 11:16:18,359\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=11124)\u001b[0m 2022-08-09 11:16:18,360\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=10368)\u001b[0m 2022-08-09 11:16:22,423\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3252)\u001b[0m 2022-08-09 11:16:23,749\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=11124)\u001b[0m 2022-08-09 11:16:27,814\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=17352)\u001b[0m 2022-08-09 11:16:33,355\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=17352)\u001b[0m 2022-08-09 11:16:33,356\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6648)\u001b[0m 2022-08-09 11:16:38,769\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=11124)\u001b[0m 2022-08-09 11:16:39,213\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=17352)\u001b[0m 2022-08-09 11:16:42,806\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=11556)\u001b[0m 2022-08-09 11:16:48,301\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=11556)\u001b[0m 2022-08-09 11:16:48,302\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17196)\u001b[0m 2022-08-09 11:16:53,740\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=17352)\u001b[0m 2022-08-09 11:16:57,540\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0758_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-16-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.499999999999915\n",
      "  episode_reward_mean: -7.299999999999998\n",
      "  episode_reward_min: -36.30000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: d3dc894fb4884b018f96995278e9ac97\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3774759769439697\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008940611034631729\n",
      "          model: {}\n",
      "          policy_loss: -0.025092337280511856\n",
      "          total_loss: 7.39013671875\n",
      "          vf_explained_var: -0.00045563263120129704\n",
      "          vf_loss: 7.413441181182861\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3809373378753662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005361040588468313\n",
      "          model: {}\n",
      "          policy_loss: -0.021676624193787575\n",
      "          total_loss: 3.986201286315918\n",
      "          vf_explained_var: 0.1469276398420334\n",
      "          vf_loss: 4.006805896759033\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 18.49642857142857\n",
      "    ram_util_percent: 46.285714285714285\n",
      "  pid: 10368\n",
      "  policy_reward_max:\n",
      "    policy1: 38.5\n",
      "    policy2: -1.1999999999999862\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.4166666666666667\n",
      "    policy2: -8.716666666666649\n",
      "  policy_reward_min:\n",
      "    policy1: -28.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07144080444559658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020965899041953458\n",
      "    mean_inference_ms: 2.6503459329169745\n",
      "    mean_raw_obs_processing_ms: 0.40145239723558934\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.499999999999915\n",
      "    episode_reward_mean: -7.299999999999998\n",
      "    episode_reward_min: -36.30000000000004\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [-7.500000000000002, -7.499999999999982, -18.000000000000004,\n",
      "        -13.499999999999973, 3.000000000000008, 22.799999999999976, -11.999999999999986,\n",
      "        -12.89999999999999, -8.399999999999997, -36.30000000000004, -8.999999999999988,\n",
      "        -4.499999999999989, 28.499999999999915, -6.299999999999998, 7.500000000000011,\n",
      "        9.00000000000001, -27.000000000000025, -5.3999999999999755, 2.100000000000014,\n",
      "        -33.000000000000064, -18.0, -5.399999999999979, 7.500000000000021, -7.19999999999998,\n",
      "        -14.999999999999984, 3.600000000000005, -7.199999999999997, -16.79999999999999,\n",
      "        -14.099999999999984, -17.99999999999998]\n",
      "      policy_policy1_reward: [-3.0, 2.5, -8.0, -3.5, 13.0, 29.5, -2.0, -4.0, 0.5, -28.5,\n",
      "        1.0, 5.5, 38.5, 1.5, 17.5, 19.0, -17.0, 3.5, 11.0, -23.0, -8.0, 3.5, 17.5, -6.0,\n",
      "        -5.0, 12.5, -0.5, -9.0, -8.5, -8.0]\n",
      "      policy_policy2_reward: [-4.500000000000001, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999985, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -1.1999999999999862, -9.99999999999998, -8.89999999999998, -6.6999999999999815,\n",
      "        -7.799999999999986, -5.599999999999997, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.5\n",
      "      policy2: -1.1999999999999862\n",
      "    policy_reward_mean:\n",
      "      policy1: 1.4166666666666667\n",
      "      policy2: -8.716666666666649\n",
      "    policy_reward_min:\n",
      "      policy1: -28.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07144080444559658\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020965899041953458\n",
      "      mean_inference_ms: 2.6503459329169745\n",
      "      mean_raw_obs_processing_ms: 0.40145239723558934\n",
      "  time_since_restore: 19.49175524711609\n",
      "  time_this_iter_s: 19.49175524711609\n",
      "  time_total_s: 19.49175524711609\n",
      "  timers:\n",
      "    learn_throughput: 300.746\n",
      "    learn_time_ms: 9975.211\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 19485.771\n",
      "  timestamp: 1660036592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0758_00000\n",
      "  warmup_time: 10.224120855331421\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11556)\u001b[0m 2022-08-09 11:16:57,744\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0758_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.00000000000001\n",
      "  episode_reward_mean: -8.799999999999994\n",
      "  episode_reward_min: -39.90000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 3b43653afbe04acfbe6caf1cbf1e78d6\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3681732416152954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018601803109049797\n",
      "          model: {}\n",
      "          policy_loss: -0.04214094206690788\n",
      "          total_loss: 6.6384358406066895\n",
      "          vf_explained_var: 0.008029840886592865\n",
      "          vf_loss: 6.67685604095459\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3701523542404175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016378428786993027\n",
      "          model: {}\n",
      "          policy_loss: -0.04480268061161041\n",
      "          total_loss: 2.933044672012329\n",
      "          vf_explained_var: 0.28952547907829285\n",
      "          vf_loss: 2.97457218170166\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 20.9\n",
      "    ram_util_percent: 54.877419354838715\n",
      "  pid: 11124\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -6.699999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 0.65\n",
      "    policy2: -9.449999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06745569470007393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.01960553871874251\n",
      "    mean_inference_ms: 3.2851087455152066\n",
      "    mean_raw_obs_processing_ms: 0.4004853759277507\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.00000000000001\n",
      "    episode_reward_mean: -8.799999999999994\n",
      "    episode_reward_min: -39.90000000000008\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [-8.399999999999979, -22.500000000000007, -7.499999999999984,\n",
      "        1.8000000000000176, -3.8999999999999964, -11.999999999999984, -34.500000000000036,\n",
      "        1.2000000000000017, 1.840194663316197e-14, -11.999999999999979, -30.000000000000036,\n",
      "        -22.799999999999997, 1.500000000000015, 5.700000000000008, 1.5000000000000098,\n",
      "        -13.49999999999997, -2.9999999999999782, -2.399999999999975, 9.000000000000028,\n",
      "        -16.499999999999986, -13.49999999999999, -22.500000000000032, -8.399999999999991,\n",
      "        -39.90000000000008, 1.7957857423311907e-14, 15.00000000000001, -14.399999999999988,\n",
      "        -7.499999999999972, 1.0408340855860843e-14, -4.4999999999999725]\n",
      "      policy_policy1_reward: [0.5, -12.5, 2.5, 8.5, 5.0, -2.0, -24.5, 9.0, 10.0, -2.0,\n",
      "        -20.0, -15.0, 11.5, 13.5, 11.5, -3.5, 7.0, 6.5, 19.0, -6.5, -3.5, -12.5, 0.5,\n",
      "        -31.0, 10.0, 25.0, -5.5, 2.5, 10.0, 5.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999994, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999982, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999989, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999984,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -6.699999999999994\n",
      "    policy_reward_mean:\n",
      "      policy1: 0.65\n",
      "      policy2: -9.449999999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -31.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06745569470007393\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.01960553871874251\n",
      "      mean_inference_ms: 3.2851087455152066\n",
      "      mean_raw_obs_processing_ms: 0.4004853759277507\n",
      "  time_since_restore: 21.722838163375854\n",
      "  time_this_iter_s: 21.722838163375854\n",
      "  time_total_s: 21.722838163375854\n",
      "  timers:\n",
      "    learn_throughput: 290.716\n",
      "    learn_time_ms: 10319.341\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 21716.854\n",
      "  timestamp: 1660036609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0758_00001\n",
      "  warmup_time: 9.458592891693115\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11556)\u001b[0m 2022-08-09 11:17:27,649\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0758_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-17-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999915\n",
      "  episode_reward_mean: -6.9\n",
      "  episode_reward_min: -36.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 3bb20841742c49d7bcfe50360fad8c16\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3725521564483643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01393173635005951\n",
      "          model: {}\n",
      "          policy_loss: -0.027482902631163597\n",
      "          total_loss: 6.912868022918701\n",
      "          vf_explained_var: 0.004622733220458031\n",
      "          vf_loss: 6.937565326690674\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3806464672088623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005680397618561983\n",
      "          model: {}\n",
      "          policy_loss: -0.01879287324845791\n",
      "          total_loss: 3.660670042037964\n",
      "          vf_explained_var: 0.21431966125965118\n",
      "          vf_loss: 3.6783266067504883\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 28.005633802816902\n",
      "    ram_util_percent: 64.82957746478877\n",
      "  pid: 17352\n",
      "  policy_reward_max:\n",
      "    policy1: 27.0\n",
      "    policy2: -3.399999999999983\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.275\n",
      "    policy2: -9.174999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -26.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07477387044764078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.017683853420189875\n",
      "    mean_inference_ms: 3.1652287285615732\n",
      "    mean_raw_obs_processing_ms: 0.40183917071574393\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999915\n",
      "    episode_reward_mean: -6.9\n",
      "    episode_reward_min: -36.00000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-13.499999999999991, -31.500000000000057, 1.5000000000000056,\n",
      "        -30.00000000000003, 4.50000000000003, -20.400000000000027, -19.500000000000007,\n",
      "        -14.999999999999975, -18.00000000000003, -17.40000000000002, -1.499999999999989,\n",
      "        -2.3999999999999875, 13.499999999999977, -18.000000000000007, 22.499999999999915,\n",
      "        -8.39999999999999, -8.999999999999998, -5.999999999999975, 6.000000000000011,\n",
      "        -5.578870698741412e-15, 9.520162436160717e-15, -19.499999999999996, -16.49999999999998,\n",
      "        -14.399999999999984, -4.499999999999998, 4.800000000000011, -14.39999999999999,\n",
      "        7.800000000000031, 12.600000000000012, -36.00000000000007, -10.499999999999995,\n",
      "        8.100000000000016, -2.999999999999994, -7.499999999999998, -7.200000000000008,\n",
      "        1.4072076837123859e-14, 9.600000000000014, -4.800000000000001, -4.499999999999977,\n",
      "        -13.499999999999979]\n",
      "      policy_policy1_reward: [-3.5, -21.5, 11.5, -20.0, 14.5, -11.5, -9.5, -5.0, -8.0,\n",
      "        -8.5, 8.5, 6.5, 23.5, -8.0, 27.0, 0.5, 1.0, 4.0, 16.0, 10.0, 10.0, -9.5, -6.5,\n",
      "        -11.0, 5.5, 11.5, -5.5, 14.5, 21.5, -26.0, -0.5, 17.0, 7.0, 2.5, -0.5, 10.0,\n",
      "        18.5, 3.0, 5.5, -3.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999988,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -3.399999999999983, -9.99999999999998, -6.699999999999992, -8.899999999999986,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -6.6999999999999895,\n",
      "        -9.99999999999998, -8.899999999999983, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.0\n",
      "      policy2: -3.399999999999983\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.275\n",
      "      policy2: -9.174999999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -26.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07477387044764078\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.017683853420189875\n",
      "      mean_inference_ms: 3.1652287285615732\n",
      "      mean_raw_obs_processing_ms: 0.40183917071574393\n",
      "  time_since_restore: 49.61376118659973\n",
      "  time_this_iter_s: 49.61376118659973\n",
      "  time_total_s: 49.61376118659973\n",
      "  timers:\n",
      "    learn_throughput: 114.721\n",
      "    learn_time_ms: 34867.266\n",
      "    synch_weights_time_ms: 1.995\n",
      "    training_iteration_time_ms: 49599.799\n",
      "  timestamp: 1660036652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0758_00002\n",
      "  warmup_time: 9.455713987350464\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.00000000000001\n",
      "  episode_reward_mean: -5.029999999999988\n",
      "  episode_reward_min: -39.90000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 3b43653afbe04acfbe6caf1cbf1e78d6\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3268342018127441\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021526338532567024\n",
      "          model: {}\n",
      "          policy_loss: -0.061961736530065536\n",
      "          total_loss: 6.448129653930664\n",
      "          vf_explained_var: 0.007846862077713013\n",
      "          vf_loss: 6.505786895751953\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3325310945510864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017039284110069275\n",
      "          model: {}\n",
      "          policy_loss: -0.056659188121557236\n",
      "          total_loss: 1.6075998544692993\n",
      "          vf_explained_var: 0.41534295678138733\n",
      "          vf_loss: 1.660851001739502\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 28.859756097560982\n",
      "    ram_util_percent: 65.94146341463416\n",
      "  pid: 11124\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -5.599999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.016666666666667\n",
      "    policy2: -9.046666666666647\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06920497689420148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020271573824170743\n",
      "    mean_inference_ms: 4.181889117295419\n",
      "    mean_raw_obs_processing_ms: 0.4098986141640314\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.00000000000001\n",
      "    episode_reward_mean: -5.029999999999988\n",
      "    episode_reward_min: -39.90000000000008\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.399999999999979, -22.500000000000007, -7.499999999999984,\n",
      "        1.8000000000000176, -3.8999999999999964, -11.999999999999984, -34.500000000000036,\n",
      "        1.2000000000000017, 1.840194663316197e-14, -11.999999999999979, -30.000000000000036,\n",
      "        -22.799999999999997, 1.500000000000015, 5.700000000000008, 1.5000000000000098,\n",
      "        -13.49999999999997, -2.9999999999999782, -2.399999999999975, 9.000000000000028,\n",
      "        -16.499999999999986, -13.49999999999999, -22.500000000000032, -8.399999999999991,\n",
      "        -39.90000000000008, 1.7957857423311907e-14, 15.00000000000001, -14.399999999999988,\n",
      "        -7.499999999999972, 1.0408340855860843e-14, -4.4999999999999725, 11.10000000000002,\n",
      "        -10.499999999999993, -0.8999999999999798, 3.600000000000023, 6.900000000000018,\n",
      "        6.900000000000027, 6.600000000000021, 5.100000000000003, -9.000000000000009,\n",
      "        3.000000000000031, -10.799999999999983, 3.0000000000000226, -10.499999999999982,\n",
      "        -20.400000000000006, -2.999999999999984, 4.200000000000024, 12.600000000000016,\n",
      "        -8.999999999999986, -10.49999999999998, -14.999999999999977, -8.999999999999975,\n",
      "        -10.799999999999976, -10.799999999999988, 0.6000000000000048, 12.300000000000002,\n",
      "        9.30000000000003, -3.300000000000001, -1.7999999999999794, 1.2000000000000264,\n",
      "        11.100000000000032]\n",
      "      policy_policy1_reward: [0.5, -12.5, 2.5, 8.5, 5.0, -2.0, -24.5, 9.0, 10.0, -2.0,\n",
      "        -20.0, -15.0, 11.5, 13.5, 11.5, -3.5, 7.0, 6.5, 19.0, -6.5, -3.5, -12.5, 0.5,\n",
      "        -31.0, 10.0, 25.0, -5.5, 2.5, 10.0, 5.5, 20.0, -0.5, 8.0, 12.5, 12.5, 12.5,\n",
      "        15.5, 14.0, 1.0, 13.0, -3.0, 13.0, -0.5, -11.5, 7.0, 12.0, 21.5, 1.0, -0.5,\n",
      "        -5.0, 1.0, -3.0, -3.0, 9.5, 19.0, 16.0, 4.5, 6.0, 9.0, 20.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999994, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999982, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999989, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999984,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -5.599999999999982,\n",
      "        -5.6, -8.899999999999986, -8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999989, -8.89999999999998, -6.6999999999999815, -6.699999999999984,\n",
      "        -7.799999999999989, -7.799999999999986, -7.79999999999999, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -5.599999999999982\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.016666666666667\n",
      "      policy2: -9.046666666666647\n",
      "    policy_reward_min:\n",
      "      policy1: -31.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06920497689420148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020271573824170743\n",
      "      mean_inference_ms: 4.181889117295419\n",
      "      mean_raw_obs_processing_ms: 0.4098986141640314\n",
      "  time_since_restore: 71.16513276100159\n",
      "  time_this_iter_s: 49.44229459762573\n",
      "  time_total_s: 71.16513276100159\n",
      "  timers:\n",
      "    learn_throughput: 160.061\n",
      "    learn_time_ms: 18742.844\n",
      "    synch_weights_time_ms: 2.993\n",
      "    training_iteration_time_ms: 35578.576\n",
      "  timestamp: 1660036667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0758_00001\n",
      "  warmup_time: 9.458592891693115\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.499999999999915\n",
      "  episode_reward_mean: -4.584999999999996\n",
      "  episode_reward_min: -36.30000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: d3dc894fb4884b018f96995278e9ac97\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3420147895812988\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011977296322584152\n",
      "          model: {}\n",
      "          policy_loss: -0.036169737577438354\n",
      "          total_loss: 7.248169422149658\n",
      "          vf_explained_var: -0.023838307708501816\n",
      "          vf_loss: 7.2819437980651855\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3539645671844482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009806361049413681\n",
      "          model: {}\n",
      "          policy_loss: -0.04069122299551964\n",
      "          total_loss: 2.1965346336364746\n",
      "          vf_explained_var: 0.3211860954761505\n",
      "          vf_loss: 2.235264778137207\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 26.91308411214953\n",
      "    ram_util_percent: 63.70093457943927\n",
      "  pid: 10368\n",
      "  policy_reward_max:\n",
      "    policy1: 38.5\n",
      "    policy2: -1.1999999999999862\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.058333333333334\n",
      "    policy2: -8.643333333333317\n",
      "  policy_reward_min:\n",
      "    policy1: -28.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0741922455208269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02096095143637812\n",
      "    mean_inference_ms: 3.719234813065238\n",
      "    mean_raw_obs_processing_ms: 0.4106291051707228\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.499999999999915\n",
      "    episode_reward_mean: -4.584999999999996\n",
      "    episode_reward_min: -36.30000000000004\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-7.500000000000002, -7.499999999999982, -18.000000000000004,\n",
      "        -13.499999999999973, 3.000000000000008, 22.799999999999976, -11.999999999999986,\n",
      "        -12.89999999999999, -8.399999999999997, -36.30000000000004, -8.999999999999988,\n",
      "        -4.499999999999989, 28.499999999999915, -6.299999999999998, 7.500000000000011,\n",
      "        9.00000000000001, -27.000000000000025, -5.3999999999999755, 2.100000000000014,\n",
      "        -33.000000000000064, -18.0, -5.399999999999979, 7.500000000000021, -7.19999999999998,\n",
      "        -14.999999999999984, 3.600000000000005, -7.199999999999997, -16.79999999999999,\n",
      "        -14.099999999999984, -17.99999999999998, -4.499999999999982, -11.099999999999982,\n",
      "        3.0000000000000293, 1.5000000000000218, -26.100000000000016, 2.1000000000000068,\n",
      "        2.9999999999999973, 10.200000000000022, -4.499999999999984, 3.0000000000000044,\n",
      "        -8.399999999999988, 2.7283730830163222e-14, -1.799999999999982, -4.4999999999999964,\n",
      "        16.499999999999922, 17.100000000000016, -2.999999999999971, 3.000000000000004,\n",
      "        -20.39999999999999, -5.999999999999988, -8.999999999999973, 2.6999999999999966,\n",
      "        -7.199999999999976, -1.799999999999994, 7.500000000000016, 13.499999999999956,\n",
      "        -3.0000000000000173, -24.000000000000036, 2.0999999999999965, -5.99999999999999]\n",
      "      policy_policy1_reward: [-3.0, 2.5, -8.0, -3.5, 13.0, 29.5, -2.0, -4.0, 0.5, -28.5,\n",
      "        1.0, 5.5, 38.5, 1.5, 17.5, 19.0, -17.0, 3.5, 11.0, -23.0, -8.0, 3.5, 17.5, -6.0,\n",
      "        -5.0, 12.5, -0.5, -9.0, -8.5, -8.0, 5.5, -5.5, 13.0, 11.5, -20.5, 5.5, 13.0,\n",
      "        18.0, 5.5, 13.0, 0.5, 10.0, 0.5, 5.5, 21.0, 26.0, 7.0, 13.0, -11.5, 4.0, 1.0,\n",
      "        10.5, -0.5, 6.0, 17.5, 23.5, 7.0, -14.0, 11.0, 4.0]\n",
      "      policy_policy2_reward: [-4.500000000000001, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999985, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -1.1999999999999862, -9.99999999999998, -8.89999999999998, -6.6999999999999815,\n",
      "        -7.799999999999986, -5.599999999999997, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999999, -9.99999999999998, -9.99999999999998, -5.599999999999984,\n",
      "        -3.400000000000003, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -9.99999999999998, -2.299999999999993,\n",
      "        -9.99999999999998, -4.499999999999982, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999982, -6.6999999999999815, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.5\n",
      "      policy2: -1.1999999999999862\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.058333333333334\n",
      "      policy2: -8.643333333333317\n",
      "    policy_reward_min:\n",
      "      policy1: -28.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0741922455208269\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02096095143637812\n",
      "      mean_inference_ms: 3.719234813065238\n",
      "      mean_raw_obs_processing_ms: 0.4106291051707228\n",
      "  time_since_restore: 69.55040073394775\n",
      "  time_this_iter_s: 50.058645486831665\n",
      "  time_total_s: 69.55040073394775\n",
      "  timers:\n",
      "    learn_throughput: 159.662\n",
      "    learn_time_ms: 18789.693\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 34769.216\n",
      "  timestamp: 1660036667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0758_00000\n",
      "  warmup_time: 10.224120855331421\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-18-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.5000000000000195\n",
      "  episode_reward_mean: -10.057499999999994\n",
      "  episode_reward_min: -28.500000000000057\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 1cb98bfd22c24e0b8d41d2e27afd2ddf\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.365856647491455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020869188010692596\n",
      "          model: {}\n",
      "          policy_loss: -0.045140571892261505\n",
      "          total_loss: 6.959005355834961\n",
      "          vf_explained_var: -0.013833796605467796\n",
      "          vf_loss: 6.999971866607666\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3681658506393433\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018313145264983177\n",
      "          model: {}\n",
      "          policy_loss: -0.0420377291738987\n",
      "          total_loss: 2.8195786476135254\n",
      "          vf_explained_var: 0.3112947344779968\n",
      "          vf_loss: 2.8579537868499756\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.046236559139782\n",
      "    ram_util_percent: 66.55483870967745\n",
      "  pid: 11556\n",
      "  policy_reward_max:\n",
      "    policy1: 17.5\n",
      "    policy2: -3.3999999999999893\n",
      "  policy_reward_mean:\n",
      "    policy1: -0.8\n",
      "    policy2: -9.257499999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07177984318236237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.017944618154543394\n",
      "    mean_inference_ms: 6.933642994967201\n",
      "    mean_raw_obs_processing_ms: 0.430209253526157\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 7.5000000000000195\n",
      "    episode_reward_mean: -10.057499999999994\n",
      "    episode_reward_min: -28.500000000000057\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-27.000000000000007, -15.899999999999988, 2.100000000000016,\n",
      "        -25.500000000000007, -13.499999999999986, 2.0999999999999965, -5.999999999999973,\n",
      "        -10.499999999999988, -28.500000000000057, -13.499999999999975, -15.00000000000001,\n",
      "        -11.99999999999998, -18.599999999999984, -7.79999999999999, -17.999999999999982,\n",
      "        1.5000000000000244, -10.799999999999983, -17.99999999999999, 3.600000000000015,\n",
      "        -7.500000000000002, -25.50000000000001, -2.999999999999997, -1.4999999999999993,\n",
      "        -22.50000000000003, -16.499999999999993, -14.999999999999996, 4.500000000000026,\n",
      "        -19.799999999999997, 2.699999999999999, 3.000000000000015, 1.529332216421153e-14,\n",
      "        -12.899999999999977, -28.500000000000018, -14.999999999999991, 5.9674487573602164e-15,\n",
      "        -2.9999999999999813, 7.5000000000000195, -7.499999999999982, 1.5000000000000235,\n",
      "        -11.99999999999999]\n",
      "      policy_policy1_reward: [-17.0, -12.5, 11.0, -15.5, -3.5, 5.5, 4.0, -0.5, -18.5,\n",
      "        -3.5, -5.0, -2.0, -13.0, 0.0, -8.0, 11.5, -3.0, -8.0, 12.5, 2.5, -15.5, 7.0,\n",
      "        8.5, -12.5, -6.5, -5.0, 14.5, -12.0, 10.5, 13.0, 10.0, -4.0, -18.5, -5.0, 10.0,\n",
      "        7.0, 17.5, 2.5, 11.5, -2.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -3.3999999999999893, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.4000000000000035, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999984, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 17.5\n",
      "      policy2: -3.3999999999999893\n",
      "    policy_reward_mean:\n",
      "      policy1: -0.8\n",
      "      policy2: -9.257499999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07177984318236237\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.017944618154543394\n",
      "      mean_inference_ms: 6.933642994967201\n",
      "      mean_raw_obs_processing_ms: 0.430209253526157\n",
      "  time_since_restore: 65.4484794139862\n",
      "  time_this_iter_s: 65.4484794139862\n",
      "  time_total_s: 65.4484794139862\n",
      "  timers:\n",
      "    learn_throughput: 112.565\n",
      "    learn_time_ms: 35534.96\n",
      "    synch_weights_time_ms: 3.99\n",
      "    training_iteration_time_ms: 65446.485\n",
      "  timestamp: 1660036683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: e0758_00003\n",
      "  warmup_time: 9.446670532226562\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00001:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-18-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.00000000000001\n",
      "  episode_reward_mean: -3.2433333333333225\n",
      "  episode_reward_min: -39.90000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 3b43653afbe04acfbe6caf1cbf1e78d6\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2763701677322388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020656730979681015\n",
      "          model: {}\n",
      "          policy_loss: -0.06032403185963631\n",
      "          total_loss: 6.6972126960754395\n",
      "          vf_explained_var: 0.06408657133579254\n",
      "          vf_loss: 6.751339912414551\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2862704992294312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02049575001001358\n",
      "          model: {}\n",
      "          policy_loss: -0.05867380648851395\n",
      "          total_loss: 1.9768407344818115\n",
      "          vf_explained_var: 0.3189891278743744\n",
      "          vf_loss: 2.0314154624938965\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.018571428571423\n",
      "    ram_util_percent: 66.62000000000002\n",
      "  pid: 11124\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -2.2999999999999976\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.461111111111111\n",
      "    policy2: -8.704444444444428\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07035783513656726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020678991967099217\n",
      "    mean_inference_ms: 4.678474823030082\n",
      "    mean_raw_obs_processing_ms: 0.4134643346216417\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.00000000000001\n",
      "    episode_reward_mean: -3.2433333333333225\n",
      "    episode_reward_min: -39.90000000000008\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.399999999999979, -22.500000000000007, -7.499999999999984,\n",
      "        1.8000000000000176, -3.8999999999999964, -11.999999999999984, -34.500000000000036,\n",
      "        1.2000000000000017, 1.840194663316197e-14, -11.999999999999979, -30.000000000000036,\n",
      "        -22.799999999999997, 1.500000000000015, 5.700000000000008, 1.5000000000000098,\n",
      "        -13.49999999999997, -2.9999999999999782, -2.399999999999975, 9.000000000000028,\n",
      "        -16.499999999999986, -13.49999999999999, -22.500000000000032, -8.399999999999991,\n",
      "        -39.90000000000008, 1.7957857423311907e-14, 15.00000000000001, -14.399999999999988,\n",
      "        -7.499999999999972, 1.0408340855860843e-14, -4.4999999999999725, 11.10000000000002,\n",
      "        -10.499999999999993, -0.8999999999999798, 3.600000000000023, 6.900000000000018,\n",
      "        6.900000000000027, 6.600000000000021, 5.100000000000003, -9.000000000000009,\n",
      "        3.000000000000031, -10.799999999999983, 3.0000000000000226, -10.499999999999982,\n",
      "        -20.400000000000006, -2.999999999999984, 4.200000000000024, 12.600000000000016,\n",
      "        -8.999999999999986, -10.49999999999998, -14.999999999999977, -8.999999999999975,\n",
      "        -10.799999999999976, -10.799999999999988, 0.6000000000000048, 12.300000000000002,\n",
      "        9.30000000000003, -3.300000000000001, -1.7999999999999794, 1.2000000000000264,\n",
      "        11.100000000000032, 10.20000000000003, 4.800000000000017, 3.0000000000000213,\n",
      "        13.49999999999999, -4.499999999999998, 6.000000000000004, 8.100000000000025,\n",
      "        -1.500000000000004, -0.29999999999999616, 3.000000000000016, -26.400000000000006,\n",
      "        1.607047828144914e-14, 5.1000000000000245, -0.29999999999999016, -5.099999999999985,\n",
      "        -5.399999999999994, 1.5000000000000235, 4.200000000000019, -3.8999999999999906,\n",
      "        -1.499999999999981, 0.6000000000000113, 2.100000000000005, -3.0000000000000013,\n",
      "        6.299999999999997, 3.000000000000005, -9.29999999999998, 6.300000000000031,\n",
      "        8.700000000000022, -7.499999999999984, -7.799999999999997]\n",
      "      policy_policy1_reward: [0.5, -12.5, 2.5, 8.5, 5.0, -2.0, -24.5, 9.0, 10.0, -2.0,\n",
      "        -20.0, -15.0, 11.5, 13.5, 11.5, -3.5, 7.0, 6.5, 19.0, -6.5, -3.5, -12.5, 0.5,\n",
      "        -31.0, 10.0, 25.0, -5.5, 2.5, 10.0, 5.5, 20.0, -0.5, 8.0, 12.5, 12.5, 12.5,\n",
      "        15.5, 14.0, 1.0, 13.0, -3.0, 13.0, -0.5, -11.5, 7.0, 12.0, 21.5, 1.0, -0.5,\n",
      "        -5.0, 1.0, -3.0, -3.0, 9.5, 19.0, 16.0, 4.5, 6.0, 9.0, 20.0, 18.0, 11.5, 13.0,\n",
      "        23.5, 5.5, 16.0, 17.0, 3.0, 7.5, 13.0, -17.5, 4.5, 14.0, 7.5, 0.5, 3.5, 11.5,\n",
      "        12.0, 5.0, 8.5, 9.5, 11.0, 7.0, 13.0, 13.0, -7.0, 13.0, 16.5, -3.0, 0.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999994, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999982, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999989, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999984,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -5.599999999999982,\n",
      "        -5.6, -8.899999999999986, -8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999989, -8.89999999999998, -6.6999999999999815, -6.699999999999984,\n",
      "        -7.799999999999989, -7.799999999999986, -7.79999999999999, -8.89999999999998,\n",
      "        -7.799999999999986, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -4.499999999999988,\n",
      "        -7.79999999999999, -9.99999999999998, -8.89999999999998, -4.49999999999999,\n",
      "        -8.89999999999998, -7.799999999999981, -5.6, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999986, -8.89999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -2.2999999999999976, -6.699999999999995, -7.7999999999999865, -4.500000000000003,\n",
      "        -7.7999999999999865]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -2.2999999999999976\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.461111111111111\n",
      "      policy2: -8.704444444444428\n",
      "    policy_reward_min:\n",
      "      policy1: -31.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07035783513656726\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020678991967099217\n",
      "      mean_inference_ms: 4.678474823030082\n",
      "      mean_raw_obs_processing_ms: 0.4134643346216417\n",
      "  time_since_restore: 120.7089147567749\n",
      "  time_this_iter_s: 49.543781995773315\n",
      "  time_total_s: 120.7089147567749\n",
      "  timers:\n",
      "    learn_throughput: 138.823\n",
      "    learn_time_ms: 21610.225\n",
      "    synch_weights_time_ms: 2.66\n",
      "    training_iteration_time_ms: 40233.313\n",
      "  timestamp: 1660036716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0758_00001\n",
      "  warmup_time: 9.458592891693115\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.499999999999915\n",
      "  episode_reward_mean: -3.0699999999999945\n",
      "  episode_reward_min: -36.30000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: d3dc894fb4884b018f96995278e9ac97\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3231052160263062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010051189921796322\n",
      "          model: {}\n",
      "          policy_loss: -0.03529670462012291\n",
      "          total_loss: 7.307828903198242\n",
      "          vf_explained_var: 0.02555014006793499\n",
      "          vf_loss: 7.341115474700928\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3163607120513916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011830263771116734\n",
      "          model: {}\n",
      "          policy_loss: -0.039261724799871445\n",
      "          total_loss: 2.525359869003296\n",
      "          vf_explained_var: 0.21606169641017914\n",
      "          vf_loss: 2.5622551441192627\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.964788732394368\n",
      "    ram_util_percent: 66.61690140845073\n",
      "  pid: 10368\n",
      "  policy_reward_max:\n",
      "    policy1: 38.5\n",
      "    policy2: -0.10000000000000467\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.316666666666666\n",
      "    policy2: -8.38666666666665\n",
      "  policy_reward_min:\n",
      "    policy1: -28.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07453974460180815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02099502085638355\n",
      "    mean_inference_ms: 4.320896686456451\n",
      "    mean_raw_obs_processing_ms: 0.414952416480626\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.499999999999915\n",
      "    episode_reward_mean: -3.0699999999999945\n",
      "    episode_reward_min: -36.30000000000004\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-7.500000000000002, -7.499999999999982, -18.000000000000004,\n",
      "        -13.499999999999973, 3.000000000000008, 22.799999999999976, -11.999999999999986,\n",
      "        -12.89999999999999, -8.399999999999997, -36.30000000000004, -8.999999999999988,\n",
      "        -4.499999999999989, 28.499999999999915, -6.299999999999998, 7.500000000000011,\n",
      "        9.00000000000001, -27.000000000000025, -5.3999999999999755, 2.100000000000014,\n",
      "        -33.000000000000064, -18.0, -5.399999999999979, 7.500000000000021, -7.19999999999998,\n",
      "        -14.999999999999984, 3.600000000000005, -7.199999999999997, -16.79999999999999,\n",
      "        -14.099999999999984, -17.99999999999998, -4.499999999999982, -11.099999999999982,\n",
      "        3.0000000000000293, 1.5000000000000218, -26.100000000000016, 2.1000000000000068,\n",
      "        2.9999999999999973, 10.200000000000022, -4.499999999999984, 3.0000000000000044,\n",
      "        -8.399999999999988, 2.7283730830163222e-14, -1.799999999999982, -4.4999999999999964,\n",
      "        16.499999999999922, 17.100000000000016, -2.999999999999971, 3.000000000000004,\n",
      "        -20.39999999999999, -5.999999999999988, -8.999999999999973, 2.6999999999999966,\n",
      "        -7.199999999999976, -1.799999999999994, 7.500000000000016, 13.499999999999956,\n",
      "        -3.0000000000000173, -24.000000000000036, 2.0999999999999965, -5.99999999999999,\n",
      "        14.999999999999947, -1.4999999999999791, -10.199999999999982, -1.7999999999999865,\n",
      "        -12.599999999999982, 10.500000000000023, -15.299999999999983, 10.500000000000023,\n",
      "        -4.499999999999983, 11.100000000000012, 14.999999999999977, -5.9999999999999805,\n",
      "        11.100000000000023, 2.5174307083375425e-14, -13.499999999999975, -7.199999999999974,\n",
      "        -4.499999999999998, -4.19999999999998, 17.09999999999993, -11.999999999999982,\n",
      "        -4.499999999999983, -1.8000000000000007, 1.5000000000000235, 2.7000000000000113,\n",
      "        0.9000000000000271, 5.700000000000033, 17.39999999999995, -12.299999999999976,\n",
      "        1.2000000000000113, -8.99999999999998]\n",
      "      policy_policy1_reward: [-3.0, 2.5, -8.0, -3.5, 13.0, 29.5, -2.0, -4.0, 0.5, -28.5,\n",
      "        1.0, 5.5, 38.5, 1.5, 17.5, 19.0, -17.0, 3.5, 11.0, -23.0, -8.0, 3.5, 17.5, -6.0,\n",
      "        -5.0, 12.5, -0.5, -9.0, -8.5, -8.0, 5.5, -5.5, 13.0, 11.5, -20.5, 5.5, 13.0,\n",
      "        18.0, 5.5, 13.0, 0.5, 10.0, 0.5, 5.5, 21.0, 26.0, 7.0, 13.0, -11.5, 4.0, 1.0,\n",
      "        10.5, -0.5, 6.0, 17.5, 23.5, 7.0, -14.0, 11.0, 4.0, 19.5, 8.5, -3.5, 6.0, -12.5,\n",
      "        20.5, -7.5, 20.5, 5.5, 20.0, 25.0, 4.0, 20.0, 4.5, -3.5, -0.5, 5.5, 2.5, 26.0,\n",
      "        -2.0, 5.5, 6.0, 11.5, 10.5, 6.5, 13.5, 23.0, -4.5, 9.0, -4.5]\n",
      "      policy_policy2_reward: [-4.500000000000001, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999985, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -1.1999999999999862, -9.99999999999998, -8.89999999999998, -6.6999999999999815,\n",
      "        -7.799999999999986, -5.599999999999997, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999999, -9.99999999999998, -9.99999999999998, -5.599999999999984,\n",
      "        -3.400000000000003, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -9.99999999999998, -2.299999999999993,\n",
      "        -9.99999999999998, -4.499999999999982, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999982, -6.6999999999999815, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -4.499999999999985, -9.99999999999998, -6.699999999999993,\n",
      "        -7.799999999999981, -0.10000000000000467, -9.99999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -4.500000000000002, -9.99999999999998,\n",
      "        -6.699999999999994, -9.99999999999998, -6.6999999999999815, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -7.799999999999989, -5.599999999999998, -7.799999999999981, -5.599999999999991,\n",
      "        -7.799999999999981, -7.799999999999981, -4.500000000000003]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.5\n",
      "      policy2: -0.10000000000000467\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.316666666666666\n",
      "      policy2: -8.38666666666665\n",
      "    policy_reward_min:\n",
      "      policy1: -28.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07453974460180815\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02099502085638355\n",
      "      mean_inference_ms: 4.320896686456451\n",
      "      mean_raw_obs_processing_ms: 0.414952416480626\n",
      "  time_since_restore: 119.83121228218079\n",
      "  time_this_iter_s: 50.28081154823303\n",
      "  time_total_s: 119.83121228218079\n",
      "  timers:\n",
      "    learn_throughput: 137.934\n",
      "    learn_time_ms: 21749.503\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 39936.091\n",
      "  timestamp: 1660036718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0758_00000\n",
      "  warmup_time: 10.224120855331421\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999915\n",
      "  episode_reward_mean: -3.982499999999996\n",
      "  episode_reward_min: -36.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 3bb20841742c49d7bcfe50360fad8c16\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3370577096939087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01232813484966755\n",
      "          model: {}\n",
      "          policy_loss: -0.030721265822649002\n",
      "          total_loss: 6.562681674957275\n",
      "          vf_explained_var: 0.004991441499441862\n",
      "          vf_loss: 6.59093713760376\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3514821529388428\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009734823368489742\n",
      "          model: {}\n",
      "          policy_loss: -0.03320500627160072\n",
      "          total_loss: 1.730455994606018\n",
      "          vf_explained_var: 0.4099683463573456\n",
      "          vf_loss: 1.761714220046997\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.71075268817205\n",
      "    ram_util_percent: 66.61290322580648\n",
      "  pid: 17352\n",
      "  policy_reward_max:\n",
      "    policy1: 27.0\n",
      "    policy2: -2.2999999999999954\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.93125\n",
      "    policy2: -8.913749999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -26.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07409186434820411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.019619602671071008\n",
      "    mean_inference_ms: 4.133665522468267\n",
      "    mean_raw_obs_processing_ms: 0.40765176614362336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999915\n",
      "    episode_reward_mean: -3.982499999999996\n",
      "    episode_reward_min: -36.00000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [-13.499999999999991, -31.500000000000057, 1.5000000000000056,\n",
      "        -30.00000000000003, 4.50000000000003, -20.400000000000027, -19.500000000000007,\n",
      "        -14.999999999999975, -18.00000000000003, -17.40000000000002, -1.499999999999989,\n",
      "        -2.3999999999999875, 13.499999999999977, -18.000000000000007, 22.499999999999915,\n",
      "        -8.39999999999999, -8.999999999999998, -5.999999999999975, 6.000000000000011,\n",
      "        -5.578870698741412e-15, 9.520162436160717e-15, -19.499999999999996, -16.49999999999998,\n",
      "        -14.399999999999984, -4.499999999999998, 4.800000000000011, -14.39999999999999,\n",
      "        7.800000000000031, 12.600000000000012, -36.00000000000007, -10.499999999999995,\n",
      "        8.100000000000016, -2.999999999999994, -7.499999999999998, -7.200000000000008,\n",
      "        1.4072076837123859e-14, 9.600000000000014, -4.800000000000001, -4.499999999999977,\n",
      "        -13.499999999999979, -5.399999999999993, -1.4999999999999798, 1.1999999999999966,\n",
      "        -18.599999999999994, -8.399999999999991, 5.100000000000032, -4.500000000000004,\n",
      "        1.5737411374061594e-14, -10.49999999999998, -5.699999999999976, -9.299999999999995,\n",
      "        7.500000000000027, -10.499999999999979, 14.100000000000009, -2.3999999999999813,\n",
      "        -13.5, -0.8999999999999932, -13.499999999999991, 7.500000000000027, 3.600000000000031,\n",
      "        14.099999999999966, -0.8999999999999817, -9.299999999999978, 4.08006961549745e-15,\n",
      "        17.09999999999995, -9.299999999999974, -3.2999999999999847, -1.4999999999999758,\n",
      "        2.699999999999999, -17.4, -1.4999999999999747, 4.799999999999988, -3.8999999999999986,\n",
      "        10.79999999999996, -8.99999999999999, 11.700000000000012, 9.000000000000034,\n",
      "        -4.499999999999996, 5.9674487573602164e-15, 13.500000000000028]\n",
      "      policy_policy1_reward: [-3.5, -21.5, 11.5, -20.0, 14.5, -11.5, -9.5, -5.0, -8.0,\n",
      "        -8.5, 8.5, 6.5, 23.5, -8.0, 27.0, 0.5, 1.0, 4.0, 16.0, 10.0, 10.0, -9.5, -6.5,\n",
      "        -11.0, 5.5, 11.5, -5.5, 14.5, 21.5, -26.0, -0.5, 17.0, 7.0, 2.5, -0.5, 10.0,\n",
      "        18.5, 3.0, 5.5, -3.5, 3.5, 8.5, 9.0, -13.0, 0.5, 14.0, 5.5, 4.5, -0.5, 1.0,\n",
      "        -7.0, 17.5, -0.5, 23.0, 6.5, -3.5, 8.0, -3.5, 17.5, 12.5, 23.0, 8.0, -1.5, 10.0,\n",
      "        26.0, -1.5, 4.5, 8.5, 10.5, -8.5, 8.5, 11.5, 5.0, 17.5, 1.0, 19.5, 19.0, 5.5,\n",
      "        10.0, 23.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999988,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -3.399999999999983, -9.99999999999998, -6.699999999999992, -8.899999999999986,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -6.6999999999999895,\n",
      "        -9.99999999999998, -8.899999999999983, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -5.599999999999982, -8.89999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -4.500000000000004, -9.99999999999998, -6.699999999999994, -2.2999999999999954,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -7.799999999999984,\n",
      "        -9.99999999999998, -7.799999999999981, -8.899999999999984, -9.99999999999998,\n",
      "        -6.699999999999985, -8.89999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -7.799999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.0\n",
      "      policy2: -2.2999999999999954\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.93125\n",
      "      policy2: -8.913749999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -26.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07409186434820411\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.019619602671071008\n",
      "      mean_inference_ms: 4.133665522468267\n",
      "      mean_raw_obs_processing_ms: 0.40765176614362336\n",
      "  time_since_restore: 115.58960366249084\n",
      "  time_this_iter_s: 65.97584247589111\n",
      "  time_total_s: 115.58960366249084\n",
      "  timers:\n",
      "    learn_throughput: 113.427\n",
      "    learn_time_ms: 35265.089\n",
      "    synch_weights_time_ms: 2.494\n",
      "    training_iteration_time_ms: 57784.33\n",
      "  timestamp: 1660036718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0758_00002\n",
      "  warmup_time: 9.455713987350464\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-19-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.000000000000014\n",
      "  episode_reward_mean: -6.356249999999991\n",
      "  episode_reward_min: -28.500000000000057\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 1cb98bfd22c24e0b8d41d2e27afd2ddf\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3201271295547485\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019898025318980217\n",
      "          model: {}\n",
      "          policy_loss: -0.051767464727163315\n",
      "          total_loss: 6.933074951171875\n",
      "          vf_explained_var: -0.03166569769382477\n",
      "          vf_loss: 6.978873252868652\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3262674808502197\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018091607838869095\n",
      "          model: {}\n",
      "          policy_loss: -0.049046289175748825\n",
      "          total_loss: 1.9566360712051392\n",
      "          vf_explained_var: 0.35884666442871094\n",
      "          vf_loss: 2.0020639896392822\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.93516483516484\n",
      "    ram_util_percent: 66.61648351648354\n",
      "  pid: 11556\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: -2.300000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.475\n",
      "    policy2: -8.831249999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07178346470252199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.019692816428823152\n",
      "    mean_inference_ms: 6.941479248480441\n",
      "    mean_raw_obs_processing_ms: 0.4211974935374146\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.000000000000014\n",
      "    episode_reward_mean: -6.356249999999991\n",
      "    episode_reward_min: -28.500000000000057\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [-27.000000000000007, -15.899999999999988, 2.100000000000016,\n",
      "        -25.500000000000007, -13.499999999999986, 2.0999999999999965, -5.999999999999973,\n",
      "        -10.499999999999988, -28.500000000000057, -13.499999999999975, -15.00000000000001,\n",
      "        -11.99999999999998, -18.599999999999984, -7.79999999999999, -17.999999999999982,\n",
      "        1.5000000000000244, -10.799999999999983, -17.99999999999999, 3.600000000000015,\n",
      "        -7.500000000000002, -25.50000000000001, -2.999999999999997, -1.4999999999999993,\n",
      "        -22.50000000000003, -16.499999999999993, -14.999999999999996, 4.500000000000026,\n",
      "        -19.799999999999997, 2.699999999999999, 3.000000000000015, 1.529332216421153e-14,\n",
      "        -12.899999999999977, -28.500000000000018, -14.999999999999991, 5.9674487573602164e-15,\n",
      "        -2.9999999999999813, 7.5000000000000195, -7.499999999999982, 1.5000000000000235,\n",
      "        -11.99999999999999, -14.99999999999998, -1.7999999999999932, -5.999999999999991,\n",
      "        -11.99999999999998, -2.3999999999999786, -11.099999999999987, -19.50000000000002,\n",
      "        -10.499999999999975, 14.700000000000028, -5.399999999999981, 4.500000000000021,\n",
      "        -10.199999999999983, 7.50000000000003, -3.300000000000002, -4.8, 8.10000000000003,\n",
      "        3.0000000000000244, -0.8999999999999932, -8.999999999999993, -3.899999999999974,\n",
      "        17.099999999999945, 3.0000000000000044, 13.499999999999966, 18.000000000000014,\n",
      "        -8.099999999999977, -16.499999999999993, -4.499999999999991, -8.399999999999999,\n",
      "        -14.999999999999973, -0.599999999999996, 0.9000000000000031, 10.500000000000014,\n",
      "        -9.599999999999978, 15.600000000000025, -2.099999999999992, 9.00000000000003,\n",
      "        -5.9999999999999805, -21.000000000000007, -2.999999999999991, -20.999999999999993]\n",
      "      policy_policy1_reward: [-17.0, -12.5, 11.0, -15.5, -3.5, 5.5, 4.0, -0.5, -18.5,\n",
      "        -3.5, -5.0, -2.0, -13.0, 0.0, -8.0, 11.5, -3.0, -8.0, 12.5, 2.5, -15.5, 7.0,\n",
      "        8.5, -12.5, -6.5, -5.0, 14.5, -12.0, 10.5, 13.0, 10.0, -4.0, -18.5, -5.0, 10.0,\n",
      "        7.0, 17.5, 2.5, 11.5, -2.0, -5.0, 6.0, 4.0, -2.0, 6.5, -5.5, -9.5, -0.5, 22.5,\n",
      "        3.5, 14.5, -3.5, 17.5, 4.5, -2.5, 17.0, 13.0, 8.0, -4.5, 5.0, 26.0, 13.0, 23.5,\n",
      "        28.0, -2.5, -6.5, 0.0, 0.5, -5.0, 5.0, 6.5, 20.5, -4.0, 24.5, 3.5, 19.0, 4.0,\n",
      "        -11.0, 7.0, -11.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -3.3999999999999893, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.4000000000000035, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999984, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999994, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999985, -8.899999999999986, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999981, -2.300000000000004,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -4.499999999999997,\n",
      "        -8.899999999999986, -8.899999999999984, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.6, -9.99999999999998, -4.499999999999986, -8.899999999999986,\n",
      "        -9.99999999999998, -5.599999999999989, -5.6, -9.99999999999998, -5.599999999999986,\n",
      "        -8.89999999999998, -5.599999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: -2.300000000000004\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.475\n",
      "      policy2: -8.831249999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07178346470252199\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.019692816428823152\n",
      "      mean_inference_ms: 6.941479248480441\n",
      "      mean_raw_obs_processing_ms: 0.4211974935374146\n",
      "  time_since_restore: 129.91799592971802\n",
      "  time_this_iter_s: 64.46951651573181\n",
      "  time_total_s: 129.91799592971802\n",
      "  timers:\n",
      "    learn_throughput: 114.095\n",
      "    learn_time_ms: 35058.558\n",
      "    synch_weights_time_ms: 3.99\n",
      "    training_iteration_time_ms: 64954.51\n",
      "  timestamp: 1660036747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: e0758_00003\n",
      "  warmup_time: 9.446670532226562\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.899999999999972\n",
      "  episode_reward_mean: -1.2749999999999897\n",
      "  episode_reward_min: -39.90000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 3b43653afbe04acfbe6caf1cbf1e78d6\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.227555513381958\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017721593379974365\n",
      "          model: {}\n",
      "          policy_loss: -0.05772843956947327\n",
      "          total_loss: 6.43147087097168\n",
      "          vf_explained_var: 0.10493534058332443\n",
      "          vf_loss: 6.48122501373291\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2388557195663452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0186460018157959\n",
      "          model: {}\n",
      "          policy_loss: -0.057930782437324524\n",
      "          total_loss: 1.8932298421859741\n",
      "          vf_explained_var: 0.39291253685951233\n",
      "          vf_loss: 1.9455667734146118\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.89428571428571\n",
      "    ram_util_percent: 66.60000000000004\n",
      "  pid: 11124\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -2.2999999999999976\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.295\n",
      "    policy2: -8.569999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07134102655882782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021296120630605354\n",
      "    mean_inference_ms: 5.352071258559063\n",
      "    mean_raw_obs_processing_ms: 0.4184115092086935\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.899999999999972\n",
      "    episode_reward_mean: -1.2749999999999897\n",
      "    episode_reward_min: -39.90000000000008\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-13.49999999999999, -22.500000000000032, -8.399999999999991,\n",
      "        -39.90000000000008, 1.7957857423311907e-14, 15.00000000000001, -14.399999999999988,\n",
      "        -7.499999999999972, 1.0408340855860843e-14, -4.4999999999999725, 11.10000000000002,\n",
      "        -10.499999999999993, -0.8999999999999798, 3.600000000000023, 6.900000000000018,\n",
      "        6.900000000000027, 6.600000000000021, 5.100000000000003, -9.000000000000009,\n",
      "        3.000000000000031, -10.799999999999983, 3.0000000000000226, -10.499999999999982,\n",
      "        -20.400000000000006, -2.999999999999984, 4.200000000000024, 12.600000000000016,\n",
      "        -8.999999999999986, -10.49999999999998, -14.999999999999977, -8.999999999999975,\n",
      "        -10.799999999999976, -10.799999999999988, 0.6000000000000048, 12.300000000000002,\n",
      "        9.30000000000003, -3.300000000000001, -1.7999999999999794, 1.2000000000000264,\n",
      "        11.100000000000032, 10.20000000000003, 4.800000000000017, 3.0000000000000213,\n",
      "        13.49999999999999, -4.499999999999998, 6.000000000000004, 8.100000000000025,\n",
      "        -1.500000000000004, -0.29999999999999616, 3.000000000000016, -26.400000000000006,\n",
      "        1.607047828144914e-14, 5.1000000000000245, -0.29999999999999016, -5.099999999999985,\n",
      "        -5.399999999999994, 1.5000000000000235, 4.200000000000019, -3.8999999999999906,\n",
      "        -1.499999999999981, 0.6000000000000113, 2.100000000000005, -3.0000000000000013,\n",
      "        6.299999999999997, 3.000000000000005, -9.29999999999998, 6.300000000000031,\n",
      "        8.700000000000022, -7.499999999999984, -7.799999999999997, -18.900000000000034,\n",
      "        2.4000000000000132, 14.099999999999909, 14.100000000000028, 11.100000000000026,\n",
      "        1.4999999999999551, -13.499999999999986, -2.9999999999999933, -0.29999999999998395,\n",
      "        -8.999999999999973, -4.799999999999976, -21.30000000000003, 7.500000000000018,\n",
      "        7.800000000000008, -8.999999999999988, -1.499999999999996, 1.5000000000000142,\n",
      "        -6.299999999999988, 15.899999999999972, -5.999999999999975, 2.095545958979983e-14,\n",
      "        6.0000000000000195, -13.499999999999993, 2.1000000000000174, 2.550737399076297e-14,\n",
      "        0.6000000000000218, 7.2000000000000295, 6.900000000000016, 7.500000000000016,\n",
      "        -2.9999999999999973]\n",
      "      policy_policy1_reward: [-3.5, -12.5, 0.5, -31.0, 10.0, 25.0, -5.5, 2.5, 10.0,\n",
      "        5.5, 20.0, -0.5, 8.0, 12.5, 12.5, 12.5, 15.5, 14.0, 1.0, 13.0, -3.0, 13.0, -0.5,\n",
      "        -11.5, 7.0, 12.0, 21.5, 1.0, -0.5, -5.0, 1.0, -3.0, -3.0, 9.5, 19.0, 16.0, 4.5,\n",
      "        6.0, 9.0, 20.0, 18.0, 11.5, 13.0, 23.5, 5.5, 16.0, 17.0, 3.0, 7.5, 13.0, -17.5,\n",
      "        4.5, 14.0, 7.5, 0.5, 3.5, 11.5, 12.0, 5.0, 8.5, 9.5, 11.0, 7.0, 13.0, 13.0,\n",
      "        -7.0, 13.0, 16.5, -3.0, 0.0, -10.0, 8.0, 23.0, 23.0, 20.0, 11.5, -3.5, 7.0,\n",
      "        7.5, 1.0, 3.0, -13.5, 17.5, 14.5, 1.0, 8.5, 11.5, 1.5, 21.5, 4.0, 10.0, 16.0,\n",
      "        -3.5, 11.0, 10.0, 9.5, 9.5, 12.5, 17.5, 7.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.899999999999984,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -5.599999999999982,\n",
      "        -5.6, -8.899999999999986, -8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999989, -8.89999999999998, -6.6999999999999815, -6.699999999999984,\n",
      "        -7.799999999999989, -7.799999999999986, -7.79999999999999, -8.89999999999998,\n",
      "        -7.799999999999986, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -4.499999999999988,\n",
      "        -7.79999999999999, -9.99999999999998, -8.89999999999998, -4.49999999999999,\n",
      "        -8.89999999999998, -7.799999999999981, -5.6, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999986, -8.89999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -2.2999999999999976, -6.699999999999995, -7.7999999999999865, -4.500000000000003,\n",
      "        -7.7999999999999865, -8.89999999999998, -5.5999999999999845, -8.899999999999986,\n",
      "        -8.899999999999986, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999989, -9.99999999999998, -7.79999999999999,\n",
      "        -7.799999999999981, -9.99999999999998, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -5.59999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -9.99999999999998, -8.899999999999986, -2.2999999999999985,\n",
      "        -5.599999999999993, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -2.2999999999999976\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.295\n",
      "      policy2: -8.569999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -31.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07134102655882782\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021296120630605354\n",
      "      mean_inference_ms: 5.352071258559063\n",
      "      mean_raw_obs_processing_ms: 0.4184115092086935\n",
      "  time_since_restore: 170.1440212726593\n",
      "  time_this_iter_s: 49.4351065158844\n",
      "  time_total_s: 170.1440212726593\n",
      "  timers:\n",
      "    learn_throughput: 130.757\n",
      "    learn_time_ms: 22943.323\n",
      "    synch_weights_time_ms: 2.743\n",
      "    training_iteration_time_ms: 42532.016\n",
      "  timestamp: 1660036766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0758_00001\n",
      "  warmup_time: 9.458592891693115\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-19-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.79999999999994\n",
      "  episode_reward_mean: -0.9569999999999904\n",
      "  episode_reward_min: -26.100000000000016\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: d3dc894fb4884b018f96995278e9ac97\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2895902395248413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011515307240188122\n",
      "          model: {}\n",
      "          policy_loss: -0.042153920978307724\n",
      "          total_loss: 6.6395182609558105\n",
      "          vf_explained_var: 0.04767061769962311\n",
      "          vf_loss: 6.67936897277832\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2728452682495117\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012654934078454971\n",
      "          model: {}\n",
      "          policy_loss: -0.04468729346990585\n",
      "          total_loss: 2.0941426753997803\n",
      "          vf_explained_var: 0.3365166485309601\n",
      "          vf_loss: 2.136298894882202\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.92394366197183\n",
      "    ram_util_percent: 66.60000000000004\n",
      "  pid: 10368\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: -0.10000000000000467\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.14\n",
      "    policy2: -8.096999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -20.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07548229570523199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021311502229968346\n",
      "    mean_inference_ms: 5.123797485209596\n",
      "    mean_raw_obs_processing_ms: 0.4210926901688545\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.79999999999994\n",
      "    episode_reward_mean: -0.9569999999999904\n",
      "    episode_reward_min: -26.100000000000016\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-18.0, -5.399999999999979, 7.500000000000021, -7.19999999999998,\n",
      "        -14.999999999999984, 3.600000000000005, -7.199999999999997, -16.79999999999999,\n",
      "        -14.099999999999984, -17.99999999999998, -4.499999999999982, -11.099999999999982,\n",
      "        3.0000000000000293, 1.5000000000000218, -26.100000000000016, 2.1000000000000068,\n",
      "        2.9999999999999973, 10.200000000000022, -4.499999999999984, 3.0000000000000044,\n",
      "        -8.399999999999988, 2.7283730830163222e-14, -1.799999999999982, -4.4999999999999964,\n",
      "        16.499999999999922, 17.100000000000016, -2.999999999999971, 3.000000000000004,\n",
      "        -20.39999999999999, -5.999999999999988, -8.999999999999973, 2.6999999999999966,\n",
      "        -7.199999999999976, -1.799999999999994, 7.500000000000016, 13.499999999999956,\n",
      "        -3.0000000000000173, -24.000000000000036, 2.0999999999999965, -5.99999999999999,\n",
      "        14.999999999999947, -1.4999999999999791, -10.199999999999982, -1.7999999999999865,\n",
      "        -12.599999999999982, 10.500000000000023, -15.299999999999983, 10.500000000000023,\n",
      "        -4.499999999999983, 11.100000000000012, 14.999999999999977, -5.9999999999999805,\n",
      "        11.100000000000023, 2.5174307083375425e-14, -13.499999999999975, -7.199999999999974,\n",
      "        -4.499999999999998, -4.19999999999998, 17.09999999999993, -11.999999999999982,\n",
      "        -4.499999999999983, -1.8000000000000007, 1.5000000000000235, 2.7000000000000113,\n",
      "        0.9000000000000271, 5.700000000000033, 17.39999999999995, -12.299999999999976,\n",
      "        1.2000000000000113, -8.99999999999998, -5.999999999999998, 0.6000000000000091,\n",
      "        -10.499999999999996, 0.9000000000000129, 8.100000000000012, 2.1000000000000263,\n",
      "        1.8000000000000265, -10.49999999999998, -4.500000000000001, 13.200000000000026,\n",
      "        -0.2999999999999735, -3.8999999999999746, 19.79999999999994, 1.500000000000019,\n",
      "        2.700000000000007, 17.70000000000001, -3.8999999999999937, -5.999999999999998,\n",
      "        -7.499999999999984, -5.39999999999999, 5.700000000000031, 2.100000000000024,\n",
      "        -1.4999999999999725, 6.600000000000021, 5.100000000000027, 4.500000000000016,\n",
      "        -1.5000000000000002, 2.700000000000014, 6.599999999999996, 12.000000000000012]\n",
      "      policy_policy1_reward: [-8.0, 3.5, 17.5, -6.0, -5.0, 12.5, -0.5, -9.0, -8.5, -8.0,\n",
      "        5.5, -5.5, 13.0, 11.5, -20.5, 5.5, 13.0, 18.0, 5.5, 13.0, 0.5, 10.0, 0.5, 5.5,\n",
      "        21.0, 26.0, 7.0, 13.0, -11.5, 4.0, 1.0, 10.5, -0.5, 6.0, 17.5, 23.5, 7.0, -14.0,\n",
      "        11.0, 4.0, 19.5, 8.5, -3.5, 6.0, -12.5, 20.5, -7.5, 20.5, 5.5, 20.0, 25.0, 4.0,\n",
      "        20.0, 4.5, -3.5, -0.5, 5.5, 2.5, 26.0, -2.0, 5.5, 6.0, 11.5, 10.5, 6.5, 13.5,\n",
      "        23.0, -4.5, 9.0, -4.5, 4.0, 9.5, -0.5, 6.5, 17.0, 11.0, 8.5, -6.0, 5.5, 21.0,\n",
      "        7.5, 5.0, 26.5, 11.5, 10.5, 25.5, 5.0, 4.0, -3.0, -2.0, 13.5, 5.5, 8.5, 15.5,\n",
      "        14.0, 9.0, 8.5, 10.5, 15.5, 22.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -1.1999999999999862, -9.99999999999998, -8.89999999999998, -6.6999999999999815,\n",
      "        -7.799999999999986, -5.599999999999997, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999999, -9.99999999999998, -9.99999999999998, -5.599999999999984,\n",
      "        -3.400000000000003, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -9.99999999999998, -2.299999999999993,\n",
      "        -9.99999999999998, -4.499999999999982, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999982, -6.6999999999999815, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -4.499999999999985, -9.99999999999998, -6.699999999999993,\n",
      "        -7.799999999999981, -0.10000000000000467, -9.99999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -4.500000000000002, -9.99999999999998,\n",
      "        -6.699999999999994, -9.99999999999998, -6.6999999999999815, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -7.799999999999989, -5.599999999999998, -7.799999999999981, -5.599999999999991,\n",
      "        -7.799999999999981, -7.799999999999981, -4.500000000000003, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999996, -8.89999999999998,\n",
      "        -8.89999999999998, -6.699999999999994, -4.499999999999985, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999989, -8.89999999999998, -6.699999999999989,\n",
      "        -9.99999999999998, -7.799999999999989, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -4.499999999999988, -3.399999999999989, -7.799999999999988,\n",
      "        -3.400000000000006, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -4.500000000000003, -9.99999999999998, -7.799999999999989, -8.89999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: -0.10000000000000467\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.14\n",
      "      policy2: -8.096999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -20.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07548229570523199\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021311502229968346\n",
      "      mean_inference_ms: 5.123797485209596\n",
      "      mean_raw_obs_processing_ms: 0.4210926901688545\n",
      "  time_since_restore: 169.5336046218872\n",
      "  time_this_iter_s: 49.70239233970642\n",
      "  time_total_s: 169.5336046218872\n",
      "  timers:\n",
      "    learn_throughput: 129.867\n",
      "    learn_time_ms: 23100.64\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 42375.921\n",
      "  timestamp: 1660036768\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0758_00000\n",
      "  warmup_time: 10.224120855331421\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-19-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999992\n",
      "  episode_reward_mean: -1.3589999999999935\n",
      "  episode_reward_min: -36.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 3bb20841742c49d7bcfe50360fad8c16\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.318493127822876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015061221085488796\n",
      "          model: {}\n",
      "          policy_loss: -0.036178961396217346\n",
      "          total_loss: 7.113546848297119\n",
      "          vf_explained_var: 0.11226260662078857\n",
      "          vf_loss: 7.146713733673096\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.316589593887329\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011725395917892456\n",
      "          model: {}\n",
      "          policy_loss: -0.03686296567320824\n",
      "          total_loss: 2.0863356590270996\n",
      "          vf_explained_var: 0.27822789549827576\n",
      "          vf_loss: 2.1208536624908447\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.14468085106383\n",
      "    ram_util_percent: 66.60106382978728\n",
      "  pid: 17352\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -1.1999999999999873\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.255\n",
      "    policy2: -8.613999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -26.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07336783096438902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020300513564589458\n",
      "    mean_inference_ms: 4.975499318382527\n",
      "    mean_raw_obs_processing_ms: 0.4125254696502485\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.59999999999992\n",
      "    episode_reward_mean: -1.3589999999999935\n",
      "    episode_reward_min: -36.00000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [9.520162436160717e-15, -19.499999999999996, -16.49999999999998,\n",
      "        -14.399999999999984, -4.499999999999998, 4.800000000000011, -14.39999999999999,\n",
      "        7.800000000000031, 12.600000000000012, -36.00000000000007, -10.499999999999995,\n",
      "        8.100000000000016, -2.999999999999994, -7.499999999999998, -7.200000000000008,\n",
      "        1.4072076837123859e-14, 9.600000000000014, -4.800000000000001, -4.499999999999977,\n",
      "        -13.499999999999979, -5.399999999999993, -1.4999999999999798, 1.1999999999999966,\n",
      "        -18.599999999999994, -8.399999999999991, 5.100000000000032, -4.500000000000004,\n",
      "        1.5737411374061594e-14, -10.49999999999998, -5.699999999999976, -9.299999999999995,\n",
      "        7.500000000000027, -10.499999999999979, 14.100000000000009, -2.3999999999999813,\n",
      "        -13.5, -0.8999999999999932, -13.499999999999991, 7.500000000000027, 3.600000000000031,\n",
      "        14.099999999999966, -0.8999999999999817, -9.299999999999978, 4.08006961549745e-15,\n",
      "        17.09999999999995, -9.299999999999974, -3.2999999999999847, -1.4999999999999758,\n",
      "        2.699999999999999, -17.4, -1.4999999999999747, 4.799999999999988, -3.8999999999999986,\n",
      "        10.79999999999996, -8.99999999999999, 11.700000000000012, 9.000000000000034,\n",
      "        -4.499999999999996, 5.9674487573602164e-15, 13.500000000000028, -7.499999999999984,\n",
      "        -8.399999999999974, -7.499999999999989, 13.500000000000018, 16.49999999999995,\n",
      "        11.99999999999998, 1.5000000000000193, 5.10000000000003, -5.399999999999983,\n",
      "        -2.400000000000003, 8.699999999999985, 2.7000000000000077, 3.000000000000033,\n",
      "        6.000000000000027, 7.800000000000017, -2.3999999999999755, 2.328692794151266e-14,\n",
      "        -9.599999999999985, 17.09999999999995, -17.399999999999984, -24.000000000000007,\n",
      "        -11.699999999999983, 2.1000000000000214, 2.100000000000011, -3.299999999999986,\n",
      "        14.999999999999979, 11.100000000000023, 18.59999999999992, 1.9845236565174673e-14,\n",
      "        -7.50000000000003, -3.8999999999999875, 13.19999999999993, -16.199999999999996,\n",
      "        2.100000000000019, -7.499999999999976, -11.999999999999998, 2.9999999999999503,\n",
      "        1.499999999999988, 7.200000000000021, -2.999999999999978]\n",
      "      policy_policy1_reward: [10.0, -9.5, -6.5, -11.0, 5.5, 11.5, -5.5, 14.5, 21.5,\n",
      "        -26.0, -0.5, 17.0, 7.0, 2.5, -0.5, 10.0, 18.5, 3.0, 5.5, -3.5, 3.5, 8.5, 9.0,\n",
      "        -13.0, 0.5, 14.0, 5.5, 4.5, -0.5, 1.0, -7.0, 17.5, -0.5, 23.0, 6.5, -3.5, 8.0,\n",
      "        -3.5, 17.5, 12.5, 23.0, 8.0, -1.5, 10.0, 26.0, -1.5, 4.5, 8.5, 10.5, -8.5, 8.5,\n",
      "        11.5, 5.0, 17.5, 1.0, 19.5, 19.0, 5.5, 10.0, 23.5, 2.5, 0.5, -3.0, 23.5, 26.5,\n",
      "        22.0, 6.0, 14.0, 3.5, 6.5, 16.5, 10.5, 13.0, 16.0, 14.5, 6.5, 10.0, -4.0, 26.0,\n",
      "        -8.5, -14.0, -5.0, 11.0, 11.0, 4.5, 25.0, 20.0, 27.5, 4.5, 2.5, 5.0, 21.0, -15.0,\n",
      "        11.0, 2.5, -2.0, 13.0, 11.5, 15.0, 7.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -3.399999999999983, -9.99999999999998, -6.699999999999992, -8.899999999999986,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -6.6999999999999895,\n",
      "        -9.99999999999998, -8.899999999999983, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -5.599999999999982, -8.89999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -4.500000000000004, -9.99999999999998, -6.699999999999994, -2.2999999999999954,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -7.799999999999984,\n",
      "        -9.99999999999998, -7.799999999999981, -8.899999999999984, -9.99999999999998,\n",
      "        -6.699999999999985, -8.89999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -7.799999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -4.499999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -4.5000000000000036,\n",
      "        -8.899999999999986, -8.899999999999986, -8.89999999999998, -7.799999999999985,\n",
      "        -7.799999999999984, -9.99999999999998, -9.99999999999998, -6.69999999999999,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999999, -8.899999999999986,\n",
      "        -8.899999999999984, -9.99999999999998, -6.6999999999999815, -8.899999999999983,\n",
      "        -8.89999999999998, -7.799999999999987, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -4.500000000000002, -9.99999999999998, -8.899999999999984,\n",
      "        -7.799999999999981, -1.1999999999999873, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: -1.1999999999999873\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.255\n",
      "      policy2: -8.613999999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -26.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07336783096438902\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020300513564589458\n",
      "      mean_inference_ms: 4.975499318382527\n",
      "      mean_raw_obs_processing_ms: 0.4125254696502485\n",
      "  time_since_restore: 181.92351150512695\n",
      "  time_this_iter_s: 66.33390784263611\n",
      "  time_total_s: 181.92351150512695\n",
      "  timers:\n",
      "    learn_throughput: 112.662\n",
      "    learn_time_ms: 35504.538\n",
      "    synch_weights_time_ms: 2.66\n",
      "    training_iteration_time_ms: 60631.53\n",
      "  timestamp: 1660036784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0758_00002\n",
      "  warmup_time: 9.455713987350464\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-20-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.99999999999997\n",
      "  episode_reward_mean: -1.80299999999999\n",
      "  episode_reward_min: -28.500000000000018\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 1cb98bfd22c24e0b8d41d2e27afd2ddf\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.282413363456726\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020683566108345985\n",
      "          model: {}\n",
      "          policy_loss: -0.0538611002266407\n",
      "          total_loss: 6.349396705627441\n",
      "          vf_explained_var: 0.09741291403770447\n",
      "          vf_loss: 6.397053241729736\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2826365232467651\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020491817966103554\n",
      "          model: {}\n",
      "          policy_loss: -0.05363612622022629\n",
      "          total_loss: 1.9846175909042358\n",
      "          vf_explained_var: 0.28940194845199585\n",
      "          vf_loss: 2.0341553688049316\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.571276595744678\n",
      "    ram_util_percent: 66.57553191489362\n",
      "  pid: 11556\n",
      "  policy_reward_max:\n",
      "    policy1: 34.0\n",
      "    policy2: -2.300000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.965\n",
      "    policy2: -8.767999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07142483346798727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020708188443623973\n",
      "    mean_inference_ms: 6.940136709329661\n",
      "    mean_raw_obs_processing_ms: 0.4182736376006875\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.99999999999997\n",
      "    episode_reward_mean: -1.80299999999999\n",
      "    episode_reward_min: -28.500000000000018\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-25.50000000000001, -2.999999999999997, -1.4999999999999993,\n",
      "        -22.50000000000003, -16.499999999999993, -14.999999999999996, 4.500000000000026,\n",
      "        -19.799999999999997, 2.699999999999999, 3.000000000000015, 1.529332216421153e-14,\n",
      "        -12.899999999999977, -28.500000000000018, -14.999999999999991, 5.9674487573602164e-15,\n",
      "        -2.9999999999999813, 7.5000000000000195, -7.499999999999982, 1.5000000000000235,\n",
      "        -11.99999999999999, -14.99999999999998, -1.7999999999999932, -5.999999999999991,\n",
      "        -11.99999999999998, -2.3999999999999786, -11.099999999999987, -19.50000000000002,\n",
      "        -10.499999999999975, 14.700000000000028, -5.399999999999981, 4.500000000000021,\n",
      "        -10.199999999999983, 7.50000000000003, -3.300000000000002, -4.8, 8.10000000000003,\n",
      "        3.0000000000000244, -0.8999999999999932, -8.999999999999993, -3.899999999999974,\n",
      "        17.099999999999945, 3.0000000000000044, 13.499999999999966, 18.000000000000014,\n",
      "        -8.099999999999977, -16.499999999999993, -4.499999999999991, -8.399999999999999,\n",
      "        -14.999999999999973, -0.599999999999996, 0.9000000000000031, 10.500000000000014,\n",
      "        -9.599999999999978, 15.600000000000025, -2.099999999999992, 9.00000000000003,\n",
      "        -5.9999999999999805, -21.000000000000007, -2.999999999999991, -20.999999999999993,\n",
      "        5.400000000000023, 0.5999999999999872, 4.200000000000016, -9.899999999999984,\n",
      "        10.500000000000021, 6.00000000000003, -16.800000000000054, -2.9999999999999796,\n",
      "        -2.9999999999999947, 1.5000000000000182, -4.499999999999975, 9.000000000000032,\n",
      "        13.500000000000028, 9.600000000000014, 6.000000000000031, 1.0852430065710905e-14,\n",
      "        -9.899999999999977, 8.999999999999991, 2.9999999999999636, 6.600000000000028,\n",
      "        23.99999999999997, 1.200000000000012, 6.000000000000027, 1.5000000000000207,\n",
      "        -8.699999999999978, 1.5000000000000053, -0.29999999999999394, 12.600000000000017,\n",
      "        4.800000000000015, -1.4999999999999907, 2.7000000000000015, -1.1999999999999762,\n",
      "        -3.8999999999999964, -9.899999999999975, 3.0000000000000293, 7.500000000000027,\n",
      "        -3.0000000000000013, 13.499999999999948, -6.89999999999999, 8.700000000000012]\n",
      "      policy_policy1_reward: [-15.5, 7.0, 8.5, -12.5, -6.5, -5.0, 14.5, -12.0, 10.5,\n",
      "        13.0, 10.0, -4.0, -18.5, -5.0, 10.0, 7.0, 17.5, 2.5, 11.5, -2.0, -5.0, 6.0,\n",
      "        4.0, -2.0, 6.5, -5.5, -9.5, -0.5, 22.5, 3.5, 14.5, -3.5, 17.5, 4.5, -2.5, 17.0,\n",
      "        13.0, 8.0, -4.5, 5.0, 26.0, 13.0, 23.5, 28.0, -2.5, -6.5, 0.0, 0.5, -5.0, 5.0,\n",
      "        6.5, 20.5, -4.0, 24.5, 3.5, 19.0, 4.0, -11.0, 7.0, -11.0, 11.0, 9.5, 12.0, -1.0,\n",
      "        20.5, 16.0, -9.0, 7.0, 7.0, 11.5, 5.5, 19.0, 23.5, 18.5, 16.0, 10.0, -1.0, 13.5,\n",
      "        7.5, 15.5, 34.0, 9.0, 10.5, 11.5, -2.0, 11.5, 7.5, 21.5, 11.5, 8.5, 10.5, 5.5,\n",
      "        5.0, -1.0, 13.0, 17.5, 7.0, 23.5, 2.0, 16.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999994, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999985, -8.899999999999986, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999981, -2.300000000000004,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -4.499999999999997,\n",
      "        -8.899999999999986, -8.899999999999984, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.6, -9.99999999999998, -4.499999999999986, -8.899999999999986,\n",
      "        -9.99999999999998, -5.599999999999989, -5.6, -9.99999999999998, -5.599999999999986,\n",
      "        -8.89999999999998, -5.599999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -8.899999999999986, -7.79999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -4.500000000000002, -4.500000000000003, -8.899999999999986, -9.99999999999998,\n",
      "        -7.79999999999999, -4.499999999999999, -9.99999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -7.7999999999999865, -8.89999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -7.799999999999989, -6.699999999999995, -8.89999999999998,\n",
      "        -8.899999999999984, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -7.79999999999999]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.0\n",
      "      policy2: -2.300000000000004\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.965\n",
      "      policy2: -8.767999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07142483346798727\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020708188443623973\n",
      "      mean_inference_ms: 6.940136709329661\n",
      "      mean_raw_obs_processing_ms: 0.4182736376006875\n",
      "  time_since_restore: 195.73601865768433\n",
      "  time_this_iter_s: 65.81802272796631\n",
      "  time_total_s: 195.73601865768433\n",
      "  timers:\n",
      "    learn_throughput: 113.043\n",
      "    learn_time_ms: 35384.705\n",
      "    synch_weights_time_ms: 3.657\n",
      "    training_iteration_time_ms: 65238.026\n",
      "  timestamp: 1660036813\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: e0758_00003\n",
      "  warmup_time: 9.446670532226562\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00001:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-20-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.29999999999997\n",
      "  episode_reward_mean: 0.5160000000000098\n",
      "  episode_reward_min: -26.400000000000006\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 3b43653afbe04acfbe6caf1cbf1e78d6\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.202102780342102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0178569033741951\n",
      "          model: {}\n",
      "          policy_loss: -0.057211413979530334\n",
      "          total_loss: 6.646420478820801\n",
      "          vf_explained_var: 0.033619754016399384\n",
      "          vf_loss: 6.695595741271973\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2055970430374146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018602000549435616\n",
      "          model: {}\n",
      "          policy_loss: -0.05841189622879028\n",
      "          total_loss: 2.1737167835235596\n",
      "          vf_explained_var: 0.2687016427516937\n",
      "          vf_loss: 2.226548194885254\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.835714285714285\n",
      "    ram_util_percent: 66.56428571428572\n",
      "  pid: 11124\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: -2.2999999999999976\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.91\n",
      "    policy2: -8.393999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -17.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07202031453057232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02147005030174885\n",
      "    mean_inference_ms: 5.857548661836335\n",
      "    mean_raw_obs_processing_ms: 0.4209042464078729\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 30.29999999999997\n",
      "    episode_reward_mean: 0.5160000000000098\n",
      "    episode_reward_min: -26.400000000000006\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.999999999999975, -10.799999999999976, -10.799999999999988,\n",
      "        0.6000000000000048, 12.300000000000002, 9.30000000000003, -3.300000000000001,\n",
      "        -1.7999999999999794, 1.2000000000000264, 11.100000000000032, 10.20000000000003,\n",
      "        4.800000000000017, 3.0000000000000213, 13.49999999999999, -4.499999999999998,\n",
      "        6.000000000000004, 8.100000000000025, -1.500000000000004, -0.29999999999999616,\n",
      "        3.000000000000016, -26.400000000000006, 1.607047828144914e-14, 5.1000000000000245,\n",
      "        -0.29999999999999016, -5.099999999999985, -5.399999999999994, 1.5000000000000235,\n",
      "        4.200000000000019, -3.8999999999999906, -1.499999999999981, 0.6000000000000113,\n",
      "        2.100000000000005, -3.0000000000000013, 6.299999999999997, 3.000000000000005,\n",
      "        -9.29999999999998, 6.300000000000031, 8.700000000000022, -7.499999999999984,\n",
      "        -7.799999999999997, -18.900000000000034, 2.4000000000000132, 14.099999999999909,\n",
      "        14.100000000000028, 11.100000000000026, 1.4999999999999551, -13.499999999999986,\n",
      "        -2.9999999999999933, -0.29999999999998395, -8.999999999999973, -4.799999999999976,\n",
      "        -21.30000000000003, 7.500000000000018, 7.800000000000008, -8.999999999999988,\n",
      "        -1.499999999999996, 1.5000000000000142, -6.299999999999988, 15.899999999999972,\n",
      "        -5.999999999999975, 2.095545958979983e-14, 6.0000000000000195, -13.499999999999993,\n",
      "        2.1000000000000174, 2.550737399076297e-14, 0.6000000000000218, 7.2000000000000295,\n",
      "        6.900000000000016, 7.500000000000016, -2.9999999999999973, -8.399999999999995,\n",
      "        -12.299999999999986, 15.000000000000002, -0.8999999999999702, -1.4999999999999984,\n",
      "        2.1000000000000143, 3.0000000000000115, -12.000000000000005, -6.899999999999984,\n",
      "        5.4000000000000234, 1.5000000000000244, 7.743805596760467e-15, 1.5000000000000084,\n",
      "        30.29999999999997, 8.100000000000012, -1.800000000000003, 19.499999999999908,\n",
      "        0.600000000000006, 5.700000000000008, 2.700000000000008, 5.4000000000000234,\n",
      "        0.30000000000002225, -0.8999999999999808, -8.09999999999998, 3.0000000000000253,\n",
      "        1.5000000000000102, 4.500000000000007, -1.4999999999999767, -8.999999999999977,\n",
      "        2.273181642920008e-14]\n",
      "      policy_policy1_reward: [1.0, -3.0, -3.0, 9.5, 19.0, 16.0, 4.5, 6.0, 9.0, 20.0,\n",
      "        18.0, 11.5, 13.0, 23.5, 5.5, 16.0, 17.0, 3.0, 7.5, 13.0, -17.5, 4.5, 14.0, 7.5,\n",
      "        0.5, 3.5, 11.5, 12.0, 5.0, 8.5, 9.5, 11.0, 7.0, 13.0, 13.0, -7.0, 13.0, 16.5,\n",
      "        -3.0, 0.0, -10.0, 8.0, 23.0, 23.0, 20.0, 11.5, -3.5, 7.0, 7.5, 1.0, 3.0, -13.5,\n",
      "        17.5, 14.5, 1.0, 8.5, 11.5, 1.5, 21.5, 4.0, 10.0, 16.0, -3.5, 11.0, 10.0, 9.5,\n",
      "        9.5, 12.5, 17.5, 7.0, 0.5, -4.5, 25.0, 8.0, 8.5, 5.5, 13.0, -2.0, 2.0, 11.0,\n",
      "        11.5, 10.0, 11.5, 37.0, 17.0, 6.0, 29.5, 9.5, 13.5, 10.5, 11.0, 7.0, 8.0, -2.5,\n",
      "        13.0, 11.5, 14.5, 8.5, 1.0, 10.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -7.799999999999981, -7.799999999999989,\n",
      "        -8.89999999999998, -6.6999999999999815, -6.699999999999984, -7.799999999999989,\n",
      "        -7.799999999999986, -7.79999999999999, -8.89999999999998, -7.799999999999986,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -4.499999999999988, -7.79999999999999,\n",
      "        -9.99999999999998, -8.89999999999998, -4.49999999999999, -8.89999999999998,\n",
      "        -7.799999999999981, -5.6, -8.89999999999998, -9.99999999999998, -7.799999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -9.99999999999998, -2.2999999999999976,\n",
      "        -6.699999999999995, -7.7999999999999865, -4.500000000000003, -7.7999999999999865,\n",
      "        -8.89999999999998, -5.5999999999999845, -8.899999999999986, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999989, -9.99999999999998, -7.79999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -5.59999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -8.899999999999986, -2.2999999999999985, -5.599999999999993,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -7.799999999999981,\n",
      "        -9.99999999999998, -8.899999999999986, -9.99999999999998, -3.3999999999999972,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -5.5999999999999845,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.69999999999999,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -8.899999999999986,\n",
      "        -7.799999999999981, -7.799999999999981, -5.599999999999984, -6.6999999999999815,\n",
      "        -8.899999999999984, -5.599999999999991, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 37.0\n",
      "      policy2: -2.2999999999999976\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.91\n",
      "      policy2: -8.393999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -17.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07202031453057232\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02147005030174885\n",
      "      mean_inference_ms: 5.857548661836335\n",
      "      mean_raw_obs_processing_ms: 0.4209042464078729\n",
      "  time_since_restore: 219.33483743667603\n",
      "  time_this_iter_s: 49.190816164016724\n",
      "  time_total_s: 219.33483743667603\n",
      "  timers:\n",
      "    learn_throughput: 126.234\n",
      "    learn_time_ms: 23765.362\n",
      "    synch_weights_time_ms: 2.793\n",
      "    training_iteration_time_ms: 43862.38\n",
      "  timestamp: 1660036815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0758_00001\n",
      "  warmup_time: 9.458592891693115\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.199999999999985\n",
      "  episode_reward_mean: 0.9810000000000099\n",
      "  episode_reward_min: -24.000000000000036\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: d3dc894fb4884b018f96995278e9ac97\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2447293996810913\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011805289424955845\n",
      "          model: {}\n",
      "          policy_loss: -0.04394349828362465\n",
      "          total_loss: 6.960835933685303\n",
      "          vf_explained_var: 0.08763283491134644\n",
      "          vf_loss: 7.002418518066406\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2356480360031128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01016547903418541\n",
      "          model: {}\n",
      "          policy_loss: -0.03500784933567047\n",
      "          total_loss: 2.0335447788238525\n",
      "          vf_explained_var: 0.3880710303783417\n",
      "          vf_loss: 2.0665194988250732\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.912857142857145\n",
      "    ram_util_percent: 66.55857142857144\n",
      "  pid: 10368\n",
      "  policy_reward_max:\n",
      "    policy1: 33.0\n",
      "    policy2: -0.10000000000000467\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.935\n",
      "    policy2: -7.953999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0751823593052782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021448077465634565\n",
      "    mean_inference_ms: 5.723190631397952\n",
      "    mean_raw_obs_processing_ms: 0.4249474750577398\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 25.199999999999985\n",
      "    episode_reward_mean: 0.9810000000000099\n",
      "    episode_reward_min: -24.000000000000036\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.999999999999973, 2.6999999999999966, -7.199999999999976, -1.799999999999994,\n",
      "        7.500000000000016, 13.499999999999956, -3.0000000000000173, -24.000000000000036,\n",
      "        2.0999999999999965, -5.99999999999999, 14.999999999999947, -1.4999999999999791,\n",
      "        -10.199999999999982, -1.7999999999999865, -12.599999999999982, 10.500000000000023,\n",
      "        -15.299999999999983, 10.500000000000023, -4.499999999999983, 11.100000000000012,\n",
      "        14.999999999999977, -5.9999999999999805, 11.100000000000023, 2.5174307083375425e-14,\n",
      "        -13.499999999999975, -7.199999999999974, -4.499999999999998, -4.19999999999998,\n",
      "        17.09999999999993, -11.999999999999982, -4.499999999999983, -1.8000000000000007,\n",
      "        1.5000000000000235, 2.7000000000000113, 0.9000000000000271, 5.700000000000033,\n",
      "        17.39999999999995, -12.299999999999976, 1.2000000000000113, -8.99999999999998,\n",
      "        -5.999999999999998, 0.6000000000000091, -10.499999999999996, 0.9000000000000129,\n",
      "        8.100000000000012, 2.1000000000000263, 1.8000000000000265, -10.49999999999998,\n",
      "        -4.500000000000001, 13.200000000000026, -0.2999999999999735, -3.8999999999999746,\n",
      "        19.79999999999994, 1.500000000000019, 2.700000000000007, 17.70000000000001,\n",
      "        -3.8999999999999937, -5.999999999999998, -7.499999999999984, -5.39999999999999,\n",
      "        5.700000000000031, 2.100000000000024, -1.4999999999999725, 6.600000000000021,\n",
      "        5.100000000000027, 4.500000000000016, -1.5000000000000002, 2.700000000000014,\n",
      "        6.599999999999996, 12.000000000000012, 10.20000000000003, 3.6000000000000263,\n",
      "        11.100000000000007, -7.799999999999979, -4.499999999999995, -2.39999999999998,\n",
      "        8.699999999999953, 15.59999999999994, 6.900000000000022, -1.4999999999999782,\n",
      "        25.199999999999985, 6.600000000000017, -0.2999999999999837, 4.800000000000031,\n",
      "        -6.299999999999995, -10.199999999999996, 1.500000000000028, 3.2999999999999767,\n",
      "        7.2000000000000295, 4.500000000000021, 4.200000000000006, 10.500000000000032,\n",
      "        0.900000000000028, 4.80000000000001, -7.499999999999975, 4.500000000000034,\n",
      "        -13.499999999999986, -5.999999999999984, -13.499999999999995, 11.700000000000015]\n",
      "      policy_policy1_reward: [1.0, 10.5, -0.5, 6.0, 17.5, 23.5, 7.0, -14.0, 11.0, 4.0,\n",
      "        19.5, 8.5, -3.5, 6.0, -12.5, 20.5, -7.5, 20.5, 5.5, 20.0, 25.0, 4.0, 20.0, 4.5,\n",
      "        -3.5, -0.5, 5.5, 2.5, 26.0, -2.0, 5.5, 6.0, 11.5, 10.5, 6.5, 13.5, 23.0, -4.5,\n",
      "        9.0, -4.5, 4.0, 9.5, -0.5, 6.5, 17.0, 11.0, 8.5, -6.0, 5.5, 21.0, 7.5, 5.0,\n",
      "        26.5, 11.5, 10.5, 25.5, 5.0, 4.0, -3.0, -2.0, 13.5, 5.5, 8.5, 15.5, 14.0, 9.0,\n",
      "        8.5, 10.5, 15.5, 22.0, 18.0, 7.0, 20.0, 0.0, 5.5, 6.5, 16.5, 24.5, 12.5, 3.0,\n",
      "        33.0, 15.5, 2.0, 11.5, 1.5, -3.5, 11.5, 10.0, 15.0, 14.5, 12.0, 20.5, 6.5, 11.5,\n",
      "        2.5, 14.5, -3.5, -1.5, -3.5, 19.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -7.799999999999982, -6.6999999999999815,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -4.499999999999985,\n",
      "        -9.99999999999998, -6.699999999999993, -7.799999999999981, -0.10000000000000467,\n",
      "        -9.99999999999998, -7.799999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -4.500000000000002, -9.99999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -6.6999999999999815, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -7.799999999999989, -5.599999999999998,\n",
      "        -7.799999999999981, -5.599999999999991, -7.799999999999981, -7.799999999999981,\n",
      "        -4.500000000000003, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -5.599999999999996, -8.89999999999998, -8.89999999999998, -6.699999999999994,\n",
      "        -4.499999999999985, -9.99999999999998, -7.799999999999981, -7.799999999999989,\n",
      "        -8.89999999999998, -6.699999999999989, -9.99999999999998, -7.799999999999989,\n",
      "        -7.799999999999981, -8.89999999999998, -9.99999999999998, -4.499999999999988,\n",
      "        -3.399999999999989, -7.799999999999988, -3.400000000000006, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999986, -4.500000000000003, -9.99999999999998,\n",
      "        -7.799999999999989, -8.89999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -3.3999999999999835, -8.89999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999989, -8.899999999999983, -5.599999999999982,\n",
      "        -4.499999999999998, -7.79999999999999, -8.89999999999998, -2.299999999999989,\n",
      "        -6.699999999999995, -7.799999999999983, -6.6999999999999815, -9.99999999999998,\n",
      "        -6.6999999999999815, -7.799999999999981, -9.99999999999998, -7.7999999999999865,\n",
      "        -9.99999999999998, -5.599999999999994, -6.699999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -4.500000000000003, -9.99999999999998,\n",
      "        -7.799999999999984]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.0\n",
      "      policy2: -0.10000000000000467\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.935\n",
      "      policy2: -7.953999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -14.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0751823593052782\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021448077465634565\n",
      "      mean_inference_ms: 5.723190631397952\n",
      "      mean_raw_obs_processing_ms: 0.4249474750577398\n",
      "  time_since_restore: 218.98472476005554\n",
      "  time_this_iter_s: 49.451120138168335\n",
      "  time_total_s: 218.98472476005554\n",
      "  timers:\n",
      "    learn_throughput: 125.255\n",
      "    learn_time_ms: 23951.056\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 43789.564\n",
      "  timestamp: 1660036817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0758_00000\n",
      "  warmup_time: 10.224120855331421\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-20-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999992\n",
      "  episode_reward_mean: 0.1230000000000095\n",
      "  episode_reward_min: -28.500000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 3bb20841742c49d7bcfe50360fad8c16\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2897876501083374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01130727306008339\n",
      "          model: {}\n",
      "          policy_loss: -0.03831830248236656\n",
      "          total_loss: 6.665258884429932\n",
      "          vf_explained_var: 0.1030181273818016\n",
      "          vf_loss: 6.7013163566589355\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.269714593887329\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014580680988729\n",
      "          model: {}\n",
      "          policy_loss: -0.04160141199827194\n",
      "          total_loss: 2.219748020172119\n",
      "          vf_explained_var: 0.28719407320022583\n",
      "          vf_loss: 2.2584331035614014\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.89148936170213\n",
      "    ram_util_percent: 66.53936170212769\n",
      "  pid: 17352\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -1.1999999999999873\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.495\n",
      "    policy2: -8.371999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07240402084493011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02047821646050008\n",
      "    mean_inference_ms: 5.766995019143565\n",
      "    mean_raw_obs_processing_ms: 0.4162540808438597\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.59999999999992\n",
      "    episode_reward_mean: 0.1230000000000095\n",
      "    episode_reward_min: -28.500000000000014\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [14.099999999999966, -0.8999999999999817, -9.299999999999978,\n",
      "        4.08006961549745e-15, 17.09999999999995, -9.299999999999974, -3.2999999999999847,\n",
      "        -1.4999999999999758, 2.699999999999999, -17.4, -1.4999999999999747, 4.799999999999988,\n",
      "        -3.8999999999999986, 10.79999999999996, -8.99999999999999, 11.700000000000012,\n",
      "        9.000000000000034, -4.499999999999996, 5.9674487573602164e-15, 13.500000000000028,\n",
      "        -7.499999999999984, -8.399999999999974, -7.499999999999989, 13.500000000000018,\n",
      "        16.49999999999995, 11.99999999999998, 1.5000000000000193, 5.10000000000003,\n",
      "        -5.399999999999983, -2.400000000000003, 8.699999999999985, 2.7000000000000077,\n",
      "        3.000000000000033, 6.000000000000027, 7.800000000000017, -2.3999999999999755,\n",
      "        2.328692794151266e-14, -9.599999999999985, 17.09999999999995, -17.399999999999984,\n",
      "        -24.000000000000007, -11.699999999999983, 2.1000000000000214, 2.100000000000011,\n",
      "        -3.299999999999986, 14.999999999999979, 11.100000000000023, 18.59999999999992,\n",
      "        1.9845236565174673e-14, -7.50000000000003, -3.8999999999999875, 13.19999999999993,\n",
      "        -16.199999999999996, 2.100000000000019, -7.499999999999976, -11.999999999999998,\n",
      "        2.9999999999999503, 1.499999999999988, 7.200000000000021, -2.999999999999978,\n",
      "        -3.899999999999978, -12.899999999999975, -5.999999999999977, -0.5999999999999744,\n",
      "        -1.7999999999999816, -10.499999999999977, 3.6000000000000063, -23.100000000000023,\n",
      "        2.858824288409778e-15, -3.299999999999981, -9.89999999999997, -28.500000000000014,\n",
      "        6.9000000000000234, 2.195466031196247e-14, 5.100000000000033, 6.6000000000000245,\n",
      "        12.000000000000032, 3.3000000000000123, 15.000000000000014, 10.200000000000031,\n",
      "        -5.999999999999982, 14.10000000000002, 2.100000000000028, 1.2000000000000113,\n",
      "        1.8000000000000247, -11.999999999999979, 2.1000000000000174, 2.700000000000023,\n",
      "        3.000000000000028, -4.499999999999993, -3.2999999999999776, 0.6000000000000264,\n",
      "        -10.499999999999977, 2.7000000000000104, -1.4999999999999796, 3.300000000000016,\n",
      "        2.400000000000017, 5.40000000000002, 1.5000000000000262, 1.8000000000000114]\n",
      "      policy_policy1_reward: [23.0, 8.0, -1.5, 10.0, 26.0, -1.5, 4.5, 8.5, 10.5, -8.5,\n",
      "        8.5, 11.5, 5.0, 17.5, 1.0, 19.5, 19.0, 5.5, 10.0, 23.5, 2.5, 0.5, -3.0, 23.5,\n",
      "        26.5, 22.0, 6.0, 14.0, 3.5, 6.5, 16.5, 10.5, 13.0, 16.0, 14.5, 6.5, 10.0, -4.0,\n",
      "        26.0, -8.5, -14.0, -5.0, 11.0, 11.0, 4.5, 25.0, 20.0, 27.5, 4.5, 2.5, 5.0, 21.0,\n",
      "        -15.0, 11.0, 2.5, -2.0, 13.0, 11.5, 15.0, 7.0, 5.0, -4.0, 4.0, 5.0, 0.5, -6.0,\n",
      "        12.5, -17.5, 10.0, 4.5, -1.0, -18.5, 12.5, 10.0, 14.0, 15.5, 22.0, 10.0, 25.0,\n",
      "        18.0, 4.0, 23.0, 11.0, 9.0, 8.5, -2.0, 11.0, 5.0, 13.0, 5.5, 4.5, 9.5, -0.5,\n",
      "        10.5, 8.5, 10.0, 8.0, 11.0, 11.5, 8.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -7.799999999999984,\n",
      "        -9.99999999999998, -7.799999999999981, -8.899999999999984, -9.99999999999998,\n",
      "        -6.699999999999985, -8.89999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -7.799999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -4.499999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -4.5000000000000036,\n",
      "        -8.899999999999986, -8.899999999999986, -8.89999999999998, -7.799999999999985,\n",
      "        -7.799999999999984, -9.99999999999998, -9.99999999999998, -6.69999999999999,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999999, -8.899999999999986,\n",
      "        -8.899999999999984, -9.99999999999998, -6.6999999999999815, -8.899999999999983,\n",
      "        -8.89999999999998, -7.799999999999987, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -4.500000000000002, -9.99999999999998, -8.899999999999984,\n",
      "        -7.799999999999981, -1.1999999999999873, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999986, -9.99999999999998,\n",
      "        -5.599999999999997, -2.2999999999999927, -4.499999999999987, -8.89999999999998,\n",
      "        -5.599999999999982, -9.99999999999998, -7.799999999999989, -8.89999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -9.99999999999998, -8.899999999999984,\n",
      "        -8.899999999999986, -9.99999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.799999999999982, -6.699999999999991, -9.99999999999998, -8.899999999999986,\n",
      "        -2.2999999999999896, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999988, -9.99999999999998,\n",
      "        -6.699999999999994, -5.599999999999989, -5.6, -9.99999999999998, -6.6999999999999815]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: -1.1999999999999873\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.495\n",
      "      policy2: -8.371999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07240402084493011\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02047821646050008\n",
      "      mean_inference_ms: 5.766995019143565\n",
      "      mean_raw_obs_processing_ms: 0.4162540808438597\n",
      "  time_since_restore: 248.10642766952515\n",
      "  time_this_iter_s: 66.1829161643982\n",
      "  time_total_s: 248.10642766952515\n",
      "  timers:\n",
      "    learn_throughput: 112.787\n",
      "    learn_time_ms: 35465.154\n",
      "    synch_weights_time_ms: 2.494\n",
      "    training_iteration_time_ms: 62017.88\n",
      "  timestamp: 1660036851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0758_00002\n",
      "  warmup_time: 9.455713987350464\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-21-05\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.29999999999997\n",
      "  episode_reward_mean: 1.830000000000004\n",
      "  episode_reward_min: -21.30000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: 3b43653afbe04acfbe6caf1cbf1e78d6\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1511667966842651\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01691400818526745\n",
      "          model: {}\n",
      "          policy_loss: -0.059708599001169205\n",
      "          total_loss: 6.859957695007324\n",
      "          vf_explained_var: 0.19025477766990662\n",
      "          vf_loss: 6.912055015563965\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1648483276367188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01780363731086254\n",
      "          model: {}\n",
      "          policy_loss: -0.055457666516304016\n",
      "          total_loss: 2.067636013031006\n",
      "          vf_explained_var: 0.3041642904281616\n",
      "          vf_loss: 2.1177525520324707\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.842857142857138\n",
      "    ram_util_percent: 66.55428571428575\n",
      "  pid: 11124\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: -2.2999999999999976\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.345\n",
      "    policy2: -8.514999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -13.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07261397527404671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021293164060895747\n",
      "    mean_inference_ms: 6.101421437363046\n",
      "    mean_raw_obs_processing_ms: 0.4208162783570943\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 30.29999999999997\n",
      "    episode_reward_mean: 1.830000000000004\n",
      "    episode_reward_min: -21.30000000000003\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [0.6000000000000113, 2.100000000000005, -3.0000000000000013, 6.299999999999997,\n",
      "        3.000000000000005, -9.29999999999998, 6.300000000000031, 8.700000000000022,\n",
      "        -7.499999999999984, -7.799999999999997, -18.900000000000034, 2.4000000000000132,\n",
      "        14.099999999999909, 14.100000000000028, 11.100000000000026, 1.4999999999999551,\n",
      "        -13.499999999999986, -2.9999999999999933, -0.29999999999998395, -8.999999999999973,\n",
      "        -4.799999999999976, -21.30000000000003, 7.500000000000018, 7.800000000000008,\n",
      "        -8.999999999999988, -1.499999999999996, 1.5000000000000142, -6.299999999999988,\n",
      "        15.899999999999972, -5.999999999999975, 2.095545958979983e-14, 6.0000000000000195,\n",
      "        -13.499999999999993, 2.1000000000000174, 2.550737399076297e-14, 0.6000000000000218,\n",
      "        7.2000000000000295, 6.900000000000016, 7.500000000000016, -2.9999999999999973,\n",
      "        -8.399999999999995, -12.299999999999986, 15.000000000000002, -0.8999999999999702,\n",
      "        -1.4999999999999984, 2.1000000000000143, 3.0000000000000115, -12.000000000000005,\n",
      "        -6.899999999999984, 5.4000000000000234, 1.5000000000000244, 7.743805596760467e-15,\n",
      "        1.5000000000000084, 30.29999999999997, 8.100000000000012, -1.800000000000003,\n",
      "        19.499999999999908, 0.600000000000006, 5.700000000000008, 2.700000000000008,\n",
      "        5.4000000000000234, 0.30000000000002225, -0.8999999999999808, -8.09999999999998,\n",
      "        3.0000000000000253, 1.5000000000000102, 4.500000000000007, -1.4999999999999767,\n",
      "        -8.999999999999977, 2.273181642920008e-14, 2.017830347256222e-14, -10.499999999999982,\n",
      "        16.49999999999992, 10.499999999999986, 17.999999999999947, -7.499999999999989,\n",
      "        5.099999999999971, 9.000000000000023, 21.599999999999945, -4.200000000000021,\n",
      "        4.80000000000001, -10.499999999999977, 10.499999999999925, 12.899999999999979,\n",
      "        4.499999999999968, 13.499999999999977, -3.299999999999984, 1.5000000000000109,\n",
      "        2.7000000000000126, -5.699999999999978, 14.999999999999977, -4.499999999999986,\n",
      "        3.30000000000003, -1.4999999999999944, 16.199999999999957, 18.0, -10.799999999999983,\n",
      "        0.6000000000000073, 13.500000000000032, 1.5000000000000226]\n",
      "      policy_policy1_reward: [9.5, 11.0, 7.0, 13.0, 13.0, -7.0, 13.0, 16.5, -3.0, 0.0,\n",
      "        -10.0, 8.0, 23.0, 23.0, 20.0, 11.5, -3.5, 7.0, 7.5, 1.0, 3.0, -13.5, 17.5, 14.5,\n",
      "        1.0, 8.5, 11.5, 1.5, 21.5, 4.0, 10.0, 16.0, -3.5, 11.0, 10.0, 9.5, 9.5, 12.5,\n",
      "        17.5, 7.0, 0.5, -4.5, 25.0, 8.0, 8.5, 5.5, 13.0, -2.0, 2.0, 11.0, 11.5, 10.0,\n",
      "        11.5, 37.0, 17.0, 6.0, 29.5, 9.5, 13.5, 10.5, 11.0, 7.0, 8.0, -2.5, 13.0, 11.5,\n",
      "        14.5, 8.5, 1.0, 10.0, 4.5, -0.5, 26.5, 20.5, 28.0, 2.5, 14.0, 19.0, 25.0, 2.5,\n",
      "        11.5, -0.5, 20.5, 18.5, 14.5, 23.5, 4.5, 11.5, 10.5, 1.0, 25.0, 5.5, 10.0, 8.5,\n",
      "        24.0, 28.0, -3.0, 9.5, 23.5, 11.5]\n",
      "      policy_policy2_reward: [-8.899999999999986, -8.89999999999998, -9.99999999999998,\n",
      "        -6.699999999999995, -9.99999999999998, -2.2999999999999976, -6.699999999999995,\n",
      "        -7.7999999999999865, -4.500000000000003, -7.7999999999999865, -8.89999999999998,\n",
      "        -5.5999999999999845, -8.899999999999986, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999989,\n",
      "        -9.99999999999998, -7.79999999999999, -7.799999999999981, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -5.59999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -8.899999999999986, -2.2999999999999985, -5.599999999999993, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -3.3999999999999972, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -5.5999999999999845, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.69999999999999, -8.89999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -8.899999999999986, -7.799999999999981,\n",
      "        -7.799999999999981, -5.599999999999984, -6.6999999999999815, -8.899999999999984,\n",
      "        -5.599999999999991, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -9.99999999999998, -3.399999999999988,\n",
      "        -6.699999999999995, -6.699999999999985, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999983, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -7.799999999999981, -6.699999999999991, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999983, -9.99999999999998, -7.799999999999989,\n",
      "        -9.99999999999998, -7.799999999999981, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 37.0\n",
      "      policy2: -2.2999999999999976\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.345\n",
      "      policy2: -8.514999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -13.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07261397527404671\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021293164060895747\n",
      "      mean_inference_ms: 6.101421437363046\n",
      "      mean_raw_obs_processing_ms: 0.4208162783570943\n",
      "  time_since_restore: 268.99345564842224\n",
      "  time_this_iter_s: 49.658618211746216\n",
      "  time_total_s: 268.99345564842224\n",
      "  timers:\n",
      "    learn_throughput: 123.109\n",
      "    learn_time_ms: 24368.582\n",
      "    synch_weights_time_ms: 2.826\n",
      "    training_iteration_time_ms: 44826.092\n",
      "  timestamp: 1660036865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: e0758_00001\n",
      "  warmup_time: 9.458592891693115\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3252)\u001b[0m E0809 11:21:05.641000000 17564 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0758_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-21-07\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.199999999999985\n",
      "  episode_reward_mean: 1.6650000000000107\n",
      "  episode_reward_min: -13.499999999999995\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: d3dc894fb4884b018f96995278e9ac97\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2045027017593384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011734535917639732\n",
      "          model: {}\n",
      "          policy_loss: -0.03639598190784454\n",
      "          total_loss: 6.61864709854126\n",
      "          vf_explained_var: 0.1052233874797821\n",
      "          vf_loss: 6.652695655822754\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2186106443405151\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009775535203516483\n",
      "          model: {}\n",
      "          policy_loss: -0.036272093653678894\n",
      "          total_loss: 2.547581195831299\n",
      "          vf_explained_var: 0.24511854350566864\n",
      "          vf_loss: 2.5818982124328613\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.70857142857142\n",
      "    ram_util_percent: 66.30857142857144\n",
      "  pid: 10368\n",
      "  policy_reward_max:\n",
      "    policy1: 33.0\n",
      "    policy2: 2.1000000000000094\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.245\n",
      "    policy2: -7.579999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -7.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0754598559162091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021640550677690832\n",
      "    mean_inference_ms: 5.999100714459789\n",
      "    mean_raw_obs_processing_ms: 0.42640794480187844\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 25.199999999999985\n",
      "    episode_reward_mean: 1.6650000000000107\n",
      "    episode_reward_min: -13.499999999999995\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-4.499999999999983, -1.8000000000000007, 1.5000000000000235,\n",
      "        2.7000000000000113, 0.9000000000000271, 5.700000000000033, 17.39999999999995,\n",
      "        -12.299999999999976, 1.2000000000000113, -8.99999999999998, -5.999999999999998,\n",
      "        0.6000000000000091, -10.499999999999996, 0.9000000000000129, 8.100000000000012,\n",
      "        2.1000000000000263, 1.8000000000000265, -10.49999999999998, -4.500000000000001,\n",
      "        13.200000000000026, -0.2999999999999735, -3.8999999999999746, 19.79999999999994,\n",
      "        1.500000000000019, 2.700000000000007, 17.70000000000001, -3.8999999999999937,\n",
      "        -5.999999999999998, -7.499999999999984, -5.39999999999999, 5.700000000000031,\n",
      "        2.100000000000024, -1.4999999999999725, 6.600000000000021, 5.100000000000027,\n",
      "        4.500000000000016, -1.5000000000000002, 2.700000000000014, 6.599999999999996,\n",
      "        12.000000000000012, 10.20000000000003, 3.6000000000000263, 11.100000000000007,\n",
      "        -7.799999999999979, -4.499999999999995, -2.39999999999998, 8.699999999999953,\n",
      "        15.59999999999994, 6.900000000000022, -1.4999999999999782, 25.199999999999985,\n",
      "        6.600000000000017, -0.2999999999999837, 4.800000000000031, -6.299999999999995,\n",
      "        -10.199999999999996, 1.500000000000028, 3.2999999999999767, 7.2000000000000295,\n",
      "        4.500000000000021, 4.200000000000006, 10.500000000000032, 0.900000000000028,\n",
      "        4.80000000000001, -7.499999999999975, 4.500000000000034, -13.499999999999986,\n",
      "        -5.999999999999984, -13.499999999999995, 11.700000000000015, 14.699999999999973,\n",
      "        -0.8999999999999803, 2.099999999999995, -3.8999999999999937, 14.999999999999929,\n",
      "        -2.9999999999999942, -7.199999999999978, -2.699999999999976, 6.000000000000032,\n",
      "        -8.699999999999974, 17.099999999999948, 4.500000000000023, -5.399999999999999,\n",
      "        -1.799999999999978, 7.799999999999974, 3.000000000000022, 11.10000000000003,\n",
      "        7.200000000000024, 1.5000000000000058, -7.499999999999977, 11.99999999999997,\n",
      "        0.9000000000000231, 10.200000000000022, -8.999999999999988, -10.799999999999983,\n",
      "        -6.8999999999999755, -1.4999999999999774, 4.500000000000028, -1.4999999999999805,\n",
      "        -6.599999999999987]\n",
      "      policy_policy1_reward: [5.5, 6.0, 11.5, 10.5, 6.5, 13.5, 23.0, -4.5, 9.0, -4.5,\n",
      "        4.0, 9.5, -0.5, 6.5, 17.0, 11.0, 8.5, -6.0, 5.5, 21.0, 7.5, 5.0, 26.5, 11.5,\n",
      "        10.5, 25.5, 5.0, 4.0, -3.0, -2.0, 13.5, 5.5, 8.5, 15.5, 14.0, 9.0, 8.5, 10.5,\n",
      "        15.5, 22.0, 18.0, 7.0, 20.0, 0.0, 5.5, 6.5, 16.5, 24.5, 12.5, 3.0, 33.0, 15.5,\n",
      "        2.0, 11.5, 1.5, -3.5, 11.5, 10.0, 15.0, 14.5, 12.0, 20.5, 6.5, 11.5, 2.5, 14.5,\n",
      "        -3.5, -1.5, -3.5, 19.5, 22.5, 8.0, 11.0, -6.0, 25.0, 7.0, -0.5, 4.0, 16.0, -7.5,\n",
      "        26.0, 14.5, 3.5, 6.0, 14.5, 2.0, 14.5, 9.5, 11.5, 2.5, 16.5, 6.5, 18.0, 1.0,\n",
      "        -3.0, 2.0, 8.5, 14.5, 8.5, -1.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -7.799999999999989, -5.599999999999998, -7.799999999999981, -5.599999999999991,\n",
      "        -7.799999999999981, -7.799999999999981, -4.500000000000003, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999996, -8.89999999999998,\n",
      "        -8.89999999999998, -6.699999999999994, -4.499999999999985, -9.99999999999998,\n",
      "        -7.799999999999981, -7.799999999999989, -8.89999999999998, -6.699999999999989,\n",
      "        -9.99999999999998, -7.799999999999989, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -4.499999999999988, -3.399999999999989, -7.799999999999988,\n",
      "        -3.400000000000006, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -4.500000000000003, -9.99999999999998, -7.799999999999989, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -3.3999999999999835, -8.89999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -8.89999999999998, -7.799999999999989,\n",
      "        -8.899999999999983, -5.599999999999982, -4.499999999999998, -7.79999999999999,\n",
      "        -8.89999999999998, -2.299999999999989, -6.699999999999995, -7.799999999999983,\n",
      "        -6.6999999999999815, -9.99999999999998, -6.6999999999999815, -7.799999999999981,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -5.599999999999994,\n",
      "        -6.699999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -4.500000000000003, -9.99999999999998, -7.799999999999984, -7.799999999999981,\n",
      "        -8.899999999999983, -8.89999999999998, 2.1000000000000094, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -6.69999999999999, -9.99999999999998,\n",
      "        -1.200000000000006, -8.89999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -7.799999999999981, -6.699999999999986, 0.999999999999997, -3.400000000000004,\n",
      "        -2.3000000000000047, -9.99999999999998, -9.99999999999998, -4.499999999999996,\n",
      "        -5.599999999999983, -7.799999999999981, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999989]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.0\n",
      "      policy2: 2.1000000000000094\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.245\n",
      "      policy2: -7.579999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -7.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0754598559162091\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021640550677690832\n",
      "      mean_inference_ms: 5.999100714459789\n",
      "      mean_raw_obs_processing_ms: 0.42640794480187844\n",
      "  time_since_restore: 268.4059793949127\n",
      "  time_this_iter_s: 49.42125463485718\n",
      "  time_total_s: 268.4059793949127\n",
      "  timers:\n",
      "    learn_throughput: 122.435\n",
      "    learn_time_ms: 24502.715\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 44726.517\n",
      "  timestamp: 1660036867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: e0758_00000\n",
      "  warmup_time: 10.224120855331421\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=15000)\u001b[0m E0809 11:21:07.628000000 10244 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15000)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0758_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.99999999999997\n",
      "  episode_reward_mean: 2.4570000000000065\n",
      "  episode_reward_min: -21.000000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 1cb98bfd22c24e0b8d41d2e27afd2ddf\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2412878274917603\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018541984260082245\n",
      "          model: {}\n",
      "          policy_loss: -0.05362188071012497\n",
      "          total_loss: 6.571496486663818\n",
      "          vf_explained_var: 0.14178690314292908\n",
      "          vf_loss: 6.616774082183838\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.253735899925232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01814180240035057\n",
      "          model: {}\n",
      "          policy_loss: -0.04999541491270065\n",
      "          total_loss: 2.166649580001831\n",
      "          vf_explained_var: 0.2068393975496292\n",
      "          vf_loss: 2.211202383041382\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 29.90240963855421\n",
      "    ram_util_percent: 64.89638554216869\n",
      "  pid: 11556\n",
      "  policy_reward_max:\n",
      "    policy1: 34.0\n",
      "    policy2: 2.0999999999999934\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.785\n",
      "    policy2: -8.327999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07171764770949585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021357456205191205\n",
      "    mean_inference_ms: 6.935718132450534\n",
      "    mean_raw_obs_processing_ms: 0.41610630414597893\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.99999999999997\n",
      "    episode_reward_mean: 2.4570000000000065\n",
      "    episode_reward_min: -21.000000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [17.099999999999945, 3.0000000000000044, 13.499999999999966, 18.000000000000014,\n",
      "        -8.099999999999977, -16.499999999999993, -4.499999999999991, -8.399999999999999,\n",
      "        -14.999999999999973, -0.599999999999996, 0.9000000000000031, 10.500000000000014,\n",
      "        -9.599999999999978, 15.600000000000025, -2.099999999999992, 9.00000000000003,\n",
      "        -5.9999999999999805, -21.000000000000007, -2.999999999999991, -20.999999999999993,\n",
      "        5.400000000000023, 0.5999999999999872, 4.200000000000016, -9.899999999999984,\n",
      "        10.500000000000021, 6.00000000000003, -16.800000000000054, -2.9999999999999796,\n",
      "        -2.9999999999999947, 1.5000000000000182, -4.499999999999975, 9.000000000000032,\n",
      "        13.500000000000028, 9.600000000000014, 6.000000000000031, 1.0852430065710905e-14,\n",
      "        -9.899999999999977, 8.999999999999991, 2.9999999999999636, 6.600000000000028,\n",
      "        23.99999999999997, 1.200000000000012, 6.000000000000027, 1.5000000000000207,\n",
      "        -8.699999999999978, 1.5000000000000053, -0.29999999999999394, 12.600000000000017,\n",
      "        4.800000000000015, -1.4999999999999907, 2.7000000000000015, -1.1999999999999762,\n",
      "        -3.8999999999999964, -9.899999999999975, 3.0000000000000293, 7.500000000000027,\n",
      "        -3.0000000000000013, 13.499999999999948, -6.89999999999999, 8.700000000000012,\n",
      "        22.499999999999925, -5.9999999999999805, 17.399999999999977, 5.100000000000017,\n",
      "        14.699999999999944, -13.499999999999975, 2.095545958979983e-14, 10.499999999999929,\n",
      "        4.800000000000022, 14.69999999999998, 1.1962653090336062e-14, 0.900000000000012,\n",
      "        3.6000000000000245, -0.2999999999999742, 19.49999999999997, 1.8000000000000078,\n",
      "        -7.499999999999977, 2.700000000000028, -2.3999999999999906, 3.0000000000000293,\n",
      "        7.800000000000022, 0.30000000000000937, 22.499999999999932, -1.4999999999999838,\n",
      "        5.999999999999998, -0.8999999999999906, 8.999999999999957, 15.300000000000015,\n",
      "        20.10000000000001, 11.099999999999984, 16.49999999999998, -11.399999999999974,\n",
      "        -0.8999999999999848, -10.499999999999988, 7.200000000000024, 8.700000000000019,\n",
      "        -6.8999999999999915, 0.9000000000000227, 3.000000000000015, -3.2999999999999794]\n",
      "      policy_policy1_reward: [26.0, 13.0, 23.5, 28.0, -2.5, -6.5, 0.0, 0.5, -5.0, 5.0,\n",
      "        6.5, 20.5, -4.0, 24.5, 3.5, 19.0, 4.0, -11.0, 7.0, -11.0, 11.0, 9.5, 12.0, -1.0,\n",
      "        20.5, 16.0, -9.0, 7.0, 7.0, 11.5, 5.5, 19.0, 23.5, 18.5, 16.0, 10.0, -1.0, 13.5,\n",
      "        7.5, 15.5, 34.0, 9.0, 10.5, 11.5, -2.0, 11.5, 7.5, 21.5, 11.5, 8.5, 10.5, 5.5,\n",
      "        5.0, -1.0, 13.0, 17.5, 7.0, 23.5, 2.0, 16.5, 32.5, 4.0, 23.0, 14.0, 22.5, -3.5,\n",
      "        10.0, 20.5, 11.5, 22.5, 10.0, 6.5, 12.5, 7.5, 29.5, 8.5, 2.5, 10.5, 6.5, 13.0,\n",
      "        14.5, 7.0, 32.5, 8.5, 16.0, 8.0, 13.5, 22.0, 29.0, 20.0, 26.5, -8.0, 8.0, -0.5,\n",
      "        15.0, 16.5, -9.0, 6.5, 13.0, -1.0]\n",
      "      policy_policy2_reward: [-8.899999999999984, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.6, -9.99999999999998, -4.499999999999986, -8.899999999999986,\n",
      "        -9.99999999999998, -5.599999999999989, -5.6, -9.99999999999998, -5.599999999999986,\n",
      "        -8.89999999999998, -5.599999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -8.899999999999986, -7.79999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -4.500000000000002, -4.500000000000003, -8.899999999999986, -9.99999999999998,\n",
      "        -7.79999999999999, -4.499999999999999, -9.99999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -7.7999999999999865, -8.89999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -7.799999999999989, -6.699999999999995, -8.89999999999998,\n",
      "        -8.899999999999984, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999997, -8.89999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999995,\n",
      "        -7.79999999999999, -9.99999999999998, -5.599999999999985, -8.89999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -6.699999999999993, -9.99999999999998,\n",
      "        -7.799999999999981, -8.899999999999986, -9.99999999999998, -6.699999999999995,\n",
      "        -6.699999999999991, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -4.500000000000001, -6.699999999999988, -8.89999999999998,\n",
      "        -8.899999999999984, -9.99999999999998, -3.400000000000004, -8.899999999999986,\n",
      "        -9.99999999999998, -7.79999999999999, -7.799999999999981, 2.0999999999999934,\n",
      "        -5.599999999999998, -9.99999999999998, -2.300000000000003]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.0\n",
      "      policy2: 2.0999999999999934\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.785\n",
      "      policy2: -8.327999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -11.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07171764770949585\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.021357456205191205\n",
      "      mean_inference_ms: 6.935718132450534\n",
      "      mean_raw_obs_processing_ms: 0.41610630414597893\n",
      "  time_since_restore: 254.36863541603088\n",
      "  time_this_iter_s: 58.63261675834656\n",
      "  time_total_s: 254.36863541603088\n",
      "  timers:\n",
      "    learn_throughput: 118.502\n",
      "    learn_time_ms: 33754.826\n",
      "    synch_weights_time_ms: 3.491\n",
      "    training_iteration_time_ms: 63585.178\n",
      "  timestamp: 1660036872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: e0758_00003\n",
      "  warmup_time: 9.446670532226562\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-21-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999993\n",
      "  episode_reward_mean: 1.0110000000000092\n",
      "  episode_reward_min: -28.500000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 3bb20841742c49d7bcfe50360fad8c16\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.254583477973938\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012961595319211483\n",
      "          model: {}\n",
      "          policy_loss: -0.03838536888360977\n",
      "          total_loss: 6.687619686126709\n",
      "          vf_explained_var: 0.1595296561717987\n",
      "          vf_loss: 6.72341251373291\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2227444648742676\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013634508475661278\n",
      "          model: {}\n",
      "          policy_loss: -0.03958398103713989\n",
      "          total_loss: 2.602707624435425\n",
      "          vf_explained_var: 0.15558744966983795\n",
      "          vf_loss: 2.6395647525787354\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 23.651785714285715\n",
      "    ram_util_percent: 56.103571428571435\n",
      "  pid: 17352\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 3.200000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.58\n",
      "    policy2: -7.568999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07211807015180581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020130101085411797\n",
      "    mean_inference_ms: 5.989160363967897\n",
      "    mean_raw_obs_processing_ms: 0.41748837437108605\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999993\n",
      "    episode_reward_mean: 1.0110000000000092\n",
      "    episode_reward_min: -28.500000000000014\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-24.000000000000007, -11.699999999999983, 2.1000000000000214,\n",
      "        2.100000000000011, -3.299999999999986, 14.999999999999979, 11.100000000000023,\n",
      "        18.59999999999992, 1.9845236565174673e-14, -7.50000000000003, -3.8999999999999875,\n",
      "        13.19999999999993, -16.199999999999996, 2.100000000000019, -7.499999999999976,\n",
      "        -11.999999999999998, 2.9999999999999503, 1.499999999999988, 7.200000000000021,\n",
      "        -2.999999999999978, -3.899999999999978, -12.899999999999975, -5.999999999999977,\n",
      "        -0.5999999999999744, -1.7999999999999816, -10.499999999999977, 3.6000000000000063,\n",
      "        -23.100000000000023, 2.858824288409778e-15, -3.299999999999981, -9.89999999999997,\n",
      "        -28.500000000000014, 6.9000000000000234, 2.195466031196247e-14, 5.100000000000033,\n",
      "        6.6000000000000245, 12.000000000000032, 3.3000000000000123, 15.000000000000014,\n",
      "        10.200000000000031, -5.999999999999982, 14.10000000000002, 2.100000000000028,\n",
      "        1.2000000000000113, 1.8000000000000247, -11.999999999999979, 2.1000000000000174,\n",
      "        2.700000000000023, 3.000000000000028, -4.499999999999993, -3.2999999999999776,\n",
      "        0.6000000000000264, -10.499999999999977, 2.7000000000000104, -1.4999999999999796,\n",
      "        3.300000000000016, 2.400000000000017, 5.40000000000002, 1.5000000000000262,\n",
      "        1.8000000000000114, -0.59999999999999, -3.5999999999999837, 0.29999999999995874,\n",
      "        -5.099999999999981, 0.6000000000000034, -2.39999999999998, 15.299999999999994,\n",
      "        6.899999999999993, 4.200000000000027, 7.500000000000034, 5.400000000000022,\n",
      "        -24.3, -9.29999999999998, 0.29999999999999905, -11.999999999999977, 7.200000000000019,\n",
      "        10.200000000000014, -0.29999999999999394, 19.199999999999953, 15.599999999999955,\n",
      "        13.499999999999963, 16.5, -1.7999999999999727, 1.5000000000000235, 19.499999999999993,\n",
      "        3.60000000000001, 5.4000000000000234, 4.800000000000016, 3.9000000000000283,\n",
      "        4.200000000000021, 19.49999999999993, 8.099999999999973, 0.6000000000000202,\n",
      "        1.2000000000000148, 3.300000000000014, -0.8999999999999784, 1.8000000000000203,\n",
      "        0.6000000000000132, 6.00000000000003, -1.199999999999978]\n",
      "      policy_policy1_reward: [-14.0, -5.0, 11.0, 11.0, 4.5, 25.0, 20.0, 27.5, 4.5, 2.5,\n",
      "        5.0, 21.0, -15.0, 11.0, 2.5, -2.0, 13.0, 11.5, 15.0, 7.0, 5.0, -4.0, 4.0, 5.0,\n",
      "        0.5, -6.0, 12.5, -17.5, 10.0, 4.5, -1.0, -18.5, 12.5, 10.0, 14.0, 15.5, 22.0,\n",
      "        10.0, 25.0, 18.0, 4.0, 23.0, 11.0, 9.0, 8.5, -2.0, 11.0, 5.0, 13.0, 5.5, 4.5,\n",
      "        9.5, -0.5, 10.5, 8.5, 10.0, 8.0, 11.0, 11.5, 8.5, 5.0, 2.0, 7.0, 0.5, 9.5, 6.5,\n",
      "        22.0, 12.5, 1.0, 17.5, 11.0, -16.5, -7.0, 1.5, -2.0, 15.0, 18.0, 7.5, 21.5,\n",
      "        24.5, 23.5, 21.0, 6.0, 11.5, 24.0, 12.5, 11.0, 11.5, 9.5, 6.5, 29.5, 17.0, 9.5,\n",
      "        9.0, 10.0, 8.0, 8.5, 4.0, 16.0, 5.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -6.6999999999999815, -8.899999999999983,\n",
      "        -8.89999999999998, -7.799999999999987, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -4.500000000000002, -9.99999999999998, -8.899999999999984,\n",
      "        -7.799999999999981, -1.1999999999999873, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999986, -9.99999999999998,\n",
      "        -5.599999999999997, -2.2999999999999927, -4.499999999999987, -8.89999999999998,\n",
      "        -5.599999999999982, -9.99999999999998, -7.799999999999989, -8.89999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -9.99999999999998, -8.899999999999984,\n",
      "        -8.899999999999986, -9.99999999999998, -6.699999999999995, -9.99999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.799999999999982, -6.699999999999991, -9.99999999999998, -8.899999999999986,\n",
      "        -2.2999999999999896, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999988, -9.99999999999998,\n",
      "        -6.699999999999994, -5.599999999999989, -5.6, -9.99999999999998, -6.6999999999999815,\n",
      "        -5.599999999999983, -5.599999999999995, -6.699999999999994, -5.599999999999987,\n",
      "        -8.89999999999998, -8.89999999999998, -6.699999999999993, -5.6, 3.200000000000008,\n",
      "        -9.99999999999998, -5.599999999999991, -7.799999999999981, -2.3000000000000056,\n",
      "        -1.1999999999999982, -9.99999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -7.799999999999981, -2.3000000000000043, -8.89999999999998, -9.99999999999998,\n",
      "        -4.499999999999997, -7.799999999999983, -9.99999999999998, -4.500000000000003,\n",
      "        -8.89999999999998, -5.59999999999999, -6.699999999999994, -5.599999999999984,\n",
      "        -2.2999999999999905, -9.99999999999998, -8.899999999999983, -8.89999999999998,\n",
      "        -7.799999999999981, -6.699999999999986, -8.89999999999998, -6.69999999999999,\n",
      "        -3.400000000000002, -9.99999999999998, -6.6999999999999815]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 3.200000000000008\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.58\n",
      "      policy2: -7.568999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07211807015180581\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.020130101085411797\n",
      "      mean_inference_ms: 5.989160363967897\n",
      "      mean_raw_obs_processing_ms: 0.41748837437108605\n",
      "  time_since_restore: 287.45201587677\n",
      "  time_this_iter_s: 39.34558820724487\n",
      "  time_total_s: 287.45201587677\n",
      "  timers:\n",
      "    learn_throughput: 127.222\n",
      "    learn_time_ms: 31441.099\n",
      "    synch_weights_time_ms: 2.394\n",
      "    training_iteration_time_ms: 57482.424\n",
      "  timestamp: 1660036890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0758_00002\n",
      "  warmup_time: 9.455713987350464\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-21-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.99999999999997\n",
      "  episode_reward_mean: 3.6060000000000074\n",
      "  episode_reward_min: -18.89999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 1cb98bfd22c24e0b8d41d2e27afd2ddf\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2132505178451538\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01718943566083908\n",
      "          model: {}\n",
      "          policy_loss: -0.049864109605550766\n",
      "          total_loss: 6.796957969665527\n",
      "          vf_explained_var: 0.1343526542186737\n",
      "          vf_loss: 6.839087009429932\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.230535864830017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018349243327975273\n",
      "          model: {}\n",
      "          policy_loss: -0.04967895522713661\n",
      "          total_loss: 1.896684169769287\n",
      "          vf_explained_var: 0.35582640767097473\n",
      "          vf_loss: 1.9408583641052246\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 19.47446808510638\n",
      "    ram_util_percent: 49.26170212765955\n",
      "  pid: 11556\n",
      "  policy_reward_max:\n",
      "    policy1: 34.0\n",
      "    policy2: 2.0999999999999934\n",
      "  policy_reward_mean:\n",
      "    policy1: 11.835\n",
      "    policy2: -8.228999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07127311271689706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0214520474978659\n",
      "    mean_inference_ms: 6.686334453321423\n",
      "    mean_raw_obs_processing_ms: 0.41559777478500254\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.99999999999997\n",
      "    episode_reward_mean: 3.6060000000000074\n",
      "    episode_reward_min: -18.89999999999999\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [23.99999999999997, 1.200000000000012, 6.000000000000027, 1.5000000000000207,\n",
      "        -8.699999999999978, 1.5000000000000053, -0.29999999999999394, 12.600000000000017,\n",
      "        4.800000000000015, -1.4999999999999907, 2.7000000000000015, -1.1999999999999762,\n",
      "        -3.8999999999999964, -9.899999999999975, 3.0000000000000293, 7.500000000000027,\n",
      "        -3.0000000000000013, 13.499999999999948, -6.89999999999999, 8.700000000000012,\n",
      "        22.499999999999925, -5.9999999999999805, 17.399999999999977, 5.100000000000017,\n",
      "        14.699999999999944, -13.499999999999975, 2.095545958979983e-14, 10.499999999999929,\n",
      "        4.800000000000022, 14.69999999999998, 1.1962653090336062e-14, 0.900000000000012,\n",
      "        3.6000000000000245, -0.2999999999999742, 19.49999999999997, 1.8000000000000078,\n",
      "        -7.499999999999977, 2.700000000000028, -2.3999999999999906, 3.0000000000000293,\n",
      "        7.800000000000022, 0.30000000000000937, 22.499999999999932, -1.4999999999999838,\n",
      "        5.999999999999998, -0.8999999999999906, 8.999999999999957, 15.300000000000015,\n",
      "        20.10000000000001, 11.099999999999984, 16.49999999999998, -11.399999999999974,\n",
      "        -0.8999999999999848, -10.499999999999988, 7.200000000000024, 8.700000000000019,\n",
      "        -6.8999999999999915, 0.9000000000000227, 3.000000000000015, -3.2999999999999794,\n",
      "        -8.099999999999993, 2.100000000000021, -3.2999999999999923, -18.89999999999999,\n",
      "        12.300000000000031, 14.999999999999947, 4.500000000000018, 8.100000000000028,\n",
      "        16.20000000000001, 14.999999999999993, -8.999999999999975, -0.5999999999999898,\n",
      "        9.00000000000003, 2.1000000000000316, 11.10000000000001, -5.399999999999984,\n",
      "        -7.799999999999978, -4.499999999999988, 8.10000000000002, 11.999999999999956,\n",
      "        6.000000000000016, 11.999999999999968, 8.400000000000025, -2.999999999999985,\n",
      "        5.700000000000019, 8.100000000000023, -2.399999999999975, 13.500000000000025,\n",
      "        -1.4999999999999738, -14.999999999999973, 15.600000000000003, 10.500000000000016,\n",
      "        -0.8999999999999915, 8.99999999999999, 0.600000000000026, 6.600000000000023,\n",
      "        7.200000000000012, 1.500000000000012, -2.9999999999999942, -12.299999999999974]\n",
      "      policy_policy1_reward: [34.0, 9.0, 10.5, 11.5, -2.0, 11.5, 7.5, 21.5, 11.5, 8.5,\n",
      "        10.5, 5.5, 5.0, -1.0, 13.0, 17.5, 7.0, 23.5, 2.0, 16.5, 32.5, 4.0, 23.0, 14.0,\n",
      "        22.5, -3.5, 10.0, 20.5, 11.5, 22.5, 10.0, 6.5, 12.5, 7.5, 29.5, 8.5, 2.5, 10.5,\n",
      "        6.5, 13.0, 14.5, 7.0, 32.5, 8.5, 16.0, 8.0, 13.5, 22.0, 29.0, 20.0, 26.5, -8.0,\n",
      "        8.0, -0.5, 15.0, 16.5, -9.0, 6.5, 13.0, -1.0, -2.5, 11.0, 4.5, -10.0, 19.0,\n",
      "        19.5, 14.5, 17.0, 24.0, 25.0, -4.5, 5.0, 19.0, 11.0, 20.0, 3.5, 0.0, 5.5, 17.0,\n",
      "        22.0, 16.0, 22.0, 14.0, 7.0, 13.5, 17.0, 6.5, 18.0, 8.5, -5.0, 24.5, 20.5, 8.0,\n",
      "        19.0, 9.5, 15.5, 15.0, 11.5, 7.0, -10.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -7.79999999999999, -4.499999999999999,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -7.7999999999999865,\n",
      "        -8.89999999999998, -6.699999999999993, -9.99999999999998, -7.799999999999989,\n",
      "        -6.699999999999995, -8.89999999999998, -8.899999999999984, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -5.599999999999997,\n",
      "        -8.89999999999998, -7.79999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -7.79999999999999, -9.99999999999998,\n",
      "        -5.599999999999985, -8.89999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -6.699999999999993, -9.99999999999998, -7.799999999999981, -8.899999999999986,\n",
      "        -9.99999999999998, -6.699999999999995, -6.699999999999991, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999983, -4.500000000000001,\n",
      "        -6.699999999999988, -8.89999999999998, -8.899999999999984, -9.99999999999998,\n",
      "        -3.400000000000004, -8.899999999999986, -9.99999999999998, -7.79999999999999,\n",
      "        -7.799999999999981, 2.0999999999999934, -5.599999999999998, -9.99999999999998,\n",
      "        -2.300000000000003, -5.599999999999988, -8.899999999999986, -7.799999999999981,\n",
      "        -8.89999999999998, -6.699999999999986, -4.500000000000003, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999988, -9.99999999999998, -4.500000000000003,\n",
      "        -5.599999999999992, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999991,\n",
      "        -9.99999999999998, -7.79999999999999, -8.899999999999984, -8.899999999999986,\n",
      "        -4.5000000000000036, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -2.3000000000000043]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.0\n",
      "      policy2: 2.0999999999999934\n",
      "    policy_reward_mean:\n",
      "      policy1: 11.835\n",
      "      policy2: -8.228999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -10.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07127311271689706\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0214520474978659\n",
      "      mean_inference_ms: 6.686334453321423\n",
      "      mean_raw_obs_processing_ms: 0.41559777478500254\n",
      "  time_since_restore: 287.47202348709106\n",
      "  time_this_iter_s: 33.10338807106018\n",
      "  time_total_s: 287.47202348709106\n",
      "  timers:\n",
      "    learn_throughput: 132.793\n",
      "    learn_time_ms: 30122.12\n",
      "    synch_weights_time_ms: 3.192\n",
      "    training_iteration_time_ms: 57487.623\n",
      "  timestamp: 1660036905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: e0758_00003\n",
      "  warmup_time: 9.446670532226562\n",
      "  \n",
      "Result for PPO_MultiAgentArena_e0758_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-22-03\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999993\n",
      "  episode_reward_mean: 2.292000000000009\n",
      "  episode_reward_min: -24.3\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 3bb20841742c49d7bcfe50360fad8c16\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2213605642318726\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01137038879096508\n",
      "          model: {}\n",
      "          policy_loss: -0.035612091422080994\n",
      "          total_loss: 7.061282157897949\n",
      "          vf_explained_var: 0.19158455729484558\n",
      "          vf_loss: 7.094620227813721\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1892495155334473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012915379367768764\n",
      "          model: {}\n",
      "          policy_loss: -0.036964043974876404\n",
      "          total_loss: 2.823784828186035\n",
      "          vf_explained_var: 0.19059352576732635\n",
      "          vf_loss: 2.858165740966797\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 19.85\n",
      "    ram_util_percent: 49.254347826086985\n",
      "  pid: 17352\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 3.200000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.245\n",
      "    policy2: -6.952999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -16.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07209385862300077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.01992125302168666\n",
      "    mean_inference_ms: 5.872424762389184\n",
      "    mean_raw_obs_processing_ms: 0.4165790434396848\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999993\n",
      "    episode_reward_mean: 2.292000000000009\n",
      "    episode_reward_min: -24.3\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-5.999999999999982, 14.10000000000002, 2.100000000000028, 1.2000000000000113,\n",
      "        1.8000000000000247, -11.999999999999979, 2.1000000000000174, 2.700000000000023,\n",
      "        3.000000000000028, -4.499999999999993, -3.2999999999999776, 0.6000000000000264,\n",
      "        -10.499999999999977, 2.7000000000000104, -1.4999999999999796, 3.300000000000016,\n",
      "        2.400000000000017, 5.40000000000002, 1.5000000000000262, 1.8000000000000114,\n",
      "        -0.59999999999999, -3.5999999999999837, 0.29999999999995874, -5.099999999999981,\n",
      "        0.6000000000000034, -2.39999999999998, 15.299999999999994, 6.899999999999993,\n",
      "        4.200000000000027, 7.500000000000034, 5.400000000000022, -24.3, -9.29999999999998,\n",
      "        0.29999999999999905, -11.999999999999977, 7.200000000000019, 10.200000000000014,\n",
      "        -0.29999999999999394, 19.199999999999953, 15.599999999999955, 13.499999999999963,\n",
      "        16.5, -1.7999999999999727, 1.5000000000000235, 19.499999999999993, 3.60000000000001,\n",
      "        5.4000000000000234, 4.800000000000016, 3.9000000000000283, 4.200000000000021,\n",
      "        19.49999999999993, 8.099999999999973, 0.6000000000000202, 1.2000000000000148,\n",
      "        3.300000000000014, -0.8999999999999784, 1.8000000000000203, 0.6000000000000132,\n",
      "        6.00000000000003, -1.199999999999978, 8.70000000000002, 9.600000000000016, -2.9999999999999964,\n",
      "        7.499999999999943, 8.099999999999982, 3.600000000000001, -13.79999999999998,\n",
      "        -11.399999999999974, -1.5000000000000075, 3.000000000000026, -5.399999999999974,\n",
      "        10.49999999999996, 16.500000000000025, 5.100000000000028, -4.199999999999985,\n",
      "        4.200000000000004, 4.800000000000026, 9.600000000000032, -0.8999999999999777,\n",
      "        10.499999999999986, 6.599999999999955, -5.099999999999975, -0.2999999999999773,\n",
      "        1.5626389071599078e-14, -10.499999999999982, 11.699999999999953, 1.5000000000000142,\n",
      "        -2.399999999999976, -1.2000000000000384, 8.100000000000007, 3.300000000000014,\n",
      "        4.500000000000002, 1.5000000000000147, -1.5681900222830336e-14, 12.299999999999981,\n",
      "        -11.999999999999975, -4.799999999999981, 7.200000000000012, 9.000000000000025,\n",
      "        -13.799999999999978]\n",
      "      policy_policy1_reward: [4.0, 23.0, 11.0, 9.0, 8.5, -2.0, 11.0, 5.0, 13.0, 5.5,\n",
      "        4.5, 9.5, -0.5, 10.5, 8.5, 10.0, 8.0, 11.0, 11.5, 8.5, 5.0, 2.0, 7.0, 0.5, 9.5,\n",
      "        6.5, 22.0, 12.5, 1.0, 17.5, 11.0, -16.5, -7.0, 1.5, -2.0, 15.0, 18.0, 7.5, 21.5,\n",
      "        24.5, 23.5, 21.0, 6.0, 11.5, 24.0, 12.5, 11.0, 11.5, 9.5, 6.5, 29.5, 17.0, 9.5,\n",
      "        9.0, 10.0, 8.0, 8.5, 4.0, 16.0, 5.5, 16.5, 18.5, 7.0, 12.0, 17.0, 1.5, -6.0,\n",
      "        -2.5, 3.0, 7.5, -2.0, 20.5, 26.5, 3.0, 2.5, 12.0, 6.0, 13.0, 2.5, 20.5, 15.5,\n",
      "        0.5, 7.5, 10.0, -0.5, 19.5, 11.5, 6.5, 5.5, 11.5, 10.0, 14.5, 11.5, 10.0, 19.0,\n",
      "        -2.0, 3.0, 4.0, 19.0, -11.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.799999999999982, -6.699999999999991, -9.99999999999998, -8.899999999999986,\n",
      "        -2.2999999999999896, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999988, -9.99999999999998,\n",
      "        -6.699999999999994, -5.599999999999989, -5.6, -9.99999999999998, -6.6999999999999815,\n",
      "        -5.599999999999983, -5.599999999999995, -6.699999999999994, -5.599999999999987,\n",
      "        -8.89999999999998, -8.89999999999998, -6.699999999999993, -5.6, 3.200000000000008,\n",
      "        -9.99999999999998, -5.599999999999991, -7.799999999999981, -2.3000000000000056,\n",
      "        -1.1999999999999982, -9.99999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -7.799999999999981, -2.3000000000000043, -8.89999999999998, -9.99999999999998,\n",
      "        -4.499999999999997, -7.799999999999983, -9.99999999999998, -4.500000000000003,\n",
      "        -8.89999999999998, -5.59999999999999, -6.699999999999994, -5.599999999999984,\n",
      "        -2.2999999999999905, -9.99999999999998, -8.899999999999983, -8.89999999999998,\n",
      "        -7.799999999999981, -6.699999999999986, -8.89999999999998, -6.69999999999999,\n",
      "        -3.400000000000002, -9.99999999999998, -6.6999999999999815, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -4.499999999999992, -8.89999999999998,\n",
      "        2.1000000000000036, -7.799999999999981, -8.899999999999986, -4.500000000000002,\n",
      "        -4.5000000000000036, -3.3999999999999977, -9.99999999999998, -9.99999999999998,\n",
      "        2.1, -6.699999999999986, -7.799999999999981, -1.199999999999993, -3.400000000000003,\n",
      "        -3.399999999999991, -9.99999999999998, -8.89999999999998, -5.5999999999999845,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -8.899999999999986, -6.699999999999995, -3.399999999999995,\n",
      "        -6.699999999999995, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999981, 3.2000000000000073,\n",
      "        -9.99999999999998, -2.300000000000004]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 3.200000000000008\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.245\n",
      "      policy2: -6.952999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -16.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07209385862300077\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.01992125302168666\n",
      "      mean_inference_ms: 5.872424762389184\n",
      "      mean_raw_obs_processing_ms: 0.4165790434396848\n",
      "  time_since_restore: 320.523556470871\n",
      "  time_this_iter_s: 33.07154059410095\n",
      "  time_total_s: 320.523556470871\n",
      "  timers:\n",
      "    learn_throughput: 139.085\n",
      "    learn_time_ms: 28759.406\n",
      "    synch_weights_time_ms: 2.327\n",
      "    training_iteration_time_ms: 53412.946\n",
      "  timestamp: 1660036923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: e0758_00002\n",
      "  warmup_time: 9.455713987350464\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=6648)\u001b[0m E0809 11:22:04.424000000  7652 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e0758_00003:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_11-22-14\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999932\n",
      "  episode_reward_mean: 3.1530000000000094\n",
      "  episode_reward_min: -18.89999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 1cb98bfd22c24e0b8d41d2e27afd2ddf\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.180142879486084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016136420890688896\n",
      "          model: {}\n",
      "          policy_loss: -0.048661764711141586\n",
      "          total_loss: 6.61419677734375\n",
      "          vf_explained_var: 0.09420862793922424\n",
      "          vf_loss: 6.655598163604736\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1831507682800293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019283108413219452\n",
      "          model: {}\n",
      "          policy_loss: -0.04982958734035492\n",
      "          total_loss: 2.035740613937378\n",
      "          vf_explained_var: 0.2426454871892929\n",
      "          vf_loss: 2.079785108566284\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 17.38048780487805\n",
      "    ram_util_percent: 46.356097560975606\n",
      "  pid: 11556\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: 3.2000000000000135\n",
      "  policy_reward_mean:\n",
      "    policy1: 11.085\n",
      "    policy2: -7.931999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -20.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07043913124154959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02134246054262813\n",
      "    mean_inference_ms: 6.2778632844360445\n",
      "    mean_raw_obs_processing_ms: 0.41290978947155776\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999932\n",
      "    episode_reward_mean: 3.1530000000000094\n",
      "    episode_reward_min: -18.89999999999999\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [7.800000000000022, 0.30000000000000937, 22.499999999999932, -1.4999999999999838,\n",
      "        5.999999999999998, -0.8999999999999906, 8.999999999999957, 15.300000000000015,\n",
      "        20.10000000000001, 11.099999999999984, 16.49999999999998, -11.399999999999974,\n",
      "        -0.8999999999999848, -10.499999999999988, 7.200000000000024, 8.700000000000019,\n",
      "        -6.8999999999999915, 0.9000000000000227, 3.000000000000015, -3.2999999999999794,\n",
      "        -8.099999999999993, 2.100000000000021, -3.2999999999999923, -18.89999999999999,\n",
      "        12.300000000000031, 14.999999999999947, 4.500000000000018, 8.100000000000028,\n",
      "        16.20000000000001, 14.999999999999993, -8.999999999999975, -0.5999999999999898,\n",
      "        9.00000000000003, 2.1000000000000316, 11.10000000000001, -5.399999999999984,\n",
      "        -7.799999999999978, -4.499999999999988, 8.10000000000002, 11.999999999999956,\n",
      "        6.000000000000016, 11.999999999999968, 8.400000000000025, -2.999999999999985,\n",
      "        5.700000000000019, 8.100000000000023, -2.399999999999975, 13.500000000000025,\n",
      "        -1.4999999999999738, -14.999999999999973, 15.600000000000003, 10.500000000000016,\n",
      "        -0.8999999999999915, 8.99999999999999, 0.600000000000026, 6.600000000000023,\n",
      "        7.200000000000012, 1.500000000000012, -2.9999999999999942, -12.299999999999974,\n",
      "        -6.899999999999984, 12.600000000000009, -3.2999999999999834, -2.9999999999999942,\n",
      "        -3.2999999999999834, 9.30000000000002, -3.2999999999999896, -3.900000000000001,\n",
      "        3.000000000000031, -0.8999999999999826, -11.399999999999979, 9.000000000000016,\n",
      "        10.199999999999994, 8.400000000000027, 6.8999999999999915, 6.600000000000012,\n",
      "        14.699999999999916, 16.49999999999999, -2.999999999999982, 6.00000000000003,\n",
      "        -15.89999999999998, 8.700000000000014, 3.000000000000024, 17.999999999999904,\n",
      "        -3.299999999999993, -4.499999999999972, 14.999999999999996, -7.799999999999974,\n",
      "        5.400000000000017, 9.90000000000003, -14.099999999999975, -16.799999999999986,\n",
      "        15.600000000000026, 6.000000000000027, 6.000000000000012, -2.999999999999976,\n",
      "        -1.5000000000000016, 6.0000000000000195, 7.19999999999996, -0.2999999999999864]\n",
      "      policy_policy1_reward: [14.5, 7.0, 32.5, 8.5, 16.0, 8.0, 13.5, 22.0, 29.0, 20.0,\n",
      "        26.5, -8.0, 8.0, -0.5, 15.0, 16.5, -9.0, 6.5, 13.0, -1.0, -2.5, 11.0, 4.5, -10.0,\n",
      "        19.0, 19.5, 14.5, 17.0, 24.0, 25.0, -4.5, 5.0, 19.0, 11.0, 20.0, 3.5, 0.0, 5.5,\n",
      "        17.0, 22.0, 16.0, 22.0, 14.0, 7.0, 13.5, 17.0, 6.5, 18.0, 8.5, -5.0, 24.5, 20.5,\n",
      "        8.0, 19.0, 9.5, 15.5, 15.0, 11.5, 7.0, -10.0, 2.0, 21.5, 4.5, 7.0, 4.5, 16.0,\n",
      "        4.5, 5.0, 13.0, 8.0, -2.5, 19.0, 18.0, 14.0, 12.5, 15.5, 22.5, 26.5, 7.0, 10.5,\n",
      "        -7.0, 16.5, 13.0, 28.0, -1.0, 5.5, 25.0, 0.0, 11.0, 15.5, -8.5, -20.0, 24.5,\n",
      "        10.5, 16.0, 7.0, 8.5, 16.0, 15.0, 7.5]\n",
      "      policy_policy2_reward: [-6.699999999999995, -6.699999999999991, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999983, -4.500000000000001,\n",
      "        -6.699999999999988, -8.89999999999998, -8.899999999999984, -9.99999999999998,\n",
      "        -3.400000000000004, -8.899999999999986, -9.99999999999998, -7.79999999999999,\n",
      "        -7.799999999999981, 2.0999999999999934, -5.599999999999998, -9.99999999999998,\n",
      "        -2.300000000000003, -5.599999999999988, -8.899999999999986, -7.799999999999981,\n",
      "        -8.89999999999998, -6.699999999999986, -4.500000000000003, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999988, -9.99999999999998, -4.500000000000003,\n",
      "        -5.599999999999992, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999991,\n",
      "        -9.99999999999998, -7.79999999999999, -8.899999999999984, -8.899999999999986,\n",
      "        -4.5000000000000036, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.899999999999986, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -2.3000000000000043, -8.899999999999986, -8.89999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -7.799999999999981, -6.699999999999994, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -8.899999999999983, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999984, -5.599999999999999, -5.599999999999987,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -4.499999999999987, -8.899999999999983, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -2.299999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -5.599999999999998, -5.599999999999998, -5.599999999999992,\n",
      "        3.2000000000000135, -8.899999999999986, -4.500000000000003, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -7.79999999999999]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: 3.2000000000000135\n",
      "    policy_reward_mean:\n",
      "      policy1: 11.085\n",
      "      policy2: -7.931999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -20.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07043913124154959\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02134246054262813\n",
      "      mean_inference_ms: 6.2778632844360445\n",
      "      mean_raw_obs_processing_ms: 0.41290978947155776\n",
      "  time_since_restore: 316.38852071762085\n",
      "  time_this_iter_s: 28.916497230529785\n",
      "  time_total_s: 316.38852071762085\n",
      "  timers:\n",
      "    learn_throughput: 148.098\n",
      "    learn_time_ms: 27009.1\n",
      "    synch_weights_time_ms: 3.159\n",
      "    training_iteration_time_ms: 52724.733\n",
      "  timestamp: 1660036934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: e0758_00003\n",
      "  warmup_time: 9.446670532226562\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=17196)\u001b[0m E0809 11:22:14.960000000 17820 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "2022-08-09 11:22:15,071\tINFO tune.py:758 -- Total run time: 378.22 seconds (377.59 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Example using Ray tune API (`tune.run()`) until some stopping condition is met.\n",
    "# This will create one (or more) Algorithms under the hood automatically w/o us having to\n",
    "# build these algos from the config.\n",
    "\n",
    "experiment_results = tune.run(\n",
    "    \"PPO\",\n",
    "\n",
    "    # training config params (translated into a python dict!)\n",
    "    config=config.to_dict(),\n",
    "\n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\n",
    "        \"training_iteration\": 6,     # stop after n training iterations (calls to `Algorithm.train()`)\n",
    "        #\"episode_reward_mean\": 400, # stop if average (sum of) rewards in an episode is 400 or more\n",
    "        #\"timesteps_total\": 100000,  # stop if reached 100,000 sampling timesteps\n",
    "    },  \n",
    "\n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=\"results\",\n",
    "         \n",
    "    # Every how many train() calls do we create a checkpoint?\n",
    "    checkpoint_freq=1,\n",
    "    # Always save last checkpoint (no matter the frequency).\n",
    "    checkpoint_at_end=True,\n",
    "\n",
    "    ###############\n",
    "    # Note about Ray Tune verbosity.\n",
    "    # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "    # 0 = silent\n",
    "    # 1 = only status updates, no logging messages\n",
    "    # 2 = status and brief trial results, includes logging messages\n",
    "    # 3 = status and detailed trial results, includes logging messages\n",
    "    # Defaults to 3.\n",
    "    ###############\n",
    "    verbose=3,\n",
    "                   \n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/folders/j4/brrn254576lgnbqqtp5p1z280000gn/T/checkpoint_tmp_6vs48i96'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the returned `experiment_results` object,\n",
    "# we can extract from it the best checkpoint according to some criterium, e.g. `episode_reward_mean`.\n",
    "\n",
    "print(\"Best checkpoint: \", experiment_results.best_checkpoint)\n",
    "\n",
    "print(\"To directory\": \", experiment_results.best_checkpoint.to_directory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details behind Ray RLlib resource allocation <a class=\"anchor\" id=\"resource_allocation\"></a>\n",
    "\n",
    "#### Why did we use 8 CPUs in the tune run above (2 CPUs per trial)?\n",
    "\n",
    "```\n",
    "== Status ==\n",
    "Current time: 2022-07-24 18:18:28 (running for 00:02:09.35)\n",
    "Memory usage on this node: 9.9/16.0 GiB\n",
    "Using FIFO scheduling algorithm.\n",
    "Resources requested: 8/16 CPUs, 0/0 GPUs, 0.5/3.97 GiB heap, 0.5/1.98 GiB objects\n",
    "```\n",
    "\n",
    "By default, the PPO Algorithm uses 2 so called `RolloutWorkers` (you can change this via `config.rollouts(num_rollout_workers=2)`) for collecting samples from\n",
    "environments in parallel.\n",
    "We changed this setting to only 1 worker via the `config.rollouts(num_rollout_workers=1)` call in the cell above.\n",
    "\n",
    "`RolloutWorkers` are Ray Actors that have their own copies of the environment and step through episodes in parallel. Each Actor in Ray normally uses a single CPU, but besides `RolloutWorker`s, an Algorithm in RLlib also always has one local process (aka. the \"driver\" process or the \"local worker\"), which - in case of PPO -\n",
    "handles the model/policy learning updates.\n",
    "\n",
    "For our experiment above, this gives us 2 CPUs (1 rollout worker + 1 local learner) per Algorithm instance.\n",
    "\n",
    "Since our config specifies two `grid_search` with 2 different learning rates AND 2 different batch sizes, we were running 4 Algorithms in parallel above (2 learning rates x 2 batch sizes = 4 trials), hence 8 CPUs were required (4 algos x 2 CPUs each = 8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt, how to:\n",
    "\n",
    "* Use Ray Tune in combination with RLlib for hyperparameter tuning\n",
    "* How RLlib and Tune determine the required computational resources for some `tune.run()` experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 03<a ></a>\n",
    "\n",
    "#### Using the `config` that we have built so far, let's run another `tune.run()`.\n",
    "\n",
    "But this time, apply the following changes to our setup:\n",
    "\n",
    "- Setup only 1 learning rate using the `config.training(lr=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size using the `config.training(train_batch_size=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set the number of RolloutWorkers to 5 using the `config.rollouts(num_rollout_workers=5)` method call, which will allow us to collect more environment samples in parallel.\n",
    "- Set the `num_envs_per_worker` config parameter to 5 using the `config.rollouts(num_envs_per_worker=...)` method call. This will batch our environment on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    " * [Tune, Scalable Hyperparameter Tuning](https://docs.ray.io/en/latest/tune/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
