{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03. Tune the hyperparameters of a RLlib Multi-Agent Model using Ray Tune\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn:\n",
    " * [How to configure Ray Tune to find solid hyperparameters more easily](#configure_ray_tune)\n",
    " * [The details behind Ray RLlib resource allocation](#resource_allocation)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to configure Ray Tune to find solid hyperparameters more easily <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "In the previous experiments, we used a single algorithm's (PPO) configuration to create\n",
    "exactly one Algorithm object and call its `train()` method manually a couple of times.\n",
    "\n",
    "A common thing to try when doing ML or RL is to look for better choices of hyperparameters, neural network architectures, or algorithm settings. This hyperparameter optimization\n",
    "problem can be tackled in a scalable fashion using Ray Tune (in combination with RLlib!).\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates, how you can setup a simple grid-search for one very important hyperparameter (the learning rate), using our already existing PPO config object and Ray Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default learning rate for PPO is: 5e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7f9daa259190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOConfig object (same as we did in the previous notebook):\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena\n",
    "\n",
    "config = PPOConfig()\n",
    "\n",
    "# Setup our config object the exact same way as before:\n",
    "# Point to our MultiAgentArena env:\n",
    "config.environment(env=MultiAgentArena)\n",
    "# Multi-agent settings:\n",
    "config.multi_agent(\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "\n",
    "# Before setting up the learning rate hyperparam sweep,\n",
    "# let's see what the default learning rate for PPO actually is:\n",
    "print(f\"Default learning rate for PPO is: {config.lr}\")\n",
    "\n",
    "# Now let's change our existing config object and add a simple\n",
    "# grid-search over two different learning rates to it:\n",
    "config.training(\n",
    "    lr=tune.grid_search([0.005, 0.0003]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:29,265\tINFO algorithm.py:1774 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:29,265\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:29,265\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RAY TUNE API .run() UNTIL STOP CONDITION\n",
    "#\n",
    "# Note about Ray Tune verbosity.\n",
    "# Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "# 0 = silent\n",
    "# 1 = only status updates, no logging messages\n",
    "# 2 = status and brief trial results, includes logging messages\n",
    "# 3 = status and detailed trial results, includes logging messages\n",
    "# Defaults to 3.\n",
    "###############\n",
    "\n",
    "verbosity = 3 # Tune logging verbosity\n",
    "\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_frequency = 1          # every how many train() calls do we create a checkpoint?\n",
    "checkpoint_at_end = True          # always save last checkpoint (no matter the frequency)\n",
    "relative_checkpoint_dir = \"multiagent_PPO_logs\" # redirect logs instead of ~/ray_results/\n",
    "\n",
    "\n",
    "experiment_results = tune.run(\"PPO\", \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\n",
    "        #\"episode_reward_mean\": 400, # stop if average (sum of) rewards in an episode is 400 or more\n",
    "        \"training_iteration\": 5,  # stop after 5 training iterations (calls to `Algorithm.train()`)\n",
    "        # \"timesteps_total\": 100000,  # stop if reached 100,000 sampling timesteps\n",
    "    },  \n",
    "          \n",
    "    # training config params\n",
    "    config=config.to_dict(),\n",
    "                    \n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=relative_checkpoint_dir,\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq=checkpoint_frequency,\n",
    "    checkpoint_at_end=checkpoint_at_end,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    verbose=verbosity,\n",
    "                   \n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")\n",
    "print(\"Best checkpoint: \", experiment_results.best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details behind Ray RLlib resource allocation <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "When running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HomeWork\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises <a ></a>\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    " * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
