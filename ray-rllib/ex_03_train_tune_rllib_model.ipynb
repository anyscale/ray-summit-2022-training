{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03. Introduction to Ray Tune and hyperparameter optimization (HPO)\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this notebook, you will learn:\n",
    " * [How to configure Ray Tune to find solid hyperparameters more easily](#configure_ray_tune)\n",
    " * [The details behind Ray RLlib resource allocation](#resource_allocation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "# Importing the very same environment class that we have coded together in\n",
    "# the previous notebook.\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena\n",
    "\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to configure Ray Tune to find solid hyperparameters more easily <a class=\"anchor\" id=\"configure_ray_tune\"></a>\n",
    "\n",
    "In the previous experiments, we used a single algorithm's (PPO) configuration to create\n",
    "exactly one Algorithm object and call its `train()` method manually a couple of times.\n",
    "\n",
    "A common thing to try when doing ML or RL is to look for better choices of hyperparameters, neural network architectures, or algorithm settings. This hyperparameter optimization\n",
    "problem can be tackled in a scalable fashion using Ray Tune (in combination with RLlib!).\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates, how you can setup a simple grid-search for one very important hyperparameter (the learning rate), using our already existing PPO config object and Ray Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7f9ea26a4790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOConfig object (same as we did in the previous notebook):\n",
    "config = PPOConfig()\n",
    "\n",
    "# Setup our config object the exact same way as before:\n",
    "# Point to our MultiAgentArena env:\n",
    "config.environment(env=MultiAgentArena)\n",
    "\n",
    "# Setup multi-agent mapping:\n",
    "\n",
    "# Environment provides M agent IDs.\n",
    "# RLlib has N policies (neural networks).\n",
    "# The `policy_mapping_fn` maps M agent IDs to N policies (M <= N).\n",
    "\n",
    "# If you don't provide a policy_mapping_fn, all agent IDs will map to \"default_policy\".\n",
    "config.multi_agent(\n",
    "    # Tell RLlib to create 2 policies with these IDs here:\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    # Tell RLlib to map agent1 to policy1 and agent2 to policy2.\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "\n",
    "# Reduce the number of workers from 2 (default) to 1 to save some resources on the expensive hyperparameter sweep.\n",
    "# IMPORTANT: More information on resource requirements for tune hyperparameter sweeps and different RLlib algorithm setups\n",
    "# below.\n",
    "config.rollouts(num_rollout_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore how a very simple hyperparameter search should be configured with RLlib and Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default learning rate for PPO is: 5e-05\n",
      "Default train batch size for PPO is: 4000\n"
     ]
    }
   ],
   "source": [
    "# Before setting up the learning rate hyperparam sweep,\n",
    "# let's see what the default learning rate and train batch size is for PPO:\n",
    "print(f\"Default learning rate for PPO is: {config.lr}\")\n",
    "print(f\"Default train batch size for PPO is: {config.train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7f9ea26a4790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's change our existing config object and add a simple\n",
    "# grid-search over two different learning rates to it:\n",
    "config.training(\n",
    "    lr=tune.grid_search([5e-5, 1e-4]),\n",
    "    train_batch_size=tune.grid_search([3000, 4000]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 12:40:46,589\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-09 12:43:38 (running for 00:02:50.72)<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/5.28 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: bafe8_00003 with episode_reward_mean=2.727000000000005 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x7f9e8f13ef40>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x7f9e8f13ed90>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f9e932b9c10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x7f9e8f13e1f0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x7f9e92ceaee0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f9e932b9c10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}<br>Result logdir: /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_wor...</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bafe8_00000</td><td>TERMINATED</td><td>127.0.0.1:63785</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         72.961 </td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">  -0.612</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               -18  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_bafe8_00001</td><td>TERMINATED</td><td>127.0.0.1:63794</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         74.022 </td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">   1.134</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                19.8</td><td style=\"text-align: right;\">               -18.3</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_bafe8_00002</td><td>TERMINATED</td><td>127.0.0.1:63797</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         94.7767</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  -1.152</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                19.8</td><td style=\"text-align: right;\">               -25.5</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_bafe8_00003</td><td>TERMINATED</td><td>127.0.0.1:63801</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         94.4711</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   2.727</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               -18  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=63785)\u001b[0m 2022-08-09 12:40:57,092\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=63785)\u001b[0m 2022-08-09 12:40:57,093\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63792)\u001b[0m 2022-08-09 12:41:04,490\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=63785)\u001b[0m 2022-08-09 12:41:07,173\tINFO trainable.py:160 -- Trainable.setup took 10.083 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=63785)\u001b[0m 2022-08-09 12:41:07,174\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=63785)\u001b[0m 2022-08-09 12:41:12,084\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=63794)\u001b[0m 2022-08-09 12:41:14,827\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=63794)\u001b[0m 2022-08-09 12:41:14,828\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63795)\u001b[0m 2022-08-09 12:41:22,411\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=63794)\u001b[0m 2022-08-09 12:41:25,189\tINFO trainable.py:160 -- Trainable.setup took 10.363 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=63794)\u001b[0m 2022-08-09 12:41:25,190\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=63794)\u001b[0m 2022-08-09 12:41:30,406\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=63797)\u001b[0m 2022-08-09 12:41:33,310\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=63797)\u001b[0m 2022-08-09 12:41:33,312\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63799)\u001b[0m 2022-08-09 12:41:41,279\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=63797)\u001b[0m 2022-08-09 12:41:44,287\tINFO trainable.py:160 -- Trainable.setup took 10.978 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=63797)\u001b[0m 2022-08-09 12:41:44,288\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=63797)\u001b[0m 2022-08-09 12:41:51,588\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=63801)\u001b[0m 2022-08-09 12:41:52,623\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=63801)\u001b[0m 2022-08-09 12:41:52,624\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=63804)\u001b[0m 2022-08-09 12:42:00,756\tWARNING env.py:235 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bafe8_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-41-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.300000000000011\n",
      "  episode_reward_mean: -11.360000000000003\n",
      "  episode_reward_min: -31.50000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 4f1310ef73844c1c99a53824a8d7ce26\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.373630404472351\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0128685487434268\n",
      "          model: {}\n",
      "          policy_loss: -0.030643977224826813\n",
      "          total_loss: 7.721398830413818\n",
      "          vf_explained_var: -0.0014218075666576624\n",
      "          vf_loss: 7.749469757080078\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.382006287574768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004326505586504936\n",
      "          model: {}\n",
      "          policy_loss: -0.018594272434711456\n",
      "          total_loss: 3.7613823413848877\n",
      "          vf_explained_var: 0.20364029705524445\n",
      "          vf_loss: 3.77911114692688\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 22.02307692307692\n",
      "    ram_util_percent: 70.83846153846153\n",
      "  pid: 63785\n",
      "  policy_reward_max:\n",
      "    policy1: 16.0\n",
      "    policy2: -4.499999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.716666666666667\n",
      "    policy2: -8.643333333333315\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08772222728023762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.037758479869909906\n",
      "    mean_inference_ms: 0.960230072590956\n",
      "    mean_raw_obs_processing_ms: 0.5091916954704063\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 9.300000000000011\n",
      "    episode_reward_mean: -11.360000000000003\n",
      "    episode_reward_min: -31.50000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [-21.000000000000043, -5.399999999999981, -25.80000000000006,\n",
      "        3.000000000000013, -15.899999999999977, -8.099999999999978, -0.29999999999997373,\n",
      "        -16.200000000000024, -13.499999999999995, 5.100000000000017, -17.99999999999998,\n",
      "        -11.399999999999983, -6.599999999999987, -28.500000000000043, -22.500000000000064,\n",
      "        -19.499999999999996, -14.699999999999985, 5.700000000000012, -1.4999999999999871,\n",
      "        -21.000000000000064, -5.999999999999997, 9.300000000000011, -24.0, -21.900000000000023,\n",
      "        -7.199999999999994, -14.999999999999977, -1.4999999999999931, -10.499999999999975,\n",
      "        -31.50000000000007, 3.5999999999999956]\n",
      "      policy_policy1_reward: [-11.0, 3.5, -18.0, 13.0, -7.0, -2.5, 7.5, -9.5, -3.5,\n",
      "        14.0, -8.0, -2.5, -1.0, -18.5, -12.5, -9.5, -8.0, 13.5, 8.5, -11.0, -1.5, 16.0,\n",
      "        -14.0, -13.0, -0.5, -5.0, 8.5, -0.5, -21.5, 12.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -7.799999999999981,\n",
      "        -6.699999999999982, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -5.599999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -4.499999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 16.0\n",
      "      policy2: -4.499999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: -2.716666666666667\n",
      "      policy2: -8.643333333333315\n",
      "    policy_reward_min:\n",
      "      policy1: -21.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08772222728023762\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.037758479869909906\n",
      "      mean_inference_ms: 0.960230072590956\n",
      "      mean_raw_obs_processing_ms: 0.5091916954704063\n",
      "  time_since_restore: 8.908966064453125\n",
      "  time_this_iter_s: 8.908966064453125\n",
      "  time_total_s: 8.908966064453125\n",
      "  timers:\n",
      "    learn_throughput: 751.571\n",
      "    learn_time_ms: 3991.637\n",
      "    synch_weights_time_ms: 2.756\n",
      "    training_iteration_time_ms: 8907.009\n",
      "  timestamp: 1660041676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: bafe8_00000\n",
      "  warmup_time: 10.087341070175171\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=63801)\u001b[0m 2022-08-09 12:42:03,722\tINFO trainable.py:160 -- Trainable.setup took 11.100 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=63801)\u001b[0m 2022-08-09 12:42:03,722\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bafe8_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-41-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.399999999999977\n",
      "  episode_reward_mean: -11.070000000000006\n",
      "  episode_reward_min: -28.80000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: ba802e21773d46369dab10c339d8b4ae\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3686600923538208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01781587116420269\n",
      "          model: {}\n",
      "          policy_loss: -0.04294942691922188\n",
      "          total_loss: 7.1038126945495605\n",
      "          vf_explained_var: 0.013475432991981506\n",
      "          vf_loss: 7.143199443817139\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.369824767112732\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016463909298181534\n",
      "          model: {}\n",
      "          policy_loss: -0.04329749569296837\n",
      "          total_loss: 3.1426119804382324\n",
      "          vf_explained_var: 0.3091248571872711\n",
      "          vf_loss: 3.1826164722442627\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 24.153846153846153\n",
      "    ram_util_percent: 71.33076923076923\n",
      "  pid: 63794\n",
      "  policy_reward_max:\n",
      "    policy1: 35.0\n",
      "    policy2: 1.0\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.316666666666667\n",
      "    policy2: -8.753333333333314\n",
      "  policy_reward_min:\n",
      "    policy1: -21.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09342012147989244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04074963598559593\n",
      "    mean_inference_ms: 1.0150489470276265\n",
      "    mean_raw_obs_processing_ms: 0.5470455904397833\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.399999999999977\n",
      "    episode_reward_mean: -11.070000000000006\n",
      "    episode_reward_min: -28.80000000000005\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [-2.3999999999999915, -14.999999999999988, -10.499999999999986,\n",
      "        -22.800000000000033, -27.00000000000001, -24.00000000000002, -24.00000000000004,\n",
      "        -10.49999999999998, -15.29999999999999, -14.999999999999975, -7.499999999999989,\n",
      "        -10.499999999999982, -12.000000000000018, -7.799999999999988, -24.60000000000005,\n",
      "        -28.80000000000005, -28.500000000000043, -5.999999999999986, 7.500000000000012,\n",
      "        -28.500000000000046, 15.000000000000012, -8.399999999999986, -4.499999999999979,\n",
      "        1.5, -22.50000000000001, -19.499999999999986, -24.000000000000004, 29.399999999999977,\n",
      "        1.500000000000012, 12.599999999999975]\n",
      "      policy_policy1_reward: [6.5, -5.0, -0.5, -15.0, -17.0, -14.0, -14.0, -0.5, -7.5,\n",
      "        -5.0, 2.5, -0.5, -2.0, 0.0, -19.0, -21.0, -18.5, 4.0, 6.5, -18.5, 25.0, 0.5,\n",
      "        5.5, 11.5, -12.5, -9.5, -14.0, 35.0, 11.5, 16.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -5.599999999999996,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, 1.0, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999984,\n",
      "        -9.99999999999998, -3.399999999999991]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 35.0\n",
      "      policy2: 1.0\n",
      "    policy_reward_mean:\n",
      "      policy1: -2.316666666666667\n",
      "      policy2: -8.753333333333314\n",
      "    policy_reward_min:\n",
      "      policy1: -21.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09342012147989244\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04074963598559593\n",
      "      mean_inference_ms: 1.0150489470276265\n",
      "      mean_raw_obs_processing_ms: 0.5470455904397833\n",
      "  time_since_restore: 9.066070079803467\n",
      "  time_this_iter_s: 9.066070079803467\n",
      "  time_total_s: 9.066070079803467\n",
      "  timers:\n",
      "    learn_throughput: 779.716\n",
      "    learn_time_ms: 3847.556\n",
      "    synch_weights_time_ms: 2.541\n",
      "    training_iteration_time_ms: 9060.534\n",
      "  timestamp: 1660041694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: bafe8_00001\n",
      "  warmup_time: 10.367733001708984\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-41-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999996\n",
      "  episode_reward_mean: -10.822499999999996\n",
      "  episode_reward_min: -37.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 2593f3a7735a4f4ab02b4cd9b49f99fc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.371561050415039\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014901818707585335\n",
      "          model: {}\n",
      "          policy_loss: -0.02905973233282566\n",
      "          total_loss: 7.118925094604492\n",
      "          vf_explained_var: -0.0020226191263645887\n",
      "          vf_loss: 7.1450042724609375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3794357776641846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006895382888615131\n",
      "          model: {}\n",
      "          policy_loss: -0.022771477699279785\n",
      "          total_loss: 3.3868792057037354\n",
      "          vf_explained_var: 0.28377214074134827\n",
      "          vf_loss: 3.4082717895507812\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 23.831578947368417\n",
      "    ram_util_percent: 69.3157894736842\n",
      "  pid: 63797\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -5.599999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -1.5375\n",
      "    policy2: -9.28499999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09836008834171463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0433902149348222\n",
      "    mean_inference_ms: 1.0870837831580618\n",
      "    mean_raw_obs_processing_ms: 0.5612661170530426\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999996\n",
      "    episode_reward_mean: -10.822499999999996\n",
      "    episode_reward_min: -37.50000000000005\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [6.000000000000018, -5.999999999999992, -27.00000000000003, -13.49999999999998,\n",
      "        -2.999999999999996, 0.6000000000000131, -30.000000000000025, -33.00000000000004,\n",
      "        -14.999999999999977, -1.4999999999999858, 7.2000000000000295, -15.000000000000002,\n",
      "        -19.200000000000003, -33.000000000000064, 5.4000000000000306, -24.000000000000007,\n",
      "        -11.999999999999986, -13.49999999999999, 1.5000000000000149, -13.499999999999975,\n",
      "        -31.500000000000036, -5.999999999999988, -7.499999999999986, -16.499999999999993,\n",
      "        -3.599999999999992, 8.631984016460592e-15, -9.29999999999998, -5.999999999999995,\n",
      "        -21.0, 0.900000000000028, -11.399999999999975, -27.900000000000027, -6.0, 2.700000000000003,\n",
      "        -37.50000000000005, 4.500000000000025, -24.00000000000001, -5.999999999999988,\n",
      "        -7.799999999999983, 19.499999999999996]\n",
      "      policy_policy1_reward: [16.0, 4.0, -17.0, -3.5, 7.0, 9.5, -20.0, -23.0, -5.0,\n",
      "        8.5, 15.0, -5.0, -12.5, -23.0, 11.0, -14.0, -2.0, -3.5, 11.5, -3.5, -21.5, 4.0,\n",
      "        2.5, -6.5, 2.0, 10.0, -1.5, 4.0, -11.0, 6.5, -2.5, -19.0, 4.0, 10.5, -27.5,\n",
      "        14.5, -14.0, 4.0, 0.0, 29.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.5999999999999845, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -5.6, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -5.599999999999982\n",
      "    policy_reward_mean:\n",
      "      policy1: -1.5375\n",
      "      policy2: -9.28499999999998\n",
      "    policy_reward_min:\n",
      "      policy1: -27.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09836008834171463\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0433902149348222\n",
      "      mean_inference_ms: 1.0870837831580618\n",
      "      mean_raw_obs_processing_ms: 0.5612661170530426\n",
      "  time_since_restore: 12.856993913650513\n",
      "  time_this_iter_s: 12.856993913650513\n",
      "  time_total_s: 12.856993913650513\n",
      "  timers:\n",
      "    learn_throughput: 720.312\n",
      "    learn_time_ms: 5553.146\n",
      "    synch_weights_time_ms: 2.923\n",
      "    training_iteration_time_ms: 12850.648\n",
      "  timestamp: 1660041717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: bafe8_00002\n",
      "  warmup_time: 10.982521057128906\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=63801)\u001b[0m 2022-08-09 12:42:13,267\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bafe8_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.299999999999947\n",
      "  episode_reward_mean: -5.254999999999996\n",
      "  episode_reward_min: -31.50000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 4f1310ef73844c1c99a53824a8d7ce26\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3507912158966064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007976443506777287\n",
      "          model: {}\n",
      "          policy_loss: -0.031042877584695816\n",
      "          total_loss: 7.243046283721924\n",
      "          vf_explained_var: 0.014940924011170864\n",
      "          vf_loss: 7.272493362426758\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3581663370132446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00897882878780365\n",
      "          model: {}\n",
      "          policy_loss: -0.034536343067884445\n",
      "          total_loss: 2.7632439136505127\n",
      "          vf_explained_var: 0.22340518236160278\n",
      "          vf_loss: 2.7968826293945312\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 24.619999999999997\n",
      "    ram_util_percent: 69.66\n",
      "  pid: 63785\n",
      "  policy_reward_max:\n",
      "    policy1: 25.5\n",
      "    policy2: 0.9999999999999934\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.875\n",
      "    policy2: -8.129999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09224627908938113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03995717454717127\n",
      "    mean_inference_ms: 1.0046892277215407\n",
      "    mean_raw_obs_processing_ms: 0.5320560353144136\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.299999999999947\n",
      "    episode_reward_mean: -5.254999999999996\n",
      "    episode_reward_min: -31.50000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-21.000000000000043, -5.399999999999981, -25.80000000000006,\n",
      "        3.000000000000013, -15.899999999999977, -8.099999999999978, -0.29999999999997373,\n",
      "        -16.200000000000024, -13.499999999999995, 5.100000000000017, -17.99999999999998,\n",
      "        -11.399999999999983, -6.599999999999987, -28.500000000000043, -22.500000000000064,\n",
      "        -19.499999999999996, -14.699999999999985, 5.700000000000012, -1.4999999999999871,\n",
      "        -21.000000000000064, -5.999999999999997, 9.300000000000011, -24.0, -21.900000000000023,\n",
      "        -7.199999999999994, -14.999999999999977, -1.4999999999999931, -10.499999999999975,\n",
      "        -31.50000000000007, 3.5999999999999956, 4.499999999999993, -10.500000000000007,\n",
      "        15.900000000000029, 2.1000000000000054, -1.4999999999999791, -4.499999999999985,\n",
      "        8.999999999999982, 14.400000000000015, -14.399999999999995, -11.999999999999979,\n",
      "        5.100000000000016, -15.899999999999974, 24.299999999999947, 4.500000000000008,\n",
      "        3.300000000000029, 3.000000000000006, 3.000000000000033, 3.6000000000000134,\n",
      "        -1.1999999999999855, -11.400000000000015, 1.5737411374061594e-14, -5.9999999999999805,\n",
      "        -4.499999999999995, -2.399999999999996, 7.200000000000031, 1.500000000000021,\n",
      "        -4.199999999999996, 2.1000000000000103, 12.000000000000023, -1.49999999999999]\n",
      "      policy_policy1_reward: [-11.0, 3.5, -18.0, 13.0, -7.0, -2.5, 7.5, -9.5, -3.5,\n",
      "        14.0, -8.0, -2.5, -1.0, -18.5, -12.5, -9.5, -8.0, 13.5, 8.5, -11.0, -1.5, 16.0,\n",
      "        -14.0, -13.0, -0.5, -5.0, 8.5, -0.5, -21.5, 12.5, 14.5, -0.5, 21.5, 11.0, 8.5,\n",
      "        5.5, 13.5, 20.0, -5.5, -2.0, 14.0, -7.0, 25.5, 14.5, 10.0, 13.0, 13.0, 12.5,\n",
      "        0.0, -8.0, 10.0, 4.0, -5.5, 6.5, 15.0, 6.0, 2.5, 11.0, 22.0, 8.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -7.799999999999981,\n",
      "        -6.699999999999982, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -5.599999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -4.499999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -4.500000000000002, -5.599999999999982, -8.899999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -1.1999999999999855,\n",
      "        -9.99999999999998, -6.6999999999999895, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -1.1999999999999993, -3.400000000000005, -9.99999999999998,\n",
      "        -9.99999999999998, 0.9999999999999934, -8.89999999999998, -7.799999999999988,\n",
      "        -4.500000000000003, -6.6999999999999815, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.5\n",
      "      policy2: 0.9999999999999934\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.875\n",
      "      policy2: -8.129999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -21.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09224627908938113\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.03995717454717127\n",
      "      mean_inference_ms: 1.0046892277215407\n",
      "      mean_raw_obs_processing_ms: 0.5320560353144136\n",
      "  time_since_restore: 20.56021213531494\n",
      "  time_this_iter_s: 11.651246070861816\n",
      "  time_total_s: 20.56021213531494\n",
      "  timers:\n",
      "    learn_throughput: 606.94\n",
      "    learn_time_ms: 4942.832\n",
      "    synch_weights_time_ms: 4.626\n",
      "    training_iteration_time_ms: 10275.062\n",
      "  timestamp: 1660041735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: bafe8_00000\n",
      "  warmup_time: 10.087341070175171\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.399999999999977\n",
      "  episode_reward_mean: -7.219999999999998\n",
      "  episode_reward_min: -30.60000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: ba802e21773d46369dab10c339d8b4ae\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.323177456855774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019627798348665237\n",
      "          model: {}\n",
      "          policy_loss: -0.05419866368174553\n",
      "          total_loss: 6.992380142211914\n",
      "          vf_explained_var: -0.0035229134373366833\n",
      "          vf_loss: 7.042652606964111\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3236840963363647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01747649908065796\n",
      "          model: {}\n",
      "          policy_loss: -0.0549771748483181\n",
      "          total_loss: 1.9954160451889038\n",
      "          vf_explained_var: 0.31923821568489075\n",
      "          vf_loss: 2.046898126602173\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 28.405000000000005\n",
      "    ram_util_percent: 68.89\n",
      "  pid: 63794\n",
      "  policy_reward_max:\n",
      "    policy1: 35.0\n",
      "    policy2: 1.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.5333333333333334\n",
      "    policy2: -8.753333333333314\n",
      "  policy_reward_min:\n",
      "    policy1: -25.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09695934879068245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04240086474827039\n",
      "    mean_inference_ms: 1.0516577289840117\n",
      "    mean_raw_obs_processing_ms: 0.5638980707451583\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.399999999999977\n",
      "    episode_reward_mean: -7.219999999999998\n",
      "    episode_reward_min: -30.60000000000003\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-2.3999999999999915, -14.999999999999988, -10.499999999999986,\n",
      "        -22.800000000000033, -27.00000000000001, -24.00000000000002, -24.00000000000004,\n",
      "        -10.49999999999998, -15.29999999999999, -14.999999999999975, -7.499999999999989,\n",
      "        -10.499999999999982, -12.000000000000018, -7.799999999999988, -24.60000000000005,\n",
      "        -28.80000000000005, -28.500000000000043, -5.999999999999986, 7.500000000000012,\n",
      "        -28.500000000000046, 15.000000000000012, -8.399999999999986, -4.499999999999979,\n",
      "        1.5, -22.50000000000001, -19.499999999999986, -24.000000000000004, 29.399999999999977,\n",
      "        1.500000000000012, 12.599999999999975, -10.499999999999986, 5.7000000000000295,\n",
      "        -13.499999999999977, 7.500000000000016, 5.400000000000016, 8.10000000000002,\n",
      "        -30.60000000000003, -2.9999999999999805, -25.500000000000043, 9.300000000000018,\n",
      "        5.10000000000001, -2.999999999999994, 4.200000000000008, -10.499999999999977,\n",
      "        -8.99999999999999, -17.999999999999986, -8.39999999999999, -0.2999999999999824,\n",
      "        4.500000000000012, 1.1296519275560968e-14, -1.799999999999975, -13.499999999999991,\n",
      "        -5.999999999999991, -0.2999999999999764, -25.50000000000002, 6.000000000000011,\n",
      "        14.100000000000016, -10.199999999999985, -5.999999999999982, 24.59999999999993]\n",
      "      policy_policy1_reward: [6.5, -5.0, -0.5, -15.0, -17.0, -14.0, -14.0, -0.5, -7.5,\n",
      "        -5.0, 2.5, -0.5, -2.0, 0.0, -19.0, -21.0, -18.5, 4.0, 6.5, -18.5, 25.0, 0.5,\n",
      "        5.5, 11.5, -12.5, -9.5, -14.0, 35.0, 11.5, 16.0, -0.5, 13.5, -3.5, 17.5, 11.0,\n",
      "        17.0, -25.0, 7.0, -15.5, 16.0, 14.0, 7.0, 12.0, -0.5, 1.0, -8.0, 0.5, 7.5, 14.5,\n",
      "        10.0, 6.0, -3.5, 4.0, 7.5, -15.5, 16.0, 23.0, -9.0, 4.0, 33.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -5.599999999999996,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, 1.0, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999984,\n",
      "        -9.99999999999998, -3.399999999999991, -9.99999999999998, -7.799999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999985, -8.899999999999986,\n",
      "        -5.599999999999982, -9.99999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999988, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -1.199999999999998, -9.99999999999998, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 35.0\n",
      "      policy2: 1.0\n",
      "    policy_reward_mean:\n",
      "      policy1: 1.5333333333333334\n",
      "      policy2: -8.753333333333314\n",
      "    policy_reward_min:\n",
      "      policy1: -25.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09695934879068245\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04240086474827039\n",
      "      mean_inference_ms: 1.0516577289840117\n",
      "      mean_raw_obs_processing_ms: 0.5638980707451583\n",
      "  time_since_restore: 21.047505855560303\n",
      "  time_this_iter_s: 11.981435775756836\n",
      "  time_total_s: 21.047505855560303\n",
      "  timers:\n",
      "    learn_throughput: 603.511\n",
      "    learn_time_ms: 4970.914\n",
      "    synch_weights_time_ms: 4.452\n",
      "    training_iteration_time_ms: 10515.999\n",
      "  timestamp: 1660041735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: bafe8_00001\n",
      "  warmup_time: 10.367733001708984\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.699999999999946\n",
      "  episode_reward_mean: -6.851249999999993\n",
      "  episode_reward_min: -37.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 2593f3a7735a4f4ab02b4cd9b49f99fc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.348755121231079\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010535013861954212\n",
      "          model: {}\n",
      "          policy_loss: -0.032000280916690826\n",
      "          total_loss: 6.850197792053223\n",
      "          vf_explained_var: 0.04993002116680145\n",
      "          vf_loss: 6.880090713500977\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3525480031967163\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009703204035758972\n",
      "          model: {}\n",
      "          policy_loss: -0.03349862992763519\n",
      "          total_loss: 1.9450156688690186\n",
      "          vf_explained_var: 0.3539165258407593\n",
      "          vf_loss: 1.9765737056732178\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 44.44117647058823\n",
      "    ram_util_percent: 67.9470588235294\n",
      "  pid: 63797\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -3.3999999999999972\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.13125\n",
      "    policy2: -8.982499999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10674808889650593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.046993654920135824\n",
      "    mean_inference_ms: 1.1718068214068331\n",
      "    mean_raw_obs_processing_ms: 0.606562534927535\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 20.699999999999946\n",
      "    episode_reward_mean: -6.851249999999993\n",
      "    episode_reward_min: -37.50000000000005\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [6.000000000000018, -5.999999999999992, -27.00000000000003, -13.49999999999998,\n",
      "        -2.999999999999996, 0.6000000000000131, -30.000000000000025, -33.00000000000004,\n",
      "        -14.999999999999977, -1.4999999999999858, 7.2000000000000295, -15.000000000000002,\n",
      "        -19.200000000000003, -33.000000000000064, 5.4000000000000306, -24.000000000000007,\n",
      "        -11.999999999999986, -13.49999999999999, 1.5000000000000149, -13.499999999999975,\n",
      "        -31.500000000000036, -5.999999999999988, -7.499999999999986, -16.499999999999993,\n",
      "        -3.599999999999992, 8.631984016460592e-15, -9.29999999999998, -5.999999999999995,\n",
      "        -21.0, 0.900000000000028, -11.399999999999975, -27.900000000000027, -6.0, 2.700000000000003,\n",
      "        -37.50000000000005, 4.500000000000025, -24.00000000000001, -5.999999999999988,\n",
      "        -7.799999999999983, 19.499999999999996, -1.4999999999999978, 1.5000000000000149,\n",
      "        -11.999999999999984, -7.499999999999997, -5.999999999999982, -10.499999999999984,\n",
      "        20.699999999999946, -4.19999999999999, -8.399999999999975, -15.599999999999989,\n",
      "        -7.499999999999984, -10.79999999999999, -6.300000000000002, -1.4999999999999976,\n",
      "        -14.699999999999974, 7.2000000000000135, -9.299999999999997, 5.700000000000003,\n",
      "        7.499999999999977, -21.0, 2.700000000000023, 1.20000000000001, -12.299999999999994,\n",
      "        4.500000000000005, 1.500000000000019, 1.5000000000000009, 4.499999999999995,\n",
      "        1.3072876114961218e-14, -10.499999999999975, 9.000000000000028, -7.499999999999974,\n",
      "        -22.500000000000007, -1.4999999999999791, 4.500000000000018, 9.000000000000032,\n",
      "        -5.399999999999984, -13.499999999999975, 1.8000000000000291, -1.4999999999999858,\n",
      "        13.499999999999973]\n",
      "      policy_policy1_reward: [16.0, 4.0, -17.0, -3.5, 7.0, 9.5, -20.0, -23.0, -5.0,\n",
      "        8.5, 15.0, -5.0, -12.5, -23.0, 11.0, -14.0, -2.0, -3.5, 11.5, -3.5, -21.5, 4.0,\n",
      "        2.5, -6.5, 2.0, 10.0, -1.5, 4.0, -11.0, 6.5, -2.5, -19.0, 4.0, 10.5, -27.5,\n",
      "        14.5, -14.0, 4.0, 0.0, 29.5, 8.5, 11.5, -2.0, 2.5, 4.0, -0.5, 28.5, 2.5, -5.0,\n",
      "        -10.0, 2.5, -3.0, 1.5, 8.5, -8.0, 15.0, -1.5, 13.5, 17.5, -11.0, 10.5, 9.0,\n",
      "        -4.5, 14.5, 11.5, 11.5, 14.5, 10.0, -0.5, 19.0, 2.5, -12.5, 8.5, 9.0, 19.0,\n",
      "        -2.0, -3.5, 8.5, 8.5, 23.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.5999999999999845, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -5.6, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -6.699999999999985,\n",
      "        -3.4000000000000004, -5.599999999999982, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999986, -9.99999999999998, -6.699999999999995, -7.799999999999986,\n",
      "        -7.79999999999999, -7.799999999999987, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -7.79999999999999, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -4.499999999999987, -9.99999999999998, -3.3999999999999972,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -3.3999999999999972\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.13125\n",
      "      policy2: -8.982499999999982\n",
      "    policy_reward_min:\n",
      "      policy1: -27.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10674808889650593\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.046993654920135824\n",
      "      mean_inference_ms: 1.1718068214068331\n",
      "      mean_raw_obs_processing_ms: 0.606562534927535\n",
      "  time_since_restore: 29.82225465774536\n",
      "  time_this_iter_s: 16.96526074409485\n",
      "  time_total_s: 29.82225465774536\n",
      "  timers:\n",
      "    learn_throughput: 616.917\n",
      "    learn_time_ms: 6483.852\n",
      "    synch_weights_time_ms: 3.804\n",
      "    training_iteration_time_ms: 14903.46\n",
      "  timestamp: 1660041740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: bafe8_00002\n",
      "  warmup_time: 10.982521057128906\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.600000000000014\n",
      "  episode_reward_mean: -12.307500000000001\n",
      "  episode_reward_min: -42.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 9960ea27f9c14636b15e67ba13cbb170\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3660567998886108\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02071431465446949\n",
      "          model: {}\n",
      "          policy_loss: -0.044074103236198425\n",
      "          total_loss: 7.045234203338623\n",
      "          vf_explained_var: 0.011638573370873928\n",
      "          vf_loss: 7.085165500640869\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3682221174240112\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018398316577076912\n",
      "          model: {}\n",
      "          policy_loss: -0.04525091126561165\n",
      "          total_loss: 2.806062936782837\n",
      "          vf_explained_var: 0.277011901140213\n",
      "          vf_loss: 2.8476338386535645\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 55.76923076923076\n",
      "    ram_util_percent: 68.07307692307693\n",
      "  pid: 63801\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -5.599999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -3.4625\n",
      "    policy2: -8.844999999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -32.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1329394824145526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05586318330924471\n",
      "    mean_inference_ms: 1.4126801365645223\n",
      "    mean_raw_obs_processing_ms: 0.7455594478264894\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 12.600000000000014\n",
      "    episode_reward_mean: -12.307500000000001\n",
      "    episode_reward_min: -42.00000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-16.500000000000085, -11.099999999999977, 0.9000000000000085,\n",
      "        8.100000000000017, -33.900000000000055, -4.799999999999985, -5.999999999999988,\n",
      "        -11.399999999999974, -4.7999999999999865, -16.49999999999998, -9.599999999999987,\n",
      "        -20.999999999999993, -14.100000000000003, -23.400000000000006, -7.499999999999991,\n",
      "        5.100000000000022, -18.000000000000036, -5.399999999999988, -30.00000000000005,\n",
      "        -16.500000000000018, -4.199999999999976, -12.599999999999973, -25.500000000000036,\n",
      "        -21.000000000000018, -16.499999999999996, -25.500000000000007, -42.00000000000007,\n",
      "        -4.799999999999983, -22.500000000000007, 0.6000000000000184, -21.000000000000007,\n",
      "        12.600000000000014, -2.399999999999981, -7.799999999999972, -16.49999999999999,\n",
      "        -22.50000000000002, -17.39999999999999, -20.400000000000006, 1.500000000000007,\n",
      "        12.000000000000032]\n",
      "      policy_policy1_reward: [-6.5, -5.5, 6.5, 17.0, -25.0, 3.0, 4.0, -2.5, 3.0, -6.5,\n",
      "        -4.0, -11.0, -8.5, -14.5, 2.5, 14.0, -8.0, 3.5, -20.0, -6.5, 2.5, -7.0, -15.5,\n",
      "        -11.0, -6.5, -15.5, -32.0, 3.0, -12.5, 9.5, -11.0, 21.5, 6.5, 0.0, -6.5, -12.5,\n",
      "        -8.5, -11.5, 11.5, 22.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -5.599999999999991, -5.599999999999992,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999989, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -5.599999999999982, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999895, -5.599999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999986, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 22.0\n",
      "      policy2: -5.599999999999982\n",
      "    policy_reward_mean:\n",
      "      policy1: -3.4625\n",
      "      policy2: -8.844999999999981\n",
      "    policy_reward_min:\n",
      "      policy1: -32.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1329394824145526\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05586318330924471\n",
      "      mean_inference_ms: 1.4126801365645223\n",
      "      mean_raw_obs_processing_ms: 0.7455594478264894\n",
      "  time_since_restore: 17.6645929813385\n",
      "  time_this_iter_s: 17.6645929813385\n",
      "  time_total_s: 17.6645929813385\n",
      "  timers:\n",
      "    learn_throughput: 493.22\n",
      "    learn_time_ms: 8109.969\n",
      "    synch_weights_time_ms: 3.368\n",
      "    training_iteration_time_ms: 17661.752\n",
      "  timestamp: 1660041741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: bafe8_00003\n",
      "  warmup_time: 11.104228258132935\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.299999999999947\n",
      "  episode_reward_mean: -3.80666666666666\n",
      "  episode_reward_min: -31.50000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 4f1310ef73844c1c99a53824a8d7ce26\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3229336738586426\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009846188127994537\n",
      "          model: {}\n",
      "          policy_loss: -0.03720417618751526\n",
      "          total_loss: 6.769473075866699\n",
      "          vf_explained_var: 0.10155760496854782\n",
      "          vf_loss: 6.804708003997803\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.322562336921692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01087478268891573\n",
      "          model: {}\n",
      "          policy_loss: -0.039417702704668045\n",
      "          total_loss: 2.434347629547119\n",
      "          vf_explained_var: 0.21719369292259216\n",
      "          vf_loss: 2.472677707672119\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 57.08947368421053\n",
      "    ram_util_percent: 67.77368421052631\n",
      "  pid: 63785\n",
      "  policy_reward_max:\n",
      "    policy1: 25.5\n",
      "    policy2: 0.9999999999999952\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.25\n",
      "    policy2: -8.05666666666665\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1001501216743757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04311990100126348\n",
      "    mean_inference_ms: 1.0838550172946932\n",
      "    mean_raw_obs_processing_ms: 0.5704966062042719\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.299999999999947\n",
      "    episode_reward_mean: -3.80666666666666\n",
      "    episode_reward_min: -31.50000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-21.000000000000043, -5.399999999999981, -25.80000000000006,\n",
      "        3.000000000000013, -15.899999999999977, -8.099999999999978, -0.29999999999997373,\n",
      "        -16.200000000000024, -13.499999999999995, 5.100000000000017, -17.99999999999998,\n",
      "        -11.399999999999983, -6.599999999999987, -28.500000000000043, -22.500000000000064,\n",
      "        -19.499999999999996, -14.699999999999985, 5.700000000000012, -1.4999999999999871,\n",
      "        -21.000000000000064, -5.999999999999997, 9.300000000000011, -24.0, -21.900000000000023,\n",
      "        -7.199999999999994, -14.999999999999977, -1.4999999999999931, -10.499999999999975,\n",
      "        -31.50000000000007, 3.5999999999999956, 4.499999999999993, -10.500000000000007,\n",
      "        15.900000000000029, 2.1000000000000054, -1.4999999999999791, -4.499999999999985,\n",
      "        8.999999999999982, 14.400000000000015, -14.399999999999995, -11.999999999999979,\n",
      "        5.100000000000016, -15.899999999999974, 24.299999999999947, 4.500000000000008,\n",
      "        3.300000000000029, 3.000000000000006, 3.000000000000033, 3.6000000000000134,\n",
      "        -1.1999999999999855, -11.400000000000015, 1.5737411374061594e-14, -5.9999999999999805,\n",
      "        -4.499999999999995, -2.399999999999996, 7.200000000000031, 1.500000000000021,\n",
      "        -4.199999999999996, 2.1000000000000103, 12.000000000000023, -1.49999999999999,\n",
      "        14.699999999999957, 3.0000000000000187, 13.500000000000025, 2.4175106361212784e-14,\n",
      "        14.10000000000003, -3.8999999999999813, 5.099999999999989, -5.999999999999995,\n",
      "        -10.79999999999998, -13.49999999999999, -2.399999999999992, 10.500000000000025,\n",
      "        6.000000000000023, -5.999999999999979, 13.499999999999988, -10.499999999999982,\n",
      "        -19.500000000000004, -2.9999999999999933, -4.499999999999988, -1.4999999999999836,\n",
      "        1.500000000000023, -2.9999999999999836, -5.399999999999984, -3.8999999999999826,\n",
      "        7.200000000000024, -3.599999999999984, -9.899999999999979, -3.2999999999999767,\n",
      "        3.300000000000032, -8.999999999999998]\n",
      "      policy_policy1_reward: [-11.0, 3.5, -18.0, 13.0, -7.0, -2.5, 7.5, -9.5, -3.5,\n",
      "        14.0, -8.0, -2.5, -1.0, -18.5, -12.5, -9.5, -8.0, 13.5, 8.5, -11.0, -1.5, 16.0,\n",
      "        -14.0, -13.0, -0.5, -5.0, 8.5, -0.5, -21.5, 12.5, 14.5, -0.5, 21.5, 11.0, 8.5,\n",
      "        5.5, 13.5, 20.0, -5.5, -2.0, 14.0, -7.0, 25.5, 14.5, 10.0, 13.0, 13.0, 12.5,\n",
      "        0.0, -8.0, 10.0, 4.0, -5.5, 6.5, 15.0, 6.0, 2.5, 11.0, 22.0, 8.5, 22.5, 7.5,\n",
      "        18.0, 10.0, 23.0, -0.5, 14.0, -7.0, -3.0, -3.5, 6.5, 15.0, 10.5, 4.0, 23.5,\n",
      "        -0.5, -9.5, 7.0, 5.5, 8.5, 11.5, 7.0, 3.5, 5.0, 15.0, 2.0, -1.0, 4.5, 10.0,\n",
      "        1.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999982, -7.799999999999981,\n",
      "        -6.699999999999982, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -5.599999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -4.499999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -4.500000000000002, -5.599999999999982, -8.899999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -1.1999999999999855,\n",
      "        -9.99999999999998, -6.6999999999999895, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -1.1999999999999993, -3.400000000000005, -9.99999999999998,\n",
      "        -9.99999999999998, 0.9999999999999934, -8.89999999999998, -7.799999999999988,\n",
      "        -4.500000000000003, -6.6999999999999815, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -4.500000000000004, -4.499999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -3.400000000000005, -8.89999999999998,\n",
      "        0.9999999999999952, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -4.499999999999982, -4.499999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -7.799999999999984, -5.599999999999998, -8.899999999999986,\n",
      "        -7.799999999999988, -6.6999999999999815, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.5\n",
      "      policy2: 0.9999999999999952\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.25\n",
      "      policy2: -8.05666666666665\n",
      "    policy_reward_min:\n",
      "      policy1: -21.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1001501216743757\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04311990100126348\n",
      "      mean_inference_ms: 1.0838550172946932\n",
      "      mean_raw_obs_processing_ms: 0.5704966062042719\n",
      "  time_since_restore: 33.78243684768677\n",
      "  time_this_iter_s: 13.222224712371826\n",
      "  time_total_s: 33.78243684768677\n",
      "  timers:\n",
      "    learn_throughput: 600.766\n",
      "    learn_time_ms: 4993.625\n",
      "    synch_weights_time_ms: 4.6\n",
      "    training_iteration_time_ms: 11254.809\n",
      "  timestamp: 1660041748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: bafe8_00000\n",
      "  warmup_time: 10.087341070175171\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00001:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.399999999999977\n",
      "  episode_reward_mean: -5.6699999999999955\n",
      "  episode_reward_min: -30.60000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: ba802e21773d46369dab10c339d8b4ae\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2756842374801636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022485511377453804\n",
      "          model: {}\n",
      "          policy_loss: -0.06508486717939377\n",
      "          total_loss: 6.535487651824951\n",
      "          vf_explained_var: -0.007282001432031393\n",
      "          vf_loss: 6.59607458114624\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.26980459690094\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021453749388456345\n",
      "          model: {}\n",
      "          policy_loss: -0.06513255089521408\n",
      "          total_loss: 2.1615827083587646\n",
      "          vf_explained_var: 0.30928701162338257\n",
      "          vf_loss: 2.222424268722534\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 57.20000000000001\n",
      "    ram_util_percent: 67.7842105263158\n",
      "  pid: 63794\n",
      "  policy_reward_max:\n",
      "    policy1: 35.0\n",
      "    policy2: 1.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.9\n",
      "    policy2: -8.569999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -25.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1038803344066528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04519142171770499\n",
      "    mean_inference_ms: 1.1217076120007572\n",
      "    mean_raw_obs_processing_ms: 0.5991544460290913\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.399999999999977\n",
      "    episode_reward_mean: -5.6699999999999955\n",
      "    episode_reward_min: -30.60000000000003\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-2.3999999999999915, -14.999999999999988, -10.499999999999986,\n",
      "        -22.800000000000033, -27.00000000000001, -24.00000000000002, -24.00000000000004,\n",
      "        -10.49999999999998, -15.29999999999999, -14.999999999999975, -7.499999999999989,\n",
      "        -10.499999999999982, -12.000000000000018, -7.799999999999988, -24.60000000000005,\n",
      "        -28.80000000000005, -28.500000000000043, -5.999999999999986, 7.500000000000012,\n",
      "        -28.500000000000046, 15.000000000000012, -8.399999999999986, -4.499999999999979,\n",
      "        1.5, -22.50000000000001, -19.499999999999986, -24.000000000000004, 29.399999999999977,\n",
      "        1.500000000000012, 12.599999999999975, -10.499999999999986, 5.7000000000000295,\n",
      "        -13.499999999999977, 7.500000000000016, 5.400000000000016, 8.10000000000002,\n",
      "        -30.60000000000003, -2.9999999999999805, -25.500000000000043, 9.300000000000018,\n",
      "        5.10000000000001, -2.999999999999994, 4.200000000000008, -10.499999999999977,\n",
      "        -8.99999999999999, -17.999999999999986, -8.39999999999999, -0.2999999999999824,\n",
      "        4.500000000000012, 1.1296519275560968e-14, -1.799999999999975, -13.499999999999991,\n",
      "        -5.999999999999991, -0.2999999999999764, -25.50000000000002, 6.000000000000011,\n",
      "        14.100000000000016, -10.199999999999985, -5.999999999999982, 24.59999999999993,\n",
      "        12.000000000000023, -4.499999999999986, -3.5999999999999823, -14.999999999999982,\n",
      "        1.8000000000000238, 13.5, 11.999999999999998, -9.599999999999978, 22.49999999999998,\n",
      "        -24.00000000000005, -17.099999999999987, -13.499999999999977, -1.4999999999999731,\n",
      "        -5.700000000000003, -20.40000000000004, -16.499999999999993, -5.999999999999988,\n",
      "        -3.2999999999999803, -1.4999999999999742, 1.5000000000000289, -8.399999999999993,\n",
      "        -7.499999999999989, 8.100000000000014, -9.89999999999999, 6.000000000000023,\n",
      "        12.600000000000033, -2.69999999999998, 0.6000000000000095, 4.500000000000002,\n",
      "        -1.5000000000000155]\n",
      "      policy_policy1_reward: [6.5, -5.0, -0.5, -15.0, -17.0, -14.0, -14.0, -0.5, -7.5,\n",
      "        -5.0, 2.5, -0.5, -2.0, 0.0, -19.0, -21.0, -18.5, 4.0, 6.5, -18.5, 25.0, 0.5,\n",
      "        5.5, 11.5, -12.5, -9.5, -14.0, 35.0, 11.5, 16.0, -0.5, 13.5, -3.5, 17.5, 11.0,\n",
      "        17.0, -25.0, 7.0, -15.5, 16.0, 14.0, 7.0, 12.0, -0.5, 1.0, -8.0, 0.5, 7.5, 14.5,\n",
      "        10.0, 6.0, -3.5, 4.0, 7.5, -15.5, 16.0, 23.0, -9.0, 4.0, 33.5, 22.0, 5.5, 2.0,\n",
      "        -5.0, 8.5, 18.0, 22.0, -4.0, 32.5, -14.0, -11.5, -3.5, 8.5, 1.0, -11.5, -6.5,\n",
      "        -1.5, 4.5, 8.5, 11.5, -5.0, -3.0, 17.0, -1.0, 16.0, 21.5, 4.0, 9.5, 14.5, 8.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -5.599999999999996,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, 1.0, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999984,\n",
      "        -9.99999999999998, -3.399999999999991, -9.99999999999998, -7.799999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999985, -8.899999999999986,\n",
      "        -5.599999999999982, -9.99999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999988, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -1.199999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999985, -9.99999999999998,\n",
      "        -6.699999999999987, -4.499999999999994, -9.99999999999998, -5.599999999999993,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999995, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999987, -8.89999999999998, -9.99999999999998,\n",
      "        -4.49999999999999, -7.79999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -3.400000000000004, -4.499999999999984, -8.899999999999984, -8.899999999999984,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999994, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 35.0\n",
      "      policy2: 1.0\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.9\n",
      "      policy2: -8.569999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -25.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1038803344066528\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04519142171770499\n",
      "      mean_inference_ms: 1.1217076120007572\n",
      "      mean_raw_obs_processing_ms: 0.5991544460290913\n",
      "  time_since_restore: 34.258604764938354\n",
      "  time_this_iter_s: 13.211098909378052\n",
      "  time_total_s: 34.258604764938354\n",
      "  timers:\n",
      "    learn_throughput: 598.821\n",
      "    learn_time_ms: 5009.841\n",
      "    synch_weights_time_ms: 4.093\n",
      "    training_iteration_time_ms: 11413.509\n",
      "  timestamp: 1660041749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: bafe8_00001\n",
      "  warmup_time: 10.367733001708984\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.699999999999946\n",
      "  episode_reward_mean: -3.5999999999999917\n",
      "  episode_reward_min: -37.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 2593f3a7735a4f4ab02b4cd9b49f99fc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3317245244979858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008313015103340149\n",
      "          model: {}\n",
      "          policy_loss: -0.03316159546375275\n",
      "          total_loss: 6.908495903015137\n",
      "          vf_explained_var: 0.066938117146492\n",
      "          vf_loss: 6.939994812011719\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3095535039901733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011691482737660408\n",
      "          model: {}\n",
      "          policy_loss: -0.03748094290494919\n",
      "          total_loss: 2.1434171199798584\n",
      "          vf_explained_var: 0.3628578186035156\n",
      "          vf_loss: 2.1785597801208496\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 50.92916666666667\n",
      "    ram_util_percent: 67.52083333333333\n",
      "  pid: 63797\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -2.3000000000000043\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.08\n",
      "    policy2: -8.679999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11516815316195536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05044482506211852\n",
      "    mean_inference_ms: 1.2574616540663879\n",
      "    mean_raw_obs_processing_ms: 0.6535789375312614\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 20.699999999999946\n",
      "    episode_reward_mean: -3.5999999999999917\n",
      "    episode_reward_min: -37.50000000000005\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-31.500000000000036, -5.999999999999988, -7.499999999999986,\n",
      "        -16.499999999999993, -3.599999999999992, 8.631984016460592e-15, -9.29999999999998,\n",
      "        -5.999999999999995, -21.0, 0.900000000000028, -11.399999999999975, -27.900000000000027,\n",
      "        -6.0, 2.700000000000003, -37.50000000000005, 4.500000000000025, -24.00000000000001,\n",
      "        -5.999999999999988, -7.799999999999983, 19.499999999999996, -1.4999999999999978,\n",
      "        1.5000000000000149, -11.999999999999984, -7.499999999999997, -5.999999999999982,\n",
      "        -10.499999999999984, 20.699999999999946, -4.19999999999999, -8.399999999999975,\n",
      "        -15.599999999999989, -7.499999999999984, -10.79999999999999, -6.300000000000002,\n",
      "        -1.4999999999999976, -14.699999999999974, 7.2000000000000135, -9.299999999999997,\n",
      "        5.700000000000003, 7.499999999999977, -21.0, 2.700000000000023, 1.20000000000001,\n",
      "        -12.299999999999994, 4.500000000000005, 1.500000000000019, 1.5000000000000009,\n",
      "        4.499999999999995, 1.3072876114961218e-14, -10.499999999999975, 9.000000000000028,\n",
      "        -7.499999999999974, -22.500000000000007, -1.4999999999999791, 4.500000000000018,\n",
      "        9.000000000000032, -5.399999999999984, -13.499999999999975, 1.8000000000000291,\n",
      "        -1.4999999999999858, 13.499999999999973, 6.900000000000029, 1.5000000000000209,\n",
      "        -5.999999999999995, 2.373101715136272e-14, 1.8000000000000118, -12.899999999999986,\n",
      "        -2.9999999999999765, -7.799999999999979, 3.000000000000017, 13.499999999999975,\n",
      "        -0.29999999999997196, 1.8000000000000047, -2.3999999999999924, -2.9999999999999853,\n",
      "        -8.99999999999998, -11.999999999999988, 7.200000000000028, 12.600000000000033,\n",
      "        -4.499999999999977, -1.499999999999989, -3.8999999999999795, 2.706168622523819e-14,\n",
      "        1.1296519275560968e-14, -4.19999999999999, -21.900000000000027, -10.499999999999988,\n",
      "        9.3, 17.99999999999997, -10.499999999999979, -1.499999999999995, -1.7999999999999794,\n",
      "        4.500000000000018, -11.999999999999991, 9.96425164601078e-15, 4.500000000000014,\n",
      "        5.099999999999964, -1.7999999999999896, -17.40000000000002, 12.300000000000018,\n",
      "        -4.499999999999984]\n",
      "      policy_policy1_reward: [-21.5, 4.0, 2.5, -6.5, 2.0, 10.0, -1.5, 4.0, -11.0, 6.5,\n",
      "        -2.5, -19.0, 4.0, 10.5, -27.5, 14.5, -14.0, 4.0, 0.0, 29.5, 8.5, 11.5, -2.0,\n",
      "        2.5, 4.0, -0.5, 28.5, 2.5, -5.0, -10.0, 2.5, -3.0, 1.5, 8.5, -8.0, 15.0, -1.5,\n",
      "        13.5, 17.5, -11.0, 10.5, 9.0, -4.5, 14.5, 11.5, 11.5, 14.5, 10.0, -0.5, 19.0,\n",
      "        2.5, -12.5, 8.5, 9.0, 19.0, -2.0, -3.5, 8.5, 8.5, 23.5, 12.5, 11.5, 4.0, 10.0,\n",
      "        8.5, -4.0, 7.0, -5.5, 7.5, 23.5, 7.5, 8.5, 6.5, 7.0, 1.0, -2.0, 15.0, 21.5,\n",
      "        5.5, 8.5, -0.5, 10.0, 10.0, 2.5, -13.0, -0.5, 16.0, 28.0, -0.5, 8.5, 6.0, 9.0,\n",
      "        -2.0, 10.0, 14.5, 14.0, 6.0, -8.5, 19.0, 5.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.5999999999999845, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -5.6, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -6.699999999999985,\n",
      "        -3.4000000000000004, -5.599999999999982, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999986, -9.99999999999998, -6.699999999999995, -7.799999999999986,\n",
      "        -7.79999999999999, -7.799999999999987, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -7.79999999999999, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -4.499999999999987, -9.99999999999998, -3.3999999999999972,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -2.3000000000000043,\n",
      "        -4.499999999999983, -9.99999999999998, -7.79999999999999, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.7999999999999865, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -3.4000000000000035, -9.99999999999998, -9.99999999999998, -6.69999999999999,\n",
      "        -8.89999999999998, -9.99999999999998, -6.699999999999985, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.7999999999999865, -4.499999999999997,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -7.799999999999981, -8.899999999999986, -6.699999999999994, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -2.3000000000000043\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.08\n",
      "      policy2: -8.679999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -27.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11516815316195536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05044482506211852\n",
      "      mean_inference_ms: 1.2574616540663879\n",
      "      mean_raw_obs_processing_ms: 0.6535789375312614\n",
      "  time_since_restore: 46.908406496047974\n",
      "  time_this_iter_s: 17.086151838302612\n",
      "  time_total_s: 46.908406496047974\n",
      "  timers:\n",
      "    learn_throughput: 603.924\n",
      "    learn_time_ms: 6623.347\n",
      "    synch_weights_time_ms: 4.119\n",
      "    training_iteration_time_ms: 15628.157\n",
      "  timestamp: 1660041758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: bafe8_00002\n",
      "  warmup_time: 10.982521057128906\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.50000000000001\n",
      "  episode_reward_mean: -6.472499999999994\n",
      "  episode_reward_min: -42.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 9960ea27f9c14636b15e67ba13cbb170\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3215736150741577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019138090312480927\n",
      "          model: {}\n",
      "          policy_loss: -0.04986068233847618\n",
      "          total_loss: 6.790638446807861\n",
      "          vf_explained_var: -0.03733423724770546\n",
      "          vf_loss: 6.834757328033447\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.330308198928833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01737838424742222\n",
      "          model: {}\n",
      "          policy_loss: -0.04839025065302849\n",
      "          total_loss: 2.4216344356536865\n",
      "          vf_explained_var: 0.25586429238319397\n",
      "          vf_loss: 2.4665486812591553\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 58.992\n",
      "    ram_util_percent: 67.52799999999999\n",
      "  pid: 63801\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: 0.9999999999999979\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.2625\n",
      "    policy2: -8.734999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -32.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13622091274795986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05717863207602993\n",
      "    mean_inference_ms: 1.44501196073232\n",
      "    mean_raw_obs_processing_ms: 0.7611733632096905\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 16.50000000000001\n",
      "    episode_reward_mean: -6.472499999999994\n",
      "    episode_reward_min: -42.00000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [-16.500000000000085, -11.099999999999977, 0.9000000000000085,\n",
      "        8.100000000000017, -33.900000000000055, -4.799999999999985, -5.999999999999988,\n",
      "        -11.399999999999974, -4.7999999999999865, -16.49999999999998, -9.599999999999987,\n",
      "        -20.999999999999993, -14.100000000000003, -23.400000000000006, -7.499999999999991,\n",
      "        5.100000000000022, -18.000000000000036, -5.399999999999988, -30.00000000000005,\n",
      "        -16.500000000000018, -4.199999999999976, -12.599999999999973, -25.500000000000036,\n",
      "        -21.000000000000018, -16.499999999999996, -25.500000000000007, -42.00000000000007,\n",
      "        -4.799999999999983, -22.500000000000007, 0.6000000000000184, -21.000000000000007,\n",
      "        12.600000000000014, -2.399999999999981, -7.799999999999972, -16.49999999999999,\n",
      "        -22.50000000000002, -17.39999999999999, -20.400000000000006, 1.500000000000007,\n",
      "        12.000000000000032, -2.399999999999986, 7.200000000000024, 4.800000000000029,\n",
      "        12.599999999999948, 16.50000000000001, 9.60000000000002, -3.8999999999999777,\n",
      "        -25.500000000000014, 9.000000000000032, 9.600000000000016, 10.50000000000002,\n",
      "        2.9999999999999707, 1.4999999999999991, -10.499999999999979, -4.499999999999979,\n",
      "        -4.4999999999999805, -14.999999999999995, -1.7999999999999945, 7.500000000000011,\n",
      "        2.700000000000021, -5.4, -7.500000000000002, -5.999999999999998, -5.999999999999979,\n",
      "        2.906008766956347e-14, 3.600000000000003, -5.399999999999984, -13.499999999999979,\n",
      "        -14.69999999999998, -1.4999999999999916, -2.999999999999975, 4.500000000000002,\n",
      "        -13.499999999999982, 2.7000000000000126, -2.3999999999999897, 6.000000000000011,\n",
      "        6.000000000000014, 1.3627987627273797e-14, -6.8999999999999915, 11.100000000000033]\n",
      "      policy_policy1_reward: [-6.5, -5.5, 6.5, 17.0, -25.0, 3.0, 4.0, -2.5, 3.0, -6.5,\n",
      "        -4.0, -11.0, -8.5, -14.5, 2.5, 14.0, -8.0, 3.5, -20.0, -6.5, 2.5, -7.0, -15.5,\n",
      "        -11.0, -6.5, -15.5, -32.0, 3.0, -12.5, 9.5, -11.0, 21.5, 6.5, 0.0, -6.5, -12.5,\n",
      "        -8.5, -11.5, 11.5, 22.0, 6.5, 15.0, 11.5, 21.5, 26.5, 18.5, 5.0, -15.5, 19.0,\n",
      "        18.5, 20.5, 13.0, 11.5, -0.5, 5.5, 0.0, -5.0, 6.0, 17.5, 10.5, -2.0, 2.5, 4.0,\n",
      "        -7.0, 10.0, 12.5, 3.5, -3.5, -8.0, 8.5, 7.0, 14.5, -3.5, 10.5, 6.5, 16.0, 16.0,\n",
      "        10.0, -3.5, 20.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -5.599999999999991, -5.599999999999992,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999989, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -5.599999999999982, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999895, -5.599999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999986, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -6.699999999999995,\n",
      "        -8.899999999999984, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999984, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -4.499999999999982, -9.99999999999998, -7.799999999999986, -9.99999999999998,\n",
      "        -7.79999999999999, -3.399999999999987, -9.99999999999998, -9.99999999999998,\n",
      "        0.9999999999999979, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -6.69999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -3.4000000000000017,\n",
      "        -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: 0.9999999999999979\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.2625\n",
      "      policy2: -8.734999999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -32.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.13622091274795986\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05717863207602993\n",
      "      mean_inference_ms: 1.44501196073232\n",
      "      mean_raw_obs_processing_ms: 0.7611733632096905\n",
      "  time_since_restore: 35.28082013130188\n",
      "  time_this_iter_s: 17.61622714996338\n",
      "  time_total_s: 35.28082013130188\n",
      "  timers:\n",
      "    learn_throughput: 519.972\n",
      "    learn_time_ms: 7692.715\n",
      "    synch_weights_time_ms: 4.446\n",
      "    training_iteration_time_ms: 17634.949\n",
      "  timestamp: 1660041759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: bafe8_00003\n",
      "  warmup_time: 11.104228258132935\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.299999999999947\n",
      "  episode_reward_mean: -1.1399999999999892\n",
      "  episode_reward_min: -31.50000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 4f1310ef73844c1c99a53824a8d7ce26\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.277124047279358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011109231039881706\n",
      "          model: {}\n",
      "          policy_loss: -0.04115000367164612\n",
      "          total_loss: 6.684436798095703\n",
      "          vf_explained_var: 0.0894772931933403\n",
      "          vf_loss: 6.723364353179932\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.28304922580719\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011474985629320145\n",
      "          model: {}\n",
      "          policy_loss: -0.04056992009282112\n",
      "          total_loss: 1.9687612056732178\n",
      "          vf_explained_var: 0.35424256324768066\n",
      "          vf_loss: 2.008183717727661\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 63.585000000000015\n",
      "    ram_util_percent: 67.36499999999998\n",
      "  pid: 63785\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 0.9999999999999952\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.1\n",
      "    policy2: -8.239999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -21.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11178218463945608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04773712916297547\n",
      "    mean_inference_ms: 1.201681085232268\n",
      "    mean_raw_obs_processing_ms: 0.6254828679947607\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.299999999999947\n",
      "    episode_reward_mean: -1.1399999999999892\n",
      "    episode_reward_min: -31.50000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-5.999999999999997, 9.300000000000011, -24.0, -21.900000000000023,\n",
      "        -7.199999999999994, -14.999999999999977, -1.4999999999999931, -10.499999999999975,\n",
      "        -31.50000000000007, 3.5999999999999956, 4.499999999999993, -10.500000000000007,\n",
      "        15.900000000000029, 2.1000000000000054, -1.4999999999999791, -4.499999999999985,\n",
      "        8.999999999999982, 14.400000000000015, -14.399999999999995, -11.999999999999979,\n",
      "        5.100000000000016, -15.899999999999974, 24.299999999999947, 4.500000000000008,\n",
      "        3.300000000000029, 3.000000000000006, 3.000000000000033, 3.6000000000000134,\n",
      "        -1.1999999999999855, -11.400000000000015, 1.5737411374061594e-14, -5.9999999999999805,\n",
      "        -4.499999999999995, -2.399999999999996, 7.200000000000031, 1.500000000000021,\n",
      "        -4.199999999999996, 2.1000000000000103, 12.000000000000023, -1.49999999999999,\n",
      "        14.699999999999957, 3.0000000000000187, 13.500000000000025, 2.4175106361212784e-14,\n",
      "        14.10000000000003, -3.8999999999999813, 5.099999999999989, -5.999999999999995,\n",
      "        -10.79999999999998, -13.49999999999999, -2.399999999999992, 10.500000000000025,\n",
      "        6.000000000000023, -5.999999999999979, 13.499999999999988, -10.499999999999982,\n",
      "        -19.500000000000004, -2.9999999999999933, -4.499999999999988, -1.4999999999999836,\n",
      "        1.500000000000023, -2.9999999999999836, -5.399999999999984, -3.8999999999999826,\n",
      "        7.200000000000024, -3.599999999999984, -9.899999999999979, -3.2999999999999767,\n",
      "        3.300000000000032, -8.999999999999998, -6.000000000000001, 19.500000000000007,\n",
      "        1.5000000000000173, -5.999999999999993, 2.1000000000000263, -3.899999999999979,\n",
      "        6.000000000000014, -5.399999999999981, 2.1000000000000227, 13.199999999999976,\n",
      "        -8.099999999999977, 3.300000000000007, 0.3000000000000218, 2.1000000000000205,\n",
      "        1.5000000000000024, -5.099999999999996, -4.500000000000001, 10.500000000000027,\n",
      "        2.017830347256222e-14, -4.4999999999999805, -5.999999999999982, -4.4999999999999964,\n",
      "        -3.9000000000000017, -18.0, -8.399999999999983, 1.7513768213461844e-14, 10.500000000000028,\n",
      "        14.700000000000019, -7.4999999999999805, -2.999999999999977]\n",
      "      policy_policy1_reward: [-1.5, 16.0, -14.0, -13.0, -0.5, -5.0, 8.5, -0.5, -21.5,\n",
      "        12.5, 14.5, -0.5, 21.5, 11.0, 8.5, 5.5, 13.5, 20.0, -5.5, -2.0, 14.0, -7.0,\n",
      "        25.5, 14.5, 10.0, 13.0, 13.0, 12.5, 0.0, -8.0, 10.0, 4.0, -5.5, 6.5, 15.0, 6.0,\n",
      "        2.5, 11.0, 22.0, 8.5, 22.5, 7.5, 18.0, 10.0, 23.0, -0.5, 14.0, -7.0, -3.0, -3.5,\n",
      "        6.5, 15.0, 10.5, 4.0, 23.5, -0.5, -9.5, 7.0, 5.5, 8.5, 11.5, 7.0, 3.5, 5.0,\n",
      "        15.0, 2.0, -1.0, 4.5, 10.0, 1.0, 4.0, 29.5, 11.5, 4.0, 11.0, 5.0, 16.0, 3.5,\n",
      "        11.0, 21.0, -2.5, 10.0, 7.0, 11.0, 11.5, 0.5, 5.5, 20.5, 10.0, 5.5, 4.0, 5.5,\n",
      "        5.0, -8.0, 0.5, 10.0, 20.5, 22.5, 2.5, 7.0]\n",
      "      policy_policy2_reward: [-4.499999999999998, -6.699999999999994, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -4.500000000000002, -5.599999999999982, -8.899999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -1.1999999999999855,\n",
      "        -9.99999999999998, -6.6999999999999895, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -1.1999999999999993, -3.400000000000005, -9.99999999999998,\n",
      "        -9.99999999999998, 0.9999999999999934, -8.89999999999998, -7.799999999999988,\n",
      "        -4.500000000000003, -6.6999999999999815, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -4.500000000000004, -4.499999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -3.400000000000005, -8.89999999999998,\n",
      "        0.9999999999999952, -7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -4.499999999999982, -4.499999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -7.799999999999984, -5.599999999999998, -8.899999999999986,\n",
      "        -7.799999999999988, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.899999999999986, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.79999999999999, -5.599999999999991, -6.6999999999999815, -6.699999999999985,\n",
      "        -8.899999999999986, -9.99999999999998, -5.599999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 0.9999999999999952\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.1\n",
      "      policy2: -8.239999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -21.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11178218463945608\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04773712916297547\n",
      "      mean_inference_ms: 1.201681085232268\n",
      "      mean_raw_obs_processing_ms: 0.6254828679947607\n",
      "  time_since_restore: 48.09267783164978\n",
      "  time_this_iter_s: 14.310240983963013\n",
      "  time_total_s: 48.09267783164978\n",
      "  timers:\n",
      "    learn_throughput: 595.001\n",
      "    learn_time_ms: 5042.006\n",
      "    synch_weights_time_ms: 4.377\n",
      "    training_iteration_time_ms: 12016.523\n",
      "  timestamp: 1660041763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: bafe8_00000\n",
      "  warmup_time: 10.087341070175171\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.399999999999977\n",
      "  episode_reward_mean: -2.069999999999991\n",
      "  episode_reward_min: -30.60000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: ba802e21773d46369dab10c339d8b4ae\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2473547458648682\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016356956213712692\n",
      "          model: {}\n",
      "          policy_loss: -0.052213240414857864\n",
      "          total_loss: 6.720391273498535\n",
      "          vf_explained_var: 0.11895407736301422\n",
      "          vf_loss: 6.767696857452393\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2141016721725464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018154991790652275\n",
      "          model: {}\n",
      "          policy_loss: -0.05714339017868042\n",
      "          total_loss: 1.7711726427078247\n",
      "          vf_explained_var: 0.3583436608314514\n",
      "          vf_loss: 1.8228695392608643\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 57.105\n",
      "    ram_util_percent: 67.35\n",
      "  pid: 63794\n",
      "  policy_reward_max:\n",
      "    policy1: 35.0\n",
      "    policy2: -1.199999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.39\n",
      "    policy2: -8.459999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -25.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11441600565086624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049562549964259565\n",
      "    mean_inference_ms: 1.2324447001727303\n",
      "    mean_raw_obs_processing_ms: 0.650436816523837\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.399999999999977\n",
      "    episode_reward_mean: -2.069999999999991\n",
      "    episode_reward_min: -30.60000000000003\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [15.000000000000012, -8.399999999999986, -4.499999999999979, 1.5,\n",
      "        -22.50000000000001, -19.499999999999986, -24.000000000000004, 29.399999999999977,\n",
      "        1.500000000000012, 12.599999999999975, -10.499999999999986, 5.7000000000000295,\n",
      "        -13.499999999999977, 7.500000000000016, 5.400000000000016, 8.10000000000002,\n",
      "        -30.60000000000003, -2.9999999999999805, -25.500000000000043, 9.300000000000018,\n",
      "        5.10000000000001, -2.999999999999994, 4.200000000000008, -10.499999999999977,\n",
      "        -8.99999999999999, -17.999999999999986, -8.39999999999999, -0.2999999999999824,\n",
      "        4.500000000000012, 1.1296519275560968e-14, -1.799999999999975, -13.499999999999991,\n",
      "        -5.999999999999991, -0.2999999999999764, -25.50000000000002, 6.000000000000011,\n",
      "        14.100000000000016, -10.199999999999985, -5.999999999999982, 24.59999999999993,\n",
      "        12.000000000000023, -4.499999999999986, -3.5999999999999823, -14.999999999999982,\n",
      "        1.8000000000000238, 13.5, 11.999999999999998, -9.599999999999978, 22.49999999999998,\n",
      "        -24.00000000000005, -17.099999999999987, -13.499999999999977, -1.4999999999999731,\n",
      "        -5.700000000000003, -20.40000000000004, -16.499999999999993, -5.999999999999988,\n",
      "        -3.2999999999999803, -1.4999999999999742, 1.5000000000000289, -8.399999999999993,\n",
      "        -7.499999999999989, 8.100000000000014, -9.89999999999999, 6.000000000000023,\n",
      "        12.600000000000033, -2.69999999999998, 0.6000000000000095, 4.500000000000002,\n",
      "        -1.5000000000000155, 4.800000000000022, -5.099999999999981, 12.000000000000004,\n",
      "        -4.499999999999987, 1.500000000000027, -9.899999999999979, 5.100000000000033,\n",
      "        -0.8999999999999913, -7.499999999999975, 5.100000000000028, -6.299999999999976,\n",
      "        1.5000000000000173, -7.799999999999976, 18.299999999999933, 0.6000000000000003,\n",
      "        8.100000000000025, 15.299999999999978, -16.500000000000007, -2.999999999999984,\n",
      "        6.600000000000005, 5.1000000000000165, -5.999999999999989, -10.799999999999992,\n",
      "        -4.4999999999999885, -11.09999999999998, 6.000000000000032, -0.2999999999999825,\n",
      "        -1.4999999999999813, 11.099999999999982, -15.299999999999992]\n",
      "      policy_policy1_reward: [25.0, 0.5, 5.5, 11.5, -12.5, -9.5, -14.0, 35.0, 11.5,\n",
      "        16.0, -0.5, 13.5, -3.5, 17.5, 11.0, 17.0, -25.0, 7.0, -15.5, 16.0, 14.0, 7.0,\n",
      "        12.0, -0.5, 1.0, -8.0, 0.5, 7.5, 14.5, 10.0, 6.0, -3.5, 4.0, 7.5, -15.5, 16.0,\n",
      "        23.0, -9.0, 4.0, 33.5, 22.0, 5.5, 2.0, -5.0, 8.5, 18.0, 22.0, -4.0, 32.5, -14.0,\n",
      "        -11.5, -3.5, 8.5, 1.0, -11.5, -6.5, -1.5, 4.5, 8.5, 11.5, -5.0, -3.0, 17.0,\n",
      "        -1.0, 16.0, 21.5, 4.0, 9.5, 14.5, 8.5, 11.5, 0.5, 22.0, 5.5, 11.5, -1.0, 14.0,\n",
      "        8.0, 2.5, 14.0, 1.5, 6.0, 0.0, 25.0, 4.0, 17.0, 22.0, -6.5, 7.0, 15.5, 14.0,\n",
      "        4.0, -3.0, 5.5, -5.5, 16.0, 7.5, 8.5, 20.0, -7.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999984, -9.99999999999998, -3.399999999999991, -9.99999999999998,\n",
      "        -7.799999999999982, -9.99999999999998, -9.99999999999998, -5.599999999999985,\n",
      "        -8.899999999999986, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -7.799999999999988,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -7.799999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -7.7999999999999865, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -1.199999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -5.599999999999985,\n",
      "        -9.99999999999998, -6.699999999999987, -4.499999999999994, -9.99999999999998,\n",
      "        -5.599999999999993, -9.99999999999998, -9.99999999999998, -5.599999999999995,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999987, -8.89999999999998,\n",
      "        -9.99999999999998, -4.49999999999999, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -3.400000000000004, -4.499999999999984, -8.899999999999984,\n",
      "        -8.899999999999984, -9.99999999999998, -8.89999999999998, -6.699999999999994,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -6.699999999999994,\n",
      "        -5.599999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.899999999999986, -8.89999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -7.799999999999981, -4.49999999999999, -7.799999999999981,\n",
      "        -6.699999999999982, -3.400000000000001, -8.899999999999984, -6.699999999999995,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -5.6, -9.99999999999998,\n",
      "        -7.799999999999985, -9.99999999999998, -8.89999999999998, -7.7999999999999865]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 35.0\n",
      "      policy2: -1.199999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.39\n",
      "      policy2: -8.459999999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -25.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11441600565086624\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.049562549964259565\n",
      "      mean_inference_ms: 1.2324447001727303\n",
      "      mean_raw_obs_processing_ms: 0.650436816523837\n",
      "  time_since_restore: 48.650858879089355\n",
      "  time_this_iter_s: 14.392254114151001\n",
      "  time_total_s: 48.650858879089355\n",
      "  timers:\n",
      "    learn_throughput: 596.997\n",
      "    learn_time_ms: 5025.147\n",
      "    synch_weights_time_ms: 3.735\n",
      "    training_iteration_time_ms: 12156.734\n",
      "  timestamp: 1660041763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: bafe8_00001\n",
      "  warmup_time: 10.367733001708984\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.500000000000007\n",
      "  episode_reward_mean: -0.7289999999999872\n",
      "  episode_reward_min: -19.500000000000004\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 4f1310ef73844c1c99a53824a8d7ce26\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2439261674880981\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012517718598246574\n",
      "          model: {}\n",
      "          policy_loss: -0.040102697908878326\n",
      "          total_loss: 7.008052349090576\n",
      "          vf_explained_var: 0.075385183095932\n",
      "          vf_loss: 7.045652389526367\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2375352382659912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011972418986260891\n",
      "          model: {}\n",
      "          policy_loss: -0.040125004947185516\n",
      "          total_loss: 3.258167266845703\n",
      "          vf_explained_var: 0.15095166862010956\n",
      "          vf_loss: 3.2970950603485107\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 58.64736842105264\n",
      "    ram_util_percent: 67.10526315789474\n",
      "  pid: 63785\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 10.900000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.95\n",
      "    policy2: -7.678999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12197175126109748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051702329926027975\n",
      "    mean_inference_ms: 1.303524562646824\n",
      "    mean_raw_obs_processing_ms: 0.6726706654356588\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.500000000000007\n",
      "    episode_reward_mean: -0.7289999999999872\n",
      "    episode_reward_min: -19.500000000000004\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [1.5737411374061594e-14, -5.9999999999999805, -4.499999999999995,\n",
      "        -2.399999999999996, 7.200000000000031, 1.500000000000021, -4.199999999999996,\n",
      "        2.1000000000000103, 12.000000000000023, -1.49999999999999, 14.699999999999957,\n",
      "        3.0000000000000187, 13.500000000000025, 2.4175106361212784e-14, 14.10000000000003,\n",
      "        -3.8999999999999813, 5.099999999999989, -5.999999999999995, -10.79999999999998,\n",
      "        -13.49999999999999, -2.399999999999992, 10.500000000000025, 6.000000000000023,\n",
      "        -5.999999999999979, 13.499999999999988, -10.499999999999982, -19.500000000000004,\n",
      "        -2.9999999999999933, -4.499999999999988, -1.4999999999999836, 1.500000000000023,\n",
      "        -2.9999999999999836, -5.399999999999984, -3.8999999999999826, 7.200000000000024,\n",
      "        -3.599999999999984, -9.899999999999979, -3.2999999999999767, 3.300000000000032,\n",
      "        -8.999999999999998, -6.000000000000001, 19.500000000000007, 1.5000000000000173,\n",
      "        -5.999999999999993, 2.1000000000000263, -3.899999999999979, 6.000000000000014,\n",
      "        -5.399999999999981, 2.1000000000000227, 13.199999999999976, -8.099999999999977,\n",
      "        3.300000000000007, 0.3000000000000218, 2.1000000000000205, 1.5000000000000024,\n",
      "        -5.099999999999996, -4.500000000000001, 10.500000000000027, 2.017830347256222e-14,\n",
      "        -4.4999999999999805, -5.999999999999982, -4.4999999999999964, -3.9000000000000017,\n",
      "        -18.0, -8.399999999999983, 1.7513768213461844e-14, 10.500000000000028, 14.700000000000019,\n",
      "        -7.4999999999999805, -2.999999999999977, 10.800000000000031, -4.499999999999979,\n",
      "        -7.199999999999994, -2.9999999999999836, 5.100000000000026, -5.999999999999982,\n",
      "        1.8000000000000242, -8.99999999999998, -7.4999999999999805, 14.999999999999947,\n",
      "        9.900000000000006, -5.099999999999985, 7.800000000000024, -4.4999999999999805,\n",
      "        -11.399999999999974, -14.699999999999992, 8.699999999999978, -12.89999999999998,\n",
      "        -1.4999999999999885, -3.59999999999998, -2.3999999999999737, 15.89999999999995,\n",
      "        0.3000000000000014, 5.079270337660091e-15, 5.099999999999971, 3.600000000000031,\n",
      "        10.200000000000015, -15.899999999999977, -17.399999999999984, -9.899999999999975]\n",
      "      policy_policy1_reward: [10.0, 4.0, -5.5, 6.5, 15.0, 6.0, 2.5, 11.0, 22.0, 8.5,\n",
      "        22.5, 7.5, 18.0, 10.0, 23.0, -0.5, 14.0, -7.0, -3.0, -3.5, 6.5, 15.0, 10.5,\n",
      "        4.0, 23.5, -0.5, -9.5, 7.0, 5.5, 8.5, 11.5, 7.0, 3.5, 5.0, 15.0, 2.0, -1.0,\n",
      "        4.5, 10.0, 1.0, 4.0, 29.5, 11.5, 4.0, 11.0, 5.0, 16.0, 3.5, 11.0, 21.0, -2.5,\n",
      "        10.0, 7.0, 11.0, 11.5, 0.5, 5.5, 20.5, 10.0, 5.5, 4.0, 5.5, 5.0, -8.0, 0.5,\n",
      "        10.0, 20.5, 22.5, 2.5, 7.0, 12.0, 5.5, -0.5, 1.5, 8.5, 4.0, 3.0, 1.0, -3.0,\n",
      "        25.0, -1.0, 0.5, 14.5, 5.5, -2.5, -8.0, 16.5, -4.0, -2.5, 2.0, 6.5, 21.5, 7.0,\n",
      "        10.0, 8.5, 7.0, 18.0, -7.0, -8.5, -1.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, 0.9999999999999934,\n",
      "        -8.89999999999998, -7.799999999999988, -4.500000000000003, -6.6999999999999815,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -4.500000000000004, -4.499999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -3.400000000000005, -8.89999999999998, 0.9999999999999952, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -4.499999999999982, -4.499999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.899999999999986, -7.799999999999984,\n",
      "        -5.599999999999998, -8.899999999999986, -7.799999999999988, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.79999999999999, -5.599999999999991,\n",
      "        -6.6999999999999815, -6.699999999999985, -8.899999999999986, -9.99999999999998,\n",
      "        -5.599999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -1.2000000000000044,\n",
      "        -9.99999999999998, -6.6999999999999815, -4.500000000000003, -3.399999999999989,\n",
      "        -9.99999999999998, -1.1999999999999977, -9.99999999999998, -4.500000000000004,\n",
      "        -9.99999999999998, 10.900000000000013, -5.599999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998, -8.899999999999986, -6.699999999999983, -7.799999999999981,\n",
      "        -8.89999999999998, 0.999999999999997, -5.599999999999987, -8.89999999999998,\n",
      "        -5.6, -6.6999999999999815, -9.99999999999998, -3.399999999999995, -3.3999999999999835,\n",
      "        -7.799999999999981, -8.899999999999986, -8.899999999999986, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 10.900000000000013\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.95\n",
      "      policy2: -7.678999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -9.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12197175126109748\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.051702329926027975\n",
      "      mean_inference_ms: 1.303524562646824\n",
      "      mean_raw_obs_processing_ms: 0.6726706654356588\n",
      "  time_since_restore: 61.248740673065186\n",
      "  time_this_iter_s: 13.156062841415405\n",
      "  time_total_s: 61.248740673065186\n",
      "  timers:\n",
      "    learn_throughput: 551.677\n",
      "    learn_time_ms: 5437.965\n",
      "    synch_weights_time_ms: 5.836\n",
      "    training_iteration_time_ms: 12243.002\n",
      "  timestamp: 1660041776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: bafe8_00000\n",
      "  warmup_time: 10.087341070175171\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.799999999999947\n",
      "  episode_reward_mean: -1.5449999999999888\n",
      "  episode_reward_min: -22.500000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 2593f3a7735a4f4ab02b4cd9b49f99fc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.295483946800232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010469823144376278\n",
      "          model: {}\n",
      "          policy_loss: -0.034892234951257706\n",
      "          total_loss: 6.954836845397949\n",
      "          vf_explained_var: 0.13727816939353943\n",
      "          vf_loss: 6.987635135650635\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.274726390838623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011590453796088696\n",
      "          model: {}\n",
      "          policy_loss: -0.04139963909983635\n",
      "          total_loss: 2.159100294113159\n",
      "          vf_explained_var: 0.25416114926338196\n",
      "          vf_loss: 2.198181629180908\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 53.270370370370365\n",
      "    ram_util_percent: 67.14814814814815\n",
      "  pid: 63797\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: -2.299999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.475\n",
      "    policy2: -8.019999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -13.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12352702293478252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05374946419756272\n",
      "    mean_inference_ms: 1.3410986321956466\n",
      "    mean_raw_obs_processing_ms: 0.6973188380146754\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.799999999999947\n",
      "    episode_reward_mean: -1.5449999999999888\n",
      "    episode_reward_min: -22.500000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [2.700000000000023, 1.20000000000001, -12.299999999999994, 4.500000000000005,\n",
      "        1.500000000000019, 1.5000000000000009, 4.499999999999995, 1.3072876114961218e-14,\n",
      "        -10.499999999999975, 9.000000000000028, -7.499999999999974, -22.500000000000007,\n",
      "        -1.4999999999999791, 4.500000000000018, 9.000000000000032, -5.399999999999984,\n",
      "        -13.499999999999975, 1.8000000000000291, -1.4999999999999858, 13.499999999999973,\n",
      "        6.900000000000029, 1.5000000000000209, -5.999999999999995, 2.373101715136272e-14,\n",
      "        1.8000000000000118, -12.899999999999986, -2.9999999999999765, -7.799999999999979,\n",
      "        3.000000000000017, 13.499999999999975, -0.29999999999997196, 1.8000000000000047,\n",
      "        -2.3999999999999924, -2.9999999999999853, -8.99999999999998, -11.999999999999988,\n",
      "        7.200000000000028, 12.600000000000033, -4.499999999999977, -1.499999999999989,\n",
      "        -3.8999999999999795, 2.706168622523819e-14, 1.1296519275560968e-14, -4.19999999999999,\n",
      "        -21.900000000000027, -10.499999999999988, 9.3, 17.99999999999997, -10.499999999999979,\n",
      "        -1.499999999999995, -1.7999999999999794, 4.500000000000018, -11.999999999999991,\n",
      "        9.96425164601078e-15, 4.500000000000014, 5.099999999999964, -1.7999999999999896,\n",
      "        -17.40000000000002, 12.300000000000018, -4.499999999999984, -8.999999999999982,\n",
      "        0.9000000000000107, -7.799999999999981, 9.299999999999962, -3.9000000000000004,\n",
      "        3.9000000000000266, -8.69999999999998, -19.799999999999997, 8.39999999999999,\n",
      "        -2.3999999999999906, -9.299999999999978, 1.5000000000000022, -5.999999999999986,\n",
      "        -10.499999999999984, 9.000000000000027, 1.8000000000000007, 4.499999999999995,\n",
      "        7.800000000000022, -7.499999999999975, -2.399999999999995, 3.0000000000000053,\n",
      "        4.80000000000002, -12.299999999999983, 4.500000000000009, 19.799999999999947,\n",
      "        0.3000000000000209, -8.399999999999974, -8.399999999999977, 14.10000000000002,\n",
      "        -21.000000000000007, -4.500000000000003, -7.79999999999999, 3.600000000000024,\n",
      "        2.100000000000021, 0.6000000000000282, -0.2999999999999986, 2.4147350785597155e-15,\n",
      "        -9.299999999999976, -6.299999999999974, -17.399999999999984]\n",
      "      policy_policy1_reward: [10.5, 9.0, -4.5, 14.5, 11.5, 11.5, 14.5, 10.0, -0.5, 19.0,\n",
      "        2.5, -12.5, 8.5, 9.0, 19.0, -2.0, -3.5, 8.5, 8.5, 23.5, 12.5, 11.5, 4.0, 10.0,\n",
      "        8.5, -4.0, 7.0, -5.5, 7.5, 23.5, 7.5, 8.5, 6.5, 7.0, 1.0, -2.0, 15.0, 21.5,\n",
      "        5.5, 8.5, -0.5, 10.0, 10.0, 2.5, -13.0, -0.5, 16.0, 28.0, -0.5, 8.5, 6.0, 9.0,\n",
      "        -2.0, 10.0, 14.5, 14.0, 6.0, -8.5, 19.0, 5.5, 1.0, 6.5, 0.0, 16.0, 5.0, 9.5,\n",
      "        -2.0, -12.0, 14.0, 6.5, -1.5, 6.0, -1.5, -6.0, 13.5, 8.5, 9.0, 14.5, -3.0, 6.5,\n",
      "        13.0, 11.5, -4.5, 14.5, 26.5, 7.0, 0.5, 0.5, 23.0, -11.0, 0.0, 0.0, 12.5, 11.0,\n",
      "        9.5, 2.0, 4.5, -1.5, 1.5, -8.5]\n",
      "      policy_policy2_reward: [-7.799999999999981, -7.79999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -4.499999999999987, -9.99999999999998,\n",
      "        -3.3999999999999972, -9.99999999999998, -6.699999999999993, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -2.3000000000000043, -4.499999999999983, -9.99999999999998, -7.79999999999999,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -3.4000000000000035, -9.99999999999998, -9.99999999999998,\n",
      "        -6.69999999999999, -8.89999999999998, -9.99999999999998, -6.699999999999985,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -4.499999999999997, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -8.899999999999986, -6.699999999999994,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999988, -7.799999999999981,\n",
      "        -6.699999999999995, -8.899999999999983, -5.599999999999999, -6.699999999999987,\n",
      "        -7.799999999999983, -5.599999999999998, -8.89999999999998, -7.79999999999999,\n",
      "        -4.499999999999983, -4.499999999999997, -4.500000000000002, -4.499999999999982,\n",
      "        -6.6999999999999815, -4.499999999999989, -6.699999999999994, -4.499999999999996,\n",
      "        -8.899999999999983, -9.99999999999998, -6.6999999999999895, -7.799999999999981,\n",
      "        -9.99999999999998, -6.6999999999999895, -6.699999999999995, -8.899999999999986,\n",
      "        -8.89999999999998, -8.89999999999998, -9.99999999999998, -4.499999999999988,\n",
      "        -7.799999999999981, -8.899999999999984, -8.899999999999986, -8.899999999999984,\n",
      "        -2.299999999999997, -4.500000000000004, -7.79999999999999, -7.799999999999981,\n",
      "        -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: -2.299999999999997\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.475\n",
      "      policy2: -8.019999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -13.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12352702293478252\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05374946419756272\n",
      "      mean_inference_ms: 1.3410986321956466\n",
      "      mean_raw_obs_processing_ms: 0.6973188380146754\n",
      "  time_since_restore: 65.48844838142395\n",
      "  time_this_iter_s: 18.580041885375977\n",
      "  time_total_s: 65.48844838142395\n",
      "  timers:\n",
      "    learn_throughput: 561.115\n",
      "    learn_time_ms: 7128.667\n",
      "    synch_weights_time_ms: 4.327\n",
      "    training_iteration_time_ms: 16363.695\n",
      "  timestamp: 1660041777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: bafe8_00002\n",
      "  warmup_time: 10.982521057128906\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00001:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.59999999999993\n",
      "  episode_reward_mean: 0.11700000000000825\n",
      "  episode_reward_min: -25.50000000000002\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: ba802e21773d46369dab10c339d8b4ae\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.199295163154602\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01995960995554924\n",
      "          model: {}\n",
      "          policy_loss: -0.06090059131383896\n",
      "          total_loss: 6.686054229736328\n",
      "          vf_explained_var: 0.07574258744716644\n",
      "          vf_loss: 6.740966796875\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1698565483093262\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020419806241989136\n",
      "          model: {}\n",
      "          policy_loss: -0.06215694174170494\n",
      "          total_loss: 1.9192254543304443\n",
      "          vf_explained_var: 0.36685290932655334\n",
      "          vf_loss: 1.975256323814392\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 60.06499999999998\n",
      "    ram_util_percent: 67.1\n",
      "  pid: 63794\n",
      "  policy_reward_max:\n",
      "    policy1: 33.5\n",
      "    policy2: -1.199999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.17\n",
      "    policy2: -8.052999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12416101882240994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05354573589472479\n",
      "    mean_inference_ms: 1.3325484704299402\n",
      "    mean_raw_obs_processing_ms: 0.6968800491630641\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.59999999999993\n",
      "    episode_reward_mean: 0.11700000000000825\n",
      "    episode_reward_min: -25.50000000000002\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-1.799999999999975, -13.499999999999991, -5.999999999999991,\n",
      "        -0.2999999999999764, -25.50000000000002, 6.000000000000011, 14.100000000000016,\n",
      "        -10.199999999999985, -5.999999999999982, 24.59999999999993, 12.000000000000023,\n",
      "        -4.499999999999986, -3.5999999999999823, -14.999999999999982, 1.8000000000000238,\n",
      "        13.5, 11.999999999999998, -9.599999999999978, 22.49999999999998, -24.00000000000005,\n",
      "        -17.099999999999987, -13.499999999999977, -1.4999999999999731, -5.700000000000003,\n",
      "        -20.40000000000004, -16.499999999999993, -5.999999999999988, -3.2999999999999803,\n",
      "        -1.4999999999999742, 1.5000000000000289, -8.399999999999993, -7.499999999999989,\n",
      "        8.100000000000014, -9.89999999999999, 6.000000000000023, 12.600000000000033,\n",
      "        -2.69999999999998, 0.6000000000000095, 4.500000000000002, -1.5000000000000155,\n",
      "        4.800000000000022, -5.099999999999981, 12.000000000000004, -4.499999999999987,\n",
      "        1.500000000000027, -9.899999999999979, 5.100000000000033, -0.8999999999999913,\n",
      "        -7.499999999999975, 5.100000000000028, -6.299999999999976, 1.5000000000000173,\n",
      "        -7.799999999999976, 18.299999999999933, 0.6000000000000003, 8.100000000000025,\n",
      "        15.299999999999978, -16.500000000000007, -2.999999999999984, 6.600000000000005,\n",
      "        5.1000000000000165, -5.999999999999989, -10.799999999999992, -4.4999999999999885,\n",
      "        -11.09999999999998, 6.000000000000032, -0.2999999999999825, -1.4999999999999813,\n",
      "        11.099999999999982, -15.299999999999992, 19.799999999999898, 1.1999999999999917,\n",
      "        -15.899999999999977, 12.00000000000002, -3.5999999999999766, -6.899999999999988,\n",
      "        2.1000000000000174, 6.600000000000033, 15.299999999999908, -8.999999999999984,\n",
      "        0.9000000000000118, 9.600000000000028, 15.600000000000014, 14.400000000000025,\n",
      "        13.799999999999947, -4.499999999999975, -4.799999999999981, 8.100000000000012,\n",
      "        7.499999999999966, -4.499999999999973, 2.100000000000019, 1.5000000000000142,\n",
      "        -1.4999999999999756, 2.017830347256222e-14, 9.299999999999994, 7.500000000000014,\n",
      "        2.1000000000000085, 10.800000000000013, 5.700000000000033, 2.1000000000000316]\n",
      "      policy_policy1_reward: [6.0, -3.5, 4.0, 7.5, -15.5, 16.0, 23.0, -9.0, 4.0, 33.5,\n",
      "        22.0, 5.5, 2.0, -5.0, 8.5, 18.0, 22.0, -4.0, 32.5, -14.0, -11.5, -3.5, 8.5,\n",
      "        1.0, -11.5, -6.5, -1.5, 4.5, 8.5, 11.5, -5.0, -3.0, 17.0, -1.0, 16.0, 21.5,\n",
      "        4.0, 9.5, 14.5, 8.5, 11.5, 0.5, 22.0, 5.5, 11.5, -1.0, 14.0, 8.0, 2.5, 14.0,\n",
      "        1.5, 6.0, 0.0, 25.0, 4.0, 17.0, 22.0, -6.5, 7.0, 15.5, 14.0, 4.0, -3.0, 5.5,\n",
      "        -5.5, 16.0, 7.5, 8.5, 20.0, -7.5, 26.5, 3.5, -7.0, 16.5, 2.0, 2.0, 11.0, 15.5,\n",
      "        22.0, 1.0, 6.5, 18.5, 24.5, 20.0, 20.5, 5.5, 3.0, 11.5, 17.5, 0.0, 11.0, 11.5,\n",
      "        3.0, 10.0, 16.0, 17.5, 11.0, 17.5, 13.5, 11.0]\n",
      "      policy_policy2_reward: [-7.799999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -7.7999999999999865, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -1.199999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999985, -9.99999999999998, -6.699999999999987,\n",
      "        -4.499999999999994, -9.99999999999998, -5.599999999999993, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999995, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999987, -8.89999999999998, -9.99999999999998, -4.49999999999999,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -3.400000000000004,\n",
      "        -4.499999999999984, -8.899999999999984, -8.899999999999984, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999994, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999994, -5.599999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -8.899999999999986,\n",
      "        -8.89999999999998, -9.99999999999998, -8.899999999999983, -7.799999999999981,\n",
      "        -4.49999999999999, -7.799999999999981, -6.699999999999982, -3.400000000000001,\n",
      "        -8.899999999999984, -6.699999999999995, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999986, -9.99999999999998, -7.7999999999999865,\n",
      "        -9.99999999999998, -5.6, -9.99999999999998, -7.799999999999985, -9.99999999999998,\n",
      "        -8.89999999999998, -7.7999999999999865, -6.6999999999999895, -2.2999999999999856,\n",
      "        -8.899999999999984, -4.499999999999997, -5.599999999999988, -8.89999999999998,\n",
      "        -8.89999999999998, -8.899999999999986, -6.699999999999995, -9.99999999999998,\n",
      "        -5.6, -8.89999999999998, -8.89999999999998, -5.5999999999999845, -6.699999999999993,\n",
      "        -9.99999999999998, -7.799999999999983, -3.4000000000000052, -9.99999999999998,\n",
      "        -4.5000000000000036, -8.89999999999998, -9.99999999999998, -4.499999999999988,\n",
      "        -9.99999999999998, -6.699999999999986, -9.99999999999998, -8.899999999999984,\n",
      "        -6.699999999999989, -7.799999999999981, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.5\n",
      "      policy2: -1.199999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.17\n",
      "      policy2: -8.052999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -15.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12416101882240994\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05354573589472479\n",
      "      mean_inference_ms: 1.3325484704299402\n",
      "      mean_raw_obs_processing_ms: 0.6968800491630641\n",
      "  time_since_restore: 62.196831941604614\n",
      "  time_this_iter_s: 13.545973062515259\n",
      "  time_total_s: 62.196831941604614\n",
      "  timers:\n",
      "    learn_throughput: 554.439\n",
      "    learn_time_ms: 5410.874\n",
      "    synch_weights_time_ms: 3.728\n",
      "    training_iteration_time_ms: 12433.492\n",
      "  timestamp: 1660041777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: bafe8_00001\n",
      "  warmup_time: 10.367733001708984\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.599999999999984\n",
      "  episode_reward_mean: -2.35199999999999\n",
      "  episode_reward_min: -42.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 9960ea27f9c14636b15e67ba13cbb170\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2749570608139038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020467912778258324\n",
      "          model: {}\n",
      "          policy_loss: -0.052486881613731384\n",
      "          total_loss: 6.747609615325928\n",
      "          vf_explained_var: 0.0017689784290269017\n",
      "          vf_loss: 6.7939558029174805\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2879531383514404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019463468343019485\n",
      "          model: {}\n",
      "          policy_loss: -0.052298981696367264\n",
      "          total_loss: 2.0763044357299805\n",
      "          vf_explained_var: 0.3277357518672943\n",
      "          vf_loss: 2.1247105598449707\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 61.776923076923076\n",
      "    ram_util_percent: 67.10000000000001\n",
      "  pid: 63801\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: 0.9999999999999979\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.185\n",
      "    policy2: -8.536999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -32.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1376100254700874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05780262086739839\n",
      "    mean_inference_ms: 1.4585542919264411\n",
      "    mean_raw_obs_processing_ms: 0.7673920684415125\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.599999999999984\n",
      "    episode_reward_mean: -2.35199999999999\n",
      "    episode_reward_min: -42.00000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-4.199999999999976, -12.599999999999973, -25.500000000000036,\n",
      "        -21.000000000000018, -16.499999999999996, -25.500000000000007, -42.00000000000007,\n",
      "        -4.799999999999983, -22.500000000000007, 0.6000000000000184, -21.000000000000007,\n",
      "        12.600000000000014, -2.399999999999981, -7.799999999999972, -16.49999999999999,\n",
      "        -22.50000000000002, -17.39999999999999, -20.400000000000006, 1.500000000000007,\n",
      "        12.000000000000032, -2.399999999999986, 7.200000000000024, 4.800000000000029,\n",
      "        12.599999999999948, 16.50000000000001, 9.60000000000002, -3.8999999999999777,\n",
      "        -25.500000000000014, 9.000000000000032, 9.600000000000016, 10.50000000000002,\n",
      "        2.9999999999999707, 1.4999999999999991, -10.499999999999979, -4.499999999999979,\n",
      "        -4.4999999999999805, -14.999999999999995, -1.7999999999999945, 7.500000000000011,\n",
      "        2.700000000000021, -5.4, -7.500000000000002, -5.999999999999998, -5.999999999999979,\n",
      "        2.906008766956347e-14, 3.600000000000003, -5.399999999999984, -13.499999999999979,\n",
      "        -14.69999999999998, -1.4999999999999916, -2.999999999999975, 4.500000000000002,\n",
      "        -13.499999999999982, 2.7000000000000126, -2.3999999999999897, 6.000000000000011,\n",
      "        6.000000000000014, 1.3627987627273797e-14, -6.8999999999999915, 11.100000000000033,\n",
      "        -3.8999999999999893, -15.299999999999992, 6.000000000000027, 5.100000000000019,\n",
      "        -13.799999999999995, 15.300000000000013, -0.8999999999999793, 4.5000000000000195,\n",
      "        3.0000000000000115, 6.299999999999997, -1.499999999999986, -2.3999999999999853,\n",
      "        7.200000000000031, 0.9000000000000262, -8.999999999999977, -2.999999999999995,\n",
      "        9.600000000000028, 2.1000000000000076, 7.5000000000000195, -4.499999999999986,\n",
      "        -0.599999999999997, -7.499999999999983, 18.599999999999984, 16.49999999999998,\n",
      "        2.100000000000016, -3.599999999999994, -8.399999999999995, -6.899999999999976,\n",
      "        9.300000000000018, -6.59999999999999, -6.299999999999991, -8.699999999999994,\n",
      "        -1.1999999999999895, 10.500000000000025, 9.600000000000016, 13.499999999999963,\n",
      "        -0.29999999999998594, 8.999999999999988, 1.5000000000000262, -7.499999999999975]\n",
      "      policy_policy1_reward: [2.5, -7.0, -15.5, -11.0, -6.5, -15.5, -32.0, 3.0, -12.5,\n",
      "        9.5, -11.0, 21.5, 6.5, 0.0, -6.5, -12.5, -8.5, -11.5, 11.5, 22.0, 6.5, 15.0,\n",
      "        11.5, 21.5, 26.5, 18.5, 5.0, -15.5, 19.0, 18.5, 20.5, 13.0, 11.5, -0.5, 5.5,\n",
      "        0.0, -5.0, 6.0, 17.5, 10.5, -2.0, 2.5, 4.0, -7.0, 10.0, 12.5, 3.5, -3.5, -8.0,\n",
      "        8.5, 7.0, 14.5, -3.5, 10.5, 6.5, 16.0, 16.0, 10.0, -3.5, 20.0, 5.0, -7.5, 16.0,\n",
      "        14.0, -6.0, 22.0, 8.0, 14.5, 13.0, 13.0, 8.5, 6.5, 15.0, 6.5, 1.0, 7.0, 18.5,\n",
      "        11.0, 17.5, 5.5, 5.0, 2.5, 27.5, 26.5, 11.0, 2.0, 0.5, 2.0, 16.0, -1.0, -4.0,\n",
      "        -2.0, 5.5, 15.0, 18.5, 18.0, 7.5, 19.0, 11.5, 2.5]\n",
      "      policy_policy2_reward: [-6.6999999999999895, -5.599999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999986, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -6.699999999999995,\n",
      "        -8.899999999999984, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999984, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -4.499999999999982, -9.99999999999998, -7.799999999999986, -9.99999999999998,\n",
      "        -7.79999999999999, -3.399999999999987, -9.99999999999998, -9.99999999999998,\n",
      "        0.9999999999999979, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -6.69999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -3.4000000000000017,\n",
      "        -8.89999999999998, -8.899999999999986, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999984, -7.7999999999999865, -6.699999999999989, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999895, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999986, -5.599999999999984, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999987, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999986, -8.89999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -5.599999999999982, -2.300000000000004,\n",
      "        -6.6999999999999815, -6.699999999999993, -4.500000000000003, -8.899999999999983,\n",
      "        -4.499999999999985, -7.799999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: 0.9999999999999979\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.185\n",
      "      policy2: -8.536999999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -32.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1376100254700874\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05780262086739839\n",
      "      mean_inference_ms: 1.4585542919264411\n",
      "      mean_raw_obs_processing_ms: 0.7673920684415125\n",
      "  time_since_restore: 53.64376425743103\n",
      "  time_this_iter_s: 18.36294412612915\n",
      "  time_total_s: 53.64376425743103\n",
      "  timers:\n",
      "    learn_throughput: 498.048\n",
      "    learn_time_ms: 8031.357\n",
      "    synch_weights_time_ms: 4.399\n",
      "    training_iteration_time_ms: 17875.463\n",
      "  timestamp: 1660041777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: bafe8_00003\n",
      "  warmup_time: 11.104228258132935\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-43-08\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.500000000000007\n",
      "  episode_reward_mean: -0.6119999999999877\n",
      "  episode_reward_min: -18.0\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: 4f1310ef73844c1c99a53824a8d7ce26\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2109540700912476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010359169915318489\n",
      "          model: {}\n",
      "          policy_loss: -0.04365573078393936\n",
      "          total_loss: 6.258693218231201\n",
      "          vf_explained_var: 0.1820053905248642\n",
      "          vf_loss: 6.300277233123779\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.201703429222107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014138085767626762\n",
      "          model: {}\n",
      "          policy_loss: -0.04395010694861412\n",
      "          total_loss: 2.328157424926758\n",
      "          vf_explained_var: 0.27645057439804077\n",
      "          vf_loss: 2.3706936836242676\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 53.170588235294126\n",
      "    ram_util_percent: 67.04705882352941\n",
      "  pid: 63785\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 10.900000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.1\n",
      "    policy2: -7.7119999999999855\n",
      "  policy_reward_min:\n",
      "    policy1: -8.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12672001400608565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05357285606920843\n",
      "    mean_inference_ms: 1.3502861188339188\n",
      "    mean_raw_obs_processing_ms: 0.6935140366856264\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.500000000000007\n",
      "    episode_reward_mean: -0.6119999999999877\n",
      "    episode_reward_min: -18.0\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [1.500000000000023, -2.9999999999999836, -5.399999999999984, -3.8999999999999826,\n",
      "        7.200000000000024, -3.599999999999984, -9.899999999999979, -3.2999999999999767,\n",
      "        3.300000000000032, -8.999999999999998, -6.000000000000001, 19.500000000000007,\n",
      "        1.5000000000000173, -5.999999999999993, 2.1000000000000263, -3.899999999999979,\n",
      "        6.000000000000014, -5.399999999999981, 2.1000000000000227, 13.199999999999976,\n",
      "        -8.099999999999977, 3.300000000000007, 0.3000000000000218, 2.1000000000000205,\n",
      "        1.5000000000000024, -5.099999999999996, -4.500000000000001, 10.500000000000027,\n",
      "        2.017830347256222e-14, -4.4999999999999805, -5.999999999999982, -4.4999999999999964,\n",
      "        -3.9000000000000017, -18.0, -8.399999999999983, 1.7513768213461844e-14, 10.500000000000028,\n",
      "        14.700000000000019, -7.4999999999999805, -2.999999999999977, 10.800000000000031,\n",
      "        -4.499999999999979, -7.199999999999994, -2.9999999999999836, 5.100000000000026,\n",
      "        -5.999999999999982, 1.8000000000000242, -8.99999999999998, -7.4999999999999805,\n",
      "        14.999999999999947, 9.900000000000006, -5.099999999999985, 7.800000000000024,\n",
      "        -4.4999999999999805, -11.399999999999974, -14.699999999999992, 8.699999999999978,\n",
      "        -12.89999999999998, -1.4999999999999885, -3.59999999999998, -2.3999999999999737,\n",
      "        15.89999999999995, 0.3000000000000014, 5.079270337660091e-15, 5.099999999999971,\n",
      "        3.600000000000031, 10.200000000000015, -15.899999999999977, -17.399999999999984,\n",
      "        -9.899999999999975, 12.000000000000005, -1.4999999999999951, -7.799999999999985,\n",
      "        -2.3999999999999835, 7.800000000000005, -0.29999999999998395, 2.506328478091291e-14,\n",
      "        -4.499999999999995, 1.200000000000022, 12.299999999999988, 3.8999999999999964,\n",
      "        3.60000000000001, 2.5618396293225487e-14, 1.5000000000000275, 2.400000000000026,\n",
      "        1.1851630787873546e-14, 6.600000000000001, -6.599999999999982, 9.299999999999969,\n",
      "        -10.199999999999976, 2.1000000000000147, 1.5000000000000306, 12.00000000000003,\n",
      "        -4.5, -7.499999999999976, -4.4999999999999805, -11.399999999999974, -8.4, -4.499999999999977,\n",
      "        12.599999999999955]\n",
      "      policy_policy1_reward: [11.5, 7.0, 3.5, 5.0, 15.0, 2.0, -1.0, 4.5, 10.0, 1.0,\n",
      "        4.0, 29.5, 11.5, 4.0, 11.0, 5.0, 16.0, 3.5, 11.0, 21.0, -2.5, 10.0, 7.0, 11.0,\n",
      "        11.5, 0.5, 5.5, 20.5, 10.0, 5.5, 4.0, 5.5, 5.0, -8.0, 0.5, 10.0, 20.5, 22.5,\n",
      "        2.5, 7.0, 12.0, 5.5, -0.5, 1.5, 8.5, 4.0, 3.0, 1.0, -3.0, 25.0, -1.0, 0.5, 14.5,\n",
      "        5.5, -2.5, -8.0, 16.5, -4.0, -2.5, 2.0, 6.5, 21.5, 7.0, 10.0, 8.5, 7.0, 18.0,\n",
      "        -7.0, -8.5, -1.0, 22.0, 8.5, 0.0, 1.0, 14.5, 7.5, 4.5, 5.5, 9.0, 19.0, 9.5,\n",
      "        7.0, 10.0, 11.5, 2.5, 10.0, 15.5, -1.0, 16.0, -3.5, 11.0, 11.5, 22.0, 5.5, 2.5,\n",
      "        5.5, -2.5, 0.5, 0.0, 21.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -7.799999999999984, -5.599999999999998, -8.899999999999986,\n",
      "        -7.799999999999988, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -8.899999999999986, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.79999999999999, -5.599999999999991, -6.6999999999999815, -6.699999999999985,\n",
      "        -8.899999999999986, -9.99999999999998, -5.599999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -1.2000000000000044, -9.99999999999998, -6.6999999999999815,\n",
      "        -4.500000000000003, -3.399999999999989, -9.99999999999998, -1.1999999999999977,\n",
      "        -9.99999999999998, -4.500000000000004, -9.99999999999998, 10.900000000000013,\n",
      "        -5.599999999999998, -6.6999999999999815, -9.99999999999998, -8.899999999999986,\n",
      "        -6.699999999999983, -7.799999999999981, -8.89999999999998, 0.999999999999997,\n",
      "        -5.599999999999987, -8.89999999999998, -5.6, -6.6999999999999815, -9.99999999999998,\n",
      "        -3.399999999999995, -3.3999999999999835, -7.799999999999981, -8.899999999999986,\n",
      "        -8.899999999999986, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999989, -3.400000000000004, -6.6999999999999815, -7.799999999999982,\n",
      "        -4.500000000000002, -9.99999999999998, -7.7999999999999865, -6.699999999999995,\n",
      "        -5.599999999999987, -3.3999999999999875, -9.99999999999998, -9.99999999999998,\n",
      "        -0.10000000000000331, -9.99999999999998, -8.89999999999998, -5.599999999999998,\n",
      "        -6.699999999999985, -6.6999999999999815, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.89999999999998, -4.499999999999994, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 10.900000000000013\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.1\n",
      "      policy2: -7.7119999999999855\n",
      "    policy_reward_min:\n",
      "      policy1: -8.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12672001400608565\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05357285606920843\n",
      "      mean_inference_ms: 1.3502861188339188\n",
      "      mean_raw_obs_processing_ms: 0.6935140366856264\n",
      "  time_since_restore: 72.96098470687866\n",
      "  time_this_iter_s: 11.712244033813477\n",
      "  time_total_s: 72.96098470687866\n",
      "  timers:\n",
      "    learn_throughput: 548.421\n",
      "    learn_time_ms: 5470.253\n",
      "    synch_weights_time_ms: 5.501\n",
      "    training_iteration_time_ms: 12152.951\n",
      "  timestamp: 1660041788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: bafe8_00000\n",
      "  warmup_time: 10.087341070175171\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-43-09\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.799999999999898\n",
      "  episode_reward_mean: 1.1340000000000081\n",
      "  episode_reward_min: -18.299999999999986\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: ba802e21773d46369dab10c339d8b4ae\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.161283016204834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02261049672961235\n",
      "          model: {}\n",
      "          policy_loss: -0.06501877307891846\n",
      "          total_loss: 6.97260856628418\n",
      "          vf_explained_var: 0.11147820949554443\n",
      "          vf_loss: 7.030844688415527\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1514028310775757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01634514518082142\n",
      "          model: {}\n",
      "          policy_loss: -0.05181280896067619\n",
      "          total_loss: 2.641278028488159\n",
      "          vf_explained_var: 0.13281409442424774\n",
      "          vf_loss: 2.6857357025146484\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 47.3\n",
      "    ram_util_percent: 66.79411764705881\n",
      "  pid: 63794\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: 5.400000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.835\n",
      "    policy2: -7.700999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -14.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12874212509297586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05546976951908714\n",
      "    mean_inference_ms: 1.3790659084785424\n",
      "    mean_raw_obs_processing_ms: 0.7174203286343527\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.799999999999898\n",
      "    episode_reward_mean: 1.1340000000000081\n",
      "    episode_reward_min: -18.299999999999986\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.399999999999993, -7.499999999999989, 8.100000000000014, -9.89999999999999,\n",
      "        6.000000000000023, 12.600000000000033, -2.69999999999998, 0.6000000000000095,\n",
      "        4.500000000000002, -1.5000000000000155, 4.800000000000022, -5.099999999999981,\n",
      "        12.000000000000004, -4.499999999999987, 1.500000000000027, -9.899999999999979,\n",
      "        5.100000000000033, -0.8999999999999913, -7.499999999999975, 5.100000000000028,\n",
      "        -6.299999999999976, 1.5000000000000173, -7.799999999999976, 18.299999999999933,\n",
      "        0.6000000000000003, 8.100000000000025, 15.299999999999978, -16.500000000000007,\n",
      "        -2.999999999999984, 6.600000000000005, 5.1000000000000165, -5.999999999999989,\n",
      "        -10.799999999999992, -4.4999999999999885, -11.09999999999998, 6.000000000000032,\n",
      "        -0.2999999999999825, -1.4999999999999813, 11.099999999999982, -15.299999999999992,\n",
      "        19.799999999999898, 1.1999999999999917, -15.899999999999977, 12.00000000000002,\n",
      "        -3.5999999999999766, -6.899999999999988, 2.1000000000000174, 6.600000000000033,\n",
      "        15.299999999999908, -8.999999999999984, 0.9000000000000118, 9.600000000000028,\n",
      "        15.600000000000014, 14.400000000000025, 13.799999999999947, -4.499999999999975,\n",
      "        -4.799999999999981, 8.100000000000012, 7.499999999999966, -4.499999999999973,\n",
      "        2.100000000000019, 1.5000000000000142, -1.4999999999999756, 2.017830347256222e-14,\n",
      "        9.299999999999994, 7.500000000000014, 2.1000000000000085, 10.800000000000013,\n",
      "        5.700000000000033, 2.1000000000000316, -4.499999999999985, -14.999999999999986,\n",
      "        10.200000000000014, 14.999999999999966, 2.1000000000000094, 13.200000000000026,\n",
      "        4.199999999999973, -6.2999999999999865, -3.8999999999999955, 10.49999999999999,\n",
      "        -11.99999999999998, 16.499999999999982, 8.099999999999982, -2.9999999999999885,\n",
      "        4.499999999999995, 7.799999999999992, 3.8999999999999875, -3.5999999999999837,\n",
      "        -18.299999999999986, -10.799999999999972, 8.699999999999948, -7.19999999999998,\n",
      "        -1.4999999999999813, 5.1000000000000245, -8.999999999999977, -0.29999999999999927,\n",
      "        -16.799999999999983, 7.500000000000025, 3.0000000000000107, -3.899999999999994]\n",
      "      policy_policy1_reward: [-5.0, -3.0, 17.0, -1.0, 16.0, 21.5, 4.0, 9.5, 14.5, 8.5,\n",
      "        11.5, 0.5, 22.0, 5.5, 11.5, -1.0, 14.0, 8.0, 2.5, 14.0, 1.5, 6.0, 0.0, 25.0,\n",
      "        4.0, 17.0, 22.0, -6.5, 7.0, 15.5, 14.0, 4.0, -3.0, 5.5, -5.5, 16.0, 7.5, 8.5,\n",
      "        20.0, -7.5, 26.5, 3.5, -7.0, 16.5, 2.0, 2.0, 11.0, 15.5, 22.0, 1.0, 6.5, 18.5,\n",
      "        24.5, 20.0, 20.5, 5.5, 3.0, 11.5, 17.5, 0.0, 11.0, 11.5, 3.0, 10.0, 16.0, 17.5,\n",
      "        11.0, 17.5, 13.5, 11.0, 5.5, -5.0, 18.0, 25.0, 5.5, 21.0, 12.0, 1.5, 5.0, 20.5,\n",
      "        -2.0, 26.5, 11.5, 7.0, 14.5, 14.5, -1.5, -3.5, -10.5, -3.0, 16.5, -0.5, 8.5,\n",
      "        14.0, -4.5, 2.0, -14.5, 17.5, 13.0, 5.0]\n",
      "      policy_policy2_reward: [-3.400000000000004, -4.499999999999984, -8.899999999999984,\n",
      "        -8.899999999999984, -9.99999999999998, -8.89999999999998, -6.699999999999994,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -6.699999999999994,\n",
      "        -5.599999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.899999999999986, -8.89999999999998, -9.99999999999998,\n",
      "        -8.899999999999983, -7.799999999999981, -4.49999999999999, -7.799999999999981,\n",
      "        -6.699999999999982, -3.400000000000001, -8.899999999999984, -6.699999999999995,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -5.6, -9.99999999999998,\n",
      "        -7.799999999999985, -9.99999999999998, -8.89999999999998, -7.7999999999999865,\n",
      "        -6.6999999999999895, -2.2999999999999856, -8.899999999999984, -4.499999999999997,\n",
      "        -5.599999999999988, -8.89999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -6.699999999999995, -9.99999999999998, -5.6, -8.89999999999998, -8.89999999999998,\n",
      "        -5.5999999999999845, -6.699999999999993, -9.99999999999998, -7.799999999999983,\n",
      "        -3.4000000000000052, -9.99999999999998, -4.5000000000000036, -8.89999999999998,\n",
      "        -9.99999999999998, -4.499999999999988, -9.99999999999998, -6.699999999999986,\n",
      "        -9.99999999999998, -8.899999999999984, -6.699999999999989, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -3.4000000000000035, -7.799999999999986, -7.799999999999981,\n",
      "        -7.79999999999999, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -3.4000000000000044, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999988, 5.400000000000013, -0.10000000000000264, -7.799999999999986,\n",
      "        -7.79999999999999, -7.79999999999999, -6.6999999999999815, -9.99999999999998,\n",
      "        -8.89999999999998, -4.500000000000002, -2.3000000000000047, -2.3000000000000007,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: 5.400000000000013\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.835\n",
      "      policy2: -7.700999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -14.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12874212509297586\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05546976951908714\n",
      "      mean_inference_ms: 1.3790659084785424\n",
      "      mean_raw_obs_processing_ms: 0.7174203286343527\n",
      "  time_since_restore: 74.02203178405762\n",
      "  time_this_iter_s: 11.825199842453003\n",
      "  time_total_s: 74.02203178405762\n",
      "  timers:\n",
      "    learn_throughput: 546.369\n",
      "    learn_time_ms: 5490.795\n",
      "    synch_weights_time_ms: 3.794\n",
      "    training_iteration_time_ms: 12330.809\n",
      "  timestamp: 1660041789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: bafe8_00001\n",
      "  warmup_time: 10.367733001708984\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.799999999999947\n",
      "  episode_reward_mean: -2.91299999999999\n",
      "  episode_reward_min: -25.500000000000025\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 2593f3a7735a4f4ab02b4cd9b49f99fc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2497870922088623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012075037695467472\n",
      "          model: {}\n",
      "          policy_loss: -0.041604626923799515\n",
      "          total_loss: 6.937272548675537\n",
      "          vf_explained_var: 0.18405094742774963\n",
      "          vf_loss: 6.976462364196777\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.240622639656067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013352811336517334\n",
      "          model: {}\n",
      "          policy_loss: -0.04359399527311325\n",
      "          total_loss: 2.7753357887268066\n",
      "          vf_explained_var: 0.20778900384902954\n",
      "          vf_loss: 2.8162591457366943\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 49.349999999999994\n",
      "    ram_util_percent: 65.00416666666666\n",
      "  pid: 63797\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: 4.300000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.26\n",
      "    policy2: -7.172999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -17.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12694345590764244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.055058362021189605\n",
      "    mean_inference_ms: 1.3742570758405008\n",
      "    mean_raw_obs_processing_ms: 0.714921267969234\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.799999999999947\n",
      "    episode_reward_mean: -2.91299999999999\n",
      "    episode_reward_min: -25.500000000000025\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-3.8999999999999795, 2.706168622523819e-14, 1.1296519275560968e-14,\n",
      "        -4.19999999999999, -21.900000000000027, -10.499999999999988, 9.3, 17.99999999999997,\n",
      "        -10.499999999999979, -1.499999999999995, -1.7999999999999794, 4.500000000000018,\n",
      "        -11.999999999999991, 9.96425164601078e-15, 4.500000000000014, 5.099999999999964,\n",
      "        -1.7999999999999896, -17.40000000000002, 12.300000000000018, -4.499999999999984,\n",
      "        -8.999999999999982, 0.9000000000000107, -7.799999999999981, 9.299999999999962,\n",
      "        -3.9000000000000004, 3.9000000000000266, -8.69999999999998, -19.799999999999997,\n",
      "        8.39999999999999, -2.3999999999999906, -9.299999999999978, 1.5000000000000022,\n",
      "        -5.999999999999986, -10.499999999999984, 9.000000000000027, 1.8000000000000007,\n",
      "        4.499999999999995, 7.800000000000022, -7.499999999999975, -2.399999999999995,\n",
      "        3.0000000000000053, 4.80000000000002, -12.299999999999983, 4.500000000000009,\n",
      "        19.799999999999947, 0.3000000000000209, -8.399999999999974, -8.399999999999977,\n",
      "        14.10000000000002, -21.000000000000007, -4.500000000000003, -7.79999999999999,\n",
      "        3.600000000000024, 2.100000000000021, 0.6000000000000282, -0.2999999999999986,\n",
      "        2.4147350785597155e-15, -9.299999999999976, -6.299999999999974, -17.399999999999984,\n",
      "        -3.900000000000007, -17.399999999999984, -0.5999999999999844, 1.500000000000012,\n",
      "        -13.199999999999983, -4.799999999999974, -14.999999999999968, -8.699999999999976,\n",
      "        -11.699999999999978, 1.5000000000000226, -8.69999999999998, 4.499999999999984,\n",
      "        1.5000000000000102, -25.500000000000025, 1.500000000000028, 6.6000000000000245,\n",
      "        0.30000000000002447, -7.79999999999999, -8.700000000000012, -4.1999999999999815,\n",
      "        1.8000000000000203, -8.39999999999998, 5.700000000000028, -0.5999999999999727,\n",
      "        -4.4999999999999805, 6.299999999999985, -7.499999999999977, 5.1000000000000245,\n",
      "        -9.299999999999978, -16.79999999999999, -9.899999999999991, 9.000000000000023,\n",
      "        10.499999999999932, -3.899999999999974, 0.30000000000002625, -11.09999999999998,\n",
      "        -8.399999999999974, 8.099999999999984, -8.699999999999974, -16.799999999999994]\n",
      "      policy_policy1_reward: [-0.5, 10.0, 10.0, 2.5, -13.0, -0.5, 16.0, 28.0, -0.5,\n",
      "        8.5, 6.0, 9.0, -2.0, 10.0, 14.5, 14.0, 6.0, -8.5, 19.0, 5.5, 1.0, 6.5, 0.0,\n",
      "        16.0, 5.0, 9.5, -2.0, -12.0, 14.0, 6.5, -1.5, 6.0, -1.5, -6.0, 13.5, 8.5, 9.0,\n",
      "        14.5, -3.0, 6.5, 13.0, 11.5, -4.5, 14.5, 26.5, 7.0, 0.5, 0.5, 23.0, -11.0, 0.0,\n",
      "        0.0, 12.5, 11.0, 9.5, 2.0, 4.5, -1.5, 1.5, -8.5, 5.0, -8.5, 5.0, 6.0, -17.5,\n",
      "        3.0, -5.0, -2.0, -5.0, 6.0, -2.0, 14.5, 11.5, -15.5, 11.5, 15.5, 7.0, 0.0, -2.0,\n",
      "        2.5, 3.0, 0.5, 8.0, 5.0, 5.5, 7.5, 2.5, 14.0, -7.0, -9.0, -1.0, 13.5, 15.0,\n",
      "        -0.5, 7.0, -11.0, 0.5, 17.0, -2.0, -9.0]\n",
      "      policy_policy2_reward: [-3.4000000000000035, -9.99999999999998, -9.99999999999998,\n",
      "        -6.69999999999999, -8.89999999999998, -9.99999999999998, -6.699999999999985,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -4.499999999999997, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -8.899999999999986, -6.699999999999994,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999988, -7.799999999999981,\n",
      "        -6.699999999999995, -8.899999999999983, -5.599999999999999, -6.699999999999987,\n",
      "        -7.799999999999983, -5.599999999999998, -8.89999999999998, -7.79999999999999,\n",
      "        -4.499999999999983, -4.499999999999997, -4.500000000000002, -4.499999999999982,\n",
      "        -6.6999999999999815, -4.499999999999989, -6.699999999999994, -4.499999999999996,\n",
      "        -8.899999999999983, -9.99999999999998, -6.6999999999999895, -7.799999999999981,\n",
      "        -9.99999999999998, -6.6999999999999895, -6.699999999999995, -8.899999999999986,\n",
      "        -8.89999999999998, -8.89999999999998, -9.99999999999998, -4.499999999999988,\n",
      "        -7.799999999999981, -8.899999999999984, -8.899999999999986, -8.899999999999984,\n",
      "        -2.299999999999997, -4.500000000000004, -7.79999999999999, -7.799999999999981,\n",
      "        -8.899999999999986, -8.899999999999986, -8.89999999999998, -5.59999999999999,\n",
      "        -4.499999999999995, 4.300000000000008, -7.799999999999986, -9.99999999999998,\n",
      "        -6.6999999999999895, -6.6999999999999815, -4.499999999999994, -6.699999999999991,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999995, -7.799999999999981, -6.699999999999986,\n",
      "        -6.699999999999992, -1.1999999999999962, -8.899999999999986, -2.3000000000000047,\n",
      "        -5.5999999999999845, -9.99999999999998, -1.2000000000000035, -9.99999999999998,\n",
      "        -8.899999999999983, -2.2999999999999834, -7.799999999999986, -8.89999999999998,\n",
      "        -4.500000000000004, -4.499999999999985, -3.3999999999999897, -6.6999999999999815,\n",
      "        -0.1000000000000042, -8.89999999999998, -8.899999999999986, -6.6999999999999815,\n",
      "        -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: 4.300000000000008\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.26\n",
      "      policy2: -7.172999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -17.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12694345590764244\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.055058362021189605\n",
      "      mean_inference_ms: 1.3742570758405008\n",
      "      mean_raw_obs_processing_ms: 0.714921267969234\n",
      "  time_since_restore: 81.83006930351257\n",
      "  time_this_iter_s: 16.341620922088623\n",
      "  time_total_s: 81.83006930351257\n",
      "  timers:\n",
      "    learn_throughput: 563.505\n",
      "    learn_time_ms: 7098.427\n",
      "    synch_weights_time_ms: 4.042\n",
      "    training_iteration_time_ms: 16358.02\n",
      "  timestamp: 1660041793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: bafe8_00002\n",
      "  warmup_time: 10.982521057128906\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.49999999999996\n",
      "  episode_reward_mean: 0.8520000000000109\n",
      "  episode_reward_min: -15.299999999999992\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 9960ea27f9c14636b15e67ba13cbb170\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2404104471206665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017810067161917686\n",
      "          model: {}\n",
      "          policy_loss: -0.04994387552142143\n",
      "          total_loss: 6.816788673400879\n",
      "          vf_explained_var: 0.08403564989566803\n",
      "          vf_loss: 6.858717918395996\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2541733980178833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018776368349790573\n",
      "          model: {}\n",
      "          policy_loss: -0.04914145544171333\n",
      "          total_loss: 2.2058186531066895\n",
      "          vf_explained_var: 0.27675163745880127\n",
      "          vf_loss: 2.2512049674987793\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 56.56521739130436\n",
      "    ram_util_percent: 65.10000000000001\n",
      "  pid: 63801\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 0.9999999999999979\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.07\n",
      "    policy2: -8.217999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1382036562360758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05809117354529391\n",
      "    mean_inference_ms: 1.4644297805678854\n",
      "    mean_raw_obs_processing_ms: 0.7713299887935453\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.49999999999996\n",
      "    episode_reward_mean: 0.8520000000000109\n",
      "    episode_reward_min: -15.299999999999992\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-5.4, -7.500000000000002, -5.999999999999998, -5.999999999999979,\n",
      "        2.906008766956347e-14, 3.600000000000003, -5.399999999999984, -13.499999999999979,\n",
      "        -14.69999999999998, -1.4999999999999916, -2.999999999999975, 4.500000000000002,\n",
      "        -13.499999999999982, 2.7000000000000126, -2.3999999999999897, 6.000000000000011,\n",
      "        6.000000000000014, 1.3627987627273797e-14, -6.8999999999999915, 11.100000000000033,\n",
      "        -3.8999999999999893, -15.299999999999992, 6.000000000000027, 5.100000000000019,\n",
      "        -13.799999999999995, 15.300000000000013, -0.8999999999999793, 4.5000000000000195,\n",
      "        3.0000000000000115, 6.299999999999997, -1.499999999999986, -2.3999999999999853,\n",
      "        7.200000000000031, 0.9000000000000262, -8.999999999999977, -2.999999999999995,\n",
      "        9.600000000000028, 2.1000000000000076, 7.5000000000000195, -4.499999999999986,\n",
      "        -0.599999999999997, -7.499999999999983, 18.599999999999984, 16.49999999999998,\n",
      "        2.100000000000016, -3.599999999999994, -8.399999999999995, -6.899999999999976,\n",
      "        9.300000000000018, -6.59999999999999, -6.299999999999991, -8.699999999999994,\n",
      "        -1.1999999999999895, 10.500000000000025, 9.600000000000016, 13.499999999999963,\n",
      "        -0.29999999999998594, 8.999999999999988, 1.5000000000000262, -7.499999999999975,\n",
      "        0.6000000000000244, 6.300000000000027, -2.999999999999976, 15.600000000000023,\n",
      "        -10.49999999999997, 5.100000000000026, -8.999999999999982, 1.1999999999999993,\n",
      "        -5.399999999999995, 12.600000000000033, -1.500000000000003, -4.199999999999982,\n",
      "        -2.9999999999999747, 3.9000000000000177, 10.500000000000023, 3.0000000000000147,\n",
      "        13.500000000000028, 19.49999999999996, -7.19999999999999, -6.8999999999999835,\n",
      "        -5.099999999999989, 7.499999999999989, -2.399999999999981, 2.450817326860033e-14,\n",
      "        10.499999999999988, -7.499999999999973, 4.500000000000007, -4.199999999999985,\n",
      "        2.7000000000000077, 11.699999999999964, 8.400000000000011, 15.000000000000025,\n",
      "        -13.499999999999986, 11.699999999999964, 1.8000000000000274, 14.999999999999972,\n",
      "        -14.399999999999972, -8.699999999999989, 18.59999999999992, -1.7999999999999774]\n",
      "      policy_policy1_reward: [-2.0, 2.5, 4.0, -7.0, 10.0, 12.5, 3.5, -3.5, -8.0, 8.5,\n",
      "        7.0, 14.5, -3.5, 10.5, 6.5, 16.0, 16.0, 10.0, -3.5, 20.0, 5.0, -7.5, 16.0, 14.0,\n",
      "        -6.0, 22.0, 8.0, 14.5, 13.0, 13.0, 8.5, 6.5, 15.0, 6.5, 1.0, 7.0, 18.5, 11.0,\n",
      "        17.5, 5.5, 5.0, 2.5, 27.5, 26.5, 11.0, 2.0, 0.5, 2.0, 16.0, -1.0, -4.0, -2.0,\n",
      "        5.5, 15.0, 18.5, 18.0, 7.5, 19.0, 11.5, 2.5, 9.5, 13.0, 7.0, 19.0, -0.5, 14.0,\n",
      "        1.0, 9.0, 3.5, 21.5, 8.5, 2.5, 7.0, 9.5, 20.5, 13.0, 23.5, 29.5, -0.5, -3.5,\n",
      "        0.5, 17.5, 6.5, 10.0, 20.5, 2.5, 14.5, -3.0, 10.5, 19.5, 14.0, 25.0, -3.5, 19.5,\n",
      "        8.5, 25.0, -5.5, -2.0, 27.5, 6.0]\n",
      "      policy_policy2_reward: [-3.399999999999987, -9.99999999999998, -9.99999999999998,\n",
      "        0.9999999999999979, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -6.69999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -3.4000000000000017,\n",
      "        -8.89999999999998, -8.899999999999986, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999984, -7.7999999999999865, -6.699999999999989, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999895, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999986, -5.599999999999984, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999987, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999986, -8.89999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -5.599999999999982, -2.300000000000004,\n",
      "        -6.6999999999999815, -6.699999999999993, -4.500000000000003, -8.899999999999983,\n",
      "        -4.499999999999985, -7.799999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999984, -6.69999999999999, -9.99999999999998,\n",
      "        -3.3999999999999964, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.79999999999999, -8.899999999999986, -8.899999999999986, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -5.599999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -3.400000000000003, -5.599999999999982, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -1.2000000000000017, -7.799999999999981, -7.799999999999981, -5.6, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -6.699999999999992, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999985, -8.89999999999998, -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 0.9999999999999979\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.07\n",
      "      policy2: -8.217999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -8.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1382036562360758\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05809117354529391\n",
      "      mean_inference_ms: 1.4644297805678854\n",
      "      mean_raw_obs_processing_ms: 0.7713299887935453\n",
      "  time_since_restore: 69.80250716209412\n",
      "  time_this_iter_s: 16.158742904663086\n",
      "  time_total_s: 69.80250716209412\n",
      "  timers:\n",
      "    learn_throughput: 525.267\n",
      "    learn_time_ms: 7615.175\n",
      "    synch_weights_time_ms: 4.03\n",
      "    training_iteration_time_ms: 17444.809\n",
      "  timestamp: 1660041793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: bafe8_00003\n",
      "  warmup_time: 11.104228258132935\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-43-26\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.799999999999947\n",
      "  episode_reward_mean: -1.151999999999993\n",
      "  episode_reward_min: -25.500000000000025\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 2593f3a7735a4f4ab02b4cd9b49f99fc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2187402248382568\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01419597677886486\n",
      "          model: {}\n",
      "          policy_loss: -0.03888927027583122\n",
      "          total_loss: 6.499823093414307\n",
      "          vf_explained_var: 0.24154379963874817\n",
      "          vf_loss: 6.5358734130859375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.193696141242981\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012979123741388321\n",
      "          model: {}\n",
      "          policy_loss: -0.040179792791604996\n",
      "          total_loss: 2.685629367828369\n",
      "          vf_explained_var: 0.2291906774044037\n",
      "          vf_loss: 2.7232131958007812\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 34.544444444444444\n",
      "    ram_util_percent: 61.73333333333332\n",
      "  pid: 63797\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 4.300000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.515\n",
      "    policy2: -6.6669999999999865\n",
      "  policy_reward_min:\n",
      "    policy1: -17.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1258744953662428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05461251821639577\n",
      "    mean_inference_ms: 1.359727411637512\n",
      "    mean_raw_obs_processing_ms: 0.7092203271837931\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.799999999999947\n",
      "    episode_reward_mean: -1.151999999999993\n",
      "    episode_reward_min: -25.500000000000025\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.0000000000000053, 4.80000000000002, -12.299999999999983, 4.500000000000009,\n",
      "        19.799999999999947, 0.3000000000000209, -8.399999999999974, -8.399999999999977,\n",
      "        14.10000000000002, -21.000000000000007, -4.500000000000003, -7.79999999999999,\n",
      "        3.600000000000024, 2.100000000000021, 0.6000000000000282, -0.2999999999999986,\n",
      "        2.4147350785597155e-15, -9.299999999999976, -6.299999999999974, -17.399999999999984,\n",
      "        -3.900000000000007, -17.399999999999984, -0.5999999999999844, 1.500000000000012,\n",
      "        -13.199999999999983, -4.799999999999974, -14.999999999999968, -8.699999999999976,\n",
      "        -11.699999999999978, 1.5000000000000226, -8.69999999999998, 4.499999999999984,\n",
      "        1.5000000000000102, -25.500000000000025, 1.500000000000028, 6.6000000000000245,\n",
      "        0.30000000000002447, -7.79999999999999, -8.700000000000012, -4.1999999999999815,\n",
      "        1.8000000000000203, -8.39999999999998, 5.700000000000028, -0.5999999999999727,\n",
      "        -4.4999999999999805, 6.299999999999985, -7.499999999999977, 5.1000000000000245,\n",
      "        -9.299999999999978, -16.79999999999999, -9.899999999999991, 9.000000000000023,\n",
      "        10.499999999999932, -3.899999999999974, 0.30000000000002625, -11.09999999999998,\n",
      "        -8.399999999999974, 8.099999999999984, -8.699999999999974, -16.799999999999994,\n",
      "        -3.299999999999985, -6.299999999999976, -5.399999999999988, 2.1000000000000316,\n",
      "        0.9000000000000065, 2.400000000000017, -0.3000000000000048, 15.89999999999993,\n",
      "        17.99999999999997, 5.999999999999998, 11.399999999999942, -0.29999999999998794,\n",
      "        12.89999999999998, -5.099999999999977, -1.4999999999999742, -8.699999999999992,\n",
      "        -17.999999999999986, -8.999999999999996, -1.799999999999991, -8.699999999999976,\n",
      "        19.199999999999918, 9.899999999999968, 18.599999999999945, 11.100000000000026,\n",
      "        0.9000000000000262, 1.2000000000000273, 3.0000000000000258, 19.499999999999908,\n",
      "        -1.4999999999999738, 1.800000000000031, -4.499999999999977, 11.699999999999967,\n",
      "        1.5000000000000098, -9.599999999999978, -10.499999999999993, -6.299999999999979,\n",
      "        10.499999999999941, 6.000000000000028, 10.19999999999997, 5.700000000000026]\n",
      "      policy_policy1_reward: [13.0, 11.5, -4.5, 14.5, 26.5, 7.0, 0.5, 0.5, 23.0, -11.0,\n",
      "        0.0, 0.0, 12.5, 11.0, 9.5, 2.0, 4.5, -1.5, 1.5, -8.5, 5.0, -8.5, 5.0, 6.0, -17.5,\n",
      "        3.0, -5.0, -2.0, -5.0, 6.0, -2.0, 14.5, 11.5, -15.5, 11.5, 15.5, 7.0, 0.0, -2.0,\n",
      "        2.5, 3.0, 0.5, 8.0, 5.0, 5.5, 7.5, 2.5, 14.0, -7.0, -9.0, -1.0, 13.5, 15.0,\n",
      "        -0.5, 7.0, -11.0, 0.5, 17.0, -2.0, -9.0, 4.5, 1.5, 3.5, 11.0, 6.5, 8.0, 7.5,\n",
      "        21.5, 28.0, 5.0, 11.5, 7.5, 18.5, 0.5, 8.5, -2.0, -8.0, -4.5, 0.5, -2.0, 21.5,\n",
      "        15.5, 22.0, 14.5, 6.5, 3.5, 13.0, 29.5, 8.5, 8.5, 5.5, 14.0, 6.0, -4.0, -0.5,\n",
      "        -4.0, 20.5, 10.5, 18.0, 13.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -6.6999999999999895, -7.799999999999981,\n",
      "        -9.99999999999998, -6.6999999999999895, -6.699999999999995, -8.899999999999986,\n",
      "        -8.89999999999998, -8.89999999999998, -9.99999999999998, -4.499999999999988,\n",
      "        -7.799999999999981, -8.899999999999984, -8.899999999999986, -8.899999999999984,\n",
      "        -2.299999999999997, -4.500000000000004, -7.79999999999999, -7.799999999999981,\n",
      "        -8.899999999999986, -8.899999999999986, -8.89999999999998, -5.59999999999999,\n",
      "        -4.499999999999995, 4.300000000000008, -7.799999999999986, -9.99999999999998,\n",
      "        -6.6999999999999895, -6.6999999999999815, -4.499999999999994, -6.699999999999991,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999995, -7.799999999999981, -6.699999999999986,\n",
      "        -6.699999999999992, -1.1999999999999962, -8.899999999999986, -2.3000000000000047,\n",
      "        -5.5999999999999845, -9.99999999999998, -1.2000000000000035, -9.99999999999998,\n",
      "        -8.899999999999983, -2.2999999999999834, -7.799999999999986, -8.89999999999998,\n",
      "        -4.500000000000004, -4.499999999999985, -3.3999999999999897, -6.6999999999999815,\n",
      "        -0.1000000000000042, -8.89999999999998, -8.899999999999986, -6.6999999999999815,\n",
      "        -7.799999999999981, -7.799999999999986, -7.79999999999999, -8.89999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -5.599999999999997, -7.79999999999999,\n",
      "        -5.599999999999988, -9.99999999999998, 0.9999999999999974, -0.0999999999999941,\n",
      "        -7.799999999999981, -5.599999999999999, -5.599999999999992, -9.99999999999998,\n",
      "        -6.699999999999986, -9.99999999999998, -4.500000000000003, -2.3000000000000043,\n",
      "        -6.699999999999995, -2.3000000000000034, -5.599999999999997, -3.400000000000005,\n",
      "        -3.399999999999996, -5.599999999999995, -2.300000000000005, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999986, -9.99999999999998,\n",
      "        -2.299999999999989, -4.499999999999984, -5.599999999999991, -9.99999999999998,\n",
      "        -2.3000000000000034, -9.99999999999998, -4.500000000000003, -7.799999999999981,\n",
      "        -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 4.300000000000008\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.515\n",
      "      policy2: -6.6669999999999865\n",
      "    policy_reward_min:\n",
      "      policy1: -17.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1258744953662428\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05461251821639577\n",
      "      mean_inference_ms: 1.359727411637512\n",
      "      mean_raw_obs_processing_ms: 0.7092203271837931\n",
      "  time_since_restore: 94.77673935890198\n",
      "  time_this_iter_s: 12.946670055389404\n",
      "  time_total_s: 94.77673935890198\n",
      "  timers:\n",
      "    learn_throughput: 580.004\n",
      "    learn_time_ms: 6896.499\n",
      "    synch_weights_time_ms: 3.986\n",
      "    training_iteration_time_ms: 15788.397\n",
      "  timestamp: 1660041806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: bafe8_00002\n",
      "  warmup_time: 10.982521057128906\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-43-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999993\n",
      "  episode_reward_mean: 2.082000000000009\n",
      "  episode_reward_min: -17.99999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 9960ea27f9c14636b15e67ba13cbb170\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1914185285568237\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017011970281600952\n",
      "          model: {}\n",
      "          policy_loss: -0.05218176171183586\n",
      "          total_loss: 6.414880275726318\n",
      "          vf_explained_var: 0.17208722233772278\n",
      "          vf_loss: 6.45940637588501\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2191962003707886\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02100237086415291\n",
      "          model: {}\n",
      "          policy_loss: -0.05399024486541748\n",
      "          total_loss: 2.244694471359253\n",
      "          vf_explained_var: 0.2851168215274811\n",
      "          vf_loss: 2.2944843769073486\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 36.56315789473684\n",
      "    ram_util_percent: 61.768421052631574\n",
      "  pid: 63801\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -1.2000000000000017\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.245\n",
      "    policy2: -8.162999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1347117994063717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05670898849631932\n",
      "    mean_inference_ms: 1.426108115402958\n",
      "    mean_raw_obs_processing_ms: 0.7549635802591006\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999993\n",
      "    episode_reward_mean: 2.082000000000009\n",
      "    episode_reward_min: -17.99999999999998\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-0.599999999999997, -7.499999999999983, 18.599999999999984, 16.49999999999998,\n",
      "        2.100000000000016, -3.599999999999994, -8.399999999999995, -6.899999999999976,\n",
      "        9.300000000000018, -6.59999999999999, -6.299999999999991, -8.699999999999994,\n",
      "        -1.1999999999999895, 10.500000000000025, 9.600000000000016, 13.499999999999963,\n",
      "        -0.29999999999998594, 8.999999999999988, 1.5000000000000262, -7.499999999999975,\n",
      "        0.6000000000000244, 6.300000000000027, -2.999999999999976, 15.600000000000023,\n",
      "        -10.49999999999997, 5.100000000000026, -8.999999999999982, 1.1999999999999993,\n",
      "        -5.399999999999995, 12.600000000000033, -1.500000000000003, -4.199999999999982,\n",
      "        -2.9999999999999747, 3.9000000000000177, 10.500000000000023, 3.0000000000000147,\n",
      "        13.500000000000028, 19.49999999999996, -7.19999999999999, -6.8999999999999835,\n",
      "        -5.099999999999989, 7.499999999999989, -2.399999999999981, 2.450817326860033e-14,\n",
      "        10.499999999999988, -7.499999999999973, 4.500000000000007, -4.199999999999985,\n",
      "        2.7000000000000077, 11.699999999999964, 8.400000000000011, 15.000000000000025,\n",
      "        -13.499999999999986, 11.699999999999964, 1.8000000000000274, 14.999999999999972,\n",
      "        -14.399999999999972, -8.699999999999989, 18.59999999999992, -1.7999999999999774,\n",
      "        3.6000000000000276, 15.00000000000001, 0.6000000000000151, 13.500000000000014,\n",
      "        5.69999999999996, -7.499999999999989, 5.100000000000023, -5.999999999999986,\n",
      "        13.200000000000008, -7.499999999999972, 6.899999999999984, 10.200000000000026,\n",
      "        -1.499999999999973, 5.100000000000028, -4.499999999999975, 13.799999999999939,\n",
      "        15.600000000000032, 2.9999999999999867, -6.299999999999997, 2.2398749521812533e-14,\n",
      "        -2.6999999999999935, 6.000000000000032, 0.6000000000000206, 19.499999999999993,\n",
      "        16.19999999999994, 5.700000000000028, 0.6000000000000273, -6.899999999999985,\n",
      "        -17.99999999999998, 1.2000000000000202, -12.299999999999983, 1.500000000000028,\n",
      "        -3.8999999999999977, 8.100000000000032, -4.499999999999979, 11.699999999999939,\n",
      "        -8.999999999999977, -7.4999999999999964, -5.9999999999999964, 6.00000000000003]\n",
      "      policy_policy1_reward: [5.0, 2.5, 27.5, 26.5, 11.0, 2.0, 0.5, 2.0, 16.0, -1.0,\n",
      "        -4.0, -2.0, 5.5, 15.0, 18.5, 18.0, 7.5, 19.0, 11.5, 2.5, 9.5, 13.0, 7.0, 19.0,\n",
      "        -0.5, 14.0, 1.0, 9.0, 3.5, 21.5, 8.5, 2.5, 7.0, 9.5, 20.5, 13.0, 23.5, 29.5,\n",
      "        -0.5, -3.5, 0.5, 17.5, 6.5, 10.0, 20.5, 2.5, 14.5, -3.0, 10.5, 19.5, 14.0, 25.0,\n",
      "        -3.5, 19.5, 8.5, 25.0, -5.5, -2.0, 27.5, 6.0, 12.5, 25.0, 4.0, 23.5, 13.5, 2.5,\n",
      "        14.0, 4.0, 21.0, 2.5, 12.5, 18.0, 8.5, 14.0, 5.5, 20.5, 24.5, 13.0, 1.5, 10.0,\n",
      "        4.0, 16.0, 9.5, 29.5, 18.5, 13.5, 9.5, 2.0, -8.0, 9.0, -4.5, 6.0, 5.0, 17.0,\n",
      "        5.5, 14.0, 1.0, 2.5, 4.0, 16.0]\n",
      "      policy_policy2_reward: [-5.599999999999987, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999986, -8.89999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -5.599999999999982, -2.300000000000004,\n",
      "        -6.6999999999999815, -6.699999999999993, -4.500000000000003, -8.899999999999983,\n",
      "        -4.499999999999985, -7.799999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999984, -6.69999999999999, -9.99999999999998,\n",
      "        -3.3999999999999964, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.79999999999999, -8.899999999999986, -8.899999999999986, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -5.599999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -3.400000000000003, -5.599999999999982, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -1.2000000000000017, -7.799999999999981, -7.799999999999981, -5.6, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -6.699999999999992, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999985, -8.89999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -3.4000000000000057, -9.99999999999998,\n",
      "        -7.799999999999985, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -5.599999999999994, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -6.699999999999993,\n",
      "        -8.899999999999983, -9.99999999999998, -7.799999999999983, -9.99999999999998,\n",
      "        -6.699999999999992, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -2.299999999999988, -7.79999999999999, -8.899999999999986, -8.899999999999986,\n",
      "        -9.99999999999998, -7.799999999999986, -7.7999999999999865, -4.499999999999982,\n",
      "        -8.89999999999998, -8.89999999999998, -9.99999999999998, -2.3000000000000047,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -1.2000000000000017\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.245\n",
      "      policy2: -8.162999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -8.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1347117994063717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05670898849631932\n",
      "      mean_inference_ms: 1.426108115402958\n",
      "      mean_raw_obs_processing_ms: 0.7549635802591006\n",
      "  time_since_restore: 82.91198229789734\n",
      "  time_this_iter_s: 13.109475135803223\n",
      "  time_total_s: 82.91198229789734\n",
      "  timers:\n",
      "    learn_throughput: 548.168\n",
      "    learn_time_ms: 7297.028\n",
      "    synch_weights_time_ms: 3.671\n",
      "    training_iteration_time_ms: 16577.287\n",
      "  timestamp: 1660041807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: bafe8_00003\n",
      "  warmup_time: 11.104228258132935\n",
      "  \n",
      "Result for PPO_MultiAgentArena_bafe8_00003:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-09_12-43-38\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999993\n",
      "  episode_reward_mean: 2.727000000000005\n",
      "  episode_reward_min: -17.99999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 9960ea27f9c14636b15e67ba13cbb170\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1555469036102295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017430666834115982\n",
      "          model: {}\n",
      "          policy_loss: -0.04689561575651169\n",
      "          total_loss: 6.518825054168701\n",
      "          vf_explained_var: 0.15633608400821686\n",
      "          vf_loss: 6.557877540588379\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.169877290725708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020568469539284706\n",
      "          model: {}\n",
      "          policy_loss: -0.05242742970585823\n",
      "          total_loss: 2.0588362216949463\n",
      "          vf_explained_var: 0.3119783103466034\n",
      "          vf_loss: 2.105093002319336\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 20.08235294117647\n",
      "    ram_util_percent: 58.53529411764706\n",
      "  pid: 63801\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: -1.2000000000000017\n",
      "  policy_reward_mean:\n",
      "    policy1: 11.11\n",
      "    policy2: -8.382999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12952416898535563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054639991915875076\n",
      "    mean_inference_ms: 1.3686725338022634\n",
      "    mean_raw_obs_processing_ms: 0.7296086489628163\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999993\n",
      "    episode_reward_mean: 2.727000000000005\n",
      "    episode_reward_min: -17.99999999999998\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-5.099999999999989, 7.499999999999989, -2.399999999999981, 2.450817326860033e-14,\n",
      "        10.499999999999988, -7.499999999999973, 4.500000000000007, -4.199999999999985,\n",
      "        2.7000000000000077, 11.699999999999964, 8.400000000000011, 15.000000000000025,\n",
      "        -13.499999999999986, 11.699999999999964, 1.8000000000000274, 14.999999999999972,\n",
      "        -14.399999999999972, -8.699999999999989, 18.59999999999992, -1.7999999999999774,\n",
      "        3.6000000000000276, 15.00000000000001, 0.6000000000000151, 13.500000000000014,\n",
      "        5.69999999999996, -7.499999999999989, 5.100000000000023, -5.999999999999986,\n",
      "        13.200000000000008, -7.499999999999972, 6.899999999999984, 10.200000000000026,\n",
      "        -1.499999999999973, 5.100000000000028, -4.499999999999975, 13.799999999999939,\n",
      "        15.600000000000032, 2.9999999999999867, -6.299999999999997, 2.2398749521812533e-14,\n",
      "        -2.6999999999999935, 6.000000000000032, 0.6000000000000206, 19.499999999999993,\n",
      "        16.19999999999994, 5.700000000000028, 0.6000000000000273, -6.899999999999985,\n",
      "        -17.99999999999998, 1.2000000000000202, -12.299999999999983, 1.500000000000028,\n",
      "        -3.8999999999999977, 8.100000000000032, -4.499999999999979, 11.699999999999939,\n",
      "        -8.999999999999977, -7.4999999999999964, -5.9999999999999964, 6.00000000000003,\n",
      "        6.60000000000001, 5.700000000000017, -3.0000000000000013, 11.09999999999996,\n",
      "        12.000000000000009, 7.199999999999994, -0.8999999999999773, 7.743805596760467e-15,\n",
      "        6.6, -14.699999999999978, -14.999999999999991, 6.3000000000000185, -5.399999999999988,\n",
      "        10.200000000000006, 11.39999999999994, -10.799999999999988, -1.499999999999988,\n",
      "        -9.299999999999999, 16.50000000000002, -0.899999999999987, 9.600000000000017,\n",
      "        4.500000000000028, 2.7000000000000246, 7.499999999999959, -0.8999999999999896,\n",
      "        1.5000000000000238, -2.699999999999997, 18.899999999999977, 1.3516965324811281e-14,\n",
      "        19.499999999999957, 3.0000000000000293, 4.199999999999989, -7.499999999999988,\n",
      "        6.600000000000019, -2.9999999999999747, 16.499999999999915, -0.8999999999999808,\n",
      "        13.200000000000033, 7.799999999999965, 6.000000000000021]\n",
      "      policy_policy1_reward: [0.5, 17.5, 6.5, 10.0, 20.5, 2.5, 14.5, -3.0, 10.5, 19.5,\n",
      "        14.0, 25.0, -3.5, 19.5, 8.5, 25.0, -5.5, -2.0, 27.5, 6.0, 12.5, 25.0, 4.0, 23.5,\n",
      "        13.5, 2.5, 14.0, 4.0, 21.0, 2.5, 12.5, 18.0, 8.5, 14.0, 5.5, 20.5, 24.5, 13.0,\n",
      "        1.5, 10.0, 4.0, 16.0, 9.5, 29.5, 18.5, 13.5, 9.5, 2.0, -8.0, 9.0, -4.5, 6.0,\n",
      "        5.0, 17.0, 5.5, 14.0, 1.0, 2.5, 4.0, 16.0, 15.5, 8.0, 7.0, 20.0, 22.0, 15.0,\n",
      "        8.0, 10.0, 15.5, -8.0, -5.0, 13.0, 3.5, 18.0, 17.0, -3.0, 8.5, -1.5, 26.5, 8.0,\n",
      "        18.5, 14.5, 10.5, 17.5, 8.0, 11.5, 4.0, 24.5, 10.0, 29.5, 13.0, 12.0, 2.5, 10.0,\n",
      "        7.0, 26.5, 8.0, 21.0, 14.5, 16.0]\n",
      "      policy_policy2_reward: [-5.599999999999982, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -1.2000000000000017, -7.799999999999981, -7.799999999999981, -5.6, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -6.699999999999992, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999985, -8.89999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -3.4000000000000057, -9.99999999999998,\n",
      "        -7.799999999999985, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -5.599999999999994, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -6.699999999999993,\n",
      "        -8.899999999999983, -9.99999999999998, -7.799999999999983, -9.99999999999998,\n",
      "        -6.699999999999992, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -2.299999999999988, -7.79999999999999, -8.899999999999986, -8.899999999999986,\n",
      "        -9.99999999999998, -7.799999999999986, -7.7999999999999865, -4.499999999999982,\n",
      "        -8.89999999999998, -8.89999999999998, -9.99999999999998, -2.3000000000000047,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -2.3000000000000016, -9.99999999999998, -8.899999999999984,\n",
      "        -9.99999999999998, -7.799999999999986, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999982, -9.99999999999998, -6.6999999999999815,\n",
      "        -8.899999999999984, -7.799999999999981, -5.599999999999982, -7.799999999999985,\n",
      "        -9.99999999999998, -7.799999999999985, -9.99999999999998, -8.899999999999983,\n",
      "        -8.899999999999986, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -6.6999999999999815, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -3.399999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999984, -6.699999999999988, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: -1.2000000000000017\n",
      "    policy_reward_mean:\n",
      "      policy1: 11.11\n",
      "      policy2: -8.382999999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -8.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12952416898535563\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.054639991915875076\n",
      "      mean_inference_ms: 1.3686725338022634\n",
      "      mean_raw_obs_processing_ms: 0.7296086489628163\n",
      "  time_since_restore: 94.4711241722107\n",
      "  time_this_iter_s: 11.559141874313354\n",
      "  time_total_s: 94.4711241722107\n",
      "  timers:\n",
      "    learn_throughput: 586.179\n",
      "    learn_time_ms: 6823.848\n",
      "    synch_weights_time_ms: 3.445\n",
      "    training_iteration_time_ms: 15740.128\n",
      "  timestamp: 1660041818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: bafe8_00003\n",
      "  warmup_time: 11.104228258132935\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 12:43:39,441\tINFO tune.py:758 -- Total run time: 171.47 seconds (170.68 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Example using Ray tune API (`tune.run()`) until some stopping condition is met.\n",
    "# This will create one (or more) Algorithms under the hood automatically w/o us having to\n",
    "# build these algos from the config.\n",
    "\n",
    "experiment_results = tune.run(\n",
    "    \"PPO\",\n",
    "\n",
    "    # training config params (translated into a python dict!)\n",
    "    config=config.to_dict(),\n",
    "\n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\n",
    "        \"training_iteration\": 6,     # stop after n training iterations (calls to `Algorithm.train()`)\n",
    "        #\"episode_reward_mean\": 400, # stop if average (sum of) rewards in an episode is 400 or more\n",
    "        #\"timesteps_total\": 100000,  # stop if reached 100,000 sampling timesteps\n",
    "    },  \n",
    "\n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=\"results\",\n",
    "         \n",
    "    # Every how many train() calls do we create a checkpoint?\n",
    "    checkpoint_freq=1,\n",
    "    # Always save last checkpoint (no matter the frequency).\n",
    "    checkpoint_at_end=True,\n",
    "\n",
    "    ###############\n",
    "    # Note about Ray Tune verbosity.\n",
    "    # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "    # 0 = silent\n",
    "    # 1 = only status updates, no logging messages\n",
    "    # 2 = status and brief trial results, includes logging messages\n",
    "    # 3 = status and detailed trial results, includes logging messages\n",
    "    # Defaults to 3.\n",
    "    ###############\n",
    "    verbose=3,\n",
    "                   \n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:  PPO_MultiAgentArena_bafe8_00003\n",
      "Best checkpoint from training: Checkpoint(local_path=/Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/results/PPO/PPO_MultiAgentArena_bafe8_00003_3_lr=0.0001,train_batch_size=4000_2022-08-09_12-41-44/checkpoint_000006)\n"
     ]
    }
   ],
   "source": [
    "# Using the returned `experiment_results` object,\n",
    "# we can extract from it the best checkpoint according to some criterium, e.g. `episode_reward_mean`.\n",
    "\n",
    "# We only had a single trial (one Algorithm instance), so this should be returned here.\n",
    "best_trial = experiment_results.get_best_trial()\n",
    "print(\"Best trial: \", best_trial)\n",
    "\n",
    "\n",
    "# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\n",
    "best_checkpoint = experiment_results.get_best_checkpoint(trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# We would expect this to be either the very last checkpoint or one close to it:\n",
    "print(f\"Best checkpoint from training: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details behind Ray RLlib resource allocation <a class=\"anchor\" id=\"resource_allocation\"></a>\n",
    "\n",
    "#### Why did we use 8 CPUs in the tune run above (2 CPUs per trial)?\n",
    "\n",
    "```\n",
    "== Status ==\n",
    "Current time: 2022-07-24 18:18:28 (running for 00:02:09.35)\n",
    "Memory usage on this node: 9.9/16.0 GiB\n",
    "Using FIFO scheduling algorithm.\n",
    "Resources requested: 8/16 CPUs, 0/0 GPUs, 0.5/3.97 GiB heap, 0.5/1.98 GiB objects\n",
    "```\n",
    "\n",
    "<img src=\"images/closer_look_at_rllib.png\" width=700 />\n",
    "\n",
    "By default, the PPO Algorithm uses 2 so called `RolloutWorkers` (you can change this via `config.rollouts(num_rollout_workers=2)`) for collecting samples from\n",
    "environments in parallel.\n",
    "We changed this setting to only 1 worker via the `config.rollouts(num_rollout_workers=1)` call in the cell above.\n",
    "\n",
    "`RolloutWorkers` are Ray Actors that have their own copies of the environment and step through episodes in parallel. Each Actor in Ray normally uses a single CPU, but besides `RolloutWorker`s, an Algorithm in RLlib also always has one local process (aka. the \"driver\" process or the \"local worker\"), which - in case of PPO -\n",
    "handles the model/policy learning updates.\n",
    "\n",
    "For our experiment above, this gives us 2 CPUs (1 rollout worker + 1 local learner) per Algorithm instance.\n",
    "\n",
    "Since our config specifies two `grid_search` with 2 different learning rates AND 2 different batch sizes, we were running 4 Algorithms in parallel above (2 learning rates x 2 batch sizes = 4 trials), hence 8 CPUs were required (4 algos x 2 CPUs each = 8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt, how to:\n",
    "\n",
    "* Use Ray Tune in combination with RLlib for hyperparameter tuning\n",
    "* How RLlib and Tune determine the required computational resources for some `tune.run()` experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 03<a ></a>\n",
    "\n",
    "#### Using the `config` that we have built so far, let's run another `tune.run()`.\n",
    "\n",
    "But this time, apply the following changes to our setup:\n",
    "\n",
    "- Setup only 1 learning rate using the `config.training(lr=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size using the `config.training(train_batch_size=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set the number of RolloutWorkers to 5 using the `config.rollouts(num_rollout_workers=5)` method call, which will allow us to collect more environment samples in parallel.\n",
    "- Set the `num_envs_per_worker` config parameter to 5 using the `config.rollouts(num_envs_per_worker=...)` method call. This will batch our environment on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    " * [Tune, Scalable Hyperparameter Tuning](https://docs.ray.io/en/latest/tune/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
