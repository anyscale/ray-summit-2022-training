{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03. Introduction to Ray Tune and hyperparameter optimization (HPO)\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this notebook, you will learn:\n",
    " * [How to configure Ray Tune to find solid hyperparameters more easily](#configure_ray_tune)\n",
    " * [The details behind Ray RLlib resource allocation](#resource_allocation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "\n",
    "# Importing the very same environment class that we have coded together in\n",
    "# the previous notebook.\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena, play_one_episode\n",
    "\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to configure Ray Tune to find solid hyperparameters more easily <a class=\"anchor\" id=\"configure_ray_tune\"></a>\n",
    "\n",
    "In the previous experiments, we used a single algorithm's (PPO) configuration to create\n",
    "exactly one Algorithm object and call its `train()` method manually a couple of times.\n",
    "\n",
    "A common thing to try when doing ML or RL is to look for better choices of hyperparameters, neural network architectures, or algorithm settings. This hyperparameter optimization\n",
    "problem can be tackled in a scalable fashion using Ray Tune (in combination with RLlib!).\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates, how you can setup a simple grid-search for one very important hyperparameter (the learning rate), using our already existing PPO config object and Ray Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x2b9d6e06970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOConfig object (same as we did in the previous notebook):\n",
    "config = PPOConfig()\n",
    "\n",
    "# Setup our config object the exact same way as before:\n",
    "# Point to our MultiAgentArena env:\n",
    "config.environment(env=MultiAgentArena)\n",
    "\n",
    "# Setup multi-agent mapping:\n",
    "\n",
    "# Environment provides M agent IDs.\n",
    "# RLlib has N policies (neural networks).\n",
    "# The `policy_mapping_fn` maps M agent IDs to N policies (M <= N).\n",
    "\n",
    "# If you don't provide a policy_mapping_fn, all agent IDs will map to \"default_policy\".\n",
    "config.multi_agent(\n",
    "    # Tell RLlib to create 2 policies with these IDs here:\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    # Tell RLlib to map agent1 to policy1 and agent2 to policy2.\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "\n",
    "# Reduce the number of workers from 2 (default) to 1 to save some resources on the expensive hyperparameter sweep.\n",
    "# IMPORTANT: More information on resource requirements for tune hyperparameter sweeps and different RLlib algorithm setups\n",
    "# below.\n",
    "config.rollouts(num_rollout_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore how a very simple hyperparameter search should be configured with RLlib and Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default learning rate for PPO is: 5e-05\n",
      "Default train batch size for PPO is: 4000\n"
     ]
    }
   ],
   "source": [
    "# Before setting up the learning rate hyperparam sweep,\n",
    "# let's see what the default learning rate and train batch size is for PPO:\n",
    "print(f\"Default learning rate for PPO is: {config.lr}\")\n",
    "print(f\"Default train batch size for PPO is: {config.train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x2b9d6e06970>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's change our existing config object and add a simple\n",
    "# grid-search over two different learning rates to it:\n",
    "config.training(\n",
    "    lr=tune.grid_search([5e-5, 1e-4]),\n",
    "    train_batch_size=tune.grid_search([3000, 4000]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 13:12:22,430\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m.\n",
      "\u001b[2m\u001b[36m(PPO pid=16724)\u001b[0m 2022-08-15 13:12:32,012\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=16724)\u001b[0m 2022-08-15 13:12:32,013\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18680)\u001b[0m 2022-08-15 13:12:37,733\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-15 13:12:25 (running for 00:00:00.29)\n",
      "Memory usage on this node: 9.9/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (3 PENDING, 1 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00001 | PENDING  |                 | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00002 | PENDING  |                 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_24664_00003 | PENDING  |                 | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=16724)\u001b[0m 2022-08-15 13:12:42,693\tINFO trainable.py:160 -- Trainable.setup took 10.681 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=16724)\u001b[0m 2022-08-15 13:12:42,693\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=5700)\u001b[0m 2022-08-15 13:12:48,704\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=5700)\u001b[0m 2022-08-15 13:12:48,705\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=16724)\u001b[0m 2022-08-15 13:12:53,459\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7184)\u001b[0m 2022-08-15 13:12:54,610\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-15 13:12:42 (running for 00:00:17.16)\n",
      "Memory usage on this node: 12.7/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (2 PENDING, 2 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00002 | PENDING  |                 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_24664_00003 | PENDING  |                 | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=5700)\u001b[0m 2022-08-15 13:12:58,901\tINFO trainable.py:160 -- Trainable.setup took 10.198 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=5700)\u001b[0m 2022-08-15 13:12:58,902\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=16440)\u001b[0m 2022-08-15 13:13:05,024\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=16440)\u001b[0m 2022-08-15 13:13:05,025\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16344)\u001b[0m 2022-08-15 13:13:10,808\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n",
      "\u001b[2m\u001b[36m(PPO pid=5700)\u001b[0m 2022-08-15 13:13:11,625\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-15 13:12:58 (running for 00:00:33.37)\n",
      "Memory usage on this node: 15.2/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (1 PENDING, 3 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_24664_00003 | PENDING  |                 | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=16440)\u001b[0m 2022-08-15 13:13:14,997\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=7072)\u001b[0m 2022-08-15 13:13:21,064\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=7072)\u001b[0m 2022-08-15 13:13:21,065\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3888)\u001b[0m 2022-08-15 13:13:26,887\tWARNING multi_agent_env.py:141 -- You environment returns observations that are MultiAgentDicts with incomplete information. Meaning that they only contain information on a subset of participating agents. Ignore this warning if this is intended, for example if your environment is a turn-based simulation.\n",
      "\u001b[2m\u001b[36m(PPO pid=16440)\u001b[0m 2022-08-15 13:13:30,720\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=7072)\u001b[0m 2022-08-15 13:13:30,901\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-15 13:13:15 (running for 00:00:49.47)\n",
      "Memory usage on this node: 17.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-13-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999922\n",
      "  episode_reward_mean: -6.100000000000002\n",
      "  episode_reward_min: -40.50000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3744503259658813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012073107995092869\n",
      "          model: {}\n",
      "          policy_loss: -0.027425741776823997\n",
      "          total_loss: 6.811229228973389\n",
      "          vf_explained_var: 0.018520427867770195\n",
      "          vf_loss: 6.836240768432617\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.381410837173462\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005071453284472227\n",
      "          model: {}\n",
      "          policy_loss: -0.017934324219822884\n",
      "          total_loss: 4.0777106285095215\n",
      "          vf_explained_var: 0.18360036611557007\n",
      "          vf_loss: 4.094630718231201\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 38.45161290322581\n",
      "    ram_util_percent: 43.45161290322581\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: -0.09999999999999634\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.8\n",
      "    policy2: -8.89999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -30.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0794254037309829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02858123156437275\n",
      "    mean_inference_ms: 3.0101111474652087\n",
      "    mean_raw_obs_processing_ms: 0.4430066661332615\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999922\n",
      "    episode_reward_mean: -6.100000000000002\n",
      "    episode_reward_min: -40.50000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [-1.4999999999999944, -14.999999999999988, -1.4999999999999958,\n",
      "        6.000000000000018, -13.499999999999991, -2.7000000000000037, -8.999999999999996,\n",
      "        -2.9999999999999867, -10.499999999999991, -2.399999999999995, -8.399999999999984,\n",
      "        12.30000000000002, -13.49999999999998, -1.4999999999999782, -17.09999999999998,\n",
      "        -2.9999999999999956, -40.50000000000007, 5.1, -4.499999999999988, -39.00000000000005,\n",
      "        -25.500000000000032, 22.499999999999922, -1.4999999999999976, -21.900000000000055,\n",
      "        -5.999999999999988, -3.599999999999999, 14.699999999999962, -6.599999999999982,\n",
      "        3.000000000000028, 5.1000000000000245]\n",
      "      policy_policy1_reward: [8.5, -5.0, 8.5, 16.0, -3.5, 4.0, 1.0, 7.0, -0.5, 6.5,\n",
      "        0.5, 19.0, -3.5, 8.5, -11.5, 7.0, -30.5, 14.0, 5.5, -29.0, -15.5, 32.5, 8.5,\n",
      "        -13.0, 4.0, 2.0, 22.5, -6.5, 13.0, 14.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999984,\n",
      "        -6.699999999999986, -9.99999999999998, -9.99999999999998, -5.599999999999985,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999984, -7.799999999999981,\n",
      "        -0.09999999999999634, -9.99999999999998, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: -0.09999999999999634\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.8\n",
      "      policy2: -8.89999999999998\n",
      "    policy_reward_min:\n",
      "      policy1: -30.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0794254037309829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02858123156437275\n",
      "      mean_inference_ms: 3.0101111474652087\n",
      "      mean_raw_obs_processing_ms: 0.4430066661332615\n",
      "  time_since_restore: 21.456650018692017\n",
      "  time_this_iter_s: 21.456650018692017\n",
      "  time_total_s: 21.456650018692017\n",
      "  timers:\n",
      "    learn_throughput: 280.743\n",
      "    learn_time_ms: 10685.939\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 21449.669\n",
      "  timestamp: 1660561984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:13:30 (running for 00:01:05.42)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |     -6.1 |             2.8 |            -8.9 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.1\n",
      "  episode_reward_mean: -7.3799999999999955\n",
      "  episode_reward_min: -35.40000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3671722412109375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01937299594283104\n",
      "          model: {}\n",
      "          policy_loss: -0.045524656772613525\n",
      "          total_loss: 6.4561767578125\n",
      "          vf_explained_var: -0.0007695736130699515\n",
      "          vf_loss: 6.497825622558594\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3696707487106323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016958117485046387\n",
      "          model: {}\n",
      "          policy_loss: -0.045632120221853256\n",
      "          total_loss: 2.8607442378997803\n",
      "          vf_explained_var: 0.3462334871292114\n",
      "          vf_loss: 2.902984619140625\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 6000\n",
      "  num_agent_steps_trained: 6000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 39.099999999999994\n",
      "    ram_util_percent: 52.04117647058823\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 26.0\n",
      "    policy2: -4.499999999999984\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.6666666666666667\n",
      "    policy2: -9.046666666666647\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07344889108517376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02625837837684476\n",
      "    mean_inference_ms: 3.6673091404122604\n",
      "    mean_raw_obs_processing_ms: 0.445302745891547\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.1\n",
      "    episode_reward_mean: -7.3799999999999955\n",
      "    episode_reward_min: -35.40000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100]\n",
      "      episode_reward: [-9.29999999999998, -24.00000000000005, -9.89999999999999, 17.1,\n",
      "        -3.5999999999999885, 2.9999999999999973, -18.00000000000001, 5.100000000000029,\n",
      "        -1.1999999999999744, -35.40000000000007, -16.500000000000014, -11.999999999999995,\n",
      "        -11.99999999999997, -11.999999999999982, 7.50000000000003, -16.50000000000004,\n",
      "        -31.50000000000003, -2.9999999999999862, -16.49999999999997, -1.4999999999999978,\n",
      "        -7.499999999999989, 1.4960255256823984e-14, -5.399999999999984, -15.000000000000009,\n",
      "        5.100000000000019, -0.8999999999999838, -9.89999999999999, 4.200000000000022,\n",
      "        2.100000000000001, -3.8999999999999866]\n",
      "      policy_policy1_reward: [-1.5, -14.0, -1.0, 26.0, 2.0, 13.0, -8.0, 14.0, 5.5, -26.5,\n",
      "        -6.5, -2.0, -2.0, -2.0, 12.0, -6.5, -21.5, 7.0, -6.5, 8.5, 2.5, 10.0, 3.5, -5.0,\n",
      "        14.0, 8.0, -1.0, 12.0, 11.0, 5.0]\n",
      "      policy_policy2_reward: [-7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999984,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999986, -8.89999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.0\n",
      "      policy2: -4.499999999999984\n",
      "    policy_reward_mean:\n",
      "      policy1: 1.6666666666666667\n",
      "      policy2: -9.046666666666647\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07344889108517376\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02625837837684476\n",
      "      mean_inference_ms: 3.6673091404122604\n",
      "      mean_raw_obs_processing_ms: 0.445302745891547\n",
      "  time_since_restore: 23.886234283447266\n",
      "  time_this_iter_s: 23.886234283447266\n",
      "  time_total_s: 23.886234283447266\n",
      "  timers:\n",
      "    learn_throughput: 268.883\n",
      "    learn_time_ms: 11157.289\n",
      "    synch_weights_time_ms: 3.989\n",
      "    training_iteration_time_ms: 23880.251\n",
      "  timestamp: 1660562002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:13:36 (running for 00:01:10.56)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:13:41 (running for 00:01:15.64)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:13:46 (running for 00:01:20.68)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:13:51 (running for 00:01:25.74)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:13:56 (running for 00:01:30.79)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=7072)\u001b[0m 2022-08-15 13:14:01,272\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:01 (running for 00:01:35.86)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |        |                  |      |          |                 |                 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-14-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.999999999999924\n",
      "  episode_reward_mean: -11.520000000000005\n",
      "  episode_reward_min: -36.00000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3732333183288574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01329946331679821\n",
      "          model: {}\n",
      "          policy_loss: -0.02344414032995701\n",
      "          total_loss: 7.042098045349121\n",
      "          vf_explained_var: -0.0014395865146070719\n",
      "          vf_loss: 7.062882900238037\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.378925085067749\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007456389255821705\n",
      "          model: {}\n",
      "          policy_loss: -0.0246359221637249\n",
      "          total_loss: 3.5015757083892822\n",
      "          vf_explained_var: 0.23919712007045746\n",
      "          vf_loss: 3.5247201919555664\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.997260273972604\n",
      "    ram_util_percent: 62.168493150684924\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -3.4000000000000044\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.5375\n",
      "    policy2: -8.982499999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -26.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08276110141165881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.025925234656606844\n",
      "    mean_inference_ms: 3.3676546235526685\n",
      "    mean_raw_obs_processing_ms: 0.42803458528678384\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 14.999999999999924\n",
      "    episode_reward_mean: -11.520000000000005\n",
      "    episode_reward_min: -36.00000000000004\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.9000000000000186, -30.90000000000005, -20.400000000000013,\n",
      "        -27.00000000000001, -5.999999999999979, -16.499999999999982, -2.9999999999999964,\n",
      "        -36.00000000000004, -2.3999999999999875, -0.2999999999999944, 2.100000000000021,\n",
      "        -3.0000000000000013, -19.199999999999996, -10.49999999999998, 14.999999999999924,\n",
      "        4.200000000000024, -21.000000000000007, -28.50000000000003, -24.00000000000003,\n",
      "        3.9000000000000123, -28.500000000000064, -10.799999999999974, 2.100000000000003,\n",
      "        -10.49999999999999, -14.999999999999998, -8.999999999999998, -19.499999999999996,\n",
      "        -3.5999999999999974, -23.400000000000013, -19.500000000000043, -24.000000000000014,\n",
      "        10.500000000000027, -24.0, -16.50000000000002, 11.700000000000008, -8.39999999999998,\n",
      "        -27.00000000000002, -5.3999999999999995, -11.999999999999982, -8.399999999999988]\n",
      "      policy_policy1_reward: [9.5, -22.0, -11.5, -17.0, 4.0, -6.5, 7.0, -26.0, 1.0,\n",
      "        7.5, 11.0, 7.0, -12.5, -0.5, 25.0, 12.0, -11.0, -18.5, -14.0, 9.5, -18.5, -3.0,\n",
      "        11.0, -0.5, -5.0, 1.0, -9.5, 2.0, -14.5, -9.5, -14.0, 20.5, -14.0, -6.5, 19.5,\n",
      "        0.5, -17.0, 3.5, -2.0, 0.5]\n",
      "      policy_policy2_reward: [-5.599999999999994, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -3.4000000000000044, -7.79999999999999, -8.89999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -9.99999999999998, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -3.4000000000000044\n",
      "    policy_reward_mean:\n",
      "      policy1: -2.5375\n",
      "      policy2: -8.982499999999982\n",
      "    policy_reward_min:\n",
      "      policy1: -26.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08276110141165881\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.025925234656606844\n",
      "      mean_inference_ms: 3.3676546235526685\n",
      "      mean_raw_obs_processing_ms: 0.42803458528678384\n",
      "  time_since_restore: 51.13941931724548\n",
      "  time_this_iter_s: 51.13941931724548\n",
      "  time_total_s: 51.13941931724548\n",
      "  timers:\n",
      "    learn_throughput: 112.999\n",
      "    learn_time_ms: 35398.393\n",
      "    synch_weights_time_ms: 3.99\n",
      "    training_iteration_time_ms: 51120.47\n",
      "  timestamp: 1660562046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:11 (running for 00:01:45.73)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |   -11.52 |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:16 (running for 00:01:50.81)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-6.100000000000002 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE130>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE310>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAE9A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EAEBB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E20040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      1 |          21.4567 | 3000 |    -6.1  |         2.8     |        -8.9     |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      1 |          23.8862 | 3000 |    -7.38 |         1.66667 |        -9.04667 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |   -11.52 |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-14-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.1\n",
      "  episode_reward_mean: -5.554999999999993\n",
      "  episode_reward_min: -35.40000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3183022737503052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021546196192502975\n",
      "          model: {}\n",
      "          policy_loss: -0.05685313418507576\n",
      "          total_loss: 6.793566703796387\n",
      "          vf_explained_var: -0.017343340441584587\n",
      "          vf_loss: 6.846110820770264\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3246138095855713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018565628677606583\n",
      "          model: {}\n",
      "          policy_loss: -0.05975031480193138\n",
      "          total_loss: 1.7175065279006958\n",
      "          vf_explained_var: 0.3714500069618225\n",
      "          vf_loss: 1.7735439538955688\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.153658536585365\n",
      "    ram_util_percent: 63.39268292682925\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 26.0\n",
      "    policy2: -0.10000000000000109\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.4\n",
      "    policy2: -8.954999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07162755881765301\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02567786897821043\n",
      "    mean_inference_ms: 4.478699468303\n",
      "    mean_raw_obs_processing_ms: 0.4425499416774619\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.1\n",
      "    episode_reward_mean: -5.554999999999993\n",
      "    episode_reward_min: -35.40000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-9.29999999999998, -24.00000000000005, -9.89999999999999, 17.1,\n",
      "        -3.5999999999999885, 2.9999999999999973, -18.00000000000001, 5.100000000000029,\n",
      "        -1.1999999999999744, -35.40000000000007, -16.500000000000014, -11.999999999999995,\n",
      "        -11.99999999999997, -11.999999999999982, 7.50000000000003, -16.50000000000004,\n",
      "        -31.50000000000003, -2.9999999999999862, -16.49999999999997, -1.4999999999999978,\n",
      "        -7.499999999999989, 1.4960255256823984e-14, -5.399999999999984, -15.000000000000009,\n",
      "        5.100000000000019, -0.8999999999999838, -9.89999999999999, 4.200000000000022,\n",
      "        2.100000000000001, -3.8999999999999866, -11.69999999999997, -32.40000000000007,\n",
      "        -8.999999999999975, -2.999999999999996, -22.50000000000002, -17.999999999999996,\n",
      "        -1.4999999999999856, 7.200000000000031, 3.000000000000005, 2.099999999999996,\n",
      "        9.30000000000002, 0.6000000000000184, -7.499999999999974, 3.6000000000000214,\n",
      "        -13.499999999999986, 2.1000000000000103, -3.899999999999985, -1.5000000000000089,\n",
      "        -9.599999999999982, 6.000000000000016, -8.999999999999996, 6.000000000000011,\n",
      "        5.100000000000014, -11.999999999999986, 2.5618396293225487e-14, -1.7999999999999847,\n",
      "        -1.4999999999999936, 1.5000000000000067, -3.5999999999999885, 3.6000000000000285]\n",
      "      policy_policy1_reward: [-1.5, -14.0, -1.0, 26.0, 2.0, 13.0, -8.0, 14.0, 5.5, -26.5,\n",
      "        -6.5, -2.0, -2.0, -2.0, 12.0, -6.5, -21.5, 7.0, -6.5, 8.5, 2.5, 10.0, 3.5, -5.0,\n",
      "        14.0, 8.0, -1.0, 12.0, 11.0, 5.0, -5.0, -23.5, 1.0, 7.0, -12.5, -8.0, 8.5, 15.0,\n",
      "        13.0, 11.0, 16.0, 9.5, 2.5, 12.5, -3.5, 11.0, 5.0, 8.5, -4.0, 16.0, 1.0, 16.0,\n",
      "        14.0, -2.0, 10.0, 6.0, 8.5, 11.5, -3.5, 12.5]\n",
      "      policy_policy2_reward: [-7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999984,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999986, -8.89999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -8.89999999999998, -6.699999999999982,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999983, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -0.10000000000000109,\n",
      "        -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.0\n",
      "      policy2: -0.10000000000000109\n",
      "    policy_reward_mean:\n",
      "      policy1: 3.4\n",
      "      policy2: -8.954999999999982\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07162755881765301\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02567786897821043\n",
      "      mean_inference_ms: 4.478699468303\n",
      "      mean_raw_obs_processing_ms: 0.4425499416774619\n",
      "  time_since_restore: 73.66914701461792\n",
      "  time_this_iter_s: 49.782912731170654\n",
      "  time_total_s: 73.66914701461792\n",
      "  timers:\n",
      "    learn_throughput: 155.549\n",
      "    learn_time_ms: 19286.473\n",
      "    synch_weights_time_ms: 3.49\n",
      "    training_iteration_time_ms: 36830.585\n",
      "  timestamp: 1660562060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-14-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999922\n",
      "  episode_reward_mean: -3.234999999999997\n",
      "  episode_reward_min: -40.50000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3516993522644043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008350875228643417\n",
      "          model: {}\n",
      "          policy_loss: -0.032837335020303726\n",
      "          total_loss: 6.607187271118164\n",
      "          vf_explained_var: 0.056506067514419556\n",
      "          vf_loss: 6.6383538246154785\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3579225540161133\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008340733125805855\n",
      "          model: {}\n",
      "          policy_loss: -0.032666806131601334\n",
      "          total_loss: 2.172516345977783\n",
      "          vf_explained_var: 0.25665953755378723\n",
      "          vf_loss: 2.20351505279541\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 6000\n",
      "    num_env_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 6000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 6000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.99266055045872\n",
      "    ram_util_percent: 60.99357798165137\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: 3.2000000000000117\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.316666666666666\n",
      "    policy2: -8.551666666666648\n",
      "  policy_reward_min:\n",
      "    policy1: -30.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07818502192121865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02692179451873402\n",
      "    mean_inference_ms: 4.012870535868279\n",
      "    mean_raw_obs_processing_ms: 0.444146794518124\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999922\n",
      "    episode_reward_mean: -3.234999999999997\n",
      "    episode_reward_min: -40.50000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-1.4999999999999944, -14.999999999999988, -1.4999999999999958,\n",
      "        6.000000000000018, -13.499999999999991, -2.7000000000000037, -8.999999999999996,\n",
      "        -2.9999999999999867, -10.499999999999991, -2.399999999999995, -8.399999999999984,\n",
      "        12.30000000000002, -13.49999999999998, -1.4999999999999782, -17.09999999999998,\n",
      "        -2.9999999999999956, -40.50000000000007, 5.1, -4.499999999999988, -39.00000000000005,\n",
      "        -25.500000000000032, 22.499999999999922, -1.4999999999999976, -21.900000000000055,\n",
      "        -5.999999999999988, -3.599999999999999, 14.699999999999962, -6.599999999999982,\n",
      "        3.000000000000028, 5.1000000000000245, 3.000000000000014, -5.9999999999999964,\n",
      "        -7.500000000000005, 6.600000000000001, -2.0999999999999748, 18.599999999999945,\n",
      "        -16.50000000000001, 1.2000000000000226, -16.499999999999996, -5.999999999999982,\n",
      "        -3.3000000000000003, 10.50000000000001, 10.50000000000003, -2.9999999999999902,\n",
      "        -7.799999999999984, 4.500000000000016, -0.8999999999999801, -16.499999999999986,\n",
      "        5.4, 8.699999999999987, -4.499999999999984, -10.499999999999995, 0.30000000000001514,\n",
      "        9.000000000000025, 4.50000000000003, 5.100000000000026, -2.0999999999999814,\n",
      "        -9.299999999999976, 17.99999999999998, -4.499999999999993]\n",
      "      policy_policy1_reward: [8.5, -5.0, 8.5, 16.0, -3.5, 4.0, 1.0, 7.0, -0.5, 6.5,\n",
      "        0.5, 19.0, -3.5, 8.5, -11.5, 7.0, -30.5, 14.0, 5.5, -29.0, -15.5, 32.5, 8.5,\n",
      "        -13.0, 4.0, 2.0, 22.5, -6.5, 13.0, 14.0, 13.0, 4.0, 2.5, 15.5, 3.5, 27.5, -6.5,\n",
      "        9.0, -6.5, 4.0, 4.5, 20.5, 15.0, 7.0, 0.0, 9.0, 8.0, -6.5, 11.0, 5.5, 5.5, -0.5,\n",
      "        7.0, 19.0, 14.5, 14.0, 3.5, -1.5, 28.0, 5.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999984,\n",
      "        -6.699999999999986, -9.99999999999998, -9.99999999999998, -5.599999999999985,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999984, -7.799999999999981,\n",
      "        -0.09999999999999634, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -5.599999999999985,\n",
      "        -8.899999999999984, -9.99999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -4.5, -9.99999999999998,\n",
      "        -7.7999999999999865, -4.499999999999992, -8.89999999999998, -9.99999999999998,\n",
      "        -5.599999999999998, 3.2000000000000117, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999982, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -5.599999999999987, -7.799999999999982, -9.99999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: 3.2000000000000117\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.316666666666666\n",
      "      policy2: -8.551666666666648\n",
      "    policy_reward_min:\n",
      "      policy1: -30.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07818502192121865\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02692179451873402\n",
      "      mean_inference_ms: 4.012870535868279\n",
      "      mean_raw_obs_processing_ms: 0.444146794518124\n",
      "  time_since_restore: 71.71429324150085\n",
      "  time_this_iter_s: 50.25764322280884\n",
      "  time_total_s: 71.71429324150085\n",
      "  timers:\n",
      "    learn_throughput: 157.1\n",
      "    learn_time_ms: 19096.176\n",
      "    synch_weights_time_ms: 3.492\n",
      "    training_iteration_time_ms: 35850.165\n",
      "  timestamp: 1660562061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:26 (running for 00:02:00.89)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:31 (running for 00:02:05.93)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:36 (running for 00:02:10.99)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |        |                  |      |          |                 |                 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-14-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.50000000000002\n",
      "  episode_reward_mean: -10.754999999999999\n",
      "  episode_reward_min: -36.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3656059503555298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021089954301714897\n",
      "          model: {}\n",
      "          policy_loss: -0.04514087736606598\n",
      "          total_loss: 6.930116653442383\n",
      "          vf_explained_var: 0.0007310908986255527\n",
      "          vf_loss: 6.971039772033691\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3697110414505005\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016949528828263283\n",
      "          model: {}\n",
      "          policy_loss: -0.042314764112234116\n",
      "          total_loss: 2.6627085208892822\n",
      "          vf_explained_var: 0.31019049882888794\n",
      "          vf_loss: 2.7016334533691406\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.979787234042558\n",
      "    ram_util_percent: 63.994680851063826\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: -4.499999999999983\n",
      "  policy_reward_mean:\n",
      "    policy1: -1.8\n",
      "    policy2: -8.95499999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0810495646886008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022435837821703022\n",
      "    mean_inference_ms: 7.024704352285648\n",
      "    mean_raw_obs_processing_ms: 0.4374461899814353\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 16.50000000000002\n",
      "    episode_reward_mean: -10.754999999999999\n",
      "    episode_reward_min: -36.00000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [4.9682480351975755e-15, -31.500000000000057, -11.999999999999975,\n",
      "        8.10000000000002, -11.999999999999986, -10.49999999999997, -13.499999999999972,\n",
      "        -0.8999999999999895, -15.900000000000002, 16.50000000000002, -24.000000000000014,\n",
      "        -36.00000000000006, -1.199999999999978, -23.4, -1.4999999999999778, -11.399999999999999,\n",
      "        -10.499999999999996, 2.095545958979983e-14, -3.299999999999994, 9.90000000000002,\n",
      "        8.400000000000023, -31.500000000000025, -30.00000000000003, 4.500000000000032,\n",
      "        -10.49999999999999, -30.000000000000057, -10.499999999999988, -10.49999999999999,\n",
      "        -35.40000000000005, 9.00000000000001, -9.599999999999982, -14.400000000000002,\n",
      "        -20.400000000000034, 6.9000000000000234, -27.00000000000001, -24.000000000000018,\n",
      "        -7.799999999999976, -5.9999999999999964, -2.9999999999999813, -15.299999999999995]\n",
      "      policy_policy1_reward: [10.0, -21.5, -2.0, 17.0, -2.0, -0.5, -3.5, 8.0, -7.0,\n",
      "        26.5, -14.0, -26.0, 5.5, -14.5, 8.5, -2.5, -0.5, 10.0, 4.5, 15.5, 14.0, -21.5,\n",
      "        -20.0, 14.5, -0.5, -20.0, -0.5, -0.5, -26.5, 19.0, -4.0, -5.5, -11.5, 12.5,\n",
      "        -17.0, -14.0, 0.0, 4.0, 1.5, -7.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999984, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999991, -8.89999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -5.599999999999994, -5.599999999999993, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -5.599999999999997,\n",
      "        -8.899999999999986, -8.89999999999998, -5.599999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -4.499999999999983,\n",
      "        -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: -4.499999999999983\n",
      "    policy_reward_mean:\n",
      "      policy1: -1.8\n",
      "      policy2: -8.95499999999998\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0810495646886008\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022435837821703022\n",
      "      mean_inference_ms: 7.024704352285648\n",
      "      mean_raw_obs_processing_ms: 0.4374461899814353\n",
      "  time_since_restore: 66.2279167175293\n",
      "  time_this_iter_s: 66.2279167175293\n",
      "  time_total_s: 66.2279167175293\n",
      "  timers:\n",
      "    learn_throughput: 111.579\n",
      "    learn_time_ms: 35849.094\n",
      "    synch_weights_time_ms: 3.99\n",
      "    training_iteration_time_ms: 66224.926\n",
      "  timestamp: 1660562077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:42 (running for 00:02:16.71)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 |  -10.755 |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:47 (running for 00:02:21.79)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 |  -10.755 |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:52 (running for 00:02:26.85)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 |  -10.755 |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:14:57 (running for 00:02:31.92)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 |  -10.755 |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:02 (running for 00:02:36.97)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 |  -10.755 |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:07 (running for 00:02:42.04)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-3.234999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F0DE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F53D60>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F532B0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E947F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DD7940>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      2 |          71.7143 | 6000 |   -3.235 |         5.31667 |        -8.55167 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      2 |          73.6691 | 6000 |   -5.555 |         3.4     |        -8.955   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      1 |          51.1394 | 4000 |  -11.52  |        -2.5375  |        -8.9825  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 |  -10.755 |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.1\n",
      "  episode_reward_mean: -4.6066666666666585\n",
      "  episode_reward_min: -35.40000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2819201946258545\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019299771636724472\n",
      "          model: {}\n",
      "          policy_loss: -0.06032547354698181\n",
      "          total_loss: 6.855311393737793\n",
      "          vf_explained_var: 0.022349048405885696\n",
      "          vf_loss: 6.909846305847168\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2720099687576294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019366485998034477\n",
      "          model: {}\n",
      "          policy_loss: -0.059561993926763535\n",
      "          total_loss: 2.2502336502075195\n",
      "          vf_explained_var: 0.23441287875175476\n",
      "          vf_loss: 2.305922269821167\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.66428571428572\n",
      "    ram_util_percent: 64.00428571428573\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 26.0\n",
      "    policy2: -0.10000000000000109\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.2444444444444445\n",
      "    policy2: -8.851111111111093\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07124732134962303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024986207013087673\n",
      "    mean_inference_ms: 4.923597102427894\n",
      "    mean_raw_obs_processing_ms: 0.44271333646264804\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.1\n",
      "    episode_reward_mean: -4.6066666666666585\n",
      "    episode_reward_min: -35.40000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-9.29999999999998, -24.00000000000005, -9.89999999999999, 17.1,\n",
      "        -3.5999999999999885, 2.9999999999999973, -18.00000000000001, 5.100000000000029,\n",
      "        -1.1999999999999744, -35.40000000000007, -16.500000000000014, -11.999999999999995,\n",
      "        -11.99999999999997, -11.999999999999982, 7.50000000000003, -16.50000000000004,\n",
      "        -31.50000000000003, -2.9999999999999862, -16.49999999999997, -1.4999999999999978,\n",
      "        -7.499999999999989, 1.4960255256823984e-14, -5.399999999999984, -15.000000000000009,\n",
      "        5.100000000000019, -0.8999999999999838, -9.89999999999999, 4.200000000000022,\n",
      "        2.100000000000001, -3.8999999999999866, -11.69999999999997, -32.40000000000007,\n",
      "        -8.999999999999975, -2.999999999999996, -22.50000000000002, -17.999999999999996,\n",
      "        -1.4999999999999856, 7.200000000000031, 3.000000000000005, 2.099999999999996,\n",
      "        9.30000000000002, 0.6000000000000184, -7.499999999999974, 3.6000000000000214,\n",
      "        -13.499999999999986, 2.1000000000000103, -3.899999999999985, -1.5000000000000089,\n",
      "        -9.599999999999982, 6.000000000000016, -8.999999999999996, 6.000000000000011,\n",
      "        5.100000000000014, -11.999999999999986, 2.5618396293225487e-14, -1.7999999999999847,\n",
      "        -1.4999999999999936, 1.5000000000000067, -3.5999999999999885, 3.6000000000000285,\n",
      "        -7.199999999999994, -5.9999999999999805, 6.600000000000019, 1.5000000000000324,\n",
      "        -13.499999999999988, -11.699999999999976, -3.2999999999999776, 8.399999999999975,\n",
      "        -3.2999999999999865, 1.4999999999999976, 3.6000000000000103, -10.199999999999989,\n",
      "        1.5000000000000155, -14.39999999999999, -14.999999999999991, 6.000000000000023,\n",
      "        -25.500000000000007, 2.1000000000000023, 5.699999999999969, -2.9999999999999782,\n",
      "        -0.9000000000000121, -13.500000000000005, 9.000000000000027, 3.3000000000000282,\n",
      "        -11.999999999999995, 12.000000000000028, 4.200000000000005, 7.499999999999995,\n",
      "        -1.799999999999986, -12.899999999999983]\n",
      "      policy_policy1_reward: [-1.5, -14.0, -1.0, 26.0, 2.0, 13.0, -8.0, 14.0, 5.5, -26.5,\n",
      "        -6.5, -2.0, -2.0, -2.0, 12.0, -6.5, -21.5, 7.0, -6.5, 8.5, 2.5, 10.0, 3.5, -5.0,\n",
      "        14.0, 8.0, -1.0, 12.0, 11.0, 5.0, -5.0, -23.5, 1.0, 7.0, -12.5, -8.0, 8.5, 15.0,\n",
      "        13.0, 11.0, 16.0, 9.5, 2.5, 12.5, -3.5, 11.0, 5.0, 8.5, -4.0, 16.0, 1.0, 16.0,\n",
      "        14.0, -2.0, 10.0, 6.0, 8.5, 11.5, -3.5, 12.5, -0.5, 4.0, 15.5, 11.5, -3.5, -5.0,\n",
      "        4.5, 14.0, 4.5, 11.5, 7.0, -3.5, 11.5, -5.5, -5.0, 16.0, -15.5, 11.0, 13.5,\n",
      "        7.0, 8.0, -3.5, 19.0, 10.0, -2.0, 22.0, 12.0, 17.5, 6.0, -4.0]\n",
      "      policy_policy2_reward: [-7.799999999999981, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999984,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999986, -8.89999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -8.89999999999998, -6.699999999999982,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999983, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -0.10000000000000109,\n",
      "        -8.89999999999998, -6.699999999999994, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -7.799999999999981,\n",
      "        -5.599999999999982, -7.799999999999981, -9.99999999999998, -3.3999999999999857,\n",
      "        -6.6999999999999815, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -9.99999999999998, -7.79999999999999, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.0\n",
      "      policy2: -0.10000000000000109\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.2444444444444445\n",
      "      policy2: -8.851111111111093\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07124732134962303\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024986207013087673\n",
      "      mean_inference_ms: 4.923597102427894\n",
      "      mean_raw_obs_processing_ms: 0.44271333646264804\n",
      "  time_since_restore: 123.23737573623657\n",
      "  time_this_iter_s: 49.56822872161865\n",
      "  time_total_s: 123.23737573623657\n",
      "  timers:\n",
      "    learn_throughput: 136.623\n",
      "    learn_time_ms: 21958.248\n",
      "    synch_weights_time_ms: 3.324\n",
      "    training_iteration_time_ms: 41073.806\n",
      "  timestamp: 1660562110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-15-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999922\n",
      "  episode_reward_mean: -1.766666666666662\n",
      "  episode_reward_min: -40.50000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3159931898117065\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010288133285939693\n",
      "          model: {}\n",
      "          policy_loss: -0.03522394970059395\n",
      "          total_loss: 6.752758979797363\n",
      "          vf_explained_var: 0.10511749982833862\n",
      "          vf_loss: 6.785924434661865\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.322205662727356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010061693377792835\n",
      "          model: {}\n",
      "          policy_loss: -0.03504713252186775\n",
      "          total_loss: 1.782465934753418\n",
      "          vf_explained_var: 0.3811359703540802\n",
      "          vf_loss: 1.8155007362365723\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_env_steps_sampled: 9000\n",
      "    num_env_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 18000\n",
      "  num_agent_steps_trained: 18000\n",
      "  num_env_steps_sampled: 9000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 9000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.622857142857143\n",
      "    ram_util_percent: 64.00857142857144\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: 3.2000000000000117\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.766666666666667\n",
      "    policy2: -8.533333333333319\n",
      "  policy_reward_min:\n",
      "    policy1: -30.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07734903662907434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026332245296225947\n",
      "    mean_inference_ms: 4.551805596439569\n",
      "    mean_raw_obs_processing_ms: 0.44321904332687756\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.499999999999922\n",
      "    episode_reward_mean: -1.766666666666662\n",
      "    episode_reward_min: -40.50000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-1.4999999999999944, -14.999999999999988, -1.4999999999999958,\n",
      "        6.000000000000018, -13.499999999999991, -2.7000000000000037, -8.999999999999996,\n",
      "        -2.9999999999999867, -10.499999999999991, -2.399999999999995, -8.399999999999984,\n",
      "        12.30000000000002, -13.49999999999998, -1.4999999999999782, -17.09999999999998,\n",
      "        -2.9999999999999956, -40.50000000000007, 5.1, -4.499999999999988, -39.00000000000005,\n",
      "        -25.500000000000032, 22.499999999999922, -1.4999999999999976, -21.900000000000055,\n",
      "        -5.999999999999988, -3.599999999999999, 14.699999999999962, -6.599999999999982,\n",
      "        3.000000000000028, 5.1000000000000245, 3.000000000000014, -5.9999999999999964,\n",
      "        -7.500000000000005, 6.600000000000001, -2.0999999999999748, 18.599999999999945,\n",
      "        -16.50000000000001, 1.2000000000000226, -16.499999999999996, -5.999999999999982,\n",
      "        -3.3000000000000003, 10.50000000000001, 10.50000000000003, -2.9999999999999902,\n",
      "        -7.799999999999984, 4.500000000000016, -0.8999999999999801, -16.499999999999986,\n",
      "        5.4, 8.699999999999987, -4.499999999999984, -10.499999999999995, 0.30000000000001514,\n",
      "        9.000000000000025, 4.50000000000003, 5.100000000000026, -2.0999999999999814,\n",
      "        -9.299999999999976, 17.99999999999998, -4.499999999999993, 7.299716386910404e-15,\n",
      "        9.59999999999998, -8.399999999999979, 1.8000000000000203, -2.3999999999999844,\n",
      "        -4.499999999999982, -2.9999999999999747, -11.999999999999982, -2.6999999999999758,\n",
      "        17.99999999999991, 16.500000000000007, -12.899999999999988, 4.500000000000014,\n",
      "        -0.8999999999999881, -5.999999999999989, 3.000000000000033, 17.69999999999996,\n",
      "        -1.7999999999999874, 9.000000000000025, -9.599999999999977, 1.8000000000000091,\n",
      "        4.500000000000018, 13.499999999999936, 5.10000000000003, 4.500000000000023,\n",
      "        -16.199999999999996, 2.6999999999999957, 3.000000000000005, -1.1999999999999913,\n",
      "        1.5000000000000133]\n",
      "      policy_policy1_reward: [8.5, -5.0, 8.5, 16.0, -3.5, 4.0, 1.0, 7.0, -0.5, 6.5,\n",
      "        0.5, 19.0, -3.5, 8.5, -11.5, 7.0, -30.5, 14.0, 5.5, -29.0, -15.5, 32.5, 8.5,\n",
      "        -13.0, 4.0, 2.0, 22.5, -6.5, 13.0, 14.0, 13.0, 4.0, 2.5, 15.5, 3.5, 27.5, -6.5,\n",
      "        9.0, -6.5, 4.0, 4.5, 20.5, 15.0, 7.0, 0.0, 9.0, 8.0, -6.5, 11.0, 5.5, 5.5, -0.5,\n",
      "        7.0, 19.0, 14.5, 14.0, 3.5, -1.5, 28.0, 5.5, 10.0, 18.5, 0.5, 8.5, 6.5, 5.5,\n",
      "        7.0, -2.0, 4.0, 28.0, 26.5, -4.0, 14.5, 2.5, -1.5, 13.0, 25.5, 6.0, 19.0, -4.0,\n",
      "        8.5, 14.5, 23.5, 14.0, 14.5, -9.5, 10.5, 13.0, 5.5, 11.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.899999999999984,\n",
      "        -6.699999999999986, -9.99999999999998, -9.99999999999998, -5.599999999999985,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999984, -7.799999999999981,\n",
      "        -0.09999999999999634, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -5.599999999999985,\n",
      "        -8.899999999999984, -9.99999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -4.5, -9.99999999999998,\n",
      "        -7.7999999999999865, -4.499999999999992, -8.89999999999998, -9.99999999999998,\n",
      "        -5.599999999999998, 3.2000000000000117, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999982, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -5.599999999999987, -7.799999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999983, -6.6999999999999815,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999895, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -3.4000000000000017, -4.500000000000003, -9.99999999999998,\n",
      "        -7.799999999999988, -7.799999999999983, -9.99999999999998, -5.59999999999999,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -6.69999999999999, -7.79999999999999, -9.99999999999998,\n",
      "        -6.699999999999993, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.5\n",
      "      policy2: 3.2000000000000117\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.766666666666667\n",
      "      policy2: -8.533333333333319\n",
      "    policy_reward_min:\n",
      "      policy1: -30.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07734903662907434\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.026332245296225947\n",
      "      mean_inference_ms: 4.551805596439569\n",
      "      mean_raw_obs_processing_ms: 0.44321904332687756\n",
      "  time_since_restore: 121.49595046043396\n",
      "  time_this_iter_s: 49.781657218933105\n",
      "  time_total_s: 121.49595046043396\n",
      "  timers:\n",
      "    learn_throughput: 136.871\n",
      "    learn_time_ms: 21918.484\n",
      "    synch_weights_time_ms: 3.325\n",
      "    training_iteration_time_ms: 40491.669\n",
      "  timestamp: 1660562111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-15-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999998\n",
      "  episode_reward_mean: -6.078749999999998\n",
      "  episode_reward_min: -36.00000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.347478985786438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009135976433753967\n",
      "          model: {}\n",
      "          policy_loss: -0.02814042940735817\n",
      "          total_loss: 6.749044895172119\n",
      "          vf_explained_var: -0.07257498055696487\n",
      "          vf_loss: 6.775358200073242\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.350757360458374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009627159684896469\n",
      "          model: {}\n",
      "          policy_loss: -0.031059997156262398\n",
      "          total_loss: 2.0338151454925537\n",
      "          vf_explained_var: 0.33974558115005493\n",
      "          vf_loss: 2.0629496574401855\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.565957446808508\n",
      "    ram_util_percent: 64.01063829787236\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -2.2999999999999954\n",
      "  policy_reward_mean:\n",
      "    policy1: 2.5875\n",
      "    policy2: -8.666249999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -26.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08138829678690095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024866680277160894\n",
      "    mean_inference_ms: 4.288585674295511\n",
      "    mean_raw_obs_processing_ms: 0.43049514946001877\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.59999999999998\n",
      "    episode_reward_mean: -6.078749999999998\n",
      "    episode_reward_min: -36.00000000000004\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [3.9000000000000186, -30.90000000000005, -20.400000000000013,\n",
      "        -27.00000000000001, -5.999999999999979, -16.499999999999982, -2.9999999999999964,\n",
      "        -36.00000000000004, -2.3999999999999875, -0.2999999999999944, 2.100000000000021,\n",
      "        -3.0000000000000013, -19.199999999999996, -10.49999999999998, 14.999999999999924,\n",
      "        4.200000000000024, -21.000000000000007, -28.50000000000003, -24.00000000000003,\n",
      "        3.9000000000000123, -28.500000000000064, -10.799999999999974, 2.100000000000003,\n",
      "        -10.49999999999999, -14.999999999999998, -8.999999999999998, -19.499999999999996,\n",
      "        -3.5999999999999974, -23.400000000000013, -19.500000000000043, -24.000000000000014,\n",
      "        10.500000000000027, -24.0, -16.50000000000002, 11.700000000000008, -8.39999999999998,\n",
      "        -27.00000000000002, -5.3999999999999995, -11.999999999999982, -8.399999999999988,\n",
      "        -1.4999999999999933, 10.200000000000015, 5.4000000000000306, 7.5000000000000195,\n",
      "        6.000000000000007, 6.000000000000021, 1.2000000000000082, 0.3000000000000116,\n",
      "        -4.499999999999988, -1.4999999999999882, -7.79999999999999, -16.49999999999999,\n",
      "        18.59999999999998, -2.0999999999999996, -8.99999999999998, -20.69999999999999,\n",
      "        -5.4, -0.8999999999999893, 5.100000000000023, 5.999999999999989, 2.7000000000000317,\n",
      "        4.199999999999989, -6.299999999999995, 7.500000000000021, 0.6000000000000103,\n",
      "        15.299999999999937, -18.299999999999983, 13.19999999999998, 4.500000000000011,\n",
      "        -2.999999999999992, -2.9999999999999982, 6.600000000000033, -11.999999999999984,\n",
      "        -1.4999999999999902, -8.999999999999972, -18.299999999999983, 1.5000000000000173,\n",
      "        -4.499999999999989, 1.529332216421153e-14, -2.099999999999994]\n",
      "      policy_policy1_reward: [9.5, -22.0, -11.5, -17.0, 4.0, -6.5, 7.0, -26.0, 1.0,\n",
      "        7.5, 11.0, 7.0, -12.5, -0.5, 25.0, 12.0, -11.0, -18.5, -14.0, 9.5, -18.5, -3.0,\n",
      "        11.0, -0.5, -5.0, 1.0, -9.5, 2.0, -14.5, -9.5, -14.0, 20.5, -14.0, -6.5, 19.5,\n",
      "        0.5, -17.0, 3.5, -2.0, 0.5, 8.5, 18.0, 11.0, 17.5, 16.0, 16.0, 3.5, 7.0, 5.5,\n",
      "        8.5, 0.0, -6.5, 27.5, 3.5, 1.0, -14.0, 3.5, 8.0, 14.0, 10.5, 10.5, 12.0, 1.5,\n",
      "        17.5, 9.5, 22.0, -16.0, 21.0, 14.5, 7.0, 7.0, 15.5, -2.0, 8.5, 1.0, -10.5, 11.5,\n",
      "        5.5, 10.0, 3.5]\n",
      "      policy_policy2_reward: [-5.599999999999994, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -3.4000000000000044, -7.79999999999999, -8.89999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -9.99999999999998, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -5.599999999999991,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -2.2999999999999954,\n",
      "        -6.699999999999994, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999997, -9.99999999999998,\n",
      "        -6.699999999999994, -8.89999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -4.500000000000001, -7.79999999999999, -7.79999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -8.899999999999983, -6.6999999999999815, -2.3, -7.79999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.6]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: -2.2999999999999954\n",
      "    policy_reward_mean:\n",
      "      policy1: 2.5875\n",
      "      policy2: -8.666249999999982\n",
      "    policy_reward_min:\n",
      "      policy1: -26.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08138829678690095\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024866680277160894\n",
      "      mean_inference_ms: 4.288585674295511\n",
      "      mean_raw_obs_processing_ms: 0.43049514946001877\n",
      "  time_since_restore: 117.66328859329224\n",
      "  time_this_iter_s: 66.52386927604675\n",
      "  time_total_s: 117.66328859329224\n",
      "  timers:\n",
      "    learn_throughput: 111.89\n",
      "    learn_time_ms: 35749.326\n",
      "    synch_weights_time_ms: 3.491\n",
      "    training_iteration_time_ms: 58818.18\n",
      "  timestamp: 1660562112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:12 (running for 00:02:47.24)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |    reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |         121.496  | 9000 |  -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |         123.237  | 9000 |  -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |         117.663  | 8000 |  -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 | -10.755   |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:17 (running for 00:02:52.38)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |    reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |         121.496  | 9000 |  -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |         123.237  | 9000 |  -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |         117.663  | 8000 |  -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 | -10.755   |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:22 (running for 00:02:57.43)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |    reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |         121.496  | 9000 |  -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |         123.237  | 9000 |  -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |         117.663  | 8000 |  -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 | -10.755   |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:28 (running for 00:03:02.49)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |    reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |         121.496  | 9000 |  -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |         123.237  | 9000 |  -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |         117.663  | 8000 |  -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 | -10.755   |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:33 (running for 00:03:07.55)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |    reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |         121.496  | 9000 |  -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |         123.237  | 9000 |  -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |         117.663  | 8000 |  -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 | -10.755   |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:38 (running for 00:03:12.62)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |    reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |         121.496  | 9000 |  -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |         123.237  | 9000 |  -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |         117.663  | 8000 |  -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 | -10.755   |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:43 (running for 00:03:17.67)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |    reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |         121.496  | 9000 |  -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |         123.237  | 9000 |  -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |         117.663  | 8000 |  -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      1 |          66.2279 | 4000 | -10.755   |        -1.8     |        -8.955   |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+-----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.399999999999977\n",
      "  episode_reward_mean: -5.448749999999996\n",
      "  episode_reward_min: -36.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3257843255996704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017495231702923775\n",
      "          model: {}\n",
      "          policy_loss: -0.045051202178001404\n",
      "          total_loss: 6.530882835388184\n",
      "          vf_explained_var: 0.06456074118614197\n",
      "          vf_loss: 6.570685386657715\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3299527168273926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017528316006064415\n",
      "          model: {}\n",
      "          policy_loss: -0.049485739320516586\n",
      "          total_loss: 2.042975902557373\n",
      "          vf_explained_var: 0.33153846859931946\n",
      "          vf_loss: 2.088956117630005\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.10744680851064\n",
      "    ram_util_percent: 64.22021276595746\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: -2.3000000000000047\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.09375\n",
      "    policy2: -8.542499999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08054306222961044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0223058876682511\n",
      "    mean_inference_ms: 7.0893998601742725\n",
      "    mean_raw_obs_processing_ms: 0.4351101852791676\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.399999999999977\n",
      "    episode_reward_mean: -5.448749999999996\n",
      "    episode_reward_min: -36.00000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100]\n",
      "      episode_reward: [4.9682480351975755e-15, -31.500000000000057, -11.999999999999975,\n",
      "        8.10000000000002, -11.999999999999986, -10.49999999999997, -13.499999999999972,\n",
      "        -0.8999999999999895, -15.900000000000002, 16.50000000000002, -24.000000000000014,\n",
      "        -36.00000000000006, -1.199999999999978, -23.4, -1.4999999999999778, -11.399999999999999,\n",
      "        -10.499999999999996, 2.095545958979983e-14, -3.299999999999994, 9.90000000000002,\n",
      "        8.400000000000023, -31.500000000000025, -30.00000000000003, 4.500000000000032,\n",
      "        -10.49999999999999, -30.000000000000057, -10.499999999999988, -10.49999999999999,\n",
      "        -35.40000000000005, 9.00000000000001, -9.599999999999982, -14.400000000000002,\n",
      "        -20.400000000000034, 6.9000000000000234, -27.00000000000001, -24.000000000000018,\n",
      "        -7.799999999999976, -5.9999999999999964, -2.9999999999999813, -15.299999999999995,\n",
      "        1.5000000000000149, 11.700000000000015, 12.299999999999985, -13.499999999999998,\n",
      "        6.599999999999994, 8.700000000000015, 0.2999999999999978, 15.299999999999995,\n",
      "        -11.399999999999991, 15.900000000000007, -8.399999999999986, -5.399999999999999,\n",
      "        -6.299999999999983, 2.700000000000013, 5.700000000000019, -7.4999999999999805,\n",
      "        -5.99999999999998, 8.700000000000022, -4.5, -8.999999999999986, -19.5, -22.5,\n",
      "        17.399999999999977, -7.499999999999986, 15.299999999999986, 2.1000000000000076,\n",
      "        10.499999999999936, 10.500000000000028, -1.1999999999999726, 9.0, 7.200000000000008,\n",
      "        0.3000000000000149, -7.499999999999992, -17.400000000000013, 4.2000000000000295,\n",
      "        -10.499999999999984, 0.6000000000000291, -15.900000000000032, -5.999999999999982,\n",
      "        7.800000000000029]\n",
      "      policy_policy1_reward: [10.0, -21.5, -2.0, 17.0, -2.0, -0.5, -3.5, 8.0, -7.0,\n",
      "        26.5, -14.0, -26.0, 5.5, -14.5, 8.5, -2.5, -0.5, 10.0, 4.5, 15.5, 14.0, -21.5,\n",
      "        -20.0, 14.5, -0.5, -20.0, -0.5, -0.5, -26.5, 19.0, -4.0, -5.5, -11.5, 12.5,\n",
      "        -17.0, -14.0, 0.0, 4.0, 1.5, -7.5, 11.5, 19.5, 19.0, -3.5, 15.5, 16.5, 7.0,\n",
      "        22.0, -2.5, 21.5, 0.5, 3.5, 1.5, 10.5, 8.0, 2.5, 4.0, 16.5, 5.5, 1.0, -9.5,\n",
      "        -12.5, 23.0, 2.5, 22.0, 5.5, 20.5, 20.5, 5.5, 19.0, 15.0, 7.0, 2.5, -14.0, 12.0,\n",
      "        -0.5, 9.5, -7.0, 4.0, 14.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999984, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999991, -8.89999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -5.599999999999994, -5.599999999999993, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -5.599999999999997,\n",
      "        -8.899999999999986, -8.89999999999998, -5.599999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -4.499999999999983,\n",
      "        -7.799999999999981, -9.99999999999998, -7.799999999999981, -6.699999999999987,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999983, -6.6999999999999815,\n",
      "        -6.699999999999987, -8.899999999999986, -5.599999999999982, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -7.799999999999981, -2.3000000000000047,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999994,\n",
      "        -9.99999999999998, -6.699999999999993, -3.3999999999999835, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999987, -9.99999999999998, -7.799999999999988,\n",
      "        -6.699999999999987, -9.99999999999998, -3.3999999999999835, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -6.699999999999983]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: -2.3000000000000047\n",
      "    policy_reward_mean:\n",
      "      policy1: 3.09375\n",
      "      policy2: -8.542499999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08054306222961044\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0223058876682511\n",
      "      mean_inference_ms: 7.0893998601742725\n",
      "      mean_raw_obs_processing_ms: 0.4351101852791676\n",
      "  time_since_restore: 132.5353627204895\n",
      "  time_this_iter_s: 66.3074460029602\n",
      "  time_total_s: 132.5353627204895\n",
      "  timers:\n",
      "    learn_throughput: 112.936\n",
      "    learn_time_ms: 35418.245\n",
      "    synch_weights_time_ms: 3.491\n",
      "    training_iteration_time_ms: 66262.695\n",
      "  timestamp: 1660562143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:48 (running for 00:03:23.13)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |          121.496 | 9000 | -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |          123.237 | 9000 | -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |          117.663 | 8000 | -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 | 8000 | -5.44875 |         3.09375 |        -8.5425  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:53 (running for 00:03:28.18)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |          121.496 | 9000 | -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |          123.237 | 9000 | -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |          117.663 | 8000 | -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 | 8000 | -5.44875 |         3.09375 |        -8.5425  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:15:58 (running for 00:03:33.25)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=-1.766666666666662 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F86700>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997F54EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB81C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |   ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      3 |          121.496 | 9000 | -1.76667 |         6.76667 |        -8.53333 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      3 |          123.237 | 9000 | -4.60667 |         4.24444 |        -8.85111 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |          117.663 | 8000 | -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 | 8000 | -5.44875 |         3.09375 |        -8.5425  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.899999999999954\n",
      "  episode_reward_mean: -1.93799999999999\n",
      "  episode_reward_min: -32.40000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.237849473953247\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01905667409300804\n",
      "          model: {}\n",
      "          policy_loss: -0.05620993673801422\n",
      "          total_loss: 6.693049430847168\n",
      "          vf_explained_var: 0.07204411923885345\n",
      "          vf_loss: 6.743541717529297\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2353378534317017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020968880504369736\n",
      "          model: {}\n",
      "          policy_loss: -0.06133314594626427\n",
      "          total_loss: 2.781359910964966\n",
      "          vf_explained_var: 0.22228868305683136\n",
      "          vf_loss: 2.8384993076324463\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.2112676056338\n",
      "    ram_util_percent: 64.36197183098591\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -0.10000000000000109\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.665\n",
      "    policy2: -8.602999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -23.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07085304259896186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024195174333327615\n",
      "    mean_inference_ms: 5.5377799359504225\n",
      "    mean_raw_obs_processing_ms: 0.44207613190849626\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.899999999999954\n",
      "    episode_reward_mean: -1.93799999999999\n",
      "    episode_reward_min: -32.40000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-7.499999999999989, 1.4960255256823984e-14, -5.399999999999984,\n",
      "        -15.000000000000009, 5.100000000000019, -0.8999999999999838, -9.89999999999999,\n",
      "        4.200000000000022, 2.100000000000001, -3.8999999999999866, -11.69999999999997,\n",
      "        -32.40000000000007, -8.999999999999975, -2.999999999999996, -22.50000000000002,\n",
      "        -17.999999999999996, -1.4999999999999856, 7.200000000000031, 3.000000000000005,\n",
      "        2.099999999999996, 9.30000000000002, 0.6000000000000184, -7.499999999999974,\n",
      "        3.6000000000000214, -13.499999999999986, 2.1000000000000103, -3.899999999999985,\n",
      "        -1.5000000000000089, -9.599999999999982, 6.000000000000016, -8.999999999999996,\n",
      "        6.000000000000011, 5.100000000000014, -11.999999999999986, 2.5618396293225487e-14,\n",
      "        -1.7999999999999847, -1.4999999999999936, 1.5000000000000067, -3.5999999999999885,\n",
      "        3.6000000000000285, -7.199999999999994, -5.9999999999999805, 6.600000000000019,\n",
      "        1.5000000000000324, -13.499999999999988, -11.699999999999976, -3.2999999999999776,\n",
      "        8.399999999999975, -3.2999999999999865, 1.4999999999999976, 3.6000000000000103,\n",
      "        -10.199999999999989, 1.5000000000000155, -14.39999999999999, -14.999999999999991,\n",
      "        6.000000000000023, -25.500000000000007, 2.1000000000000023, 5.699999999999969,\n",
      "        -2.9999999999999782, -0.9000000000000121, -13.500000000000005, 9.000000000000027,\n",
      "        3.3000000000000282, -11.999999999999995, 12.000000000000028, 4.200000000000005,\n",
      "        7.499999999999995, -1.799999999999986, -12.899999999999983, -4.499999999999986,\n",
      "        7.500000000000027, -5.999999999999987, 8.999999999999963, -9.299999999999983,\n",
      "        8.100000000000032, 13.799999999999988, -4.499999999999977, -10.499999999999984,\n",
      "        15.000000000000021, 14.999999999999975, 4.500000000000012, -6.599999999999982,\n",
      "        8.100000000000026, 9.300000000000017, 3.299999999999991, 11.10000000000002,\n",
      "        -2.3999999999999835, -16.199999999999985, 7.500000000000012, 4.500000000000005,\n",
      "        3.0000000000000115, -11.999999999999988, -2.9999999999999853, 15.899999999999954,\n",
      "        -2.399999999999989, 2.100000000000019, -11.699999999999983, -17.99999999999998,\n",
      "        1.1296519275560968e-14]\n",
      "      policy_policy1_reward: [2.5, 10.0, 3.5, -5.0, 14.0, 8.0, -1.0, 12.0, 11.0, 5.0,\n",
      "        -5.0, -23.5, 1.0, 7.0, -12.5, -8.0, 8.5, 15.0, 13.0, 11.0, 16.0, 9.5, 2.5, 12.5,\n",
      "        -3.5, 11.0, 5.0, 8.5, -4.0, 16.0, 1.0, 16.0, 14.0, -2.0, 10.0, 6.0, 8.5, 11.5,\n",
      "        -3.5, 12.5, -0.5, 4.0, 15.5, 11.5, -3.5, -5.0, 4.5, 14.0, 4.5, 11.5, 7.0, -3.5,\n",
      "        11.5, -5.5, -5.0, 16.0, -15.5, 11.0, 13.5, 7.0, 8.0, -3.5, 19.0, 10.0, -2.0,\n",
      "        22.0, 12.0, 17.5, 6.0, -4.0, 5.5, 17.5, 4.0, 19.0, -1.5, 17.0, 15.0, 5.5, -0.5,\n",
      "        25.0, 25.0, 14.5, -1.0, 11.5, 10.5, 10.0, 20.0, 6.5, -9.5, 17.5, 14.5, 7.5,\n",
      "        -2.0, 7.0, 21.5, 6.5, 11.0, -5.0, -8.0, 10.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999986, -8.89999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -8.89999999999998, -6.699999999999982,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999983, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -0.10000000000000109,\n",
      "        -8.89999999999998, -6.699999999999994, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -7.799999999999981,\n",
      "        -5.599999999999982, -7.799999999999981, -9.99999999999998, -3.3999999999999857,\n",
      "        -6.6999999999999815, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -9.99999999999998, -7.79999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -1.1999999999999875, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -3.4000000000000044, -1.2000000000000048, -6.699999999999986, -8.89999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -4.499999999999982, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -8.89999999999998, -8.899999999999986, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -0.10000000000000109\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.665\n",
      "      policy2: -8.602999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -23.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07085304259896186\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024195174333327615\n",
      "      mean_inference_ms: 5.5377799359504225\n",
      "      mean_raw_obs_processing_ms: 0.44207613190849626\n",
      "  time_since_restore: 173.4684989452362\n",
      "  time_this_iter_s: 50.231123208999634\n",
      "  time_total_s: 173.4684989452362\n",
      "  timers:\n",
      "    learn_throughput: 128.812\n",
      "    learn_time_ms: 23289.814\n",
      "    synch_weights_time_ms: 3.241\n",
      "    training_iteration_time_ms: 43362.637\n",
      "  timestamp: 1660562160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.59999999999996\n",
      "  episode_reward_mean: 0.9960000000000052\n",
      "  episode_reward_min: -25.500000000000032\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.291355013847351\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010387702845036983\n",
      "          model: {}\n",
      "          policy_loss: -0.0375373400747776\n",
      "          total_loss: 6.467782974243164\n",
      "          vf_explained_var: 0.18747752904891968\n",
      "          vf_loss: 6.5032429695129395\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.287182331085205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011936833150684834\n",
      "          model: {}\n",
      "          policy_loss: -0.044002432376146317\n",
      "          total_loss: 1.8401789665222168\n",
      "          vf_explained_var: 0.3437495529651642\n",
      "          vf_loss: 1.8817940950393677\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.24788732394366\n",
      "    ram_util_percent: 64.36197183098591\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 33.5\n",
      "    policy2: 3.2000000000000117\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.335\n",
      "    policy2: -8.338999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07616713904444071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02528807388190146\n",
      "    mean_inference_ms: 5.284470010793085\n",
      "    mean_raw_obs_processing_ms: 0.44231636995847196\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.59999999999996\n",
      "    episode_reward_mean: 0.9960000000000052\n",
      "    episode_reward_min: -25.500000000000032\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-25.500000000000032, 22.499999999999922, -1.4999999999999976,\n",
      "        -21.900000000000055, -5.999999999999988, -3.599999999999999, 14.699999999999962,\n",
      "        -6.599999999999982, 3.000000000000028, 5.1000000000000245, 3.000000000000014,\n",
      "        -5.9999999999999964, -7.500000000000005, 6.600000000000001, -2.0999999999999748,\n",
      "        18.599999999999945, -16.50000000000001, 1.2000000000000226, -16.499999999999996,\n",
      "        -5.999999999999982, -3.3000000000000003, 10.50000000000001, 10.50000000000003,\n",
      "        -2.9999999999999902, -7.799999999999984, 4.500000000000016, -0.8999999999999801,\n",
      "        -16.499999999999986, 5.4, 8.699999999999987, -4.499999999999984, -10.499999999999995,\n",
      "        0.30000000000001514, 9.000000000000025, 4.50000000000003, 5.100000000000026,\n",
      "        -2.0999999999999814, -9.299999999999976, 17.99999999999998, -4.499999999999993,\n",
      "        7.299716386910404e-15, 9.59999999999998, -8.399999999999979, 1.8000000000000203,\n",
      "        -2.3999999999999844, -4.499999999999982, -2.9999999999999747, -11.999999999999982,\n",
      "        -2.6999999999999758, 17.99999999999991, 16.500000000000007, -12.899999999999988,\n",
      "        4.500000000000014, -0.8999999999999881, -5.999999999999989, 3.000000000000033,\n",
      "        17.69999999999996, -1.7999999999999874, 9.000000000000025, -9.599999999999977,\n",
      "        1.8000000000000091, 4.500000000000018, 13.499999999999936, 5.10000000000003,\n",
      "        4.500000000000023, -16.199999999999996, 2.6999999999999957, 3.000000000000005,\n",
      "        -1.1999999999999913, 1.5000000000000133, 9.000000000000027, 7.5000000000000195,\n",
      "        9.90000000000002, 1.1296519275560968e-14, 0.30000000000002536, -3.000000000000035,\n",
      "        7.200000000000001, -13.499999999999986, -1.1999999999999762, -3.899999999999987,\n",
      "        -3.2999999999999794, 3.3000000000000176, -1.7999999999999798, 16.199999999999903,\n",
      "        -14.999999999999988, -2.9999999999999885, 8.099999999999989, -8.999999999999982,\n",
      "        12.000000000000007, 2.273181642920008e-14, 24.59999999999996, 11.999999999999991,\n",
      "        10.500000000000004, -2.399999999999987, 0.6000000000000202, 1.2000000000000064,\n",
      "        11.100000000000032, 10.2, 4.799999999999992, 3.0000000000000053]\n",
      "      policy_policy1_reward: [-15.5, 32.5, 8.5, -13.0, 4.0, 2.0, 22.5, -6.5, 13.0, 14.0,\n",
      "        13.0, 4.0, 2.5, 15.5, 3.5, 27.5, -6.5, 9.0, -6.5, 4.0, 4.5, 20.5, 15.0, 7.0,\n",
      "        0.0, 9.0, 8.0, -6.5, 11.0, 5.5, 5.5, -0.5, 7.0, 19.0, 14.5, 14.0, 3.5, -1.5,\n",
      "        28.0, 5.5, 10.0, 18.5, 0.5, 8.5, 6.5, 5.5, 7.0, -2.0, 4.0, 28.0, 26.5, -4.0,\n",
      "        14.5, 2.5, -1.5, 13.0, 25.5, 6.0, 19.0, -4.0, 8.5, 14.5, 23.5, 14.0, 14.5, -9.5,\n",
      "        10.5, 13.0, 5.5, 11.5, 19.0, 17.5, 15.5, 10.0, 7.0, 7.0, 15.0, -3.5, 5.5, -0.5,\n",
      "        4.5, 10.0, 6.0, 24.0, -5.0, 7.0, 17.0, 1.0, 16.5, 10.0, 33.5, 22.0, 20.5, 6.5,\n",
      "        9.5, 9.0, 20.0, 18.0, 11.5, 13.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999984, -7.799999999999981,\n",
      "        -0.09999999999999634, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -5.599999999999985,\n",
      "        -8.899999999999984, -9.99999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -4.5, -9.99999999999998,\n",
      "        -7.7999999999999865, -4.499999999999992, -8.89999999999998, -9.99999999999998,\n",
      "        -5.599999999999998, 3.2000000000000117, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999982, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -5.599999999999987, -7.799999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, -8.899999999999983, -6.6999999999999815,\n",
      "        -8.899999999999983, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999895, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -3.4000000000000017, -4.500000000000003, -9.99999999999998,\n",
      "        -7.799999999999988, -7.799999999999983, -9.99999999999998, -5.59999999999999,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -6.69999999999999, -7.79999999999999, -9.99999999999998,\n",
      "        -6.699999999999993, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -9.99999999999998, -6.699999999999984, -9.99999999999998,\n",
      "        -7.799999999999989, -9.99999999999998, -6.69999999999999, -3.400000000000004,\n",
      "        -7.7999999999999865, -6.699999999999991, -7.79999999999999, -7.7999999999999865,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -4.499999999999982, -9.99999999999998, -8.899999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -7.7999999999999865,\n",
      "        -8.89999999999998, -7.79999999999999, -6.699999999999995, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.5\n",
      "      policy2: 3.2000000000000117\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.335\n",
      "      policy2: -8.338999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -15.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07616713904444071\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02528807388190146\n",
      "      mean_inference_ms: 5.284470010793085\n",
      "      mean_raw_obs_processing_ms: 0.44231636995847196\n",
      "  time_since_restore: 171.28425860404968\n",
      "  time_this_iter_s: 49.78830814361572\n",
      "  time_total_s: 171.28425860404968\n",
      "  timers:\n",
      "    learn_throughput: 129.085\n",
      "    learn_time_ms: 23240.543\n",
      "    synch_weights_time_ms: 3.242\n",
      "    training_iteration_time_ms: 42812.836\n",
      "  timestamp: 1660562160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:06 (running for 00:03:40.53)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |        -8.339   |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |        -8.603   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |          117.663 |  8000 | -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |        -8.5425  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:11 (running for 00:03:45.65)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |        -8.339   |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |        -8.603   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |          117.663 |  8000 | -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |        -8.5425  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:16 (running for 00:03:50.71)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |        -8.339   |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |        -8.603   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      2 |          117.663 |  8000 | -6.07875 |         2.5875  |        -8.66625 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |        -8.5425  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999998\n",
      "  episode_reward_mean: -2.765999999999991\n",
      "  episode_reward_min: -28.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3103502988815308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010615496896207333\n",
      "          model: {}\n",
      "          policy_loss: -0.03033759631216526\n",
      "          total_loss: 7.081586837768555\n",
      "          vf_explained_var: -0.05958202853798866\n",
      "          vf_loss: 7.109801292419434\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3168665170669556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010594990104436874\n",
      "          model: {}\n",
      "          policy_loss: -0.034776464104652405\n",
      "          total_loss: 1.913361668586731\n",
      "          vf_explained_var: 0.3551611602306366\n",
      "          vf_loss: 1.9460192918777466\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.30957446808511\n",
      "    ram_util_percent: 64.32978723404256\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: -2.2999999999999954\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.87\n",
      "    policy2: -8.635999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07979978965671455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024282000201537635\n",
      "    mean_inference_ms: 5.076661823009217\n",
      "    mean_raw_obs_processing_ms: 0.43279564402607074\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.59999999999998\n",
      "    episode_reward_mean: -2.765999999999991\n",
      "    episode_reward_min: -28.500000000000064\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-28.500000000000064, -10.799999999999974, 2.100000000000003,\n",
      "        -10.49999999999999, -14.999999999999998, -8.999999999999998, -19.499999999999996,\n",
      "        -3.5999999999999974, -23.400000000000013, -19.500000000000043, -24.000000000000014,\n",
      "        10.500000000000027, -24.0, -16.50000000000002, 11.700000000000008, -8.39999999999998,\n",
      "        -27.00000000000002, -5.3999999999999995, -11.999999999999982, -8.399999999999988,\n",
      "        -1.4999999999999933, 10.200000000000015, 5.4000000000000306, 7.5000000000000195,\n",
      "        6.000000000000007, 6.000000000000021, 1.2000000000000082, 0.3000000000000116,\n",
      "        -4.499999999999988, -1.4999999999999882, -7.79999999999999, -16.49999999999999,\n",
      "        18.59999999999998, -2.0999999999999996, -8.99999999999998, -20.69999999999999,\n",
      "        -5.4, -0.8999999999999893, 5.100000000000023, 5.999999999999989, 2.7000000000000317,\n",
      "        4.199999999999989, -6.299999999999995, 7.500000000000021, 0.6000000000000103,\n",
      "        15.299999999999937, -18.299999999999983, 13.19999999999998, 4.500000000000011,\n",
      "        -2.999999999999992, -2.9999999999999982, 6.600000000000033, -11.999999999999984,\n",
      "        -1.4999999999999902, -8.999999999999972, -18.299999999999983, 1.5000000000000173,\n",
      "        -4.499999999999989, 1.529332216421153e-14, -2.099999999999994, -7.499999999999989,\n",
      "        5.100000000000003, -19.8, -1.4999999999999756, 8.631984016460592e-15, 13.499999999999952,\n",
      "        -6.59999999999998, 14.100000000000003, 10.500000000000004, -2.0999999999999766,\n",
      "        10.200000000000024, 7.500000000000034, 3.0000000000000275, 1.5000000000000289,\n",
      "        1.2000000000000235, -7.499999999999973, -11.39999999999998, 9.000000000000023,\n",
      "        9.300000000000024, 1.2000000000000077, -2.5118795932144167e-14, -2.9999999999999973,\n",
      "        5.1000000000000245, -7.7999999999999865, -10.199999999999976, 0.30000000000002225,\n",
      "        5.100000000000032, 1.7291723608536813e-14, -11.999999999999972, -3.8999999999999795,\n",
      "        -11.999999999999984, 7.200000000000026, -1.4999999999999818, 4.5000000000000115,\n",
      "        -3.5999999999999917, -5.999999999999995, 15.000000000000028, 6.000000000000034,\n",
      "        -5.39999999999999, -17.4]\n",
      "      policy_policy1_reward: [-18.5, -3.0, 11.0, -0.5, -5.0, 1.0, -9.5, 2.0, -14.5,\n",
      "        -9.5, -14.0, 20.5, -14.0, -6.5, 19.5, 0.5, -17.0, 3.5, -2.0, 0.5, 8.5, 18.0,\n",
      "        11.0, 17.5, 16.0, 16.0, 3.5, 7.0, 5.5, 8.5, 0.0, -6.5, 27.5, 3.5, 1.0, -14.0,\n",
      "        3.5, 8.0, 14.0, 10.5, 10.5, 12.0, 1.5, 17.5, 9.5, 22.0, -16.0, 21.0, 14.5, 7.0,\n",
      "        7.0, 15.5, -2.0, 8.5, 1.0, -10.5, 11.5, 5.5, 10.0, 3.5, 2.5, 14.0, -12.0, 8.5,\n",
      "        10.0, 23.5, -1.0, 23.0, 20.5, 3.5, 18.0, 17.5, 13.0, 11.5, 9.0, 2.5, -2.5, 19.0,\n",
      "        16.0, 3.5, 10.0, 7.0, 14.0, 0.0, -3.5, 7.0, 14.0, 10.0, -2.0, 5.0, -2.0, 15.0,\n",
      "        8.5, 14.5, 2.0, 4.0, 25.0, 16.0, 3.5, -14.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -7.799999999999981, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -5.599999999999991,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -2.2999999999999954,\n",
      "        -6.699999999999994, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -5.599999999999997, -9.99999999999998,\n",
      "        -6.699999999999994, -8.89999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -4.500000000000001, -7.79999999999999, -7.79999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -8.899999999999983, -6.6999999999999815, -2.3, -7.79999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.6, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.6, -8.89999999999998, -9.99999999999998, -5.599999999999992,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999983, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -6.699999999999995, -2.2999999999999994, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999983, -6.6999999999999895, -6.699999999999994,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -3.4]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: -2.2999999999999954\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.87\n",
      "      policy2: -8.635999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07979978965671455\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024282000201537635\n",
      "      mean_inference_ms: 5.076661823009217\n",
      "      mean_raw_obs_processing_ms: 0.43279564402607074\n",
      "  time_since_restore: 183.99533581733704\n",
      "  time_this_iter_s: 66.3320472240448\n",
      "  time_total_s: 183.99533581733704\n",
      "  timers:\n",
      "    learn_throughput: 111.403\n",
      "    learn_time_ms: 35905.754\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 61320.808\n",
      "  timestamp: 1660562179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:24 (running for 00:03:58.75)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |         -8.339  |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |         -8.603  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 | -2.766   |         5.87    |         -8.636  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |         -8.5425 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:29 (running for 00:04:03.81)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |         -8.339  |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |         -8.603  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 | -2.766   |         5.87    |         -8.636  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |         -8.5425 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:34 (running for 00:04:08.88)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |         -8.339  |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |         -8.603  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 | -2.766   |         5.87    |         -8.636  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |         -8.5425 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:39 (running for 00:04:13.94)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |         -8.339  |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |         -8.603  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 | -2.766   |         5.87    |         -8.636  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |         -8.5425 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:44 (running for 00:04:19.02)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |         -8.339  |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |         -8.603  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 | -2.766   |         5.87    |         -8.636  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |         -8.5425 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:49 (running for 00:04:24.08)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=0.9960000000000052 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980027F0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980021F0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD69A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FD6880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037F70>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      4 |          171.284 | 12000 |  0.996   |         9.335   |         -8.339  |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      4 |          173.468 | 12000 | -1.938   |         6.665   |         -8.603  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 | -2.766   |         5.87    |         -8.636  |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      2 |          132.535 |  8000 | -5.44875 |         3.09375 |         -8.5425 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-16-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.999999999999908\n",
      "  episode_reward_mean: -2.405999999999993\n",
      "  episode_reward_min: -35.40000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2885066270828247\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018023841083049774\n",
      "          model: {}\n",
      "          policy_loss: -0.046217553317546844\n",
      "          total_loss: 6.708960056304932\n",
      "          vf_explained_var: 0.12299440056085587\n",
      "          vf_loss: 6.749770164489746\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2843635082244873\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02013770304620266\n",
      "          model: {}\n",
      "          policy_loss: -0.050635043531656265\n",
      "          total_loss: 2.347994565963745\n",
      "          vf_explained_var: 0.3080383837223053\n",
      "          vf_loss: 2.3946022987365723\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.272340425531908\n",
      "    ram_util_percent: 64.16276595744682\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 26.0\n",
      "    policy2: 0.9999999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.625\n",
      "    policy2: -8.030999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07844476025681432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02219313128086496\n",
      "    mean_inference_ms: 7.1301289376276165\n",
      "    mean_raw_obs_processing_ms: 0.43112926806150575\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 26.999999999999908\n",
      "    episode_reward_mean: -2.405999999999993\n",
      "    episode_reward_min: -35.40000000000005\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [8.400000000000023, -31.500000000000025, -30.00000000000003, 4.500000000000032,\n",
      "        -10.49999999999999, -30.000000000000057, -10.499999999999988, -10.49999999999999,\n",
      "        -35.40000000000005, 9.00000000000001, -9.599999999999982, -14.400000000000002,\n",
      "        -20.400000000000034, 6.9000000000000234, -27.00000000000001, -24.000000000000018,\n",
      "        -7.799999999999976, -5.9999999999999964, -2.9999999999999813, -15.299999999999995,\n",
      "        1.5000000000000149, 11.700000000000015, 12.299999999999985, -13.499999999999998,\n",
      "        6.599999999999994, 8.700000000000015, 0.2999999999999978, 15.299999999999995,\n",
      "        -11.399999999999991, 15.900000000000007, -8.399999999999986, -5.399999999999999,\n",
      "        -6.299999999999983, 2.700000000000013, 5.700000000000019, -7.4999999999999805,\n",
      "        -5.99999999999998, 8.700000000000022, -4.5, -8.999999999999986, -19.5, -22.5,\n",
      "        17.399999999999977, -7.499999999999986, 15.299999999999986, 2.1000000000000076,\n",
      "        10.499999999999936, 10.500000000000028, -1.1999999999999726, 9.0, 7.200000000000008,\n",
      "        0.3000000000000149, -7.499999999999992, -17.400000000000013, 4.2000000000000295,\n",
      "        -10.499999999999984, 0.6000000000000291, -15.900000000000032, -5.999999999999982,\n",
      "        7.800000000000029, -5.399999999999997, -0.8999999999999937, -3.8999999999999857,\n",
      "        -10.199999999999983, 10.200000000000022, 5.190292640122607e-15, 26.999999999999908,\n",
      "        1.0963452368173421e-14, -1.5000000000000004, 4.200000000000018, -6.299999999999981,\n",
      "        -5.399999999999991, 12.000000000000002, -14.99999999999998, 11.400000000000025,\n",
      "        0.600000000000022, 3.0000000000000058, -10.499999999999993, -4.199999999999979,\n",
      "        -5.9999999999999885, -13.199999999999982, 4.500000000000009, 1.5000000000000226,\n",
      "        4.199999999999999, -4.499999999999974, -11.999999999999995, 4.500000000000028,\n",
      "        15.000000000000025, -3.59999999999998, 2.4000000000000212, 1.2000000000000006,\n",
      "        14.700000000000015, -0.8999999999999878, 6.3000000000000185, -8.699999999999994,\n",
      "        -3.299999999999985, -1.4999999999999845, 14.399999999999995, -3.5999999999999814,\n",
      "        5.700000000000024]\n",
      "      policy_policy1_reward: [14.0, -21.5, -20.0, 14.5, -0.5, -20.0, -0.5, -0.5, -26.5,\n",
      "        19.0, -4.0, -5.5, -11.5, 12.5, -17.0, -14.0, 0.0, 4.0, 1.5, -7.5, 11.5, 19.5,\n",
      "        19.0, -3.5, 15.5, 16.5, 7.0, 22.0, -2.5, 21.5, 0.5, 3.5, 1.5, 10.5, 8.0, 2.5,\n",
      "        4.0, 16.5, 5.5, 1.0, -9.5, -12.5, 23.0, 2.5, 22.0, 5.5, 20.5, 20.5, 5.5, 19.0,\n",
      "        15.0, 7.0, 2.5, -14.0, 12.0, -0.5, 9.5, -7.0, 4.0, 14.5, 3.5, 8.0, -0.5, -3.5,\n",
      "        12.5, 4.5, 26.0, 10.0, 8.5, 12.0, -4.0, 3.5, 22.0, -5.0, 17.0, 9.5, 13.0, -0.5,\n",
      "        2.5, 4.0, -6.5, 14.5, 11.5, 12.0, 5.5, -2.0, 14.5, 25.0, 2.0, 8.0, 9.0, 22.5,\n",
      "        8.0, 13.0, -2.0, 4.5, 8.5, 20.0, 2.0, 13.5]\n",
      "      policy_policy2_reward: [-5.599999999999993, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -5.599999999999997,\n",
      "        -8.899999999999986, -8.89999999999998, -5.599999999999983, -9.99999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -4.499999999999983,\n",
      "        -7.799999999999981, -9.99999999999998, -7.799999999999981, -6.699999999999987,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999983, -6.6999999999999815,\n",
      "        -6.699999999999987, -8.899999999999986, -5.599999999999982, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -7.799999999999981, -2.3000000000000047,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999994,\n",
      "        -9.99999999999998, -6.699999999999993, -3.3999999999999835, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999987, -9.99999999999998, -7.799999999999988,\n",
      "        -6.699999999999987, -9.99999999999998, -3.3999999999999835, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -6.699999999999983, -8.899999999999986, -8.89999999999998, -3.4000000000000052,\n",
      "        -6.699999999999987, -2.300000000000003, -4.499999999999999, 0.9999999999999974,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -2.299999999999994,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999984,\n",
      "        -9.99999999999998, -6.699999999999995, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -5.599999999999984, -7.7999999999999865,\n",
      "        -7.799999999999981, -8.899999999999986, -6.699999999999986, -6.699999999999993,\n",
      "        -7.79999999999999, -9.99999999999998, -5.599999999999982, -5.599999999999997,\n",
      "        -7.799999999999989]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.0\n",
      "      policy2: 0.9999999999999974\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.625\n",
      "      policy2: -8.030999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07844476025681432\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02219313128086496\n",
      "      mean_inference_ms: 7.1301289376276165\n",
      "      mean_raw_obs_processing_ms: 0.43112926806150575\n",
      "  time_since_restore: 199.3949990272522\n",
      "  time_this_iter_s: 66.8596363067627\n",
      "  time_total_s: 199.3949990272522\n",
      "  timers:\n",
      "    learn_throughput: 112.207\n",
      "    learn_time_ms: 35648.297\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 66461.011\n",
      "  timestamp: 1660562210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-16-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999998\n",
      "  episode_reward_mean: -0.578999999999992\n",
      "  episode_reward_min: -25.500000000000007\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2043787240982056\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019742904230952263\n",
      "          model: {}\n",
      "          policy_loss: -0.056391842663288116\n",
      "          total_loss: 6.829800128936768\n",
      "          vf_explained_var: 0.11571041494607925\n",
      "          vf_loss: 6.880269527435303\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1837486028671265\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020968707278370857\n",
      "          model: {}\n",
      "          policy_loss: -0.05846991017460823\n",
      "          total_loss: 2.8543548583984375\n",
      "          vf_explained_var: 0.10148607939481735\n",
      "          vf_loss: 2.906534194946289\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.530985915492963\n",
      "    ram_util_percent: 64.13943661971832\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: 4.300000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.21\n",
      "    policy2: -7.788999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07125111728478015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02367322715989755\n",
      "    mean_inference_ms: 5.997707172547936\n",
      "    mean_raw_obs_processing_ms: 0.44189221515386845\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 18.59999999999998\n",
      "    episode_reward_mean: -0.578999999999992\n",
      "    episode_reward_min: -25.500000000000007\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.999999999999996, 6.000000000000011, 5.100000000000014, -11.999999999999986,\n",
      "        2.5618396293225487e-14, -1.7999999999999847, -1.4999999999999936, 1.5000000000000067,\n",
      "        -3.5999999999999885, 3.6000000000000285, -7.199999999999994, -5.9999999999999805,\n",
      "        6.600000000000019, 1.5000000000000324, -13.499999999999988, -11.699999999999976,\n",
      "        -3.2999999999999776, 8.399999999999975, -3.2999999999999865, 1.4999999999999976,\n",
      "        3.6000000000000103, -10.199999999999989, 1.5000000000000155, -14.39999999999999,\n",
      "        -14.999999999999991, 6.000000000000023, -25.500000000000007, 2.1000000000000023,\n",
      "        5.699999999999969, -2.9999999999999782, -0.9000000000000121, -13.500000000000005,\n",
      "        9.000000000000027, 3.3000000000000282, -11.999999999999995, 12.000000000000028,\n",
      "        4.200000000000005, 7.499999999999995, -1.799999999999986, -12.899999999999983,\n",
      "        -4.499999999999986, 7.500000000000027, -5.999999999999987, 8.999999999999963,\n",
      "        -9.299999999999983, 8.100000000000032, 13.799999999999988, -4.499999999999977,\n",
      "        -10.499999999999984, 15.000000000000021, 14.999999999999975, 4.500000000000012,\n",
      "        -6.599999999999982, 8.100000000000026, 9.300000000000017, 3.299999999999991,\n",
      "        11.10000000000002, -2.3999999999999835, -16.199999999999985, 7.500000000000012,\n",
      "        4.500000000000005, 3.0000000000000115, -11.999999999999988, -2.9999999999999853,\n",
      "        15.899999999999954, -2.399999999999989, 2.100000000000019, -11.699999999999983,\n",
      "        -17.99999999999998, 1.1296519275560968e-14, -13.499999999999993, 15.59999999999994,\n",
      "        18.000000000000025, -2.699999999999992, -18.599999999999994, -17.999999999999982,\n",
      "        -0.5999999999999958, 10.199999999999964, 5.099999999999984, 1.2000000000000273,\n",
      "        4.799999999999997, -1.4999999999999827, 13.199999999999964, -8.99999999999998,\n",
      "        3.3000000000000194, 14.699999999999937, 2.7000000000000086, 18.59999999999998,\n",
      "        10.500000000000028, -7.199999999999976, -16.19999999999998, -2.700000000000003,\n",
      "        1.5000000000000233, 11.100000000000017, -11.699999999999992, -9.299999999999992,\n",
      "        5.700000000000015, -10.49999999999999, -10.499999999999977, 0.3000000000000014]\n",
      "      policy_policy1_reward: [1.0, 16.0, 14.0, -2.0, 10.0, 6.0, 8.5, 11.5, -3.5, 12.5,\n",
      "        -0.5, 4.0, 15.5, 11.5, -3.5, -5.0, 4.5, 14.0, 4.5, 11.5, 7.0, -3.5, 11.5, -5.5,\n",
      "        -5.0, 16.0, -15.5, 11.0, 13.5, 7.0, 8.0, -3.5, 19.0, 10.0, -2.0, 22.0, 12.0,\n",
      "        17.5, 6.0, -4.0, 5.5, 17.5, 4.0, 19.0, -1.5, 17.0, 15.0, 5.5, -0.5, 25.0, 25.0,\n",
      "        14.5, -1.0, 11.5, 10.5, 10.0, 20.0, 6.5, -9.5, 17.5, 14.5, 7.5, -2.0, 7.0, 21.5,\n",
      "        6.5, 11.0, -5.0, -8.0, 10.0, -3.5, 19.0, 28.0, -7.0, -13.0, -8.0, 5.0, 12.5,\n",
      "        14.0, 9.0, 11.5, 8.5, 21.0, 1.0, 10.0, 22.5, -0.5, 16.5, 20.5, -0.5, -15.0,\n",
      "        4.0, 11.5, 20.0, -5.0, -1.5, 8.0, -0.5, -0.5, 7.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -0.10000000000000109, -8.89999999999998, -6.699999999999994,\n",
      "        -9.99999999999998, -8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -7.799999999999981, -5.599999999999982, -7.799999999999981,\n",
      "        -9.99999999999998, -3.3999999999999857, -6.6999999999999815, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.79999999999999, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -7.7999999999999865, -9.99999999999998, -7.79999999999999,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -8.89999999999998, -1.1999999999999875,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999998, -3.4000000000000044, -1.2000000000000048,\n",
      "        -6.699999999999986, -8.89999999999998, -8.89999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -8.89999999999998, -8.899999999999986,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -3.399999999999995, -9.99999999999998, 4.300000000000008, -5.599999999999997,\n",
      "        -9.99999999999998, -5.599999999999998, -2.3, -8.89999999999998, -7.7999999999999865,\n",
      "        -6.699999999999988, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -6.699999999999994, -7.79999999999999, 3.200000000000011, 2.100000000000002,\n",
      "        -9.99999999999998, -6.6999999999999815, -1.2000000000000026, -6.699999999999995,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999995, -7.799999999999983,\n",
      "        -2.3000000000000034, -9.99999999999998, -9.99999999999998, -6.699999999999988]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 28.0\n",
      "      policy2: 4.300000000000008\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.21\n",
      "      policy2: -7.788999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -15.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07125111728478015\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02367322715989755\n",
      "      mean_inference_ms: 5.997707172547936\n",
      "      mean_raw_obs_processing_ms: 0.44189221515386845\n",
      "  time_since_restore: 223.10969305038452\n",
      "  time_this_iter_s: 49.641194105148315\n",
      "  time_total_s: 223.10969305038452\n",
      "  timers:\n",
      "    learn_throughput: 124.482\n",
      "    learn_time_ms: 24099.823\n",
      "    synch_weights_time_ms: 3.191\n",
      "    training_iteration_time_ms: 44617.151\n",
      "  timestamp: 1660562210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-16-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.59999999999996\n",
      "  episode_reward_mean: 1.2360000000000093\n",
      "  episode_reward_min: -17.99999999999998\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2527241706848145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009681749157607555\n",
      "          model: {}\n",
      "          policy_loss: -0.04170193895697594\n",
      "          total_loss: 6.677282810211182\n",
      "          vf_explained_var: 0.11032506823539734\n",
      "          vf_loss: 6.717047691345215\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.266761064529419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011997257359325886\n",
      "          model: {}\n",
      "          policy_loss: -0.04639517515897751\n",
      "          total_loss: 2.5352590084075928\n",
      "          vf_explained_var: 0.26555943489074707\n",
      "          vf_loss: 2.5792548656463623\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_env_steps_sampled: 15000\n",
      "    num_env_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30000\n",
      "  num_agent_steps_trained: 30000\n",
      "  num_env_steps_sampled: 15000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 15000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.50714285714285\n",
      "    ram_util_percent: 64.1357142857143\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 33.5\n",
      "    policy2: -2.2999999999999834\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.63\n",
      "    policy2: -8.393999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07535165043395224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02491754586515609\n",
      "    mean_inference_ms: 5.824159302596626\n",
      "    mean_raw_obs_processing_ms: 0.44114722002306017\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.59999999999996\n",
      "    episode_reward_mean: 1.2360000000000093\n",
      "    episode_reward_min: -17.99999999999998\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-4.499999999999984, -10.499999999999995, 0.30000000000001514,\n",
      "        9.000000000000025, 4.50000000000003, 5.100000000000026, -2.0999999999999814,\n",
      "        -9.299999999999976, 17.99999999999998, -4.499999999999993, 7.299716386910404e-15,\n",
      "        9.59999999999998, -8.399999999999979, 1.8000000000000203, -2.3999999999999844,\n",
      "        -4.499999999999982, -2.9999999999999747, -11.999999999999982, -2.6999999999999758,\n",
      "        17.99999999999991, 16.500000000000007, -12.899999999999988, 4.500000000000014,\n",
      "        -0.8999999999999881, -5.999999999999989, 3.000000000000033, 17.69999999999996,\n",
      "        -1.7999999999999874, 9.000000000000025, -9.599999999999977, 1.8000000000000091,\n",
      "        4.500000000000018, 13.499999999999936, 5.10000000000003, 4.500000000000023,\n",
      "        -16.199999999999996, 2.6999999999999957, 3.000000000000005, -1.1999999999999913,\n",
      "        1.5000000000000133, 9.000000000000027, 7.5000000000000195, 9.90000000000002,\n",
      "        1.1296519275560968e-14, 0.30000000000002536, -3.000000000000035, 7.200000000000001,\n",
      "        -13.499999999999986, -1.1999999999999762, -3.899999999999987, -3.2999999999999794,\n",
      "        3.3000000000000176, -1.7999999999999798, 16.199999999999903, -14.999999999999988,\n",
      "        -2.9999999999999885, 8.099999999999989, -8.999999999999982, 12.000000000000007,\n",
      "        2.273181642920008e-14, 24.59999999999996, 11.999999999999991, 10.500000000000004,\n",
      "        -2.399999999999987, 0.6000000000000202, 1.2000000000000064, 11.100000000000032,\n",
      "        10.2, 4.799999999999992, 3.0000000000000053, -8.4, -7.499999999999975, 3.000000000000024,\n",
      "        -14.999999999999995, 5.100000000000023, 2.095545958979983e-14, 1.5000000000000262,\n",
      "        7.800000000000033, -6.0, 7.500000000000012, -17.99999999999998, 4.200000000000019,\n",
      "        -1.4999999999999953, -2.3999999999999915, 15.000000000000034, -0.5999999999999861,\n",
      "        -2.3999999999999795, -4.199999999999976, -1.4999999999999911, -11.999999999999973,\n",
      "        -11.999999999999986, 6.300000000000024, 20.999999999999993, 3.000000000000023,\n",
      "        -7.5, 5.100000000000028, -9.29999999999999, 4.199999999999978, 16.199999999999964,\n",
      "        -4.499999999999979]\n",
      "      policy_policy1_reward: [5.5, -0.5, 7.0, 19.0, 14.5, 14.0, 3.5, -1.5, 28.0, 5.5,\n",
      "        10.0, 18.5, 0.5, 8.5, 6.5, 5.5, 7.0, -2.0, 4.0, 28.0, 26.5, -4.0, 14.5, 2.5,\n",
      "        -1.5, 13.0, 25.5, 6.0, 19.0, -4.0, 8.5, 14.5, 23.5, 14.0, 14.5, -9.5, 10.5,\n",
      "        13.0, 5.5, 11.5, 19.0, 17.5, 15.5, 10.0, 7.0, 7.0, 15.0, -3.5, 5.5, -0.5, 4.5,\n",
      "        10.0, 6.0, 24.0, -5.0, 7.0, 17.0, 1.0, 16.5, 10.0, 33.5, 22.0, 20.5, 6.5, 9.5,\n",
      "        9.0, 20.0, 18.0, 11.5, 13.0, 0.5, 2.5, 13.0, -5.0, 8.5, 10.0, 11.5, 14.5, 4.0,\n",
      "        17.5, -8.0, 6.5, 8.5, 6.5, 25.0, 5.0, 6.5, 2.5, 3.0, -2.0, -7.5, 13.0, 31.0,\n",
      "        13.0, 2.5, 14.0, -1.5, 6.5, 24.0, 5.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -6.699999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -5.599999999999987,\n",
      "        -7.799999999999982, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -8.899999999999983, -6.6999999999999815, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.6999999999999895,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -3.4000000000000017, -4.500000000000003, -9.99999999999998, -7.799999999999988,\n",
      "        -7.799999999999983, -9.99999999999998, -5.59999999999999, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -6.69999999999999, -7.79999999999999, -9.99999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -6.699999999999984, -9.99999999999998, -7.799999999999989,\n",
      "        -9.99999999999998, -6.69999999999999, -3.400000000000004, -7.7999999999999865,\n",
      "        -6.699999999999991, -7.79999999999999, -7.7999999999999865, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -4.499999999999982,\n",
      "        -9.99999999999998, -8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.7999999999999865, -8.89999999999998,\n",
      "        -7.79999999999999, -6.699999999999995, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -3.3999999999999915,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -2.2999999999999985, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999982, -8.89999999999998,\n",
      "        -6.69999999999999, -4.500000000000003, -9.99999999999998, -4.5, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -7.799999999999981, -2.2999999999999834, -7.799999999999989, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.5\n",
      "      policy2: -2.2999999999999834\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.63\n",
      "      policy2: -8.393999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -9.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07535165043395224\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02491754586515609\n",
      "      mean_inference_ms: 5.824159302596626\n",
      "      mean_raw_obs_processing_ms: 0.44114722002306017\n",
      "  time_since_restore: 220.91448140144348\n",
      "  time_this_iter_s: 49.6302227973938\n",
      "  time_total_s: 220.91448140144348\n",
      "  timers:\n",
      "    learn_throughput: 124.686\n",
      "    learn_time_ms: 24060.406\n",
      "    synch_weights_time_ms: 3.192\n",
      "    training_iteration_time_ms: 44174.917\n",
      "  timestamp: 1660562210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:16:55 (running for 00:04:30.31)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 |   -2.766 |           5.87  |          -8.636 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:00 (running for 00:04:35.36)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 |   -2.766 |           5.87  |          -8.636 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:05 (running for 00:04:40.43)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 |   -2.766 |           5.87  |          -8.636 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:11 (running for 00:04:45.49)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 |   -2.766 |           5.87  |          -8.636 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:16 (running for 00:04:50.57)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 |   -2.766 |           5.87  |          -8.636 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:21 (running for 00:04:55.63)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      3 |          183.995 | 12000 |   -2.766 |           5.87  |          -8.636 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-17-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.299999999999937\n",
      "  episode_reward_mean: -0.7559999999999867\n",
      "  episode_reward_min: -19.8\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.296664834022522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008727996610105038\n",
      "          model: {}\n",
      "          policy_loss: -0.0295390784740448\n",
      "          total_loss: 6.89946174621582\n",
      "          vf_explained_var: -0.04526242986321449\n",
      "          vf_loss: 6.927255153656006\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2743135690689087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01259362418204546\n",
      "          model: {}\n",
      "          policy_loss: -0.03942999988794327\n",
      "          total_loss: 2.6158714294433594\n",
      "          vf_explained_var: 0.18790312111377716\n",
      "          vf_loss: 2.652782678604126\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.507608695652177\n",
      "    ram_util_percent: 64.00869565217388\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -0.10000000000000309\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.33\n",
      "    policy2: -8.085999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07850462288581156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02448269459742969\n",
      "    mean_inference_ms: 5.8111662947981975\n",
      "    mean_raw_obs_processing_ms: 0.4339333298328233\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.299999999999937\n",
      "    episode_reward_mean: -0.7559999999999867\n",
      "    episode_reward_min: -19.8\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [2.7000000000000317, 4.199999999999989, -6.299999999999995, 7.500000000000021,\n",
      "        0.6000000000000103, 15.299999999999937, -18.299999999999983, 13.19999999999998,\n",
      "        4.500000000000011, -2.999999999999992, -2.9999999999999982, 6.600000000000033,\n",
      "        -11.999999999999984, -1.4999999999999902, -8.999999999999972, -18.299999999999983,\n",
      "        1.5000000000000173, -4.499999999999989, 1.529332216421153e-14, -2.099999999999994,\n",
      "        -7.499999999999989, 5.100000000000003, -19.8, -1.4999999999999756, 8.631984016460592e-15,\n",
      "        13.499999999999952, -6.59999999999998, 14.100000000000003, 10.500000000000004,\n",
      "        -2.0999999999999766, 10.200000000000024, 7.500000000000034, 3.0000000000000275,\n",
      "        1.5000000000000289, 1.2000000000000235, -7.499999999999973, -11.39999999999998,\n",
      "        9.000000000000023, 9.300000000000024, 1.2000000000000077, -2.5118795932144167e-14,\n",
      "        -2.9999999999999973, 5.1000000000000245, -7.7999999999999865, -10.199999999999976,\n",
      "        0.30000000000002225, 5.100000000000032, 1.7291723608536813e-14, -11.999999999999972,\n",
      "        -3.8999999999999795, -11.999999999999984, 7.200000000000026, -1.4999999999999818,\n",
      "        4.5000000000000115, -3.5999999999999917, -5.999999999999995, 15.000000000000028,\n",
      "        6.000000000000034, -5.39999999999999, -17.4, -11.999999999999998, 6.000000000000016,\n",
      "        1.840194663316197e-14, 1.5000000000000173, 7.19999999999996, -7.499999999999975,\n",
      "        -4.799999999999976, 2.1000000000000196, 3.900000000000016, 6.300000000000024,\n",
      "        0.9000000000000209, 13.500000000000025, -7.4999999999999805, 0.30000000000001914,\n",
      "        0.3000000000000218, -1.5000000000000182, -13.49999999999999, -8.399999999999979,\n",
      "        -5.999999999999988, -10.499999999999979, -8.399999999999993, 11.400000000000032,\n",
      "        2.1000000000000187, -17.09999999999998, -8.99999999999998, -4.499999999999989,\n",
      "        2.399999999999976, -1.4999999999999818, 8.700000000000005, -2.999999999999995,\n",
      "        -4.799999999999992, 5.100000000000004, -11.699999999999985, 2.7727820040013285e-14,\n",
      "        5.400000000000022, -1.4999999999999907, -0.5999999999999709, -5.399999999999979,\n",
      "        6.599999999999973, 11.700000000000028]\n",
      "      policy_policy1_reward: [10.5, 12.0, 1.5, 17.5, 9.5, 22.0, -16.0, 21.0, 14.5, 7.0,\n",
      "        7.0, 15.5, -2.0, 8.5, 1.0, -10.5, 11.5, 5.5, 10.0, 3.5, 2.5, 14.0, -12.0, 8.5,\n",
      "        10.0, 23.5, -1.0, 23.0, 20.5, 3.5, 18.0, 17.5, 13.0, 11.5, 9.0, 2.5, -2.5, 19.0,\n",
      "        16.0, 3.5, 10.0, 7.0, 14.0, 0.0, -3.5, 7.0, 14.0, 10.0, -2.0, 5.0, -2.0, 15.0,\n",
      "        8.5, 14.5, 2.0, 4.0, 25.0, 16.0, 3.5, -14.0, -2.0, 16.0, 4.5, 11.5, 15.0, 2.5,\n",
      "        -2.5, 11.0, 9.5, 13.0, 6.5, 18.0, 2.5, 7.0, 7.0, 8.5, -3.5, 0.5, -1.5, -0.5,\n",
      "        0.5, 11.5, 11.0, -11.5, -4.5, 5.5, 8.0, 8.5, 16.5, 1.5, 3.0, 14.0, -5.0, 10.0,\n",
      "        5.5, 8.5, 5.0, 3.5, 15.5, 19.5]\n",
      "      policy_policy2_reward: [-7.79999999999999, -7.79999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -8.899999999999983, -6.6999999999999815, -2.3, -7.79999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.6, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.6, -8.89999999999998, -9.99999999999998, -5.599999999999992,\n",
      "        -7.79999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999983, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -6.699999999999995, -2.2999999999999994, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999983, -6.6999999999999895, -6.699999999999994,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -3.4, -9.99999999999998, -9.99999999999998, -4.499999999999982,\n",
      "        -9.99999999999998, -7.79999999999999, -9.99999999999998, -2.3000000000000047,\n",
      "        -8.899999999999986, -5.599999999999992, -6.699999999999987, -5.599999999999995,\n",
      "        -4.499999999999991, -9.99999999999998, -6.6999999999999815, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -4.500000000000004,\n",
      "        -9.99999999999998, -8.89999999999998, -0.10000000000000375, -8.899999999999986,\n",
      "        -5.599999999999998, -4.499999999999998, -9.99999999999998, -5.5999999999999925,\n",
      "        -9.99999999999998, -7.799999999999981, -4.499999999999996, -7.799999999999989,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -0.10000000000000309,\n",
      "        -9.99999999999998, -5.599999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.799999999999982]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: -0.10000000000000309\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.33\n",
      "      policy2: -8.085999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -16.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07850462288581156\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02448269459742969\n",
      "      mean_inference_ms: 5.8111662947981975\n",
      "      mean_raw_obs_processing_ms: 0.4339333298328233\n",
      "  time_since_restore: 249.40334916114807\n",
      "  time_this_iter_s: 65.40801334381104\n",
      "  time_total_s: 249.40334916114807\n",
      "  timers:\n",
      "    learn_throughput: 112.281\n",
      "    learn_time_ms: 35625.052\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 62340.365\n",
      "  timestamp: 1660562244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:29 (running for 00:05:04.21)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |           7.33  |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:34 (running for 00:05:09.27)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |           7.33  |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:39 (running for 00:05:14.34)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.2360000000000093 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FB85E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805ECA0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805EB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998037E50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      5 |          220.914 | 15000 |    1.236 |           9.63  |          -8.394 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      5 |          223.11  | 15000 |   -0.579 |           7.21  |          -7.789 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |           7.33  |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-17-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.499999999999986\n",
      "  episode_reward_mean: 0.7380000000000073\n",
      "  episode_reward_min: -18.599999999999994\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1565179824829102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019411947578191757\n",
      "          model: {}\n",
      "          policy_loss: -0.0581362210214138\n",
      "          total_loss: 6.785131931304932\n",
      "          vf_explained_var: 0.06113891676068306\n",
      "          vf_loss: 6.837444305419922\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.145967721939087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016757085919380188\n",
      "          model: {}\n",
      "          policy_loss: -0.05415492504835129\n",
      "          total_loss: 2.759702444076538\n",
      "          vf_explained_var: 0.10056202113628387\n",
      "          vf_loss: 2.806316375732422\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.585915492957753\n",
      "    ram_util_percent: 63.940845070422526\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.12\n",
      "    policy2: -7.381999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07141841010065739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02332658252863735\n",
      "    mean_inference_ms: 6.2260967660504525\n",
      "    mean_raw_obs_processing_ms: 0.44199828271456165\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.499999999999986\n",
      "    episode_reward_mean: 0.7380000000000073\n",
      "    episode_reward_min: -18.599999999999994\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-0.9000000000000121, -13.500000000000005, 9.000000000000027,\n",
      "        3.3000000000000282, -11.999999999999995, 12.000000000000028, 4.200000000000005,\n",
      "        7.499999999999995, -1.799999999999986, -12.899999999999983, -4.499999999999986,\n",
      "        7.500000000000027, -5.999999999999987, 8.999999999999963, -9.299999999999983,\n",
      "        8.100000000000032, 13.799999999999988, -4.499999999999977, -10.499999999999984,\n",
      "        15.000000000000021, 14.999999999999975, 4.500000000000012, -6.599999999999982,\n",
      "        8.100000000000026, 9.300000000000017, 3.299999999999991, 11.10000000000002,\n",
      "        -2.3999999999999835, -16.199999999999985, 7.500000000000012, 4.500000000000005,\n",
      "        3.0000000000000115, -11.999999999999988, -2.9999999999999853, 15.899999999999954,\n",
      "        -2.399999999999989, 2.100000000000019, -11.699999999999983, -17.99999999999998,\n",
      "        1.1296519275560968e-14, -13.499999999999993, 15.59999999999994, 18.000000000000025,\n",
      "        -2.699999999999992, -18.599999999999994, -17.999999999999982, -0.5999999999999958,\n",
      "        10.199999999999964, 5.099999999999984, 1.2000000000000273, 4.799999999999997,\n",
      "        -1.4999999999999827, 13.199999999999964, -8.99999999999998, 3.3000000000000194,\n",
      "        14.699999999999937, 2.7000000000000086, 18.59999999999998, 10.500000000000028,\n",
      "        -7.199999999999976, -16.19999999999998, -2.700000000000003, 1.5000000000000233,\n",
      "        11.100000000000017, -11.699999999999992, -9.299999999999992, 5.700000000000015,\n",
      "        -10.49999999999999, -10.499999999999977, 0.3000000000000014, -8.999999999999982,\n",
      "        9.300000000000026, 4.499999999999998, -0.8999999999999824, -8.69999999999999,\n",
      "        -3.8999999999999946, 19.499999999999986, 8.70000000000003, 3.300000000000016,\n",
      "        -7.4999999999999805, -14.99999999999999, 10.199999999999942, 3.5999999999999983,\n",
      "        0.600000000000022, 10.500000000000034, -1.499999999999996, 6.900000000000022,\n",
      "        -11.999999999999991, -7.79999999999999, 14.39999999999998, 6.000000000000007,\n",
      "        11.69999999999995, 1.5000000000000306, -2.9999999999999902, -9.899999999999977,\n",
      "        -3.899999999999984, 10.19999999999997, -5.999999999999988, 7.500000000000032,\n",
      "        4.500000000000028]\n",
      "      policy_policy1_reward: [8.0, -3.5, 19.0, 10.0, -2.0, 22.0, 12.0, 17.5, 6.0, -4.0,\n",
      "        5.5, 17.5, 4.0, 19.0, -1.5, 17.0, 15.0, 5.5, -0.5, 25.0, 25.0, 14.5, -1.0, 11.5,\n",
      "        10.5, 10.0, 20.0, 6.5, -9.5, 17.5, 14.5, 7.5, -2.0, 7.0, 21.5, 6.5, 11.0, -5.0,\n",
      "        -8.0, 10.0, -3.5, 19.0, 28.0, -7.0, -13.0, -8.0, 5.0, 12.5, 14.0, 9.0, 11.5,\n",
      "        8.5, 21.0, 1.0, 10.0, 22.5, -0.5, 16.5, 20.5, -0.5, -15.0, 4.0, 11.5, 20.0,\n",
      "        -5.0, -1.5, 8.0, -0.5, -0.5, 7.0, 1.0, 10.5, 9.0, 8.0, -2.0, 5.0, 29.5, 16.5,\n",
      "        10.0, 2.5, -5.0, 18.0, 7.0, 9.5, 20.5, 8.5, 12.5, -2.0, -5.5, 20.0, 10.5, 19.5,\n",
      "        11.5, 7.0, -1.0, 5.0, 18.0, -12.5, 12.0, 14.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -9.99999999999998, -7.79999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -1.1999999999999875, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -3.4000000000000044, -1.2000000000000048, -6.699999999999986, -8.89999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -4.499999999999982, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -8.89999999999998, -8.899999999999986, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -3.399999999999995, -9.99999999999998,\n",
      "        4.300000000000008, -5.599999999999997, -9.99999999999998, -5.599999999999998,\n",
      "        -2.3, -8.89999999999998, -7.7999999999999865, -6.699999999999988, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -6.699999999999994, -7.79999999999999,\n",
      "        3.200000000000011, 2.100000000000002, -9.99999999999998, -6.6999999999999815,\n",
      "        -1.2000000000000026, -6.699999999999995, -9.99999999999998, -8.89999999999998,\n",
      "        -6.699999999999995, -7.799999999999983, -2.3000000000000034, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999988, -9.99999999999998, -1.1999999999999833,\n",
      "        -4.500000000000002, -8.89999999999998, -6.6999999999999815, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -3.399999999999993, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999995, -9.99999999999998,\n",
      "        -2.3000000000000034, -5.599999999999994, -4.5000000000000036, -7.799999999999987,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.799999999999986, 6.500000000000011, -4.500000000000001, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 6.500000000000011\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.12\n",
      "      policy2: -7.381999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -15.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07141841010065739\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02332658252863735\n",
      "      mean_inference_ms: 6.2260967660504525\n",
      "      mean_raw_obs_processing_ms: 0.44199828271456165\n",
      "  time_since_restore: 273.1787476539612\n",
      "  time_this_iter_s: 50.06905460357666\n",
      "  time_total_s: 273.1787476539612\n",
      "  timers:\n",
      "    learn_throughput: 121.661\n",
      "    learn_time_ms: 24658.612\n",
      "    synch_weights_time_ms: 3.158\n",
      "    training_iteration_time_ms: 45524.638\n",
      "  timestamp: 1660562260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-17-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.59999999999996\n",
      "  episode_reward_mean: 1.1640000000000106\n",
      "  episode_reward_min: -17.99999999999998\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 180\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2002251148223877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013652165420353413\n",
      "          model: {}\n",
      "          policy_loss: -0.04661579057574272\n",
      "          total_loss: 7.023128032684326\n",
      "          vf_explained_var: 0.12738284468650818\n",
      "          vf_loss: 7.067013740539551\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2319360971450806\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010867956094443798\n",
      "          model: {}\n",
      "          policy_loss: -0.03947197645902634\n",
      "          total_loss: 3.4796407222747803\n",
      "          vf_explained_var: 0.18266145884990692\n",
      "          vf_loss: 3.516939401626587\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 18000\n",
      "    num_env_steps_trained: 18000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 18000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 18000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.584507042253534\n",
      "    ram_util_percent: 63.93521126760563\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 33.5\n",
      "    policy2: 4.30000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.92\n",
      "    policy2: -7.755999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07471295099864028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02484095406668603\n",
      "    mean_inference_ms: 6.082558819913943\n",
      "    mean_raw_obs_processing_ms: 0.4412093070808284\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.59999999999996\n",
      "    episode_reward_mean: 1.1640000000000106\n",
      "    episode_reward_min: -17.99999999999998\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [1.8000000000000091, 4.500000000000018, 13.499999999999936, 5.10000000000003,\n",
      "        4.500000000000023, -16.199999999999996, 2.6999999999999957, 3.000000000000005,\n",
      "        -1.1999999999999913, 1.5000000000000133, 9.000000000000027, 7.5000000000000195,\n",
      "        9.90000000000002, 1.1296519275560968e-14, 0.30000000000002536, -3.000000000000035,\n",
      "        7.200000000000001, -13.499999999999986, -1.1999999999999762, -3.899999999999987,\n",
      "        -3.2999999999999794, 3.3000000000000176, -1.7999999999999798, 16.199999999999903,\n",
      "        -14.999999999999988, -2.9999999999999885, 8.099999999999989, -8.999999999999982,\n",
      "        12.000000000000007, 2.273181642920008e-14, 24.59999999999996, 11.999999999999991,\n",
      "        10.500000000000004, -2.399999999999987, 0.6000000000000202, 1.2000000000000064,\n",
      "        11.100000000000032, 10.2, 4.799999999999992, 3.0000000000000053, -8.4, -7.499999999999975,\n",
      "        3.000000000000024, -14.999999999999995, 5.100000000000023, 2.095545958979983e-14,\n",
      "        1.5000000000000262, 7.800000000000033, -6.0, 7.500000000000012, -17.99999999999998,\n",
      "        4.200000000000019, -1.4999999999999953, -2.3999999999999915, 15.000000000000034,\n",
      "        -0.5999999999999861, -2.3999999999999795, -4.199999999999976, -1.4999999999999911,\n",
      "        -11.999999999999973, -11.999999999999986, 6.300000000000024, 20.999999999999993,\n",
      "        3.000000000000023, -7.5, 5.100000000000028, -9.29999999999999, 4.199999999999978,\n",
      "        16.199999999999964, -4.499999999999979, 4.2000000000000215, -13.499999999999982,\n",
      "        3.600000000000017, -11.999999999999984, -1.8000000000000331, 0.6000000000000238,\n",
      "        -1.199999999999978, -2.099999999999986, 7.800000000000026, -2.999999999999983,\n",
      "        -5.999999999999982, -4.499999999999972, -7.799999999999978, -3.2999999999999905,\n",
      "        12.00000000000003, 1.8000000000000154, -5.999999999999973, -14.99999999999998,\n",
      "        -2.999999999999993, 16.800000000000026, 5.100000000000028, 1.5000000000000113,\n",
      "        9.00000000000002, -1.7999999999999856, -11.699999999999978, 11.100000000000032,\n",
      "        14.399999999999961, 13.500000000000005, 3.3000000000000034, 2.70000000000002]\n",
      "      policy_policy1_reward: [8.5, 14.5, 23.5, 14.0, 14.5, -9.5, 10.5, 13.0, 5.5, 11.5,\n",
      "        19.0, 17.5, 15.5, 10.0, 7.0, 7.0, 15.0, -3.5, 5.5, -0.5, 4.5, 10.0, 6.0, 24.0,\n",
      "        -5.0, 7.0, 17.0, 1.0, 16.5, 10.0, 33.5, 22.0, 20.5, 6.5, 9.5, 9.0, 20.0, 18.0,\n",
      "        11.5, 13.0, 0.5, 2.5, 13.0, -5.0, 8.5, 10.0, 11.5, 14.5, 4.0, 17.5, -8.0, 6.5,\n",
      "        8.5, 6.5, 25.0, 5.0, 6.5, 2.5, 3.0, -2.0, -7.5, 13.0, 31.0, 13.0, 2.5, 14.0,\n",
      "        -1.5, 6.5, 24.0, 5.5, 12.0, -3.5, 12.5, -2.0, 6.0, 9.5, 5.5, -2.0, 3.5, 7.0,\n",
      "        4.0, 5.5, -5.5, 4.5, 22.0, 8.5, 4.0, -5.0, 1.5, 23.5, 8.5, 11.5, 19.0, 0.5,\n",
      "        -10.5, 20.0, 14.5, 23.5, 4.5, 5.0]\n",
      "      policy_policy2_reward: [-6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -6.69999999999999, -7.79999999999999,\n",
      "        -9.99999999999998, -6.699999999999993, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -9.99999999999998, -6.699999999999984,\n",
      "        -9.99999999999998, -7.799999999999989, -9.99999999999998, -6.69999999999999,\n",
      "        -3.400000000000004, -7.7999999999999865, -6.699999999999991, -7.79999999999999,\n",
      "        -7.7999999999999865, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -4.499999999999982, -9.99999999999998, -8.899999999999983,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.7999999999999865, -8.89999999999998, -7.79999999999999, -6.699999999999995,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -3.3999999999999915, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999982, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -2.2999999999999985, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -5.599999999999982, -8.89999999999998, -6.69999999999999, -4.500000000000003,\n",
      "        -9.99999999999998, -4.5, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999983, -7.799999999999981, -2.2999999999999834,\n",
      "        -7.799999999999989, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999986, -8.899999999999983,\n",
      "        -6.699999999999995, -0.09999999999999187, 4.30000000000001, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -2.3000000000000034, -7.79999999999999,\n",
      "        -9.99999999999998, -6.69999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -4.500000000000001, -6.6999999999999815, -3.3999999999999924, -9.99999999999998,\n",
      "        -9.99999999999998, -2.299999999999989, -1.1999999999999869, -8.89999999999998,\n",
      "        -0.09999999999999423, -9.99999999999998, -1.199999999999999, -2.3000000000000056]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.5\n",
      "      policy2: 4.30000000000001\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.92\n",
      "      policy2: -7.755999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -10.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07471295099864028\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02484095406668603\n",
      "      mean_inference_ms: 6.082558819913943\n",
      "      mean_raw_obs_processing_ms: 0.4412093070808284\n",
      "  time_since_restore: 270.9955017566681\n",
      "  time_this_iter_s: 50.08102035522461\n",
      "  time_total_s: 270.9955017566681\n",
      "  timers:\n",
      "    learn_throughput: 121.852\n",
      "    learn_time_ms: 24620.113\n",
      "    synch_weights_time_ms: 3.158\n",
      "    training_iteration_time_ms: 45158.104\n",
      "  timestamp: 1660562260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 6\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:45 (running for 00:05:20.38)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |           8.92  |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |           8.12  |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |           7.33  |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:51 (running for 00:05:25.50)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |           8.92  |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |           8.12  |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |           7.33  |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:17:56 (running for 00:05:30.56)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |           8.92  |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |           8.12  |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |           7.33  |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      3 |          199.395 | 12000 |   -2.406 |           5.625 |          -8.031 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-17-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.999999999999908\n",
      "  episode_reward_mean: -0.0059999999999894625\n",
      "  episode_reward_min: -22.5\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2677934169769287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01822894997894764\n",
      "          model: {}\n",
      "          policy_loss: -0.04515327885746956\n",
      "          total_loss: 6.81857967376709\n",
      "          vf_explained_var: 0.11277937144041061\n",
      "          vf_loss: 6.858264446258545\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2487167119979858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017998307943344116\n",
      "          model: {}\n",
      "          policy_loss: -0.04769115149974823\n",
      "          total_loss: 2.868675470352173\n",
      "          vf_explained_var: 0.14340965449810028\n",
      "          vf_loss: 2.9109671115875244\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.772631578947365\n",
      "    ram_util_percent: 63.93684210526313\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 26.0\n",
      "    policy2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.2\n",
      "    policy2: -7.205999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -14.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07637304878853912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022266378787140245\n",
      "    mean_inference_ms: 7.158441614556969\n",
      "    mean_raw_obs_processing_ms: 0.42863868131809013\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 26.999999999999908\n",
      "    episode_reward_mean: -0.0059999999999894625\n",
      "    episode_reward_min: -22.5\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-19.5, -22.5, 17.399999999999977, -7.499999999999986, 15.299999999999986,\n",
      "        2.1000000000000076, 10.499999999999936, 10.500000000000028, -1.1999999999999726,\n",
      "        9.0, 7.200000000000008, 0.3000000000000149, -7.499999999999992, -17.400000000000013,\n",
      "        4.2000000000000295, -10.499999999999984, 0.6000000000000291, -15.900000000000032,\n",
      "        -5.999999999999982, 7.800000000000029, -5.399999999999997, -0.8999999999999937,\n",
      "        -3.8999999999999857, -10.199999999999983, 10.200000000000022, 5.190292640122607e-15,\n",
      "        26.999999999999908, 1.0963452368173421e-14, -1.5000000000000004, 4.200000000000018,\n",
      "        -6.299999999999981, -5.399999999999991, 12.000000000000002, -14.99999999999998,\n",
      "        11.400000000000025, 0.600000000000022, 3.0000000000000058, -10.499999999999993,\n",
      "        -4.199999999999979, -5.9999999999999885, -13.199999999999982, 4.500000000000009,\n",
      "        1.5000000000000226, 4.199999999999999, -4.499999999999974, -11.999999999999995,\n",
      "        4.500000000000028, 15.000000000000025, -3.59999999999998, 2.4000000000000212,\n",
      "        1.2000000000000006, 14.700000000000015, -0.8999999999999878, 6.3000000000000185,\n",
      "        -8.699999999999994, -3.299999999999985, -1.4999999999999845, 14.399999999999995,\n",
      "        -3.5999999999999814, 5.700000000000024, 11.99999999999995, -2.09999999999998,\n",
      "        5.700000000000033, -11.699999999999976, 1.2739809207573671e-14, 7.500000000000028,\n",
      "        -0.8999999999999923, -19.500000000000007, -0.5999999999999869, -10.19999999999998,\n",
      "        1.174060848541103e-14, 7.800000000000015, -4.799999999999978, -2.699999999999996,\n",
      "        -6.899999999999979, 9.599999999999978, 10.500000000000034, 8.400000000000023,\n",
      "        1.1851630787873546e-14, -10.199999999999974, 7.800000000000027, -4.499999999999986,\n",
      "        3.000000000000001, -7.7999999999999865, -6.299999999999983, 8.700000000000026,\n",
      "        13.499999999999984, -8.399999999999984, 6.900000000000027, -2.999999999999973,\n",
      "        2.7000000000000166, -0.5999999999999946, 2.400000000000015, 7.200000000000012,\n",
      "        3.300000000000007, -5.699999999999985, -7.7999999999999865, 6.000000000000018,\n",
      "        1.200000000000013, -10.199999999999982]\n",
      "      policy_policy1_reward: [-9.5, -12.5, 23.0, 2.5, 22.0, 5.5, 20.5, 20.5, 5.5, 19.0,\n",
      "        15.0, 7.0, 2.5, -14.0, 12.0, -0.5, 9.5, -7.0, 4.0, 14.5, 3.5, 8.0, -0.5, -3.5,\n",
      "        12.5, 4.5, 26.0, 10.0, 8.5, 12.0, -4.0, 3.5, 22.0, -5.0, 17.0, 9.5, 13.0, -0.5,\n",
      "        2.5, 4.0, -6.5, 14.5, 11.5, 12.0, 5.5, -2.0, 14.5, 25.0, 2.0, 8.0, 9.0, 22.5,\n",
      "        8.0, 13.0, -2.0, 4.5, 8.5, 20.0, 2.0, 13.5, 22.0, 3.5, 13.5, -10.5, 10.0, 17.5,\n",
      "        2.5, -9.5, 5.0, -14.5, 4.5, 14.5, 3.0, 4.0, 2.0, 18.5, 20.5, 14.0, 10.0, -3.5,\n",
      "        9.0, 5.5, -3.5, -11.0, 1.5, 16.5, 23.5, 0.5, 12.5, 7.0, 10.5, 5.0, 8.0, 15.0,\n",
      "        10.0, 1.0, 0.0, 16.0, 9.0, -9.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -5.599999999999994,\n",
      "        -9.99999999999998, -6.699999999999993, -3.3999999999999835, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999987, -9.99999999999998, -7.799999999999988,\n",
      "        -6.699999999999987, -9.99999999999998, -3.3999999999999835, -7.799999999999981,\n",
      "        -9.99999999999998, -8.89999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -6.699999999999983, -8.899999999999986, -8.89999999999998, -3.4000000000000052,\n",
      "        -6.699999999999987, -2.300000000000003, -4.499999999999999, 0.9999999999999974,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999981, -2.299999999999994,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999984,\n",
      "        -9.99999999999998, -6.699999999999995, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -5.599999999999984, -7.7999999999999865,\n",
      "        -7.799999999999981, -8.899999999999986, -6.699999999999986, -6.699999999999993,\n",
      "        -7.79999999999999, -9.99999999999998, -5.599999999999982, -5.599999999999997,\n",
      "        -7.799999999999989, -9.99999999999998, -5.599999999999997, -7.799999999999981,\n",
      "        -1.2000000000000035, -9.99999999999998, -9.99999999999998, -3.399999999999993,\n",
      "        -9.99999999999998, -5.59999999999999, 4.299999999999998, -4.499999999999984,\n",
      "        -6.699999999999986, -7.799999999999988, -6.699999999999988, -8.89999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -5.599999999999995, -9.99999999999998,\n",
      "        -6.699999999999988, -1.2000000000000013, -9.99999999999998, 6.500000000000011,\n",
      "        3.2000000000000055, -7.799999999999981, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999984, -5.6, -9.99999999999998, -7.799999999999989, -5.599999999999982,\n",
      "        -5.599999999999997, -7.799999999999989, -6.69999999999999, -6.699999999999988,\n",
      "        -7.799999999999984, -9.99999999999998, -7.79999999999999, -1.199999999999997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.0\n",
      "      policy2: 6.500000000000011\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.2\n",
      "      policy2: -7.205999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -14.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07637304878853912\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022266378787140245\n",
      "      mean_inference_ms: 7.158441614556969\n",
      "      mean_raw_obs_processing_ms: 0.42863868131809013\n",
      "  time_since_restore: 266.14143419265747\n",
      "  time_this_iter_s: 66.74643516540527\n",
      "  time_total_s: 266.14143419265747\n",
      "  timers:\n",
      "    learn_throughput: 112.021\n",
      "    learn_time_ms: 35707.472\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 66530.621\n",
      "  timestamp: 1660562277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:02 (running for 00:05:36.90)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |            8.92 |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |            8.12 |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |            7.33 |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |            7.2  |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:07 (running for 00:05:41.95)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |            8.92 |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |            8.12 |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |            7.33 |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |            7.2  |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:12 (running for 00:05:47.03)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |            8.92 |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |            8.12 |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |            7.33 |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |            7.2  |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:17 (running for 00:05:52.09)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |            8.92 |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |            8.12 |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |            7.33 |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |            7.2  |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:22 (running for 00:05:57.17)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |            8.92 |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |            8.12 |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |            7.33 |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |            7.2  |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:27 (running for 00:06:02.22)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00000 with episode_reward_mean=1.1640000000000106 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980A1C10>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998094340>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980943A0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980AE820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997F29160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      6 |          270.996 | 18000 |    1.164 |            8.92 |          -7.756 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      6 |          273.179 | 18000 |    0.738 |            8.12 |          -7.382 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      4 |          249.403 | 16000 |   -0.756 |            7.33 |          -8.086 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |            7.2  |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 42000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-18-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.499999999999893\n",
      "  episode_reward_mean: 2.1960000000000006\n",
      "  episode_reward_min: -21.000000000000043\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 210\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.128420114517212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019358403980731964\n",
      "          model: {}\n",
      "          policy_loss: -0.05329977720975876\n",
      "          total_loss: 6.804091930389404\n",
      "          vf_explained_var: 0.11954955756664276\n",
      "          vf_loss: 6.851583480834961\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1145806312561035\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019221313297748566\n",
      "          model: {}\n",
      "          policy_loss: -0.06022941693663597\n",
      "          total_loss: 2.366826057434082\n",
      "          vf_explained_var: 0.23641815781593323\n",
      "          vf_loss: 2.418405771255493\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 42000\n",
      "  num_agent_steps_trained: 42000\n",
      "  num_env_steps_sampled: 21000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 21000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.322857142857142\n",
      "    ram_util_percent: 63.942857142857115\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 38.5\n",
      "    policy2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.325\n",
      "    policy2: -7.128999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07143452424092035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023224417020774712\n",
      "    mean_inference_ms: 6.358052508973471\n",
      "    mean_raw_obs_processing_ms: 0.44111913919230056\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.499999999999893\n",
      "    episode_reward_mean: 2.1960000000000006\n",
      "    episode_reward_min: -21.000000000000043\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [4.500000000000005, 3.0000000000000115, -11.999999999999988, -2.9999999999999853,\n",
      "        15.899999999999954, -2.399999999999989, 2.100000000000019, -11.699999999999983,\n",
      "        -17.99999999999998, 1.1296519275560968e-14, -13.499999999999993, 15.59999999999994,\n",
      "        18.000000000000025, -2.699999999999992, -18.599999999999994, -17.999999999999982,\n",
      "        -0.5999999999999958, 10.199999999999964, 5.099999999999984, 1.2000000000000273,\n",
      "        4.799999999999997, -1.4999999999999827, 13.199999999999964, -8.99999999999998,\n",
      "        3.3000000000000194, 14.699999999999937, 2.7000000000000086, 18.59999999999998,\n",
      "        10.500000000000028, -7.199999999999976, -16.19999999999998, -2.700000000000003,\n",
      "        1.5000000000000233, 11.100000000000017, -11.699999999999992, -9.299999999999992,\n",
      "        5.700000000000015, -10.49999999999999, -10.499999999999977, 0.3000000000000014,\n",
      "        -8.999999999999982, 9.300000000000026, 4.499999999999998, -0.8999999999999824,\n",
      "        -8.69999999999999, -3.8999999999999946, 19.499999999999986, 8.70000000000003,\n",
      "        3.300000000000016, -7.4999999999999805, -14.99999999999999, 10.199999999999942,\n",
      "        3.5999999999999983, 0.600000000000022, 10.500000000000034, -1.499999999999996,\n",
      "        6.900000000000022, -11.999999999999991, -7.79999999999999, 14.39999999999998,\n",
      "        6.000000000000007, 11.69999999999995, 1.5000000000000306, -2.9999999999999902,\n",
      "        -9.899999999999977, -3.899999999999984, 10.19999999999997, -5.999999999999988,\n",
      "        7.500000000000032, 4.500000000000028, 5.699999999999974, -5.699999999999985,\n",
      "        7.80000000000001, 13.499999999999993, 7.800000000000026, 23.99999999999998,\n",
      "        11.400000000000006, 9.900000000000025, -0.8999999999999807, -3.0000000000000053,\n",
      "        15.299999999999905, 4.199999999999978, -9.299999999999985, 3.6000000000000187,\n",
      "        -21.000000000000043, -5.9999999999999805, 16.79999999999992, 6.000000000000011,\n",
      "        12.599999999999943, 1.5000000000000244, 15.599999999999921, 28.499999999999893,\n",
      "        13.799999999999983, -8.999999999999979, 18.59999999999997, -0.29999999999997373,\n",
      "        9.000000000000012, -5.399999999999979, 19.199999999999996, 8.700000000000019]\n",
      "      policy_policy1_reward: [14.5, 7.5, -2.0, 7.0, 21.5, 6.5, 11.0, -5.0, -8.0, 10.0,\n",
      "        -3.5, 19.0, 28.0, -7.0, -13.0, -8.0, 5.0, 12.5, 14.0, 9.0, 11.5, 8.5, 21.0,\n",
      "        1.0, 10.0, 22.5, -0.5, 16.5, 20.5, -0.5, -15.0, 4.0, 11.5, 20.0, -5.0, -1.5,\n",
      "        8.0, -0.5, -0.5, 7.0, 1.0, 10.5, 9.0, 8.0, -2.0, 5.0, 29.5, 16.5, 10.0, 2.5,\n",
      "        -5.0, 18.0, 7.0, 9.5, 20.5, 8.5, 12.5, -2.0, -5.5, 20.0, 10.5, 19.5, 11.5, 7.0,\n",
      "        -1.0, 5.0, 18.0, -12.5, 12.0, 14.5, 8.0, 1.0, 14.5, 23.5, 14.5, 28.5, 17.0,\n",
      "        15.5, 8.0, 7.0, 22.0, 12.0, -7.0, 12.5, -11.0, 4.0, 23.5, 10.5, 21.5, 11.5,\n",
      "        24.5, 38.5, 20.5, 1.0, 27.5, 7.5, 19.0, 3.5, 27.0, 11.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -8.89999999999998, -8.899999999999986,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -3.399999999999995, -9.99999999999998, 4.300000000000008, -5.599999999999997,\n",
      "        -9.99999999999998, -5.599999999999998, -2.3, -8.89999999999998, -7.7999999999999865,\n",
      "        -6.699999999999988, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -6.699999999999994, -7.79999999999999, 3.200000000000011, 2.100000000000002,\n",
      "        -9.99999999999998, -6.6999999999999815, -1.2000000000000026, -6.699999999999995,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999995, -7.799999999999983,\n",
      "        -2.3000000000000034, -9.99999999999998, -9.99999999999998, -6.699999999999988,\n",
      "        -9.99999999999998, -1.1999999999999833, -4.500000000000002, -8.89999999999998,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -7.79999999999999,\n",
      "        -3.399999999999993, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999995, -9.99999999999998, -2.3000000000000034, -5.599999999999994,\n",
      "        -4.5000000000000036, -7.799999999999987, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.799999999999986, 6.500000000000011,\n",
      "        -4.500000000000001, -9.99999999999998, -2.300000000000005, -6.699999999999994,\n",
      "        -6.699999999999987, -9.99999999999998, -6.699999999999985, -4.500000000000002,\n",
      "        -5.599999999999982, -5.6, -8.89999999999998, -9.99999999999998, -6.699999999999995,\n",
      "        -7.799999999999981, -2.300000000000003, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999988, -4.499999999999997, -8.899999999999986,\n",
      "        -9.99999999999998, -8.899999999999984, -9.99999999999998, -6.699999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -2.3000000000000047]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.5\n",
      "      policy2: 6.500000000000011\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.325\n",
      "      policy2: -7.128999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -15.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07143452424092035\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023224417020774712\n",
      "      mean_inference_ms: 6.358052508973471\n",
      "      mean_raw_obs_processing_ms: 0.44111913919230056\n",
      "  time_since_restore: 322.82714343070984\n",
      "  time_this_iter_s: 49.64839577674866\n",
      "  time_total_s: 322.82714343070984\n",
      "  timers:\n",
      "    learn_throughput: 119.731\n",
      "    learn_time_ms: 25056.211\n",
      "    synch_weights_time_ms: 3.134\n",
      "    training_iteration_time_ms: 46112.606\n",
      "  timestamp: 1660562310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 7\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-18-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.59999999999996\n",
      "  episode_reward_mean: 0.7440000000000111\n",
      "  episode_reward_min: -17.99999999999998\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 210\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1671197414398193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011466152034699917\n",
      "          model: {}\n",
      "          policy_loss: -0.04138234630227089\n",
      "          total_loss: 6.529102802276611\n",
      "          vf_explained_var: 0.15306663513183594\n",
      "          vf_loss: 6.568192005157471\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.196876049041748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013626879081130028\n",
      "          model: {}\n",
      "          policy_loss: -0.04265325516462326\n",
      "          total_loss: 2.265439510345459\n",
      "          vf_explained_var: 0.26114270091056824\n",
      "          vf_loss: 2.3053674697875977\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_env_steps_sampled: 21000\n",
      "    num_env_steps_trained: 21000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 42000\n",
      "  num_agent_steps_trained: 42000\n",
      "  num_env_steps_sampled: 21000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 21000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.32\n",
      "    ram_util_percent: 63.94142857142854\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 33.5\n",
      "    policy2: 4.30000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.39\n",
      "    policy2: -7.645999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07443736258835748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024793317884527637\n",
      "    mean_inference_ms: 6.236848539841222\n",
      "    mean_raw_obs_processing_ms: 0.4413624516966662\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 24.59999999999996\n",
      "    episode_reward_mean: 0.7440000000000111\n",
      "    episode_reward_min: -17.99999999999998\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [24.59999999999996, 11.999999999999991, 10.500000000000004, -2.399999999999987,\n",
      "        0.6000000000000202, 1.2000000000000064, 11.100000000000032, 10.2, 4.799999999999992,\n",
      "        3.0000000000000053, -8.4, -7.499999999999975, 3.000000000000024, -14.999999999999995,\n",
      "        5.100000000000023, 2.095545958979983e-14, 1.5000000000000262, 7.800000000000033,\n",
      "        -6.0, 7.500000000000012, -17.99999999999998, 4.200000000000019, -1.4999999999999953,\n",
      "        -2.3999999999999915, 15.000000000000034, -0.5999999999999861, -2.3999999999999795,\n",
      "        -4.199999999999976, -1.4999999999999911, -11.999999999999973, -11.999999999999986,\n",
      "        6.300000000000024, 20.999999999999993, 3.000000000000023, -7.5, 5.100000000000028,\n",
      "        -9.29999999999999, 4.199999999999978, 16.199999999999964, -4.499999999999979,\n",
      "        4.2000000000000215, -13.499999999999982, 3.600000000000017, -11.999999999999984,\n",
      "        -1.8000000000000331, 0.6000000000000238, -1.199999999999978, -2.099999999999986,\n",
      "        7.800000000000026, -2.999999999999983, -5.999999999999982, -4.499999999999972,\n",
      "        -7.799999999999978, -3.2999999999999905, 12.00000000000003, 1.8000000000000154,\n",
      "        -5.999999999999973, -14.99999999999998, -2.999999999999993, 16.800000000000026,\n",
      "        5.100000000000028, 1.5000000000000113, 9.00000000000002, -1.7999999999999856,\n",
      "        -11.699999999999978, 11.100000000000032, 14.399999999999961, 13.500000000000005,\n",
      "        3.3000000000000034, 2.70000000000002, 2.700000000000028, -4.499999999999988,\n",
      "        3.300000000000014, -7.49999999999998, 19.199999999999925, 7.500000000000032,\n",
      "        -5.999999999999995, -4.199999999999978, -17.4, 6.599999999999978, -5.999999999999979,\n",
      "        -6.8999999999999755, -9.59999999999998, -8.399999999999991, 2.999999999999992,\n",
      "        -7.799999999999976, 7.500000000000025, -5.999999999999991, 7.500000000000032,\n",
      "        -11.999999999999984, 9.299999999999944, 2.7000000000000317, -2.999999999999987,\n",
      "        -13.49999999999998, 1.2000000000000264, -0.8999999999999955, 11.400000000000006,\n",
      "        1.5000000000000013, 22.199999999999925, 5.100000000000021]\n",
      "      policy_policy1_reward: [33.5, 22.0, 20.5, 6.5, 9.5, 9.0, 20.0, 18.0, 11.5, 13.0,\n",
      "        0.5, 2.5, 13.0, -5.0, 8.5, 10.0, 11.5, 14.5, 4.0, 17.5, -8.0, 6.5, 8.5, 6.5,\n",
      "        25.0, 5.0, 6.5, 2.5, 3.0, -2.0, -7.5, 13.0, 31.0, 13.0, 2.5, 14.0, -1.5, 6.5,\n",
      "        24.0, 5.5, 12.0, -3.5, 12.5, -2.0, 6.0, 9.5, 5.5, -2.0, 3.5, 7.0, 4.0, 5.5,\n",
      "        -5.5, 4.5, 22.0, 8.5, 4.0, -5.0, 1.5, 23.5, 8.5, 11.5, 19.0, 0.5, -10.5, 20.0,\n",
      "        14.5, 23.5, 4.5, 5.0, 10.5, 5.5, 4.5, -3.0, 27.0, 17.5, -7.0, 2.5, -8.5, 15.5,\n",
      "        4.0, 2.0, -4.0, 0.5, 13.0, 0.0, 17.5, 4.0, 17.5, -2.0, 16.0, 10.5, 7.0, -3.5,\n",
      "        9.0, 8.0, 17.0, 11.5, 30.0, 14.0]\n",
      "      policy_policy2_reward: [-8.899999999999983, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -7.7999999999999865, -8.89999999999998,\n",
      "        -7.79999999999999, -6.699999999999995, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -3.3999999999999915,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -2.2999999999999985, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999982, -8.89999999999998,\n",
      "        -6.69999999999999, -4.500000000000003, -9.99999999999998, -4.5, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.899999999999983,\n",
      "        -7.799999999999981, -2.2999999999999834, -7.799999999999989, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999986, -8.899999999999983, -6.699999999999995, -0.09999999999999187,\n",
      "        4.30000000000001, -9.99999999999998, -9.99999999999998, -9.99999999999998, -2.3000000000000034,\n",
      "        -7.79999999999999, -9.99999999999998, -6.69999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -4.500000000000001, -6.6999999999999815, -3.3999999999999924,\n",
      "        -9.99999999999998, -9.99999999999998, -2.299999999999989, -1.1999999999999869,\n",
      "        -8.89999999999998, -0.09999999999999423, -9.99999999999998, -1.199999999999999,\n",
      "        -2.3000000000000056, -7.799999999999986, -9.99999999999998, -1.1999999999999886,\n",
      "        -4.5000000000000036, -7.79999999999999, -9.99999999999998, 0.9999999999999959,\n",
      "        -6.6999999999999815, -8.899999999999986, -8.899999999999983, -9.99999999999998,\n",
      "        -8.89999999999998, -5.599999999999993, -8.89999999999998, -9.99999999999998,\n",
      "        -7.7999999999999865, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -8.89999999999998, -5.599999999999988,\n",
      "        -9.99999999999998, -7.799999999999981, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 33.5\n",
      "      policy2: 4.30000000000001\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.39\n",
      "      policy2: -7.645999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -10.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07443736258835748\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024793317884527637\n",
      "      mean_inference_ms: 6.236848539841222\n",
      "      mean_raw_obs_processing_ms: 0.4413624516966662\n",
      "  time_since_restore: 320.6329274177551\n",
      "  time_this_iter_s: 49.637425661087036\n",
      "  time_total_s: 320.6329274177551\n",
      "  timers:\n",
      "    learn_throughput: 119.904\n",
      "    learn_time_ms: 25020.078\n",
      "    synch_weights_time_ms: 3.135\n",
      "    training_iteration_time_ms: 45797.153\n",
      "  timestamp: 1660562310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 7\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-18-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000028\n",
      "  episode_reward_mean: -1.6109999999999867\n",
      "  episode_reward_min: -19.5\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2573984861373901\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011766966432332993\n",
      "          model: {}\n",
      "          policy_loss: -0.036276139318943024\n",
      "          total_loss: 7.0852532386779785\n",
      "          vf_explained_var: -0.005922498181462288\n",
      "          vf_loss: 7.119176387786865\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2375972270965576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011021708138287067\n",
      "          model: {}\n",
      "          policy_loss: -0.035153649747371674\n",
      "          total_loss: 2.889319896697998\n",
      "          vf_explained_var: 0.18672624230384827\n",
      "          vf_loss: 2.922269105911255\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.136170212765954\n",
      "    ram_util_percent: 63.94574468085103\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: 4.3000000000000025\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.98\n",
      "    policy2: -7.590999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0782502766923327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02496496356401852\n",
      "    mean_inference_ms: 6.133910202805202\n",
      "    mean_raw_obs_processing_ms: 0.4335670135871566\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.000000000000028\n",
      "    episode_reward_mean: -1.6109999999999867\n",
      "    episode_reward_min: -19.5\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-2.5118795932144167e-14, -2.9999999999999973, 5.1000000000000245,\n",
      "        -7.7999999999999865, -10.199999999999976, 0.30000000000002225, 5.100000000000032,\n",
      "        1.7291723608536813e-14, -11.999999999999972, -3.8999999999999795, -11.999999999999984,\n",
      "        7.200000000000026, -1.4999999999999818, 4.5000000000000115, -3.5999999999999917,\n",
      "        -5.999999999999995, 15.000000000000028, 6.000000000000034, -5.39999999999999,\n",
      "        -17.4, -11.999999999999998, 6.000000000000016, 1.840194663316197e-14, 1.5000000000000173,\n",
      "        7.19999999999996, -7.499999999999975, -4.799999999999976, 2.1000000000000196,\n",
      "        3.900000000000016, 6.300000000000024, 0.9000000000000209, 13.500000000000025,\n",
      "        -7.4999999999999805, 0.30000000000001914, 0.3000000000000218, -1.5000000000000182,\n",
      "        -13.49999999999999, -8.399999999999979, -5.999999999999988, -10.499999999999979,\n",
      "        -8.399999999999993, 11.400000000000032, 2.1000000000000187, -17.09999999999998,\n",
      "        -8.99999999999998, -4.499999999999989, 2.399999999999976, -1.4999999999999818,\n",
      "        8.700000000000005, -2.999999999999995, -4.799999999999992, 5.100000000000004,\n",
      "        -11.699999999999985, 2.7727820040013285e-14, 5.400000000000022, -1.4999999999999907,\n",
      "        -0.5999999999999709, -5.399999999999979, 6.599999999999973, 11.700000000000028,\n",
      "        2.3999999999999875, 6.600000000000005, 0.6000000000000066, -7.799999999999974,\n",
      "        2.0999999999999917, -7.199999999999982, -7.799999999999978, 9.000000000000025,\n",
      "        6.299999999999979, 1.5000000000000107, -12.899999999999991, -8.39999999999999,\n",
      "        -0.8999999999999932, -19.5, -14.09999999999998, -7.499999999999989, -7.19999999999999,\n",
      "        3.0000000000000164, 1.5000000000000147, -1.499999999999973, 6.600000000000016,\n",
      "        -0.8999999999999856, 4.500000000000012, -8.999999999999977, -1.799999999999986,\n",
      "        -4.799999999999978, 9.000000000000012, -14.99999999999999, 13.800000000000013,\n",
      "        -0.8999999999999924, -10.499999999999973, 3.000000000000015, 1.5000000000000226,\n",
      "        7.500000000000032, 3.9000000000000266, -12.89999999999998, -6.5999999999999766,\n",
      "        6.38378239159465e-16, 1.199999999999974, -4.499999999999986]\n",
      "      policy_policy1_reward: [10.0, 7.0, 14.0, 0.0, -3.5, 7.0, 14.0, 10.0, -2.0, 5.0,\n",
      "        -2.0, 15.0, 8.5, 14.5, 2.0, 4.0, 25.0, 16.0, 3.5, -14.0, -2.0, 16.0, 4.5, 11.5,\n",
      "        15.0, 2.5, -2.5, 11.0, 9.5, 13.0, 6.5, 18.0, 2.5, 7.0, 7.0, 8.5, -3.5, 0.5,\n",
      "        -1.5, -0.5, 0.5, 11.5, 11.0, -11.5, -4.5, 5.5, 8.0, 8.5, 16.5, 1.5, 3.0, 14.0,\n",
      "        -5.0, 10.0, 5.5, 8.5, 5.0, 3.5, 15.5, 19.5, 8.0, 15.5, 4.0, 0.0, 11.0, -6.0,\n",
      "        0.0, 19.0, 13.0, 11.5, -4.0, 0.5, 8.0, -9.5, -8.5, 2.5, -11.5, 7.5, 6.0, 8.5,\n",
      "        15.5, 8.0, 14.5, -4.5, 6.0, -2.5, 19.0, -5.0, 20.5, 8.0, -0.5, 13.0, 11.5, 17.5,\n",
      "        4.0, -9.5, -1.0, 10.0, 9.0, 5.5]\n",
      "      policy_policy2_reward: [-9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -7.799999999999983, -6.6999999999999895, -6.699999999999994, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -5.599999999999982,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -3.4, -9.99999999999998, -9.99999999999998, -4.499999999999982, -9.99999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -2.3000000000000047, -8.899999999999986,\n",
      "        -5.599999999999992, -6.699999999999987, -5.599999999999995, -4.499999999999991,\n",
      "        -9.99999999999998, -6.6999999999999815, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -4.500000000000004, -9.99999999999998,\n",
      "        -8.89999999999998, -0.10000000000000375, -8.899999999999986, -5.599999999999998,\n",
      "        -4.499999999999998, -9.99999999999998, -5.5999999999999925, -9.99999999999998,\n",
      "        -7.799999999999981, -4.499999999999996, -7.799999999999989, -8.89999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -0.10000000000000309, -9.99999999999998,\n",
      "        -5.599999999999998, -8.89999999999998, -8.89999999999998, -7.799999999999982,\n",
      "        -5.5999999999999925, -8.899999999999984, -3.4000000000000057, -7.799999999999981,\n",
      "        -8.899999999999986, -1.2000000000000035, -7.7999999999999865, -9.99999999999998,\n",
      "        -6.699999999999992, -9.99999999999998, -8.899999999999983, -8.89999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -5.599999999999997, -9.99999999999998,\n",
      "        4.3000000000000025, -4.499999999999989, -4.499999999999982, -9.99999999999998,\n",
      "        -8.899999999999983, -8.89999999999998, -9.99999999999998, -4.499999999999995,\n",
      "        -7.799999999999981, -2.299999999999997, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -0.09999999999999898, -3.4000000000000035,\n",
      "        -5.599999999999982, -9.99999999999998, -7.799999999999981, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: 4.3000000000000025\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.98\n",
      "      policy2: -7.590999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -14.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0782502766923327\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02496496356401852\n",
      "      mean_inference_ms: 6.133910202805202\n",
      "      mean_raw_obs_processing_ms: 0.4335670135871566\n",
      "  time_since_restore: 316.08916902542114\n",
      "  time_this_iter_s: 66.68581986427307\n",
      "  time_total_s: 316.08916902542114\n",
      "  timers:\n",
      "    learn_throughput: 111.882\n",
      "    learn_time_ms: 35752.084\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 63206.265\n",
      "  timestamp: 1660562311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:36 (running for 00:06:11.04)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |           7.2   |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:41 (running for 00:06:16.08)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |           7.2   |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:46 (running for 00:06:21.16)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |           7.2   |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:51 (running for 00:06:26.21)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |           7.2   |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:18:56 (running for 00:06:31.28)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |           7.2   |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:01 (running for 00:06:36.34)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      4 |          266.141 | 16000 |   -0.006 |           7.2   |          -7.206 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-19-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.19999999999999\n",
      "  episode_reward_mean: -0.14999999999998848\n",
      "  episode_reward_min: -24.000000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2183212041854858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020165609195828438\n",
      "          model: {}\n",
      "          policy_loss: -0.054817963391542435\n",
      "          total_loss: 6.809563159942627\n",
      "          vf_explained_var: 0.07597889751195908\n",
      "          vf_loss: 6.858331203460693\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2114993333816528\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019910994917154312\n",
      "          model: {}\n",
      "          policy_loss: -0.05231647938489914\n",
      "          total_loss: 2.546151876449585\n",
      "          vf_explained_var: 0.2344440072774887\n",
      "          vf_loss: 2.5924952030181885\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.38494623655914\n",
      "    ram_util_percent: 63.94516129032256\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.1\n",
      "    policy2: -7.249999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -14.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07465959144648426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022504415003706743\n",
      "    mean_inference_ms: 7.1683753680040025\n",
      "    mean_raw_obs_processing_ms: 0.42832782535511527\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 16.19999999999999\n",
      "    episode_reward_mean: -0.14999999999998848\n",
      "    episode_reward_min: -24.000000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-13.199999999999982, 4.500000000000009, 1.5000000000000226, 4.199999999999999,\n",
      "        -4.499999999999974, -11.999999999999995, 4.500000000000028, 15.000000000000025,\n",
      "        -3.59999999999998, 2.4000000000000212, 1.2000000000000006, 14.700000000000015,\n",
      "        -0.8999999999999878, 6.3000000000000185, -8.699999999999994, -3.299999999999985,\n",
      "        -1.4999999999999845, 14.399999999999995, -3.5999999999999814, 5.700000000000024,\n",
      "        11.99999999999995, -2.09999999999998, 5.700000000000033, -11.699999999999976,\n",
      "        1.2739809207573671e-14, 7.500000000000028, -0.8999999999999923, -19.500000000000007,\n",
      "        -0.5999999999999869, -10.19999999999998, 1.174060848541103e-14, 7.800000000000015,\n",
      "        -4.799999999999978, -2.699999999999996, -6.899999999999979, 9.599999999999978,\n",
      "        10.500000000000034, 8.400000000000023, 1.1851630787873546e-14, -10.199999999999974,\n",
      "        7.800000000000027, -4.499999999999986, 3.000000000000001, -7.7999999999999865,\n",
      "        -6.299999999999983, 8.700000000000026, 13.499999999999984, -8.399999999999984,\n",
      "        6.900000000000027, -2.999999999999973, 2.7000000000000166, -0.5999999999999946,\n",
      "        2.400000000000015, 7.200000000000012, 3.300000000000007, -5.699999999999985,\n",
      "        -7.7999999999999865, 6.000000000000018, 1.200000000000013, -10.199999999999982,\n",
      "        10.500000000000027, 8.699999999999985, 9.299999999999967, 13.199999999999932,\n",
      "        4.5000000000000195, -3.899999999999976, -8.099999999999998, -8.99999999999998,\n",
      "        3.000000000000007, -11.999999999999984, -5.999999999999986, -10.199999999999978,\n",
      "        -7.4999999999999805, -13.499999999999979, -4.4999999999999805, -21.000000000000036,\n",
      "        14.700000000000017, -10.499999999999977, 9.299999999999997, -7.499999999999989,\n",
      "        3.600000000000002, 0.6000000000000193, -24.000000000000007, -5.999999999999986,\n",
      "        -12.899999999999977, 3.6000000000000223, 6.000000000000025, 7.200000000000017,\n",
      "        16.19999999999999, -1.4999999999999836, 1.2000000000000137, 3.8999999999999964,\n",
      "        16.199999999999974, -10.499999999999979, -4.499999999999975, -2.9999999999999973,\n",
      "        -4.199999999999986, 2.400000000000016, 0.6000000000000184, 7.19999999999998]\n",
      "      policy_policy1_reward: [-6.5, 14.5, 11.5, 12.0, 5.5, -2.0, 14.5, 25.0, 2.0, 8.0,\n",
      "        9.0, 22.5, 8.0, 13.0, -2.0, 4.5, 8.5, 20.0, 2.0, 13.5, 22.0, 3.5, 13.5, -10.5,\n",
      "        10.0, 17.5, 2.5, -9.5, 5.0, -14.5, 4.5, 14.5, 3.0, 4.0, 2.0, 18.5, 20.5, 14.0,\n",
      "        10.0, -3.5, 9.0, 5.5, -3.5, -11.0, 1.5, 16.5, 23.5, 0.5, 12.5, 7.0, 10.5, 5.0,\n",
      "        8.0, 15.0, 10.0, 1.0, 0.0, 16.0, 9.0, -9.0, 20.5, 11.0, 16.0, 21.0, 14.5, -0.5,\n",
      "        -2.5, 1.0, 13.0, -2.0, 4.0, -3.5, 2.5, -3.5, 5.5, -11.0, 22.5, -6.0, 16.0, 2.5,\n",
      "        7.0, 9.5, -14.0, 4.0, -9.5, 12.5, 16.0, 15.0, 24.0, 3.0, 9.0, 9.5, 24.0, -0.5,\n",
      "        5.5, 7.0, 2.5, 8.0, 4.0, 15.0]\n",
      "      policy_policy2_reward: [-6.699999999999995, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999982, -5.599999999999984, -7.7999999999999865,\n",
      "        -7.799999999999981, -8.899999999999986, -6.699999999999986, -6.699999999999993,\n",
      "        -7.79999999999999, -9.99999999999998, -5.599999999999982, -5.599999999999997,\n",
      "        -7.799999999999989, -9.99999999999998, -5.599999999999997, -7.799999999999981,\n",
      "        -1.2000000000000035, -9.99999999999998, -9.99999999999998, -3.399999999999993,\n",
      "        -9.99999999999998, -5.59999999999999, 4.299999999999998, -4.499999999999984,\n",
      "        -6.699999999999986, -7.799999999999988, -6.699999999999988, -8.89999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -5.599999999999995, -9.99999999999998,\n",
      "        -6.699999999999988, -1.2000000000000013, -9.99999999999998, 6.500000000000011,\n",
      "        3.2000000000000055, -7.799999999999981, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999984, -5.6, -9.99999999999998, -7.799999999999989, -5.599999999999982,\n",
      "        -5.599999999999997, -7.799999999999989, -6.69999999999999, -6.699999999999988,\n",
      "        -7.799999999999984, -9.99999999999998, -7.79999999999999, -1.199999999999997,\n",
      "        -9.99999999999998, -2.299999999999991, -6.699999999999983, -7.79999999999999,\n",
      "        -9.99999999999998, -3.400000000000002, -5.599999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -4.499999999999995, -6.699999999999985, -9.99999999999998,\n",
      "        -3.4000000000000035, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -3.400000000000001, -8.899999999999986, -9.99999999999998, -7.799999999999983,\n",
      "        -7.79999999999999, -4.500000000000003, -7.799999999999981, -5.599999999999999,\n",
      "        -7.799999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -5.599999999999992, -3.4000000000000026, -7.799999999999989]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.0\n",
      "      policy2: 6.500000000000011\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.1\n",
      "      policy2: -7.249999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -14.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07465959144648426\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022504415003706743\n",
      "      mean_inference_ms: 7.1683753680040025\n",
      "      mean_raw_obs_processing_ms: 0.42832782535511527\n",
      "  time_since_restore: 332.584445476532\n",
      "  time_this_iter_s: 66.44301128387451\n",
      "  time_total_s: 332.584445476532\n",
      "  timers:\n",
      "    learn_throughput: 112.318\n",
      "    learn_time_ms: 35613.034\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 66511.903\n",
      "  timestamp: 1660562343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:08 (running for 00:06:43.42)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:14 (running for 00:06:48.49)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:19 (running for 00:06:53.56)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=2.1960000000000006 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3EE0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980E3E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980ECAC0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980EC970>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      7 |          320.633 | 21000 |    0.744 |           8.39  |          -7.646 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      7 |          322.827 | 21000 |    2.196 |           9.325 |          -7.129 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-19-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.199999999999925\n",
      "  episode_reward_mean: 0.681000000000011\n",
      "  episode_reward_min: -17.4\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 240\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.136597990989685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011221167631447315\n",
      "          model: {}\n",
      "          policy_loss: -0.04002900794148445\n",
      "          total_loss: 6.694581508636475\n",
      "          vf_explained_var: 0.18336144089698792\n",
      "          vf_loss: 6.73236608505249\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1609917879104614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011695065535604954\n",
      "          model: {}\n",
      "          policy_loss: -0.03987015038728714\n",
      "          total_loss: 1.9700212478637695\n",
      "          vf_explained_var: 0.43486717343330383\n",
      "          vf_loss: 2.0075523853302\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.42714285714285\n",
      "    ram_util_percent: 63.964285714285715\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: 4.30000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.14\n",
      "    policy2: -7.458999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07416969709223861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024725640858926853\n",
      "    mean_inference_ms: 6.340673079735127\n",
      "    mean_raw_obs_processing_ms: 0.44070722084210695\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 22.199999999999925\n",
      "    episode_reward_mean: 0.681000000000011\n",
      "    episode_reward_min: -17.4\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-11.999999999999986, 6.300000000000024, 20.999999999999993, 3.000000000000023,\n",
      "        -7.5, 5.100000000000028, -9.29999999999999, 4.199999999999978, 16.199999999999964,\n",
      "        -4.499999999999979, 4.2000000000000215, -13.499999999999982, 3.600000000000017,\n",
      "        -11.999999999999984, -1.8000000000000331, 0.6000000000000238, -1.199999999999978,\n",
      "        -2.099999999999986, 7.800000000000026, -2.999999999999983, -5.999999999999982,\n",
      "        -4.499999999999972, -7.799999999999978, -3.2999999999999905, 12.00000000000003,\n",
      "        1.8000000000000154, -5.999999999999973, -14.99999999999998, -2.999999999999993,\n",
      "        16.800000000000026, 5.100000000000028, 1.5000000000000113, 9.00000000000002,\n",
      "        -1.7999999999999856, -11.699999999999978, 11.100000000000032, 14.399999999999961,\n",
      "        13.500000000000005, 3.3000000000000034, 2.70000000000002, 2.700000000000028,\n",
      "        -4.499999999999988, 3.300000000000014, -7.49999999999998, 19.199999999999925,\n",
      "        7.500000000000032, -5.999999999999995, -4.199999999999978, -17.4, 6.599999999999978,\n",
      "        -5.999999999999979, -6.8999999999999755, -9.59999999999998, -8.399999999999991,\n",
      "        2.999999999999992, -7.799999999999976, 7.500000000000025, -5.999999999999991,\n",
      "        7.500000000000032, -11.999999999999984, 9.299999999999944, 2.7000000000000317,\n",
      "        -2.999999999999987, -13.49999999999998, 1.2000000000000264, -0.8999999999999955,\n",
      "        11.400000000000006, 1.5000000000000013, 22.199999999999925, 5.100000000000021,\n",
      "        2.7283730830163222e-14, -1.8000000000000154, -5.399999999999981, -10.199999999999978,\n",
      "        9.000000000000025, 7.499999999999998, 8.965050923848139e-15, 0.3000000000000005,\n",
      "        14.099999999999937, -2.9999999999999933, 1.200000000000029, 8.099999999999998,\n",
      "        -10.799999999999978, -0.2999999999999914, 9.00000000000003, 12.00000000000003,\n",
      "        -4.199999999999973, 0.6000000000000291, -2.099999999999978, 6.0000000000000195,\n",
      "        3.600000000000021, -11.999999999999977, -4.199999999999988, -7.799999999999981,\n",
      "        5.700000000000028, -11.999999999999972, 3.300000000000029, 19.499999999999936,\n",
      "        6.3000000000000185, 1.500000000000028]\n",
      "      policy_policy1_reward: [-7.5, 13.0, 31.0, 13.0, 2.5, 14.0, -1.5, 6.5, 24.0, 5.5,\n",
      "        12.0, -3.5, 12.5, -2.0, 6.0, 9.5, 5.5, -2.0, 3.5, 7.0, 4.0, 5.5, -5.5, 4.5,\n",
      "        22.0, 8.5, 4.0, -5.0, 1.5, 23.5, 8.5, 11.5, 19.0, 0.5, -10.5, 20.0, 14.5, 23.5,\n",
      "        4.5, 5.0, 10.5, 5.5, 4.5, -3.0, 27.0, 17.5, -7.0, 2.5, -8.5, 15.5, 4.0, 2.0,\n",
      "        -4.0, 0.5, 13.0, 0.0, 17.5, 4.0, 17.5, -2.0, 16.0, 10.5, 7.0, -3.5, 9.0, 8.0,\n",
      "        17.0, 11.5, 30.0, 14.0, 10.0, 6.0, -2.0, -3.5, 19.0, 17.5, 10.0, 7.0, 23.0,\n",
      "        7.0, 9.0, 17.0, -3.0, 7.5, 19.0, 22.0, 2.5, 4.0, 3.5, 16.0, 12.5, -2.0, 2.5,\n",
      "        0.0, 13.5, -2.0, 10.0, 24.0, 13.0, 6.0]\n",
      "      policy_policy2_reward: [-4.5, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999983, -7.799999999999981, -2.2999999999999834,\n",
      "        -7.799999999999989, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999986, -8.899999999999983,\n",
      "        -6.699999999999995, -0.09999999999999187, 4.30000000000001, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -2.3000000000000034, -7.79999999999999,\n",
      "        -9.99999999999998, -6.69999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -4.500000000000001, -6.6999999999999815, -3.3999999999999924, -9.99999999999998,\n",
      "        -9.99999999999998, -2.299999999999989, -1.1999999999999869, -8.89999999999998,\n",
      "        -0.09999999999999423, -9.99999999999998, -1.199999999999999, -2.3000000000000056,\n",
      "        -7.799999999999986, -9.99999999999998, -1.1999999999999886, -4.5000000000000036,\n",
      "        -7.79999999999999, -9.99999999999998, 0.9999999999999959, -6.6999999999999815,\n",
      "        -8.899999999999986, -8.899999999999983, -9.99999999999998, -8.89999999999998,\n",
      "        -5.599999999999993, -8.89999999999998, -9.99999999999998, -7.7999999999999865,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999995, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -5.599999999999988, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -3.4000000000000066, -6.69999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999994, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -8.899999999999986, -7.799999999999981, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999995, -3.400000000000001,\n",
      "        -5.6, -9.99999999999998, -8.89999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -7.799999999999982, -7.799999999999981, -9.99999999999998, -6.69999999999999,\n",
      "        -4.499999999999987, -6.6999999999999815, -4.499999999999987]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 31.0\n",
      "      policy2: 4.30000000000001\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.14\n",
      "      policy2: -7.458999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -10.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07416969709223861\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024725640858926853\n",
      "      mean_inference_ms: 6.340673079735127\n",
      "      mean_raw_obs_processing_ms: 0.44070722084210695\n",
      "  time_since_restore: 370.0492687225342\n",
      "  time_this_iter_s: 49.41634130477905\n",
      "  time_total_s: 370.0492687225342\n",
      "  timers:\n",
      "    learn_throughput: 118.633\n",
      "    learn_time_ms: 25288.11\n",
      "    synch_weights_time_ms: 3.117\n",
      "    training_iteration_time_ms: 46248.678\n",
      "  timestamp: 1660562360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 8\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-19-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.299999999999976\n",
      "  episode_reward_mean: 3.701999999999997\n",
      "  episode_reward_min: -21.000000000000043\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 240\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0890227556228638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020198805257678032\n",
      "          model: {}\n",
      "          policy_loss: -0.05690683797001839\n",
      "          total_loss: 6.83250617980957\n",
      "          vf_explained_var: 0.1650695651769638\n",
      "          vf_loss: 6.883352279663086\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.077040433883667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018466828390955925\n",
      "          model: {}\n",
      "          policy_loss: -0.06017360836267471\n",
      "          total_loss: 2.562284469604492\n",
      "          vf_explained_var: 0.22985106706619263\n",
      "          vf_loss: 2.614147901535034\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.40845070422536\n",
      "    ram_util_percent: 63.96338028169015\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 38.5\n",
      "    policy2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    policy1: 11.095\n",
      "    policy2: -7.392999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07126952798766069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022939207176581933\n",
      "    mean_inference_ms: 6.446409369838123\n",
      "    mean_raw_obs_processing_ms: 0.440414938790508\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 30.299999999999976\n",
      "    episode_reward_mean: 3.701999999999997\n",
      "    episode_reward_min: -21.000000000000043\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-16.19999999999998, -2.700000000000003, 1.5000000000000233, 11.100000000000017,\n",
      "        -11.699999999999992, -9.299999999999992, 5.700000000000015, -10.49999999999999,\n",
      "        -10.499999999999977, 0.3000000000000014, -8.999999999999982, 9.300000000000026,\n",
      "        4.499999999999998, -0.8999999999999824, -8.69999999999999, -3.8999999999999946,\n",
      "        19.499999999999986, 8.70000000000003, 3.300000000000016, -7.4999999999999805,\n",
      "        -14.99999999999999, 10.199999999999942, 3.5999999999999983, 0.600000000000022,\n",
      "        10.500000000000034, -1.499999999999996, 6.900000000000022, -11.999999999999991,\n",
      "        -7.79999999999999, 14.39999999999998, 6.000000000000007, 11.69999999999995,\n",
      "        1.5000000000000306, -2.9999999999999902, -9.899999999999977, -3.899999999999984,\n",
      "        10.19999999999997, -5.999999999999988, 7.500000000000032, 4.500000000000028,\n",
      "        5.699999999999974, -5.699999999999985, 7.80000000000001, 13.499999999999993,\n",
      "        7.800000000000026, 23.99999999999998, 11.400000000000006, 9.900000000000025,\n",
      "        -0.8999999999999807, -3.0000000000000053, 15.299999999999905, 4.199999999999978,\n",
      "        -9.299999999999985, 3.6000000000000187, -21.000000000000043, -5.9999999999999805,\n",
      "        16.79999999999992, 6.000000000000011, 12.599999999999943, 1.5000000000000244,\n",
      "        15.599999999999921, 28.499999999999893, 13.799999999999983, -8.999999999999979,\n",
      "        18.59999999999997, -0.29999999999997373, 9.000000000000012, -5.399999999999979,\n",
      "        19.199999999999996, 8.700000000000019, 5.999999999999947, -10.799999999999981,\n",
      "        11.999999999999966, 15.599999999999959, 5.100000000000022, 14.699999999999944,\n",
      "        -13.49999999999998, 6.000000000000011, 20.399999999999906, 26.999999999999922,\n",
      "        -7.1999999999999815, 3.599999999999971, 6.000000000000007, 8.700000000000024,\n",
      "        0.6000000000000215, -1.4999999999999782, -8.399999999999977, 8.100000000000007,\n",
      "        -1.4999999999999747, -2.999999999999975, 14.699999999999969, -11.699999999999982,\n",
      "        17.999999999999915, -7.499999999999973, 6.000000000000027, 9.000000000000027,\n",
      "        30.299999999999976, 8.099999999999927, 1.484923295436147e-14, 20.999999999999915]\n",
      "      policy_policy1_reward: [-15.0, 4.0, 11.5, 20.0, -5.0, -1.5, 8.0, -0.5, -0.5, 7.0,\n",
      "        1.0, 10.5, 9.0, 8.0, -2.0, 5.0, 29.5, 16.5, 10.0, 2.5, -5.0, 18.0, 7.0, 9.5,\n",
      "        20.5, 8.5, 12.5, -2.0, -5.5, 20.0, 10.5, 19.5, 11.5, 7.0, -1.0, 5.0, 18.0, -12.5,\n",
      "        12.0, 14.5, 8.0, 1.0, 14.5, 23.5, 14.5, 28.5, 17.0, 15.5, 8.0, 7.0, 22.0, 12.0,\n",
      "        -7.0, 12.5, -11.0, 4.0, 23.5, 10.5, 21.5, 11.5, 24.5, 38.5, 20.5, 1.0, 27.5,\n",
      "        7.5, 19.0, 3.5, 27.0, 11.0, 16.0, -8.5, 22.0, 24.5, 8.5, 17.0, -3.5, 10.5, 26.0,\n",
      "        37.0, -6.0, 12.5, 10.5, 16.5, 9.5, 8.5, 0.5, 17.0, 8.5, 7.0, 22.5, -5.0, 28.0,\n",
      "        2.5, 16.0, 19.0, 37.0, 17.0, 10.0, 25.5]\n",
      "      policy_policy2_reward: [-1.2000000000000026, -6.699999999999995, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999995, -7.799999999999983, -2.3000000000000034,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999988, -9.99999999999998,\n",
      "        -1.1999999999999833, -4.500000000000002, -8.89999999999998, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -7.79999999999999, -3.399999999999993,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999995,\n",
      "        -9.99999999999998, -2.3000000000000034, -5.599999999999994, -4.5000000000000036,\n",
      "        -7.799999999999987, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.89999999999998, -7.799999999999986, 6.500000000000011, -4.500000000000001,\n",
      "        -9.99999999999998, -2.300000000000005, -6.699999999999994, -6.699999999999987,\n",
      "        -9.99999999999998, -6.699999999999985, -4.500000000000002, -5.599999999999982,\n",
      "        -5.6, -8.89999999999998, -9.99999999999998, -6.699999999999995, -7.799999999999981,\n",
      "        -2.300000000000003, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999988, -4.499999999999997, -8.899999999999986, -9.99999999999998,\n",
      "        -8.899999999999984, -9.99999999999998, -6.699999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999986, -9.99999999999998, -8.89999999999998,\n",
      "        -7.799999999999981, -2.3000000000000047, -9.99999999999998, -2.3000000000000043,\n",
      "        -9.99999999999998, -8.89999999999998, -3.400000000000005, -2.3000000000000047,\n",
      "        -9.99999999999998, -4.499999999999998, -5.599999999999999, -9.99999999999998,\n",
      "        -1.200000000000003, -8.89999999999998, -4.499999999999982, -7.79999999999999,\n",
      "        -8.899999999999983, -9.99999999999998, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999989, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999985, -8.89999999999998, -9.99999999999998, -4.500000000000004]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.5\n",
      "      policy2: 6.500000000000011\n",
      "    policy_reward_mean:\n",
      "      policy1: 11.095\n",
      "      policy2: -7.392999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -15.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07126952798766069\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022939207176581933\n",
      "      mean_inference_ms: 6.446409369838123\n",
      "      mean_raw_obs_processing_ms: 0.440414938790508\n",
      "  time_since_restore: 372.75112652778625\n",
      "  time_this_iter_s: 49.923983097076416\n",
      "  time_total_s: 372.75112652778625\n",
      "  timers:\n",
      "    learn_throughput: 118.327\n",
      "    learn_time_ms: 25353.511\n",
      "    synch_weights_time_ms: 3.116\n",
      "    training_iteration_time_ms: 46588.281\n",
      "  timestamp: 1660562360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 8\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:25 (running for 00:06:59.90)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:30 (running for 00:07:05.04)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:35 (running for 00:07:10.10)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      5 |          316.089 | 20000 |   -1.611 |           5.98  |          -7.591 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-19-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.599999999999945\n",
      "  episode_reward_mean: -0.48299999999998905\n",
      "  episode_reward_min: -19.5\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.22789466381073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011083769612014294\n",
      "          model: {}\n",
      "          policy_loss: -0.034938745200634\n",
      "          total_loss: 7.126927852630615\n",
      "          vf_explained_var: 0.009180993773043156\n",
      "          vf_loss: 7.159649848937988\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2072699069976807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012786424718797207\n",
      "          model: {}\n",
      "          policy_loss: -0.039569079875946045\n",
      "          total_loss: 2.686042308807373\n",
      "          vf_explained_var: 0.1500360071659088\n",
      "          vf_loss: 2.7230541706085205\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.74042553191489\n",
      "    ram_util_percent: 63.99787234042552\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 25.5\n",
      "    policy2: 4.3000000000000025\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.745\n",
      "    policy2: -7.227999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -12.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07832571114022638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0250918835632815\n",
      "    mean_inference_ms: 6.316368794619705\n",
      "    mean_raw_obs_processing_ms: 0.43287934842205383\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 27.599999999999945\n",
      "    episode_reward_mean: -0.48299999999998905\n",
      "    episode_reward_min: -19.5\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-8.399999999999993, 11.400000000000032, 2.1000000000000187, -17.09999999999998,\n",
      "        -8.99999999999998, -4.499999999999989, 2.399999999999976, -1.4999999999999818,\n",
      "        8.700000000000005, -2.999999999999995, -4.799999999999992, 5.100000000000004,\n",
      "        -11.699999999999985, 2.7727820040013285e-14, 5.400000000000022, -1.4999999999999907,\n",
      "        -0.5999999999999709, -5.399999999999979, 6.599999999999973, 11.700000000000028,\n",
      "        2.3999999999999875, 6.600000000000005, 0.6000000000000066, -7.799999999999974,\n",
      "        2.0999999999999917, -7.199999999999982, -7.799999999999978, 9.000000000000025,\n",
      "        6.299999999999979, 1.5000000000000107, -12.899999999999991, -8.39999999999999,\n",
      "        -0.8999999999999932, -19.5, -14.09999999999998, -7.499999999999989, -7.19999999999999,\n",
      "        3.0000000000000164, 1.5000000000000147, -1.499999999999973, 6.600000000000016,\n",
      "        -0.8999999999999856, 4.500000000000012, -8.999999999999977, -1.799999999999986,\n",
      "        -4.799999999999978, 9.000000000000012, -14.99999999999999, 13.800000000000013,\n",
      "        -0.8999999999999924, -10.499999999999973, 3.000000000000015, 1.5000000000000226,\n",
      "        7.500000000000032, 3.9000000000000266, -12.89999999999998, -6.5999999999999766,\n",
      "        6.38378239159465e-16, 1.199999999999974, -4.499999999999986, -5.399999999999981,\n",
      "        -5.699999999999985, -12.599999999999982, -2.9999999999999876, -1.7999999999999807,\n",
      "        6.300000000000027, 27.599999999999945, 2.4000000000000252, -14.39999999999998,\n",
      "        11.099999999999971, 3.0000000000000133, 14.09999999999998, 1.5000000000000022,\n",
      "        -0.8999999999999886, -10.799999999999999, 5.700000000000033, -9.299999999999988,\n",
      "        11.099999999999993, 12.599999999999934, -0.8999999999999798, 10.800000000000033,\n",
      "        4.500000000000021, 4.5000000000000275, 2.100000000000025, 2.4000000000000226,\n",
      "        -13.499999999999977, -4.500000000000012, 10.500000000000016, 0.599999999999958,\n",
      "        13.799999999999995, 4.50000000000003, -3.5999999999999766, 2.9999999999999987,\n",
      "        -10.499999999999988, -11.09999999999998, 3.0000000000000044, 9.000000000000032,\n",
      "        1.500000000000008, -5.6999999999999815, -8.399999999999979]\n",
      "      policy_policy1_reward: [0.5, 11.5, 11.0, -11.5, -4.5, 5.5, 8.0, 8.5, 16.5, 1.5,\n",
      "        3.0, 14.0, -5.0, 10.0, 5.5, 8.5, 5.0, 3.5, 15.5, 19.5, 8.0, 15.5, 4.0, 0.0,\n",
      "        11.0, -6.0, 0.0, 19.0, 13.0, 11.5, -4.0, 0.5, 8.0, -9.5, -8.5, 2.5, -11.5, 7.5,\n",
      "        6.0, 8.5, 15.5, 8.0, 14.5, -4.5, 6.0, -2.5, 19.0, -5.0, 20.5, 8.0, -0.5, 13.0,\n",
      "        11.5, 17.5, 4.0, -9.5, -1.0, 10.0, 9.0, 5.5, 3.5, -4.5, -12.5, 7.0, 0.5, 13.0,\n",
      "        25.5, 8.0, -5.5, 20.0, 13.0, 23.0, 11.5, 8.0, -3.0, 13.5, -1.5, 20.0, 21.5,\n",
      "        8.0, 17.5, 14.5, 14.5, 11.0, 8.0, -3.5, 5.5, 9.5, 9.5, 20.5, 14.5, 2.0, 13.0,\n",
      "        -0.5, -5.5, 7.5, 13.5, 11.5, 1.0, 0.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -0.10000000000000375, -8.899999999999986,\n",
      "        -5.599999999999998, -4.499999999999998, -9.99999999999998, -5.5999999999999925,\n",
      "        -9.99999999999998, -7.799999999999981, -4.499999999999996, -7.799999999999989,\n",
      "        -8.89999999999998, -6.6999999999999815, -9.99999999999998, -0.10000000000000309,\n",
      "        -9.99999999999998, -5.599999999999998, -8.89999999999998, -8.89999999999998,\n",
      "        -7.799999999999982, -5.5999999999999925, -8.899999999999984, -3.4000000000000057,\n",
      "        -7.799999999999981, -8.899999999999986, -1.2000000000000035, -7.7999999999999865,\n",
      "        -9.99999999999998, -6.699999999999992, -9.99999999999998, -8.899999999999983,\n",
      "        -8.89999999999998, -8.899999999999986, -9.99999999999998, -5.599999999999997,\n",
      "        -9.99999999999998, 4.3000000000000025, -4.499999999999989, -4.499999999999982,\n",
      "        -9.99999999999998, -8.899999999999983, -8.89999999999998, -9.99999999999998,\n",
      "        -4.499999999999995, -7.799999999999981, -2.299999999999997, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -0.09999999999999898,\n",
      "        -3.4000000000000035, -5.599999999999982, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.899999999999986, -1.2000000000000055, -0.09999999999999487,\n",
      "        -9.99999999999998, -2.3000000000000047, -6.699999999999995, 2.099999999999996,\n",
      "        -5.599999999999999, -8.899999999999986, -8.899999999999986, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -8.89999999999998, -7.79999999999999,\n",
      "        -7.799999999999981, -7.799999999999981, -8.89999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -6.699999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        0.9999999999999952, -8.899999999999983, -6.69999999999999, -9.99999999999998,\n",
      "        -5.5999999999999925, -9.99999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -4.499999999999995, -4.500000000000003, -9.99999999999998, -6.699999999999986,\n",
      "        -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 25.5\n",
      "      policy2: 4.3000000000000025\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.745\n",
      "      policy2: -7.227999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -12.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07832571114022638\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0250918835632815\n",
      "      mean_inference_ms: 6.316368794619705\n",
      "      mean_raw_obs_processing_ms: 0.43287934842205383\n",
      "  time_since_restore: 381.9101448059082\n",
      "  time_this_iter_s: 65.82097578048706\n",
      "  time_total_s: 381.9101448059082\n",
      "  timers:\n",
      "    learn_throughput: 111.852\n",
      "    learn_time_ms: 35761.69\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 63640.886\n",
      "  timestamp: 1660562377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:42 (running for 00:07:16.88)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:47 (running for 00:07:21.94)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:52 (running for 00:07:27.02)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:19:57 (running for 00:07:32.06)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:02 (running for 00:07:37.14)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:07 (running for 00:07:42.19)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=3.701999999999997 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AE20>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815A580>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AA00>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99815AF70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997DF2C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      8 |          370.049 | 24000 |    0.681 |           8.14  |          -7.459 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      8 |          372.751 | 24000 |    3.702 |          11.095 |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      5 |          332.584 | 20000 |   -0.15  |           7.1   |          -7.25  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-20-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: 2.097000000000009\n",
      "  episode_reward_min: -18.29999999999999\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 270\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.099258542060852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01301400363445282\n",
      "          model: {}\n",
      "          policy_loss: -0.04405347630381584\n",
      "          total_loss: 7.063788414001465\n",
      "          vf_explained_var: 0.11632663756608963\n",
      "          vf_loss: 7.105238914489746\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.137921690940857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01163367461413145\n",
      "          model: {}\n",
      "          policy_loss: -0.04120894521474838\n",
      "          total_loss: 2.674889087677002\n",
      "          vf_explained_var: 0.23913991451263428\n",
      "          vf_loss: 2.7137715816497803\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 54000\n",
      "  num_agent_steps_trained: 54000\n",
      "  num_env_steps_sampled: 27000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 27000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.529999999999998\n",
      "    ram_util_percent: 63.98999999999999\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 3.200000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.435\n",
      "    policy2: -7.337999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07411070276946587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024584838106144372\n",
      "    mean_inference_ms: 6.411816013243701\n",
      "    mean_raw_obs_processing_ms: 0.4397515660241475\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.099999999999945\n",
      "    episode_reward_mean: 2.097000000000009\n",
      "    episode_reward_min: -18.29999999999999\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [5.100000000000028, 1.5000000000000113, 9.00000000000002, -1.7999999999999856,\n",
      "        -11.699999999999978, 11.100000000000032, 14.399999999999961, 13.500000000000005,\n",
      "        3.3000000000000034, 2.70000000000002, 2.700000000000028, -4.499999999999988,\n",
      "        3.300000000000014, -7.49999999999998, 19.199999999999925, 7.500000000000032,\n",
      "        -5.999999999999995, -4.199999999999978, -17.4, 6.599999999999978, -5.999999999999979,\n",
      "        -6.8999999999999755, -9.59999999999998, -8.399999999999991, 2.999999999999992,\n",
      "        -7.799999999999976, 7.500000000000025, -5.999999999999991, 7.500000000000032,\n",
      "        -11.999999999999984, 9.299999999999944, 2.7000000000000317, -2.999999999999987,\n",
      "        -13.49999999999998, 1.2000000000000264, -0.8999999999999955, 11.400000000000006,\n",
      "        1.5000000000000013, 22.199999999999925, 5.100000000000021, 2.7283730830163222e-14,\n",
      "        -1.8000000000000154, -5.399999999999981, -10.199999999999978, 9.000000000000025,\n",
      "        7.499999999999998, 8.965050923848139e-15, 0.3000000000000005, 14.099999999999937,\n",
      "        -2.9999999999999933, 1.200000000000029, 8.099999999999998, -10.799999999999978,\n",
      "        -0.2999999999999914, 9.00000000000003, 12.00000000000003, -4.199999999999973,\n",
      "        0.6000000000000291, -2.099999999999978, 6.0000000000000195, 3.600000000000021,\n",
      "        -11.999999999999977, -4.199999999999988, -7.799999999999981, 5.700000000000028,\n",
      "        -11.999999999999972, 3.300000000000029, 19.499999999999936, 6.3000000000000185,\n",
      "        1.500000000000028, 5.100000000000021, 15.900000000000007, -18.29999999999999,\n",
      "        2.2842838731662596e-14, 18.60000000000001, -9.599999999999977, -8.999999999999972,\n",
      "        8.100000000000032, 18.59999999999995, 20.09999999999997, 9.90000000000002, 7.200000000000031,\n",
      "        12.000000000000018, 1.2000000000000148, -0.2999999999999897, -7.499999999999977,\n",
      "        19.49999999999998, -0.9000000000000041, -6.299999999999978, 8.09999999999992,\n",
      "        0.9000000000000209, -6.900000000000004, 15.899999999999975, -0.8999999999999718,\n",
      "        6.000000000000027, 1.3961054534661343e-14, 23.099999999999945, 6.599999999999991,\n",
      "        1.5000000000000209, -6.899999999999977]\n",
      "      policy_policy1_reward: [8.5, 11.5, 19.0, 0.5, -10.5, 20.0, 14.5, 23.5, 4.5, 5.0,\n",
      "        10.5, 5.5, 4.5, -3.0, 27.0, 17.5, -7.0, 2.5, -8.5, 15.5, 4.0, 2.0, -4.0, 0.5,\n",
      "        13.0, 0.0, 17.5, 4.0, 17.5, -2.0, 16.0, 10.5, 7.0, -3.5, 9.0, 8.0, 17.0, 11.5,\n",
      "        30.0, 14.0, 10.0, 6.0, -2.0, -3.5, 19.0, 17.5, 10.0, 7.0, 23.0, 7.0, 9.0, 17.0,\n",
      "        -3.0, 7.5, 19.0, 22.0, 2.5, 4.0, 3.5, 16.0, 12.5, -2.0, 2.5, 0.0, 13.5, -2.0,\n",
      "        10.0, 24.0, 13.0, 6.0, 14.0, 21.5, -10.5, 10.0, 27.5, -4.0, 1.0, 17.0, 27.5,\n",
      "        29.0, 15.5, 15.0, 16.5, 3.5, 7.5, 2.5, 29.5, 8.0, -9.5, 17.0, 6.5, 2.0, 16.0,\n",
      "        2.5, 10.5, 10.0, 32.0, 10.0, 11.5, 2.0]\n",
      "      policy_policy2_reward: [-3.3999999999999924, -9.99999999999998, -9.99999999999998,\n",
      "        -2.299999999999989, -1.1999999999999869, -8.89999999999998, -0.09999999999999423,\n",
      "        -9.99999999999998, -1.199999999999999, -2.3000000000000056, -7.799999999999986,\n",
      "        -9.99999999999998, -1.1999999999999886, -4.5000000000000036, -7.79999999999999,\n",
      "        -9.99999999999998, 0.9999999999999959, -6.6999999999999815, -8.899999999999986,\n",
      "        -8.899999999999983, -9.99999999999998, -8.89999999999998, -5.599999999999993,\n",
      "        -8.89999999999998, -9.99999999999998, -7.7999999999999865, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999995,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -5.599999999999988, -9.99999999999998, -7.799999999999981,\n",
      "        -8.89999999999998, -9.99999999999998, -7.799999999999981, -3.4000000000000066,\n",
      "        -6.69999999999999, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999994, -8.89999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -8.899999999999986, -7.799999999999981, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999995, -3.400000000000001, -5.6, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -6.6999999999999815, -7.799999999999982,\n",
      "        -7.799999999999981, -9.99999999999998, -6.69999999999999, -4.499999999999987,\n",
      "        -6.6999999999999815, -4.499999999999987, -8.89999999999998, -5.599999999999999,\n",
      "        -7.79999999999999, -9.99999999999998, -8.89999999999998, -5.6, -9.99999999999998,\n",
      "        -8.899999999999984, -8.89999999999998, -8.89999999999998, -5.599999999999993,\n",
      "        -7.799999999999981, -4.499999999999998, -2.2999999999999976, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, 3.200000000000001,\n",
      "        -8.899999999999986, -5.5999999999999845, -8.89999999999998, -0.10000000000000109,\n",
      "        -3.399999999999999, -4.500000000000002, -9.99999999999998, -8.89999999999998,\n",
      "        -3.4, -9.99999999999998, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.0\n",
      "      policy2: 3.200000000000001\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.435\n",
      "      policy2: -7.337999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -10.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07411070276946587\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024584838106144372\n",
      "      mean_inference_ms: 6.411816013243701\n",
      "      mean_raw_obs_processing_ms: 0.4397515660241475\n",
      "  time_since_restore: 419.78772592544556\n",
      "  time_this_iter_s: 49.73845720291138\n",
      "  time_total_s: 419.78772592544556\n",
      "  timers:\n",
      "    learn_throughput: 117.459\n",
      "    learn_time_ms: 25540.793\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 46635.767\n",
      "  timestamp: 1660562409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 9\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 54000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.299999999999976\n",
      "  episode_reward_mean: 5.525999999999994\n",
      "  episode_reward_min: -21.000000000000043\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 270\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.042231559753418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019840843975543976\n",
      "          model: {}\n",
      "          policy_loss: -0.05992204695940018\n",
      "          total_loss: 6.345377445220947\n",
      "          vf_explained_var: 0.23646777868270874\n",
      "          vf_loss: 6.396370887756348\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0498954057693481\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018740452826023102\n",
      "          model: {}\n",
      "          policy_loss: -0.05551488697528839\n",
      "          total_loss: 2.187725067138672\n",
      "          vf_explained_var: 0.30505481362342834\n",
      "          vf_loss: 2.234807014465332\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_env_steps_sampled: 27000\n",
      "    num_env_steps_trained: 27000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 54000\n",
      "  num_agent_steps_trained: 54000\n",
      "  num_env_steps_sampled: 27000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 27000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.514285714285716\n",
      "    ram_util_percent: 63.98857142857141\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 38.5\n",
      "    policy2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    policy1: 13.095\n",
      "    policy2: -7.568999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -12.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07147007649208827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02291411438635179\n",
      "    mean_inference_ms: 6.51097284006049\n",
      "    mean_raw_obs_processing_ms: 0.4394989798447919\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 30.299999999999976\n",
      "    episode_reward_mean: 5.525999999999994\n",
      "    episode_reward_min: -21.000000000000043\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [6.000000000000007, 11.69999999999995, 1.5000000000000306, -2.9999999999999902,\n",
      "        -9.899999999999977, -3.899999999999984, 10.19999999999997, -5.999999999999988,\n",
      "        7.500000000000032, 4.500000000000028, 5.699999999999974, -5.699999999999985,\n",
      "        7.80000000000001, 13.499999999999993, 7.800000000000026, 23.99999999999998,\n",
      "        11.400000000000006, 9.900000000000025, -0.8999999999999807, -3.0000000000000053,\n",
      "        15.299999999999905, 4.199999999999978, -9.299999999999985, 3.6000000000000187,\n",
      "        -21.000000000000043, -5.9999999999999805, 16.79999999999992, 6.000000000000011,\n",
      "        12.599999999999943, 1.5000000000000244, 15.599999999999921, 28.499999999999893,\n",
      "        13.799999999999983, -8.999999999999979, 18.59999999999997, -0.29999999999997373,\n",
      "        9.000000000000012, -5.399999999999979, 19.199999999999996, 8.700000000000019,\n",
      "        5.999999999999947, -10.799999999999981, 11.999999999999966, 15.599999999999959,\n",
      "        5.100000000000022, 14.699999999999944, -13.49999999999998, 6.000000000000011,\n",
      "        20.399999999999906, 26.999999999999922, -7.1999999999999815, 3.599999999999971,\n",
      "        6.000000000000007, 8.700000000000024, 0.6000000000000215, -1.4999999999999782,\n",
      "        -8.399999999999977, 8.100000000000007, -1.4999999999999747, -2.999999999999975,\n",
      "        14.699999999999969, -11.699999999999982, 17.999999999999915, -7.499999999999973,\n",
      "        6.000000000000027, 9.000000000000027, 30.299999999999976, 8.099999999999927,\n",
      "        1.484923295436147e-14, 20.999999999999915, -2.7000000000000295, 5.700000000000015,\n",
      "        -1.499999999999981, 11.999999999999941, 9.59999999999995, 5.7000000000000215,\n",
      "        3.600000000000026, 6.899999999999961, 3.000000000000033, 12.6, 3.30000000000003,\n",
      "        6.60000000000003, 4.500000000000007, -0.8999999999999861, 3.0000000000000258,\n",
      "        17.09999999999995, 1.5000000000000049, 3.0000000000000275, 4.499999999999975,\n",
      "        4.800000000000015, 8.10000000000003, -1.799999999999971, 7.500000000000034,\n",
      "        -8.699999999999978, 18.599999999999948, 17.99999999999993, 15.899999999999965,\n",
      "        3.299999999999968, -17.999999999999986, 20.099999999999895]\n",
      "      policy_policy1_reward: [10.5, 19.5, 11.5, 7.0, -1.0, 5.0, 18.0, -12.5, 12.0, 14.5,\n",
      "        8.0, 1.0, 14.5, 23.5, 14.5, 28.5, 17.0, 15.5, 8.0, 7.0, 22.0, 12.0, -7.0, 12.5,\n",
      "        -11.0, 4.0, 23.5, 10.5, 21.5, 11.5, 24.5, 38.5, 20.5, 1.0, 27.5, 7.5, 19.0,\n",
      "        3.5, 27.0, 11.0, 16.0, -8.5, 22.0, 24.5, 8.5, 17.0, -3.5, 10.5, 26.0, 37.0,\n",
      "        -6.0, 12.5, 10.5, 16.5, 9.5, 8.5, 0.5, 17.0, 8.5, 7.0, 22.5, -5.0, 28.0, 2.5,\n",
      "        16.0, 19.0, 37.0, 17.0, 10.0, 25.5, 4.0, 13.5, 8.5, 22.0, 18.5, 8.0, 12.5, 7.0,\n",
      "        13.0, 21.5, 10.0, 15.5, 14.5, 8.0, 7.5, 26.0, 11.5, 13.0, 14.5, 11.5, 17.0,\n",
      "        6.0, 17.5, -2.0, 22.0, 28.0, 21.5, 10.0, -8.0, 29.0]\n",
      "      policy_policy2_reward: [-4.5000000000000036, -7.799999999999987, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.89999999999998, -7.799999999999986,\n",
      "        6.500000000000011, -4.500000000000001, -9.99999999999998, -2.300000000000005,\n",
      "        -6.699999999999994, -6.699999999999987, -9.99999999999998, -6.699999999999985,\n",
      "        -4.500000000000002, -5.599999999999982, -5.6, -8.89999999999998, -9.99999999999998,\n",
      "        -6.699999999999995, -7.799999999999981, -2.300000000000003, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999988, -4.499999999999997,\n",
      "        -8.899999999999986, -9.99999999999998, -8.899999999999984, -9.99999999999998,\n",
      "        -6.699999999999986, -9.99999999999998, -8.89999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999981, -2.3000000000000047,\n",
      "        -9.99999999999998, -2.3000000000000043, -9.99999999999998, -8.89999999999998,\n",
      "        -3.400000000000005, -2.3000000000000047, -9.99999999999998, -4.499999999999998,\n",
      "        -5.599999999999999, -9.99999999999998, -1.200000000000003, -8.89999999999998,\n",
      "        -4.499999999999982, -7.79999999999999, -8.899999999999983, -9.99999999999998,\n",
      "        -8.89999999999998, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999989, -6.6999999999999815, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999985, -8.89999999999998,\n",
      "        -9.99999999999998, -4.500000000000004, -6.69999999999999, -7.799999999999981,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999984, -2.299999999999995,\n",
      "        -8.89999999999998, -0.1000000000000042, -9.99999999999998, -8.89999999999998,\n",
      "        -6.6999999999999815, -8.89999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -4.499999999999997, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999988, -8.89999999999998, -7.79999999999999,\n",
      "        -9.99999999999998, -6.699999999999982, -3.4000000000000057, -9.99999999999998,\n",
      "        -5.599999999999998, -6.699999999999989, -9.99999999999998, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.5\n",
      "      policy2: 6.500000000000011\n",
      "    policy_reward_mean:\n",
      "      policy1: 13.095\n",
      "      policy2: -7.568999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -12.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07147007649208827\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02291411438635179\n",
      "      mean_inference_ms: 6.51097284006049\n",
      "      mean_raw_obs_processing_ms: 0.4394989798447919\n",
      "  time_since_restore: 422.7349259853363\n",
      "  time_this_iter_s: 49.98379945755005\n",
      "  time_total_s: 422.7349259853363\n",
      "  timers:\n",
      "    learn_throughput: 117.273\n",
      "    learn_time_ms: 25581.419\n",
      "    synch_weights_time_ms: 3.103\n",
      "    training_iteration_time_ms: 46964.896\n",
      "  timestamp: 1660562410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 9\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-20-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.19999999999999\n",
      "  episode_reward_mean: 0.9390000000000113\n",
      "  episode_reward_min: -24.000000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1866399049758911\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01788230612874031\n",
      "          model: {}\n",
      "          policy_loss: -0.04919588193297386\n",
      "          total_loss: 6.7157301902771\n",
      "          vf_explained_var: 0.09981973469257355\n",
      "          vf_loss: 6.756878852844238\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1607000827789307\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019952882081270218\n",
      "          model: {}\n",
      "          policy_loss: -0.053953852504491806\n",
      "          total_loss: 2.5053443908691406\n",
      "          vf_explained_var: 0.20789991319179535\n",
      "          vf_loss: 2.553312063217163\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.18958333333333\n",
      "    ram_util_percent: 63.99166666666665\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 24.0\n",
      "    policy2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.925\n",
      "    policy2: -6.985999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0739505639985902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022974200850186914\n",
      "    mean_inference_ms: 7.1800265524500695\n",
      "    mean_raw_obs_processing_ms: 0.4277854905369475\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 16.19999999999999\n",
      "    episode_reward_mean: 0.9390000000000113\n",
      "    episode_reward_min: -24.000000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [7.800000000000027, -4.499999999999986, 3.000000000000001, -7.7999999999999865,\n",
      "        -6.299999999999983, 8.700000000000026, 13.499999999999984, -8.399999999999984,\n",
      "        6.900000000000027, -2.999999999999973, 2.7000000000000166, -0.5999999999999946,\n",
      "        2.400000000000015, 7.200000000000012, 3.300000000000007, -5.699999999999985,\n",
      "        -7.7999999999999865, 6.000000000000018, 1.200000000000013, -10.199999999999982,\n",
      "        10.500000000000027, 8.699999999999985, 9.299999999999967, 13.199999999999932,\n",
      "        4.5000000000000195, -3.899999999999976, -8.099999999999998, -8.99999999999998,\n",
      "        3.000000000000007, -11.999999999999984, -5.999999999999986, -10.199999999999978,\n",
      "        -7.4999999999999805, -13.499999999999979, -4.4999999999999805, -21.000000000000036,\n",
      "        14.700000000000017, -10.499999999999977, 9.299999999999997, -7.499999999999989,\n",
      "        3.600000000000002, 0.6000000000000193, -24.000000000000007, -5.999999999999986,\n",
      "        -12.899999999999977, 3.6000000000000223, 6.000000000000025, 7.200000000000017,\n",
      "        16.19999999999999, -1.4999999999999836, 1.2000000000000137, 3.8999999999999964,\n",
      "        16.199999999999974, -10.499999999999979, -4.499999999999975, -2.9999999999999973,\n",
      "        -4.199999999999986, 2.400000000000016, 0.6000000000000184, 7.19999999999998,\n",
      "        1.5000000000000249, 7.800000000000031, 10.200000000000012, 6.900000000000025,\n",
      "        -8.99999999999999, 15.299999999999997, 6.6000000000000245, -8.39999999999998,\n",
      "        3.0000000000000187, 14.70000000000003, 3.000000000000024, -4.199999999999992,\n",
      "        0.30000000000002625, -0.9000000000000181, 5.700000000000026, 1.8000000000000291,\n",
      "        4.500000000000028, 6.899999999999963, 14.700000000000001, -3.299999999999985,\n",
      "        3.900000000000032, 1.4999999999999774, 3.000000000000016, 5.70000000000001,\n",
      "        12.600000000000005, 2.700000000000019, -6.299999999999983, 5.7000000000000295,\n",
      "        4.800000000000027, 13.800000000000026, 1.5000000000000298, 1.5000000000000127,\n",
      "        -0.29999999999997906, -7.199999999999996, 6.599999999999973, -5.399999999999977,\n",
      "        6.600000000000026, -15.599999999999994, -0.5999999999999915, 12.299999999999986]\n",
      "      policy_policy1_reward: [9.0, 5.5, -3.5, -11.0, 1.5, 16.5, 23.5, 0.5, 12.5, 7.0,\n",
      "        10.5, 5.0, 8.0, 15.0, 10.0, 1.0, 0.0, 16.0, 9.0, -9.0, 20.5, 11.0, 16.0, 21.0,\n",
      "        14.5, -0.5, -2.5, 1.0, 13.0, -2.0, 4.0, -3.5, 2.5, -3.5, 5.5, -11.0, 22.5, -6.0,\n",
      "        16.0, 2.5, 7.0, 9.5, -14.0, 4.0, -9.5, 12.5, 16.0, 15.0, 24.0, 3.0, 9.0, 9.5,\n",
      "        24.0, -0.5, 5.5, 7.0, 2.5, 8.0, 4.0, 15.0, 0.5, 14.5, 18.0, 12.5, 1.0, 22.0,\n",
      "        15.5, -5.0, 13.0, 22.5, 13.0, 2.5, 1.5, 2.5, 8.0, 8.5, 14.5, 7.0, 22.5, 4.5,\n",
      "        9.5, 6.0, 13.0, 13.5, 21.5, 10.5, 1.5, 13.5, 11.5, 15.0, 11.5, 11.5, 7.5, -0.5,\n",
      "        15.5, 3.5, 15.5, -10.0, 5.0, 19.0]\n",
      "      policy_policy2_reward: [-1.2000000000000013, -9.99999999999998, 6.500000000000011,\n",
      "        3.2000000000000055, -7.799999999999981, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999984, -5.6, -9.99999999999998, -7.799999999999989, -5.599999999999982,\n",
      "        -5.599999999999997, -7.799999999999989, -6.69999999999999, -6.699999999999988,\n",
      "        -7.799999999999984, -9.99999999999998, -7.79999999999999, -1.199999999999997,\n",
      "        -9.99999999999998, -2.299999999999991, -6.699999999999983, -7.79999999999999,\n",
      "        -9.99999999999998, -3.400000000000002, -5.599999999999982, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -4.499999999999995, -6.699999999999985, -9.99999999999998,\n",
      "        -3.4000000000000035, -8.899999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -3.400000000000001, -8.899999999999986, -9.99999999999998, -7.799999999999983,\n",
      "        -7.79999999999999, -4.500000000000003, -7.799999999999981, -5.599999999999999,\n",
      "        -7.799999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -5.599999999999992, -3.4000000000000026, -7.799999999999989,\n",
      "        0.999999999999997, -6.699999999999986, -7.799999999999981, -5.599999999999983,\n",
      "        -9.99999999999998, -6.699999999999995, -8.899999999999986, -3.4000000000000035,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -6.699999999999994,\n",
      "        -1.200000000000002, -3.4000000000000057, -2.3000000000000043, -6.699999999999989,\n",
      "        -9.99999999999998, -0.09999999999999282, -7.799999999999988, -7.799999999999981,\n",
      "        -5.599999999999982, -4.499999999999999, -9.99999999999998, -7.799999999999981,\n",
      "        -8.899999999999984, -7.799999999999981, -7.799999999999981, -7.79999999999999,\n",
      "        -6.699999999999986, -1.2000000000000026, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999984, -6.699999999999986, -8.899999999999986, -8.899999999999986,\n",
      "        -8.89999999999998, -5.6, -5.599999999999991, -6.6999999999999815]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 24.0\n",
      "      policy2: 6.500000000000011\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.925\n",
      "      policy2: -6.985999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -14.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0739505639985902\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022974200850186914\n",
      "      mean_inference_ms: 7.1800265524500695\n",
      "      mean_raw_obs_processing_ms: 0.4277854905369475\n",
      "  time_since_restore: 399.85499811172485\n",
      "  time_this_iter_s: 67.27055263519287\n",
      "  time_total_s: 399.85499811172485\n",
      "  timers:\n",
      "    learn_throughput: 111.99\n",
      "    learn_time_ms: 35717.37\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 66637.014\n",
      "  timestamp: 1660562411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:16 (running for 00:07:50.82)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:21 (running for 00:07:55.88)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:26 (running for 00:08:00.96)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:31 (running for 00:08:06.01)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:36 (running for 00:08:11.10)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:41 (running for 00:08:16.15)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      6 |          381.91  | 24000 |   -0.483 |           6.745 |          -7.228 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 56000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-20-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.599999999999945\n",
      "  episode_reward_mean: 0.9510000000000095\n",
      "  episode_reward_min: -18.299999999999983\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1901997327804565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011903629638254642\n",
      "          model: {}\n",
      "          policy_loss: -0.03618089482188225\n",
      "          total_loss: 6.912899971008301\n",
      "          vf_explained_var: 0.058321237564086914\n",
      "          vf_loss: 6.946700096130371\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1705327033996582\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01035651471465826\n",
      "          model: {}\n",
      "          policy_loss: -0.033988919109106064\n",
      "          total_loss: 2.3524653911590576\n",
      "          vf_explained_var: 0.22288918495178223\n",
      "          vf_loss: 2.384382963180542\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 28000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.474999999999998\n",
      "    ram_util_percent: 63.95760869565219\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 27.5\n",
      "    policy2: 3.1999999999999966\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.355\n",
      "    policy2: -7.4039999999999875\n",
      "  policy_reward_min:\n",
      "    policy1: -12.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07791927879480808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02481575851521587\n",
      "    mean_inference_ms: 6.433871886235388\n",
      "    mean_raw_obs_processing_ms: 0.4321901642538203\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 27.599999999999945\n",
      "    episode_reward_mean: 0.9510000000000095\n",
      "    episode_reward_min: -18.299999999999983\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [6.600000000000016, -0.8999999999999856, 4.500000000000012, -8.999999999999977,\n",
      "        -1.799999999999986, -4.799999999999978, 9.000000000000012, -14.99999999999999,\n",
      "        13.800000000000013, -0.8999999999999924, -10.499999999999973, 3.000000000000015,\n",
      "        1.5000000000000226, 7.500000000000032, 3.9000000000000266, -12.89999999999998,\n",
      "        -6.5999999999999766, 6.38378239159465e-16, 1.199999999999974, -4.499999999999986,\n",
      "        -5.399999999999981, -5.699999999999985, -12.599999999999982, -2.9999999999999876,\n",
      "        -1.7999999999999807, 6.300000000000027, 27.599999999999945, 2.4000000000000252,\n",
      "        -14.39999999999998, 11.099999999999971, 3.0000000000000133, 14.09999999999998,\n",
      "        1.5000000000000022, -0.8999999999999886, -10.799999999999999, 5.700000000000033,\n",
      "        -9.299999999999988, 11.099999999999993, 12.599999999999934, -0.8999999999999798,\n",
      "        10.800000000000033, 4.500000000000021, 4.5000000000000275, 2.100000000000025,\n",
      "        2.4000000000000226, -13.499999999999977, -4.500000000000012, 10.500000000000016,\n",
      "        0.599999999999958, 13.799999999999995, 4.50000000000003, -3.5999999999999766,\n",
      "        2.9999999999999987, -10.499999999999988, -11.09999999999998, 3.0000000000000044,\n",
      "        9.000000000000032, 1.500000000000008, -5.6999999999999815, -8.399999999999979,\n",
      "        5.099999999999972, 9.29999999999997, -1.4999999999999738, -6.29999999999999,\n",
      "        -9.599999999999978, -4.499999999999977, -5.399999999999979, 6.600000000000017,\n",
      "        -2.999999999999984, 6.600000000000021, -1.4999999999999862, -18.299999999999983,\n",
      "        15.000000000000032, 3.3000000000000176, -6.899999999999974, -6.899999999999974,\n",
      "        13.199999999999966, 2.700000000000005, 9.3, 1.2073675392798577e-14, 5.700000000000031,\n",
      "        -3.8999999999999795, 3.300000000000014, -1.1999999999999738, 14.699999999999958,\n",
      "        10.799999999999969, 18.59999999999993, 6.600000000000028, -7.499999999999976,\n",
      "        -18.0, -5.099999999999978, 14.999999999999954, -7.799999999999981, 1.8846035843012032e-14,\n",
      "        4.200000000000028, 2.7000000000000237, 2.4000000000000257, 6.000000000000021,\n",
      "        11.39999999999998, 2.4000000000000203]\n",
      "      policy_policy1_reward: [15.5, 8.0, 14.5, -4.5, 6.0, -2.5, 19.0, -5.0, 20.5, 8.0,\n",
      "        -0.5, 13.0, 11.5, 17.5, 4.0, -9.5, -1.0, 10.0, 9.0, 5.5, 3.5, -4.5, -12.5, 7.0,\n",
      "        0.5, 13.0, 25.5, 8.0, -5.5, 20.0, 13.0, 23.0, 11.5, 8.0, -3.0, 13.5, -1.5, 20.0,\n",
      "        21.5, 8.0, 17.5, 14.5, 14.5, 11.0, 8.0, -3.5, 5.5, 9.5, 9.5, 20.5, 14.5, 2.0,\n",
      "        13.0, -0.5, -5.5, 7.5, 13.5, 11.5, 1.0, 0.5, 14.0, 16.0, 8.5, -9.5, -4.0, 5.5,\n",
      "        3.5, 15.5, 7.0, 10.0, 8.5, -10.5, 25.0, 4.5, 2.0, 2.0, 21.0, 10.5, 10.5, 10.0,\n",
      "        13.5, 5.0, 10.0, 5.5, 22.5, 12.0, 27.5, 15.5, 2.5, -8.0, 0.5, 25.0, 0.0, 10.0,\n",
      "        12.0, 10.5, 8.0, 16.0, 17.0, 8.0]\n",
      "      policy_policy2_reward: [-8.899999999999983, -8.89999999999998, -9.99999999999998,\n",
      "        -4.499999999999995, -7.799999999999981, -2.299999999999997, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -0.09999999999999898,\n",
      "        -3.4000000000000035, -5.599999999999982, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -8.899999999999986, -1.2000000000000055, -0.09999999999999487,\n",
      "        -9.99999999999998, -2.3000000000000047, -6.699999999999995, 2.099999999999996,\n",
      "        -5.599999999999999, -8.899999999999986, -8.899999999999986, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -8.89999999999998, -7.79999999999999,\n",
      "        -7.799999999999981, -7.799999999999981, -8.89999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -6.699999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        0.9999999999999952, -8.899999999999983, -6.69999999999999, -9.99999999999998,\n",
      "        -5.5999999999999925, -9.99999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -4.499999999999995, -4.500000000000003, -9.99999999999998, -6.699999999999986,\n",
      "        -8.89999999999998, -8.899999999999986, -6.699999999999983, -9.99999999999998,\n",
      "        3.1999999999999966, -5.6, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -3.4000000000000066, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -1.2000000000000035, -8.89999999999998, -8.899999999999986,\n",
      "        -7.799999999999989, -7.799999999999985, -1.2000000000000006, -9.99999999999998,\n",
      "        -7.799999999999984, -8.899999999999986, -6.6999999999999815, -6.699999999999995,\n",
      "        -7.799999999999988, -1.2000000000000048, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -5.6, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -7.799999999999981, -7.79999999999999, -5.599999999999999,\n",
      "        -9.99999999999998, -5.599999999999982, -5.599999999999995]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 27.5\n",
      "      policy2: 3.1999999999999966\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.355\n",
      "      policy2: -7.4039999999999875\n",
      "    policy_reward_min:\n",
      "      policy1: -12.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07791927879480808\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02481575851521587\n",
      "      mean_inference_ms: 6.433871886235388\n",
      "      mean_raw_obs_processing_ms: 0.4321901642538203\n",
      "  time_since_restore: 447.59393095970154\n",
      "  time_this_iter_s: 65.68378615379333\n",
      "  time_total_s: 447.59393095970154\n",
      "  timers:\n",
      "    learn_throughput: 112.085\n",
      "    learn_time_ms: 35687.196\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 63930.734\n",
      "  timestamp: 1660562443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:48 (running for 00:08:22.65)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:53 (running for 00:08:27.71)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:20:58 (running for 00:08:32.79)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=5.525999999999994 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998120730>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107250>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9980C3070>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816C0A0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9980BF790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |      9 |          419.788 | 27000 |    2.097 |           9.435 |          -7.338 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |      9 |          422.735 | 27000 |    5.526 |          13.095 |          -7.569 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_env_steps_sampled: 30000\n",
      "    num_env_steps_trained: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-20-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: 3.258000000000006\n",
      "  episode_reward_min: -18.29999999999999\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 300\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.068947672843933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015081462450325489\n",
      "          model: {}\n",
      "          policy_loss: -0.041702352464199066\n",
      "          total_loss: 6.48184871673584\n",
      "          vf_explained_var: 0.14776983857154846\n",
      "          vf_loss: 6.520534992218018\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.091320514678955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011844041757285595\n",
      "          model: {}\n",
      "          policy_loss: -0.03498711436986923\n",
      "          total_loss: 3.4468181133270264\n",
      "          vf_explained_var: 0.14802905917167664\n",
      "          vf_loss: 3.4794366359710693\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_env_steps_sampled: 30000\n",
      "    num_env_steps_trained: 30000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 30000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 30000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.533802816901403\n",
      "    ram_util_percent: 63.93943661971827\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 17.49999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.65\n",
      "    policy2: -6.391999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07411040018399184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024501201983097413\n",
      "    mean_inference_ms: 6.466217189161716\n",
      "    mean_raw_obs_processing_ms: 0.4390420712382982\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.099999999999945\n",
      "    episode_reward_mean: 3.258000000000006\n",
      "    episode_reward_min: -18.29999999999999\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [9.299999999999944, 2.7000000000000317, -2.999999999999987, -13.49999999999998,\n",
      "        1.2000000000000264, -0.8999999999999955, 11.400000000000006, 1.5000000000000013,\n",
      "        22.199999999999925, 5.100000000000021, 2.7283730830163222e-14, -1.8000000000000154,\n",
      "        -5.399999999999981, -10.199999999999978, 9.000000000000025, 7.499999999999998,\n",
      "        8.965050923848139e-15, 0.3000000000000005, 14.099999999999937, -2.9999999999999933,\n",
      "        1.200000000000029, 8.099999999999998, -10.799999999999978, -0.2999999999999914,\n",
      "        9.00000000000003, 12.00000000000003, -4.199999999999973, 0.6000000000000291,\n",
      "        -2.099999999999978, 6.0000000000000195, 3.600000000000021, -11.999999999999977,\n",
      "        -4.199999999999988, -7.799999999999981, 5.700000000000028, -11.999999999999972,\n",
      "        3.300000000000029, 19.499999999999936, 6.3000000000000185, 1.500000000000028,\n",
      "        5.100000000000021, 15.900000000000007, -18.29999999999999, 2.2842838731662596e-14,\n",
      "        18.60000000000001, -9.599999999999977, -8.999999999999972, 8.100000000000032,\n",
      "        18.59999999999995, 20.09999999999997, 9.90000000000002, 7.200000000000031, 12.000000000000018,\n",
      "        1.2000000000000148, -0.2999999999999897, -7.499999999999977, 19.49999999999998,\n",
      "        -0.9000000000000041, -6.299999999999978, 8.09999999999992, 0.9000000000000209,\n",
      "        -6.900000000000004, 15.899999999999975, -0.8999999999999718, 6.000000000000027,\n",
      "        1.3961054534661343e-14, 23.099999999999945, 6.599999999999991, 1.5000000000000209,\n",
      "        -6.899999999999977, 6.599999999999945, 4.800000000000024, -8.999999999999979,\n",
      "        2.683964162031316e-14, -2.9999999999999956, 9.000000000000028, 5.399999999999963,\n",
      "        7.500000000000007, 17.69999999999991, 2.7283730830163222e-14, 14.999999999999975,\n",
      "        6.599999999999991, 1.8000000000000278, 1.2000000000000215, 9.299999999999969,\n",
      "        -5.700000000000003, 1.4999999999999876, 2.7000000000000157, 10.79999999999992,\n",
      "        2.0999999999999788, 3.9000000000000283, 2.373101715136272e-14, -1.1999999999999846,\n",
      "        -0.2999999999999853, 11.700000000000017, 8.400000000000025, -1.1999999999999922,\n",
      "        13.500000000000012, 14.100000000000028, -8.999999999999986]\n",
      "      policy_policy1_reward: [16.0, 10.5, 7.0, -3.5, 9.0, 8.0, 17.0, 11.5, 30.0, 14.0,\n",
      "        10.0, 6.0, -2.0, -3.5, 19.0, 17.5, 10.0, 7.0, 23.0, 7.0, 9.0, 17.0, -3.0, 7.5,\n",
      "        19.0, 22.0, 2.5, 4.0, 3.5, 16.0, 12.5, -2.0, 2.5, 0.0, 13.5, -2.0, 10.0, 24.0,\n",
      "        13.0, 6.0, 14.0, 21.5, -10.5, 10.0, 27.5, -4.0, 1.0, 17.0, 27.5, 29.0, 15.5,\n",
      "        15.0, 16.5, 3.5, 7.5, 2.5, 29.5, 8.0, -9.5, 17.0, 6.5, 2.0, 16.0, 2.5, 10.5,\n",
      "        10.0, 32.0, 10.0, 11.5, 2.0, -1.0, 11.5, 1.0, 10.0, 7.0, 8.0, 11.0, 17.5, 20.0,\n",
      "        10.0, 25.0, 15.5, 8.5, 9.0, 16.0, 1.0, -5.0, 10.5, 17.5, 11.0, 4.0, 10.0, 5.5,\n",
      "        -9.0, -2.5, 3.0, 5.5, 23.5, 17.5, -26.5]\n",
      "      policy_policy2_reward: [-6.699999999999995, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -8.89999999999998, -5.599999999999988,\n",
      "        -9.99999999999998, -7.799999999999981, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -3.4000000000000066, -6.69999999999999, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999994, -8.89999999999998,\n",
      "        -9.99999999999998, -7.799999999999981, -8.899999999999986, -7.799999999999981,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -6.699999999999995,\n",
      "        -3.400000000000001, -5.6, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -7.799999999999982, -7.799999999999981, -9.99999999999998,\n",
      "        -6.69999999999999, -4.499999999999987, -6.6999999999999815, -4.499999999999987,\n",
      "        -8.89999999999998, -5.599999999999999, -7.79999999999999, -9.99999999999998,\n",
      "        -8.89999999999998, -5.6, -9.99999999999998, -8.899999999999984, -8.89999999999998,\n",
      "        -8.89999999999998, -5.599999999999993, -7.799999999999981, -4.499999999999998,\n",
      "        -2.2999999999999976, -7.799999999999981, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, 3.200000000000001, -8.899999999999986, -5.5999999999999845,\n",
      "        -8.89999999999998, -0.10000000000000109, -3.399999999999999, -4.500000000000002,\n",
      "        -9.99999999999998, -8.89999999999998, -3.4, -9.99999999999998, -8.89999999999998,\n",
      "        7.60000000000001, -6.699999999999991, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, 1.0000000000000022, -5.599999999999991, -9.99999999999998,\n",
      "        -2.3000000000000047, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -6.699999999999995, -7.7999999999999865, -6.699999999999995, -6.6999999999999815,\n",
      "        6.500000000000014, -7.799999999999981, -6.699999999999995, -8.89999999999998,\n",
      "        -0.0999999999999841, -9.99999999999998, -6.69999999999999, 8.700000000000024,\n",
      "        14.200000000000003, 5.4000000000000075, -6.69999999999999, -9.99999999999998,\n",
      "        -3.399999999999999, 17.49999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.0\n",
      "      policy2: 17.49999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.65\n",
      "      policy2: -6.391999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07411040018399184\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024501201983097413\n",
      "      mean_inference_ms: 6.466217189161716\n",
      "      mean_raw_obs_processing_ms: 0.4390420712382982\n",
      "  time_since_restore: 469.5471136569977\n",
      "  time_this_iter_s: 49.759387731552124\n",
      "  time_total_s: 469.5471136569977\n",
      "  timers:\n",
      "    learn_throughput: 116.666\n",
      "    learn_time_ms: 25714.516\n",
      "    synch_weights_time_ms: 2.892\n",
      "    training_iteration_time_ms: 46946.034\n",
      "  timestamp: 1660562459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 10\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 60000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_env_steps_sampled: 30000\n",
      "    num_env_steps_trained: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-21-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.299999999999976\n",
      "  episode_reward_mean: 7.046999999999986\n",
      "  episode_reward_min: -17.999999999999986\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 300\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.011735200881958\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019047293812036514\n",
      "          model: {}\n",
      "          policy_loss: -0.05495165288448334\n",
      "          total_loss: 6.745706558227539\n",
      "          vf_explained_var: 0.2582670748233795\n",
      "          vf_loss: 6.792087078094482\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9978364109992981\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01810850389301777\n",
      "          model: {}\n",
      "          policy_loss: -0.05258254334330559\n",
      "          total_loss: 2.877471923828125\n",
      "          vf_explained_var: 0.17330804467201233\n",
      "          vf_loss: 2.921905755996704\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_env_steps_sampled: 30000\n",
      "    num_env_steps_trained: 30000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 30000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 30000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.578873239436625\n",
      "    ram_util_percent: 63.93943661971827\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 38.5\n",
      "    policy2: 6.500000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 14.44\n",
      "    policy2: -7.392999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07177147627510906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023112470640191774\n",
      "    mean_inference_ms: 6.562757348508004\n",
      "    mean_raw_obs_processing_ms: 0.4389921359616426\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 30.299999999999976\n",
      "    episode_reward_mean: 7.046999999999986\n",
      "    episode_reward_min: -17.999999999999986\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [15.599999999999921, 28.499999999999893, 13.799999999999983, -8.999999999999979,\n",
      "        18.59999999999997, -0.29999999999997373, 9.000000000000012, -5.399999999999979,\n",
      "        19.199999999999996, 8.700000000000019, 5.999999999999947, -10.799999999999981,\n",
      "        11.999999999999966, 15.599999999999959, 5.100000000000022, 14.699999999999944,\n",
      "        -13.49999999999998, 6.000000000000011, 20.399999999999906, 26.999999999999922,\n",
      "        -7.1999999999999815, 3.599999999999971, 6.000000000000007, 8.700000000000024,\n",
      "        0.6000000000000215, -1.4999999999999782, -8.399999999999977, 8.100000000000007,\n",
      "        -1.4999999999999747, -2.999999999999975, 14.699999999999969, -11.699999999999982,\n",
      "        17.999999999999915, -7.499999999999973, 6.000000000000027, 9.000000000000027,\n",
      "        30.299999999999976, 8.099999999999927, 1.484923295436147e-14, 20.999999999999915,\n",
      "        -2.7000000000000295, 5.700000000000015, -1.499999999999981, 11.999999999999941,\n",
      "        9.59999999999995, 5.7000000000000215, 3.600000000000026, 6.899999999999961,\n",
      "        3.000000000000033, 12.6, 3.30000000000003, 6.60000000000003, 4.500000000000007,\n",
      "        -0.8999999999999861, 3.0000000000000258, 17.09999999999995, 1.5000000000000049,\n",
      "        3.0000000000000275, 4.499999999999975, 4.800000000000015, 8.10000000000003,\n",
      "        -1.799999999999971, 7.500000000000034, -8.699999999999978, 18.599999999999948,\n",
      "        17.99999999999993, 15.899999999999965, 3.299999999999968, -17.999999999999986,\n",
      "        20.099999999999895, 5.999999999999988, 7.500000000000034, 8.999999999999934,\n",
      "        20.99999999999993, 17.099999999999916, 0.6000000000000224, 23.099999999999923,\n",
      "        -4.499999999999984, 12.6, 9.29999999999998, 3.000000000000022, 5.400000000000002,\n",
      "        5.399999999999936, 18.29999999999992, 2.7283730830163222e-14, 6.599999999999982,\n",
      "        15.600000000000016, -4.20000000000001, -5.699999999999982, 14.699999999999916,\n",
      "        9.000000000000032, 9.599999999999925, 16.799999999999933, 1.7999999999999936,\n",
      "        15.899999999999972, 19.499999999999915, 9.00000000000003, 7.500000000000007,\n",
      "        -0.5999999999999762, 15.600000000000017]\n",
      "      policy_policy1_reward: [24.5, 38.5, 20.5, 1.0, 27.5, 7.5, 19.0, 3.5, 27.0, 11.0,\n",
      "        16.0, -8.5, 22.0, 24.5, 8.5, 17.0, -3.5, 10.5, 26.0, 37.0, -6.0, 12.5, 10.5,\n",
      "        16.5, 9.5, 8.5, 0.5, 17.0, 8.5, 7.0, 22.5, -5.0, 28.0, 2.5, 16.0, 19.0, 37.0,\n",
      "        17.0, 10.0, 25.5, 4.0, 13.5, 8.5, 22.0, 18.5, 8.0, 12.5, 7.0, 13.0, 21.5, 10.0,\n",
      "        15.5, 14.5, 8.0, 7.5, 26.0, 11.5, 13.0, 14.5, 11.5, 17.0, 6.0, 17.5, -2.0, 22.0,\n",
      "        28.0, 21.5, 10.0, -8.0, 29.0, 16.0, 17.5, 19.0, 31.0, 26.0, 9.5, 32.0, -11.0,\n",
      "        21.5, 10.5, 13.0, 5.5, 11.0, 25.0, 10.0, 15.5, 24.5, 2.5, 1.0, 11.5, 19.0, 13.0,\n",
      "        23.5, 8.5, 16.0, 29.5, 13.5, 12.0, 5.0, 24.5]\n",
      "      policy_policy2_reward: [-8.899999999999984, -9.99999999999998, -6.699999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -7.799999999999981, -2.3000000000000047, -9.99999999999998,\n",
      "        -2.3000000000000043, -9.99999999999998, -8.89999999999998, -3.400000000000005,\n",
      "        -2.3000000000000047, -9.99999999999998, -4.499999999999998, -5.599999999999999,\n",
      "        -9.99999999999998, -1.200000000000003, -8.89999999999998, -4.499999999999982,\n",
      "        -7.79999999999999, -8.899999999999983, -9.99999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -7.799999999999989,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999985, -8.89999999999998, -9.99999999999998,\n",
      "        -4.500000000000004, -6.69999999999999, -7.799999999999981, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999984, -2.299999999999995, -8.89999999999998,\n",
      "        -0.1000000000000042, -9.99999999999998, -8.89999999999998, -6.6999999999999815,\n",
      "        -8.89999999999998, -9.99999999999998, -8.89999999999998, -4.499999999999997,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.699999999999988, -8.89999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -6.699999999999982, -3.4000000000000057, -9.99999999999998, -5.599999999999998,\n",
      "        -6.699999999999989, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -8.89999999999998, 6.500000000000002, -8.899999999999986,\n",
      "        -1.2000000000000008, -9.99999999999998, -0.1000000000000022, -5.599999999999994,\n",
      "        -6.699999999999987, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -6.699999999999994, -6.699999999999994, 3.199999999999997, -9.99999999999998,\n",
      "        -3.4000000000000044, -6.6999999999999815, -6.699999999999995, -0.10000000000000175,\n",
      "        -9.99999999999998, -4.499999999999982, -4.499999999999997, -5.599999999999999,\n",
      "        -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.5\n",
      "      policy2: 6.500000000000002\n",
      "    policy_reward_mean:\n",
      "      policy1: 14.44\n",
      "      policy2: -7.392999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -11.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07177147627510906\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023112470640191774\n",
      "      mean_inference_ms: 6.562757348508004\n",
      "      mean_raw_obs_processing_ms: 0.4389921359616426\n",
      "  time_since_restore: 472.52024388313293\n",
      "  time_this_iter_s: 49.78531789779663\n",
      "  time_total_s: 472.52024388313293\n",
      "  timers:\n",
      "    learn_throughput: 116.55\n",
      "    learn_time_ms: 25740.009\n",
      "    synch_weights_time_ms: 3.092\n",
      "    training_iteration_time_ms: 47246.24\n",
      "  timestamp: 1660562460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 10\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:05 (running for 00:08:39.80)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:10 (running for 00:08:44.97)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:15 (running for 00:08:50.03)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      6 |          399.855 | 24000 |    0.939 |           7.925 |          -6.986 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 56000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-21-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.699999999999996\n",
      "  episode_reward_mean: 3.3090000000000113\n",
      "  episode_reward_min: -24.000000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1593353748321533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017796562984585762\n",
      "          model: {}\n",
      "          policy_loss: -0.04975755140185356\n",
      "          total_loss: 6.639917850494385\n",
      "          vf_explained_var: 0.16817890107631683\n",
      "          vf_loss: 6.681666851043701\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1356838941574097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01851319521665573\n",
      "          model: {}\n",
      "          policy_loss: -0.048850253224372864\n",
      "          total_loss: 2.2166950702667236\n",
      "          vf_explained_var: 0.23764120042324066\n",
      "          vf_loss: 2.259991407394409\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 28000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.918279569892473\n",
      "    ram_util_percent: 63.93118279569891\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 26.5\n",
      "    policy2: 0.999999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.24\n",
      "    policy2: -6.930999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07362806688003612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023029841288685935\n",
      "    mean_inference_ms: 7.18070397206463\n",
      "    mean_raw_obs_processing_ms: 0.4269611206659494\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 17.699999999999996\n",
      "    episode_reward_mean: 3.3090000000000113\n",
      "    episode_reward_min: -24.000000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.600000000000002, 0.6000000000000193, -24.000000000000007, -5.999999999999986,\n",
      "        -12.899999999999977, 3.6000000000000223, 6.000000000000025, 7.200000000000017,\n",
      "        16.19999999999999, -1.4999999999999836, 1.2000000000000137, 3.8999999999999964,\n",
      "        16.199999999999974, -10.499999999999979, -4.499999999999975, -2.9999999999999973,\n",
      "        -4.199999999999986, 2.400000000000016, 0.6000000000000184, 7.19999999999998,\n",
      "        1.5000000000000249, 7.800000000000031, 10.200000000000012, 6.900000000000025,\n",
      "        -8.99999999999999, 15.299999999999997, 6.6000000000000245, -8.39999999999998,\n",
      "        3.0000000000000187, 14.70000000000003, 3.000000000000024, -4.199999999999992,\n",
      "        0.30000000000002625, -0.9000000000000181, 5.700000000000026, 1.8000000000000291,\n",
      "        4.500000000000028, 6.899999999999963, 14.700000000000001, -3.299999999999985,\n",
      "        3.900000000000032, 1.4999999999999774, 3.000000000000016, 5.70000000000001,\n",
      "        12.600000000000005, 2.700000000000019, -6.299999999999983, 5.7000000000000295,\n",
      "        4.800000000000027, 13.800000000000026, 1.5000000000000298, 1.5000000000000127,\n",
      "        -0.29999999999997906, -7.199999999999996, 6.599999999999973, -5.399999999999977,\n",
      "        6.600000000000026, -15.599999999999994, -0.5999999999999915, 12.299999999999986,\n",
      "        13.500000000000016, 3.300000000000023, 7.799999999999969, 11.40000000000002,\n",
      "        17.699999999999996, 0.30000000000002447, 2.1000000000000005, 13.799999999999983,\n",
      "        5.400000000000009, 15.299999999999986, 17.699999999999974, 5.400000000000013,\n",
      "        10.500000000000007, -10.499999999999972, -12.899999999999986, -4.4999999999999805,\n",
      "        5.100000000000032, 2.700000000000014, 4.500000000000016, 9.900000000000025,\n",
      "        6.6000000000000245, 12.000000000000025, 3.000000000000001, 9.000000000000027,\n",
      "        13.799999999999931, -1.499999999999989, 10.799999999999992, 6.600000000000023,\n",
      "        3.9000000000000283, -9.899999999999979, 11.700000000000017, -6.900000000000004,\n",
      "        -2.6999999999999953, 3.90000000000003, -14.699999999999992, 2.095545958979983e-14,\n",
      "        14.700000000000019, 16.500000000000007, 10.500000000000028, -0.8999999999999961]\n",
      "      policy_policy1_reward: [7.0, 9.5, -14.0, 4.0, -9.5, 12.5, 16.0, 15.0, 24.0, 3.0,\n",
      "        9.0, 9.5, 24.0, -0.5, 5.5, 7.0, 2.5, 8.0, 4.0, 15.0, 0.5, 14.5, 18.0, 12.5,\n",
      "        1.0, 22.0, 15.5, -5.0, 13.0, 22.5, 13.0, 2.5, 1.5, 2.5, 8.0, 8.5, 14.5, 7.0,\n",
      "        22.5, 4.5, 9.5, 6.0, 13.0, 13.5, 21.5, 10.5, 1.5, 13.5, 11.5, 15.0, 11.5, 11.5,\n",
      "        7.5, -0.5, 15.5, 3.5, 15.5, -10.0, 5.0, 19.0, 23.5, 10.0, 14.5, 17.0, 20.0,\n",
      "        7.0, 11.0, 20.5, 11.0, 22.0, 25.5, 11.0, 20.5, -0.5, -4.0, 5.5, 14.0, 10.5,\n",
      "        9.0, 15.5, 10.0, 22.0, 13.0, 19.0, 20.5, 8.5, 17.5, 10.0, 9.5, -1.0, 19.5, -3.5,\n",
      "        4.0, 9.5, -8.0, 4.5, 22.5, 26.5, 9.5, 2.5]\n",
      "      policy_policy2_reward: [-3.4000000000000035, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -3.400000000000001, -8.899999999999986, -9.99999999999998,\n",
      "        -7.799999999999983, -7.79999999999999, -4.500000000000003, -7.799999999999981,\n",
      "        -5.599999999999999, -7.799999999999986, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -5.599999999999992, -3.4000000000000026,\n",
      "        -7.799999999999989, 0.999999999999997, -6.699999999999986, -7.799999999999981,\n",
      "        -5.599999999999983, -9.99999999999998, -6.699999999999995, -8.899999999999986,\n",
      "        -3.4000000000000035, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -6.699999999999994, -1.200000000000002, -3.4000000000000057, -2.3000000000000043,\n",
      "        -6.699999999999989, -9.99999999999998, -0.09999999999999282, -7.799999999999988,\n",
      "        -7.799999999999981, -5.599999999999982, -4.499999999999999, -9.99999999999998,\n",
      "        -7.799999999999981, -8.899999999999984, -7.799999999999981, -7.799999999999981,\n",
      "        -7.79999999999999, -6.699999999999986, -1.2000000000000026, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999984, -6.699999999999986, -8.899999999999986,\n",
      "        -8.899999999999986, -8.89999999999998, -5.6, -5.599999999999991, -6.6999999999999815,\n",
      "        -9.99999999999998, -6.6999999999999815, -6.699999999999993, -5.599999999999986,\n",
      "        -2.3000000000000047, -6.699999999999983, -8.89999999999998, -6.699999999999994,\n",
      "        -5.599999999999998, -6.699999999999992, -7.799999999999981, -5.5999999999999925,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.7999999999999865, -4.500000000000004, -5.599999999999982,\n",
      "        -3.4000000000000026, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -6.699999999999995, -3.4000000000000035,\n",
      "        -5.5999999999999845, -8.89999999999998, -7.799999999999981, -3.4000000000000057,\n",
      "        -6.699999999999989, -5.599999999999991, -6.699999999999995, -4.5000000000000036,\n",
      "        -7.7999999999999865, -9.99999999999998, 0.9999999999999957, -3.400000000000002]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 26.5\n",
      "      policy2: 0.999999999999997\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.24\n",
      "      policy2: -6.930999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -14.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07362806688003612\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023029841288685935\n",
      "      mean_inference_ms: 7.18070397206463\n",
      "      mean_raw_obs_processing_ms: 0.4269611206659494\n",
      "  time_since_restore: 465.8020796775818\n",
      "  time_this_iter_s: 65.94708156585693\n",
      "  time_total_s: 465.8020796775818\n",
      "  timers:\n",
      "    learn_throughput: 112.089\n",
      "    learn_time_ms: 35686.036\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 66537.455\n",
      "  timestamp: 1660562477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:22 (running for 00:08:56.77)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:27 (running for 00:09:01.81)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:32 (running for 00:09:06.88)\n",
      "Memory usage on this node: 20.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:37 (running for 00:09:11.94)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:42 (running for 00:09:17.01)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:47 (running for 00:09:22.07)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=7.046999999999986 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99816CAF0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA1C0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DA370>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981DAB20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998152AF0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     10 |          469.547 | 30000 |    3.258 |           9.65  |          -6.392 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     10 |          472.52  | 30000 |    7.047 |          14.44  |          -7.393 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      7 |          447.594 | 28000 |    0.951 |           8.355 |          -7.404 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_env_steps_sampled: 33000\n",
      "    num_env_steps_trained: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-21-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.199999999999918\n",
      "  episode_reward_mean: 4.299000000000006\n",
      "  episode_reward_min: -18.29999999999999\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 330\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.041691780090332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012295895256102085\n",
      "          model: {}\n",
      "          policy_loss: -0.04138442873954773\n",
      "          total_loss: 6.966303825378418\n",
      "          vf_explained_var: 0.1744290441274643\n",
      "          vf_loss: 7.005228519439697\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0818440914154053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011602021753787994\n",
      "          model: {}\n",
      "          policy_loss: -0.03621944412589073\n",
      "          total_loss: 2.916816473007202\n",
      "          vf_explained_var: 0.1825157105922699\n",
      "          vf_loss: 2.9507155418395996\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_env_steps_sampled: 33000\n",
      "    num_env_steps_trained: 33000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 66000\n",
      "  num_agent_steps_trained: 66000\n",
      "  num_env_steps_sampled: 33000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 33000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.233333333333334\n",
      "    ram_util_percent: 64.05942028985505\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 36.0\n",
      "    policy2: 17.49999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.24\n",
      "    policy2: -5.9409999999999865\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07434591850289012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02457182493202588\n",
      "    mean_inference_ms: 6.507959809080643\n",
      "    mean_raw_obs_processing_ms: 0.43915785689192915\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.199999999999918\n",
      "    episode_reward_mean: 4.299000000000006\n",
      "    episode_reward_min: -18.29999999999999\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.600000000000021, -11.999999999999977, -4.199999999999988, -7.799999999999981,\n",
      "        5.700000000000028, -11.999999999999972, 3.300000000000029, 19.499999999999936,\n",
      "        6.3000000000000185, 1.500000000000028, 5.100000000000021, 15.900000000000007,\n",
      "        -18.29999999999999, 2.2842838731662596e-14, 18.60000000000001, -9.599999999999977,\n",
      "        -8.999999999999972, 8.100000000000032, 18.59999999999995, 20.09999999999997,\n",
      "        9.90000000000002, 7.200000000000031, 12.000000000000018, 1.2000000000000148,\n",
      "        -0.2999999999999897, -7.499999999999977, 19.49999999999998, -0.9000000000000041,\n",
      "        -6.299999999999978, 8.09999999999992, 0.9000000000000209, -6.900000000000004,\n",
      "        15.899999999999975, -0.8999999999999718, 6.000000000000027, 1.3961054534661343e-14,\n",
      "        23.099999999999945, 6.599999999999991, 1.5000000000000209, -6.899999999999977,\n",
      "        6.599999999999945, 4.800000000000024, -8.999999999999979, 2.683964162031316e-14,\n",
      "        -2.9999999999999956, 9.000000000000028, 5.399999999999963, 7.500000000000007,\n",
      "        17.69999999999991, 2.7283730830163222e-14, 14.999999999999975, 6.599999999999991,\n",
      "        1.8000000000000278, 1.2000000000000215, 9.299999999999969, -5.700000000000003,\n",
      "        1.4999999999999876, 2.7000000000000157, 10.79999999999992, 2.0999999999999788,\n",
      "        3.9000000000000283, 2.373101715136272e-14, -1.1999999999999846, -0.2999999999999853,\n",
      "        11.700000000000017, 8.400000000000025, -1.1999999999999922, 13.500000000000012,\n",
      "        14.100000000000028, -8.999999999999986, 6.000000000000012, 23.999999999999936,\n",
      "        1.5000000000000187, -17.399999999999977, -8.699999999999985, -1.4999999999999805,\n",
      "        6.9000000000000306, 8.10000000000001, -3.8999999999999777, 12.299999999999928,\n",
      "        7.800000000000031, 7.500000000000032, 6.900000000000032, 1.2000000000000246,\n",
      "        14.699999999999994, 16.49999999999999, 15.600000000000023, 8.099999999999934,\n",
      "        8.700000000000019, 14.699999999999958, -6.299999999999977, 4.500000000000025,\n",
      "        -1.4999999999999787, 28.199999999999918, -1.1999999999999762, 6.900000000000025,\n",
      "        11.100000000000009, -4.4999999999999805, -1.7999999999999785, 5.700000000000017]\n",
      "      policy_policy1_reward: [12.5, -2.0, 2.5, 0.0, 13.5, -2.0, 10.0, 24.0, 13.0, 6.0,\n",
      "        14.0, 21.5, -10.5, 10.0, 27.5, -4.0, 1.0, 17.0, 27.5, 29.0, 15.5, 15.0, 16.5,\n",
      "        3.5, 7.5, 2.5, 29.5, 8.0, -9.5, 17.0, 6.5, 2.0, 16.0, 2.5, 10.5, 10.0, 32.0,\n",
      "        10.0, 11.5, 2.0, -1.0, 11.5, 1.0, 10.0, 7.0, 8.0, 11.0, 17.5, 20.0, 10.0, 25.0,\n",
      "        15.5, 8.5, 9.0, 16.0, 1.0, -5.0, 10.5, 17.5, 11.0, 4.0, 10.0, 5.5, -9.0, -2.5,\n",
      "        3.0, 5.5, 23.5, 17.5, -26.5, 16.0, 23.0, 11.5, -8.5, -2.0, 8.5, 7.0, 17.0, 5.0,\n",
      "        19.0, 14.5, 17.5, 12.5, 9.0, 6.0, 26.5, 24.5, 17.0, 16.5, 22.5, -4.0, 14.5,\n",
      "        8.5, 36.0, 5.5, 12.5, 14.5, 5.5, 0.5, 13.5]\n",
      "      policy_policy2_reward: [-8.89999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -7.799999999999982, -7.799999999999981, -9.99999999999998, -6.69999999999999,\n",
      "        -4.499999999999987, -6.6999999999999815, -4.499999999999987, -8.89999999999998,\n",
      "        -5.599999999999999, -7.79999999999999, -9.99999999999998, -8.89999999999998,\n",
      "        -5.6, -9.99999999999998, -8.899999999999984, -8.89999999999998, -8.89999999999998,\n",
      "        -5.599999999999993, -7.799999999999981, -4.499999999999998, -2.2999999999999976,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        3.200000000000001, -8.899999999999986, -5.5999999999999845, -8.89999999999998,\n",
      "        -0.10000000000000109, -3.399999999999999, -4.500000000000002, -9.99999999999998,\n",
      "        -8.89999999999998, -3.4, -9.99999999999998, -8.89999999999998, 7.60000000000001,\n",
      "        -6.699999999999991, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        1.0000000000000022, -5.599999999999991, -9.99999999999998, -2.3000000000000047,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -6.699999999999995,\n",
      "        -7.7999999999999865, -6.699999999999995, -6.6999999999999815, 6.500000000000014,\n",
      "        -7.799999999999981, -6.699999999999995, -8.89999999999998, -0.0999999999999841,\n",
      "        -9.99999999999998, -6.69999999999999, 8.700000000000024, 14.200000000000003,\n",
      "        5.4000000000000075, -6.69999999999999, -9.99999999999998, -3.399999999999999,\n",
      "        17.49999999999998, -9.99999999999998, 0.999999999999998, -9.99999999999998,\n",
      "        -8.899999999999986, -6.699999999999988, -9.99999999999998, -0.09999999999998677,\n",
      "        -8.89999999999998, -8.89999999999998, -6.69999999999999, -6.69999999999999,\n",
      "        -9.99999999999998, -5.599999999999999, -7.799999999999981, 8.70000000000001,\n",
      "        -9.99999999999998, -8.899999999999986, -8.89999999999998, -7.799999999999981,\n",
      "        -7.799999999999981, -2.299999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999986, -6.6999999999999815, -5.6, -3.399999999999987, -9.99999999999998,\n",
      "        -2.3000000000000016, -7.799999999999985]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 36.0\n",
      "      policy2: 17.49999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.24\n",
      "      policy2: -5.9409999999999865\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07434591850289012\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02457182493202588\n",
      "      mean_inference_ms: 6.507959809080643\n",
      "      mean_raw_obs_processing_ms: 0.43915785689192915\n",
      "  time_since_restore: 518.7325265407562\n",
      "  time_this_iter_s: 49.185412883758545\n",
      "  time_total_s: 518.7325265407562\n",
      "  timers:\n",
      "    learn_throughput: 109.709\n",
      "    learn_time_ms: 27345.001\n",
      "    synch_weights_time_ms: 2.892\n",
      "    training_iteration_time_ms: 49719.011\n",
      "  timestamp: 1660562508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 11\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 64000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-21-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.59999999999996\n",
      "  episode_reward_mean: 3.0060000000000078\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1588622331619263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013084284029901028\n",
      "          model: {}\n",
      "          policy_loss: -0.03666471689939499\n",
      "          total_loss: 6.984848976135254\n",
      "          vf_explained_var: 0.08362455666065216\n",
      "          vf_loss: 7.018896579742432\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1375681161880493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012084710411727428\n",
      "          model: {}\n",
      "          policy_loss: -0.03796739503741264\n",
      "          total_loss: 2.355794668197632\n",
      "          vf_explained_var: 0.22725142538547516\n",
      "          vf_loss: 2.391345500946045\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 32000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.75157894736842\n",
      "    ram_util_percent: 64.01684210526312\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 30.5\n",
      "    policy2: 3.1999999999999966\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.355\n",
      "    policy2: -7.348999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -10.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07715684644081591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02445272494741676\n",
      "    mean_inference_ms: 6.521856186064281\n",
      "    mean_raw_obs_processing_ms: 0.43154803598089103\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 21.59999999999996\n",
      "    episode_reward_mean: 3.0060000000000078\n",
      "    episode_reward_min: -19.499999999999986\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [10.800000000000033, 4.500000000000021, 4.5000000000000275, 2.100000000000025,\n",
      "        2.4000000000000226, -13.499999999999977, -4.500000000000012, 10.500000000000016,\n",
      "        0.599999999999958, 13.799999999999995, 4.50000000000003, -3.5999999999999766,\n",
      "        2.9999999999999987, -10.499999999999988, -11.09999999999998, 3.0000000000000044,\n",
      "        9.000000000000032, 1.500000000000008, -5.6999999999999815, -8.399999999999979,\n",
      "        5.099999999999972, 9.29999999999997, -1.4999999999999738, -6.29999999999999,\n",
      "        -9.599999999999978, -4.499999999999977, -5.399999999999979, 6.600000000000017,\n",
      "        -2.999999999999984, 6.600000000000021, -1.4999999999999862, -18.299999999999983,\n",
      "        15.000000000000032, 3.3000000000000176, -6.899999999999974, -6.899999999999974,\n",
      "        13.199999999999966, 2.700000000000005, 9.3, 1.2073675392798577e-14, 5.700000000000031,\n",
      "        -3.8999999999999795, 3.300000000000014, -1.1999999999999738, 14.699999999999958,\n",
      "        10.799999999999969, 18.59999999999993, 6.600000000000028, -7.499999999999976,\n",
      "        -18.0, -5.099999999999978, 14.999999999999954, -7.799999999999981, 1.8846035843012032e-14,\n",
      "        4.200000000000028, 2.7000000000000237, 2.4000000000000257, 6.000000000000021,\n",
      "        11.39999999999998, 2.4000000000000203, 9.299999999999995, 5.40000000000001,\n",
      "        9.300000000000004, 5.1000000000000245, 7.200000000000024, 3.2999999999999674,\n",
      "        11.699999999999992, 0.6000000000000165, 5.7000000000000295, 3.900000000000025,\n",
      "        -8.399999999999999, 13.499999999999948, 13.499999999999948, 9.299999999999962,\n",
      "        13.500000000000023, 11.099999999999977, 6.000000000000027, 18.900000000000002,\n",
      "        4.500000000000018, -0.9000000000000075, 3.300000000000015, 2.099999999999998,\n",
      "        13.499999999999952, 0.9000000000000127, -2.999999999999976, 6.900000000000025,\n",
      "        1.740274591099933e-14, 17.700000000000017, 0.6000000000000113, -19.499999999999986,\n",
      "        21.59999999999996, 1.8000000000000238, 5.100000000000016, 7.5000000000000195,\n",
      "        8.400000000000025, -1.799999999999978, -0.6000000000000051, -0.29999999999998084,\n",
      "        -2.6999999999999766, 16.199999999999925]\n",
      "      policy_policy1_reward: [17.5, 14.5, 14.5, 11.0, 8.0, -3.5, 5.5, 9.5, 9.5, 20.5,\n",
      "        14.5, 2.0, 13.0, -0.5, -5.5, 7.5, 13.5, 11.5, 1.0, 0.5, 14.0, 16.0, 8.5, -9.5,\n",
      "        -4.0, 5.5, 3.5, 15.5, 7.0, 10.0, 8.5, -10.5, 25.0, 4.5, 2.0, 2.0, 21.0, 10.5,\n",
      "        10.5, 10.0, 13.5, 5.0, 10.0, 5.5, 22.5, 12.0, 27.5, 15.5, 2.5, -8.0, 0.5, 25.0,\n",
      "        0.0, 10.0, 12.0, 10.5, 8.0, 16.0, 17.0, 8.0, 16.0, 11.0, 16.0, 14.0, 15.0, 10.0,\n",
      "        19.5, 9.5, 13.5, 9.5, 0.5, 18.0, 12.5, 16.0, 23.5, 20.0, 16.0, 24.5, 14.5, 8.0,\n",
      "        10.0, 5.5, 23.5, 6.5, 7.0, 7.0, 10.0, 25.5, 9.5, -9.5, 30.5, 8.5, 14.0, 17.5,\n",
      "        14.0, 0.5, 5.0, 7.5, 4.0, 24.0]\n",
      "      policy_policy2_reward: [-6.699999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -5.599999999999982, -9.99999999999998, -9.99999999999998,\n",
      "        0.9999999999999952, -8.899999999999983, -6.69999999999999, -9.99999999999998,\n",
      "        -5.5999999999999925, -9.99999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -4.499999999999995, -4.500000000000003, -9.99999999999998, -6.699999999999986,\n",
      "        -8.89999999999998, -8.899999999999986, -6.699999999999983, -9.99999999999998,\n",
      "        3.1999999999999966, -5.6, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -9.99999999999998, -3.4000000000000066, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -1.2000000000000035, -8.89999999999998, -8.899999999999986,\n",
      "        -7.799999999999989, -7.799999999999985, -1.2000000000000006, -9.99999999999998,\n",
      "        -7.799999999999984, -8.899999999999986, -6.6999999999999815, -6.699999999999995,\n",
      "        -7.799999999999988, -1.2000000000000048, -8.89999999999998, -8.899999999999986,\n",
      "        -9.99999999999998, -9.99999999999998, -5.6, -9.99999999999998, -7.799999999999981,\n",
      "        -9.99999999999998, -7.799999999999981, -7.79999999999999, -5.599999999999999,\n",
      "        -9.99999999999998, -5.599999999999982, -5.599999999999995, -6.6999999999999815,\n",
      "        -5.599999999999995, -6.699999999999986, -8.89999999999998, -7.799999999999986,\n",
      "        -6.6999999999999815, -7.79999999999999, -8.899999999999983, -7.799999999999981,\n",
      "        -5.599999999999995, -8.899999999999984, -4.500000000000002, 0.9999999999999983,\n",
      "        -6.6999999999999815, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -5.5999999999999925, -9.99999999999998, -8.899999999999986, -6.699999999999985,\n",
      "        -3.399999999999988, -9.99999999999998, -5.5999999999999845, -9.99999999999998,\n",
      "        -0.1000000000000022, -9.99999999999998, -7.79999999999999, -8.899999999999986,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999995, -8.89999999999998,\n",
      "        -9.99999999999998, -5.599999999999998, -2.300000000000002, -5.599999999999989,\n",
      "        -7.799999999999989, -6.699999999999985, -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 30.5\n",
      "      policy2: 3.1999999999999966\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.355\n",
      "      policy2: -7.348999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -10.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07715684644081591\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02445272494741676\n",
      "      mean_inference_ms: 6.521856186064281\n",
      "      mean_raw_obs_processing_ms: 0.43154803598089103\n",
      "  time_since_restore: 514.4271309375763\n",
      "  time_this_iter_s: 66.83319997787476\n",
      "  time_total_s: 514.4271309375763\n",
      "  timers:\n",
      "    learn_throughput: 111.841\n",
      "    learn_time_ms: 35764.904\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 64292.67\n",
      "  timestamp: 1660562509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 66000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_env_steps_sampled: 33000\n",
      "    num_env_steps_trained: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-21-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.299999999999976\n",
      "  episode_reward_mean: 8.159999999999988\n",
      "  episode_reward_min: -17.999999999999986\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 330\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9695741534233093\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019059766083955765\n",
      "          model: {}\n",
      "          policy_loss: -0.060230910778045654\n",
      "          total_loss: 7.1679511070251465\n",
      "          vf_explained_var: 0.15205499529838562\n",
      "          vf_loss: 7.219605922698975\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9542282819747925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018743030726909637\n",
      "          model: {}\n",
      "          policy_loss: -0.05704605206847191\n",
      "          total_loss: 2.5620312690734863\n",
      "          vf_explained_var: 0.21545295417308807\n",
      "          vf_loss: 2.610643148422241\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_env_steps_sampled: 33000\n",
      "    num_env_steps_trained: 33000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 66000\n",
      "  num_agent_steps_trained: 66000\n",
      "  num_env_steps_sampled: 33000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 33000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 32.15428571428571\n",
      "    ram_util_percent: 64.0685714285714\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 6.500000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 15.41\n",
      "    policy2: -7.249999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07219368072145133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023355222903107115\n",
      "    mean_inference_ms: 6.603622834341402\n",
      "    mean_raw_obs_processing_ms: 0.4390401588278607\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 30.299999999999976\n",
      "    episode_reward_mean: 8.159999999999988\n",
      "    episode_reward_min: -17.999999999999986\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [14.699999999999969, -11.699999999999982, 17.999999999999915,\n",
      "        -7.499999999999973, 6.000000000000027, 9.000000000000027, 30.299999999999976,\n",
      "        8.099999999999927, 1.484923295436147e-14, 20.999999999999915, -2.7000000000000295,\n",
      "        5.700000000000015, -1.499999999999981, 11.999999999999941, 9.59999999999995,\n",
      "        5.7000000000000215, 3.600000000000026, 6.899999999999961, 3.000000000000033,\n",
      "        12.6, 3.30000000000003, 6.60000000000003, 4.500000000000007, -0.8999999999999861,\n",
      "        3.0000000000000258, 17.09999999999995, 1.5000000000000049, 3.0000000000000275,\n",
      "        4.499999999999975, 4.800000000000015, 8.10000000000003, -1.799999999999971,\n",
      "        7.500000000000034, -8.699999999999978, 18.599999999999948, 17.99999999999993,\n",
      "        15.899999999999965, 3.299999999999968, -17.999999999999986, 20.099999999999895,\n",
      "        5.999999999999988, 7.500000000000034, 8.999999999999934, 20.99999999999993,\n",
      "        17.099999999999916, 0.6000000000000224, 23.099999999999923, -4.499999999999984,\n",
      "        12.6, 9.29999999999998, 3.000000000000022, 5.400000000000002, 5.399999999999936,\n",
      "        18.29999999999992, 2.7283730830163222e-14, 6.599999999999982, 15.600000000000016,\n",
      "        -4.20000000000001, -5.699999999999982, 14.699999999999916, 9.000000000000032,\n",
      "        9.599999999999925, 16.799999999999933, 1.7999999999999936, 15.899999999999972,\n",
      "        19.499999999999915, 9.00000000000003, 7.500000000000007, -0.5999999999999762,\n",
      "        15.600000000000017, 22.499999999999957, 15.000000000000012, 14.999999999999911,\n",
      "        7.19999999999996, 17.70000000000001, 29.999999999999943, 3.3000000000000114,\n",
      "        13.499999999999927, -3.299999999999975, 11.999999999999991, 8.700000000000028,\n",
      "        11.699999999999976, 8.099999999999998, 1.8000000000000167, 11.700000000000026,\n",
      "        20.099999999999916, 1.800000000000022, -2.9999999999999916, 4.500000000000027,\n",
      "        7.500000000000018, 15.599999999999998, 10.500000000000021, 11.999999999999984,\n",
      "        8.999999999999979, 7.799999999999974, 12.60000000000001, 11.399999999999933,\n",
      "        5.999999999999941, 1.662558979376172e-14, 7.199999999999934]\n",
      "      policy_policy1_reward: [22.5, -5.0, 28.0, 2.5, 16.0, 19.0, 37.0, 17.0, 10.0, 25.5,\n",
      "        4.0, 13.5, 8.5, 22.0, 18.5, 8.0, 12.5, 7.0, 13.0, 21.5, 10.0, 15.5, 14.5, 8.0,\n",
      "        7.5, 26.0, 11.5, 13.0, 14.5, 11.5, 17.0, 6.0, 17.5, -2.0, 22.0, 28.0, 21.5,\n",
      "        10.0, -8.0, 29.0, 16.0, 17.5, 19.0, 31.0, 26.0, 9.5, 32.0, -11.0, 21.5, 10.5,\n",
      "        13.0, 5.5, 11.0, 25.0, 10.0, 15.5, 24.5, 2.5, 1.0, 11.5, 19.0, 13.0, 23.5, 8.5,\n",
      "        16.0, 29.5, 13.5, 12.0, 5.0, 24.5, 32.5, 19.5, 25.0, 15.0, 25.5, 34.5, 10.0,\n",
      "        23.5, 4.5, 22.0, 16.5, 14.0, 17.0, 3.0, 19.5, 29.0, 8.5, 7.0, 9.0, 17.5, 24.5,\n",
      "        15.0, 22.0, 19.0, 14.5, 16.0, 11.5, 16.0, 10.0, 9.5]\n",
      "      policy_policy2_reward: [-7.799999999999989, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -6.699999999999985,\n",
      "        -8.89999999999998, -9.99999999999998, -4.500000000000004, -6.69999999999999,\n",
      "        -7.799999999999981, -9.99999999999998, -9.99999999999998, -8.899999999999984,\n",
      "        -2.299999999999995, -8.89999999999998, -0.1000000000000042, -9.99999999999998,\n",
      "        -8.89999999999998, -6.6999999999999815, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -4.499999999999997, -8.899999999999986, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -6.699999999999988, -8.89999999999998,\n",
      "        -7.79999999999999, -9.99999999999998, -6.699999999999982, -3.4000000000000057,\n",
      "        -9.99999999999998, -5.599999999999998, -6.699999999999989, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        6.500000000000002, -8.899999999999986, -1.2000000000000008, -9.99999999999998,\n",
      "        -0.1000000000000022, -5.599999999999994, -6.699999999999987, -9.99999999999998,\n",
      "        -8.899999999999986, -8.89999999999998, -6.699999999999994, -6.699999999999994,\n",
      "        3.199999999999997, -9.99999999999998, -3.4000000000000044, -6.6999999999999815,\n",
      "        -6.699999999999995, -0.10000000000000175, -9.99999999999998, -4.499999999999982,\n",
      "        -4.499999999999997, -5.599999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -4.499999999999997, -9.99999999999998, -7.799999999999983, -7.79999999999999,\n",
      "        -4.499999999999987, -6.6999999999999815, -9.99999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -7.799999999999989, -2.2999999999999896, -8.89999999999998,\n",
      "        -1.2000000000000035, -7.79999999999999, -8.89999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -4.5000000000000036, -9.99999999999998, -8.899999999999986,\n",
      "        -4.499999999999991, -9.99999999999998, -9.99999999999998, -6.699999999999986,\n",
      "        -3.400000000000005, -0.1000000000000042, -9.99999999999998, -9.99999999999998,\n",
      "        -2.2999999999999834]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 37.0\n",
      "      policy2: 6.500000000000002\n",
      "    policy_reward_mean:\n",
      "      policy1: 15.41\n",
      "      policy2: -7.249999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -11.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07219368072145133\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023355222903107115\n",
      "      mean_inference_ms: 6.603622834341402\n",
      "      mean_raw_obs_processing_ms: 0.4390401588278607\n",
      "  time_since_restore: 522.3299870491028\n",
      "  time_this_iter_s: 49.80974316596985\n",
      "  time_total_s: 522.3299870491028\n",
      "  timers:\n",
      "    learn_throughput: 109.632\n",
      "    learn_time_ms: 27364.15\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 49838.491\n",
      "  timestamp: 1660562510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 11\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:21:55 (running for 00:09:29.86)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:00 (running for 00:09:34.91)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:05 (running for 00:09:40.00)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:10 (running for 00:09:45.05)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:15 (running for 00:09:50.13)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:20 (running for 00:09:55.19)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      7 |          465.802 | 28000 |    3.309 |          10.24  |          -6.931 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 64000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-22-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.09999999999996\n",
      "  episode_reward_mean: 4.116000000000011\n",
      "  episode_reward_min: -15.599999999999994\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1570671796798706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01655583269894123\n",
      "          model: {}\n",
      "          policy_loss: -0.05063411220908165\n",
      "          total_loss: 6.972736835479736\n",
      "          vf_explained_var: 0.15675362944602966\n",
      "          vf_loss: 7.015921115875244\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0918129682540894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01849987544119358\n",
      "          model: {}\n",
      "          policy_loss: -0.04421568661928177\n",
      "          total_loss: 2.995983839035034\n",
      "          vf_explained_var: 0.1779404580593109\n",
      "          vf_loss: 3.034649610519409\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 32000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.847872340425535\n",
      "    ram_util_percent: 64.29042553191489\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.805\n",
      "    policy2: -6.688999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07347555649135762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02266334874794665\n",
      "    mean_inference_ms: 7.1802886045251055\n",
      "    mean_raw_obs_processing_ms: 0.4265721579481773\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 23.09999999999996\n",
      "    episode_reward_mean: 4.116000000000011\n",
      "    episode_reward_min: -15.599999999999994\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.900000000000032, 1.4999999999999774, 3.000000000000016, 5.70000000000001,\n",
      "        12.600000000000005, 2.700000000000019, -6.299999999999983, 5.7000000000000295,\n",
      "        4.800000000000027, 13.800000000000026, 1.5000000000000298, 1.5000000000000127,\n",
      "        -0.29999999999997906, -7.199999999999996, 6.599999999999973, -5.399999999999977,\n",
      "        6.600000000000026, -15.599999999999994, -0.5999999999999915, 12.299999999999986,\n",
      "        13.500000000000016, 3.300000000000023, 7.799999999999969, 11.40000000000002,\n",
      "        17.699999999999996, 0.30000000000002447, 2.1000000000000005, 13.799999999999983,\n",
      "        5.400000000000009, 15.299999999999986, 17.699999999999974, 5.400000000000013,\n",
      "        10.500000000000007, -10.499999999999972, -12.899999999999986, -4.4999999999999805,\n",
      "        5.100000000000032, 2.700000000000014, 4.500000000000016, 9.900000000000025,\n",
      "        6.6000000000000245, 12.000000000000025, 3.000000000000001, 9.000000000000027,\n",
      "        13.799999999999931, -1.499999999999989, 10.799999999999992, 6.600000000000023,\n",
      "        3.9000000000000283, -9.899999999999979, 11.700000000000017, -6.900000000000004,\n",
      "        -2.6999999999999953, 3.90000000000003, -14.699999999999992, 2.095545958979983e-14,\n",
      "        14.700000000000019, 16.500000000000007, 10.500000000000028, -0.8999999999999961,\n",
      "        -7.49999999999999, -2.6999999999999873, 12.000000000000032, 3.299999999999976,\n",
      "        13.499999999999982, 0.5999999999999996, 10.199999999999994, 4.2000000000000295,\n",
      "        1.5000000000000067, -9.299999999999981, 7.500000000000032, 8.100000000000033,\n",
      "        15.00000000000003, 12.00000000000003, 12.600000000000025, 9.000000000000023,\n",
      "        5.100000000000017, -11.999999999999979, -15.29999999999998, 23.09999999999996,\n",
      "        18.299999999999947, 8.100000000000021, -8.999999999999973, 1.8000000000000185,\n",
      "        -12.899999999999993, -3.2999999999999847, 2.7000000000000086, 12.30000000000002,\n",
      "        12.300000000000017, 2.400000000000026, 6.300000000000033, 2.373101715136272e-14,\n",
      "        9.000000000000021, 18.900000000000006, -8.999999999999986, 14.10000000000003,\n",
      "        -5.699999999999977, -4.4999999999999964, -1.4999999999999933, 8.699999999999983]\n",
      "      policy_policy1_reward: [9.5, 6.0, 13.0, 13.5, 21.5, 10.5, 1.5, 13.5, 11.5, 15.0,\n",
      "        11.5, 11.5, 7.5, -0.5, 15.5, 3.5, 15.5, -10.0, 5.0, 19.0, 23.5, 10.0, 14.5,\n",
      "        17.0, 20.0, 7.0, 11.0, 20.5, 11.0, 22.0, 25.5, 11.0, 20.5, -0.5, -4.0, 5.5,\n",
      "        14.0, 10.5, 9.0, 15.5, 10.0, 22.0, 13.0, 19.0, 20.5, 8.5, 17.5, 10.0, 9.5, -1.0,\n",
      "        19.5, -3.5, 4.0, 9.5, -8.0, 4.5, 22.5, 26.5, 9.5, 2.5, 2.5, 4.0, 16.5, 4.5,\n",
      "        18.0, 9.5, 18.0, 12.0, 11.5, -1.5, 17.5, 17.0, 19.5, 22.0, 21.5, 8.0, 14.0,\n",
      "        -2.0, -18.5, 32.0, 19.5, 17.0, 1.0, -8.0, -4.0, 4.5, 10.5, 19.0, 19.0, 2.5,\n",
      "        13.0, 4.5, 19.0, 19.0, 1.0, 17.5, 1.0, 5.5, 3.0, 16.5]\n",
      "      policy_policy2_reward: [-5.599999999999982, -4.499999999999999, -9.99999999999998,\n",
      "        -7.799999999999981, -8.899999999999984, -7.799999999999981, -7.799999999999981,\n",
      "        -7.79999999999999, -6.699999999999986, -1.2000000000000026, -9.99999999999998,\n",
      "        -9.99999999999998, -7.799999999999984, -6.699999999999986, -8.899999999999986,\n",
      "        -8.899999999999986, -8.89999999999998, -5.6, -5.599999999999991, -6.6999999999999815,\n",
      "        -9.99999999999998, -6.6999999999999815, -6.699999999999993, -5.599999999999986,\n",
      "        -2.3000000000000047, -6.699999999999983, -8.89999999999998, -6.699999999999994,\n",
      "        -5.599999999999998, -6.699999999999992, -7.799999999999981, -5.5999999999999925,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -8.89999999999998, -7.7999999999999865, -4.500000000000004, -5.599999999999982,\n",
      "        -3.4000000000000026, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -6.699999999999995, -3.4000000000000035,\n",
      "        -5.5999999999999845, -8.89999999999998, -7.799999999999981, -3.4000000000000057,\n",
      "        -6.699999999999989, -5.599999999999991, -6.699999999999995, -4.5000000000000036,\n",
      "        -7.7999999999999865, -9.99999999999998, 0.9999999999999957, -3.400000000000002,\n",
      "        -9.99999999999998, -6.699999999999995, -4.4999999999999885, -1.200000000000004,\n",
      "        -4.499999999999997, -8.89999999999998, -7.79999999999999, -7.79999999999999,\n",
      "        -9.99999999999998, -7.799999999999981, -9.99999999999998, -8.899999999999986,\n",
      "        -4.500000000000001, -9.99999999999998, -8.899999999999986, 1.0000000000000004,\n",
      "        -8.89999999999998, -9.99999999999998, 3.2000000000000046, -8.89999999999998,\n",
      "        -1.2000000000000048, -8.899999999999983, -9.99999999999998, 9.800000000000006,\n",
      "        -8.89999999999998, -7.799999999999981, -7.799999999999981, -6.6999999999999815,\n",
      "        -6.6999999999999815, -0.09999999999998954, -6.69999999999999, -4.499999999999988,\n",
      "        -9.99999999999998, -0.09999999999999593, -9.99999999999998, -3.3999999999999986,\n",
      "        -6.6999999999999815, -9.99999999999998, -4.5, -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 32.0\n",
      "      policy2: 9.800000000000006\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.805\n",
      "      policy2: -6.688999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07347555649135762\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02266334874794665\n",
      "      mean_inference_ms: 7.1802886045251055\n",
      "      mean_raw_obs_processing_ms: 0.4265721579481773\n",
      "  time_since_restore: 532.5205867290497\n",
      "  time_this_iter_s: 66.7185070514679\n",
      "  time_total_s: 532.5205867290497\n",
      "  timers:\n",
      "    learn_throughput: 112.144\n",
      "    learn_time_ms: 35668.395\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 66559.339\n",
      "  timestamp: 1660562543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:29 (running for 00:10:03.57)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:34 (running for 00:10:08.63)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=8.159999999999988 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107520>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B998107F40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B9981AEFD0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202EE0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     11 |          518.733 | 33000 |    4.299 |          10.24  |          -5.941 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     11 |          522.33  | 33000 |    8.16  |          15.41  |          -7.25  |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-22-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.199999999999918\n",
      "  episode_reward_mean: 4.479000000000005\n",
      "  episode_reward_min: -17.399999999999977\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 360\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9933236241340637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01148010790348053\n",
      "          model: {}\n",
      "          policy_loss: -0.03915372118353844\n",
      "          total_loss: 6.535118103027344\n",
      "          vf_explained_var: 0.21137899160385132\n",
      "          vf_loss: 6.571976661682129\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0368194580078125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01215177122503519\n",
      "          model: {}\n",
      "          policy_loss: -0.03656945750117302\n",
      "          total_loss: 2.486356735229492\n",
      "          vf_explained_var: 0.2722625434398651\n",
      "          vf_loss: 2.520495891571045\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 36000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.290140845070415\n",
      "    ram_util_percent: 64.39436619718307\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 36.0\n",
      "    policy2: 17.49999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.31\n",
      "    policy2: -5.830999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07450191451992529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024749431320832425\n",
      "    mean_inference_ms: 6.542436289529703\n",
      "    mean_raw_obs_processing_ms: 0.4393425509717261\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.199999999999918\n",
      "    episode_reward_mean: 4.479000000000005\n",
      "    episode_reward_min: -17.399999999999977\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [0.9000000000000209, -6.900000000000004, 15.899999999999975, -0.8999999999999718,\n",
      "        6.000000000000027, 1.3961054534661343e-14, 23.099999999999945, 6.599999999999991,\n",
      "        1.5000000000000209, -6.899999999999977, 6.599999999999945, 4.800000000000024,\n",
      "        -8.999999999999979, 2.683964162031316e-14, -2.9999999999999956, 9.000000000000028,\n",
      "        5.399999999999963, 7.500000000000007, 17.69999999999991, 2.7283730830163222e-14,\n",
      "        14.999999999999975, 6.599999999999991, 1.8000000000000278, 1.2000000000000215,\n",
      "        9.299999999999969, -5.700000000000003, 1.4999999999999876, 2.7000000000000157,\n",
      "        10.79999999999992, 2.0999999999999788, 3.9000000000000283, 2.373101715136272e-14,\n",
      "        -1.1999999999999846, -0.2999999999999853, 11.700000000000017, 8.400000000000025,\n",
      "        -1.1999999999999922, 13.500000000000012, 14.100000000000028, -8.999999999999986,\n",
      "        6.000000000000012, 23.999999999999936, 1.5000000000000187, -17.399999999999977,\n",
      "        -8.699999999999985, -1.4999999999999805, 6.9000000000000306, 8.10000000000001,\n",
      "        -3.8999999999999777, 12.299999999999928, 7.800000000000031, 7.500000000000032,\n",
      "        6.900000000000032, 1.2000000000000246, 14.699999999999994, 16.49999999999999,\n",
      "        15.600000000000023, 8.099999999999934, 8.700000000000019, 14.699999999999958,\n",
      "        -6.299999999999977, 4.500000000000025, -1.4999999999999787, 28.199999999999918,\n",
      "        -1.1999999999999762, 6.900000000000025, 11.100000000000009, -4.4999999999999805,\n",
      "        -1.7999999999999785, 5.700000000000017, 12.599999999999989, 7.2000000000000135,\n",
      "        -3.8999999999999777, 7.200000000000008, -1.1999999999999829, 7.800000000000031,\n",
      "        -5.999999999999989, 7.800000000000027, 0.6000000000000119, 3.6000000000000214,\n",
      "        0.3000000000000227, 12.000000000000021, 14.999999999999932, 6.599999999999943,\n",
      "        17.100000000000016, -5.099999999999985, -8.399999999999975, -1.4999999999999891,\n",
      "        5.100000000000023, 10.499999999999995, -1.4999999999999956, 3.90000000000003,\n",
      "        2.017830347256222e-14, 6.900000000000032, 3.000000000000009, 6.600000000000028,\n",
      "        -2.4000000000000052, 18.299999999999983, -4.799999999999977, -2.9999999999999867]\n",
      "      policy_policy1_reward: [6.5, 2.0, 16.0, 2.5, 10.5, 10.0, 32.0, 10.0, 11.5, 2.0,\n",
      "        -1.0, 11.5, 1.0, 10.0, 7.0, 8.0, 11.0, 17.5, 20.0, 10.0, 25.0, 15.5, 8.5, 9.0,\n",
      "        16.0, 1.0, -5.0, 10.5, 17.5, 11.0, 4.0, 10.0, 5.5, -9.0, -2.5, 3.0, 5.5, 23.5,\n",
      "        17.5, -26.5, 16.0, 23.0, 11.5, -8.5, -2.0, 8.5, 7.0, 17.0, 5.0, 19.0, 14.5,\n",
      "        17.5, 12.5, 9.0, 6.0, 26.5, 24.5, 17.0, 16.5, 22.5, -4.0, 14.5, 8.5, 36.0, 5.5,\n",
      "        12.5, 14.5, 5.5, 0.5, 13.5, 16.0, 9.5, 5.0, 15.0, 5.5, 14.5, 4.0, 14.5, 4.0,\n",
      "        12.5, 7.0, 22.0, 25.0, 15.5, 9.5, 0.5, -5.0, 8.5, 14.0, 20.5, 8.5, 9.5, 10.0,\n",
      "        12.5, 13.0, 10.0, 6.5, 25.0, 3.0, 7.0]\n",
      "      policy_policy2_reward: [-5.5999999999999845, -8.89999999999998, -0.10000000000000109,\n",
      "        -3.399999999999999, -4.500000000000002, -9.99999999999998, -8.89999999999998,\n",
      "        -3.4, -9.99999999999998, -8.89999999999998, 7.60000000000001, -6.699999999999991,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, 1.0000000000000022,\n",
      "        -5.599999999999991, -9.99999999999998, -2.3000000000000047, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -6.699999999999995, -7.7999999999999865,\n",
      "        -6.699999999999995, -6.6999999999999815, 6.500000000000014, -7.799999999999981,\n",
      "        -6.699999999999995, -8.89999999999998, -0.0999999999999841, -9.99999999999998,\n",
      "        -6.69999999999999, 8.700000000000024, 14.200000000000003, 5.4000000000000075,\n",
      "        -6.69999999999999, -9.99999999999998, -3.399999999999999, 17.49999999999998,\n",
      "        -9.99999999999998, 0.999999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        -6.699999999999988, -9.99999999999998, -0.09999999999998677, -8.89999999999998,\n",
      "        -8.89999999999998, -6.69999999999999, -6.69999999999999, -9.99999999999998,\n",
      "        -5.599999999999999, -7.799999999999981, 8.70000000000001, -9.99999999999998,\n",
      "        -8.899999999999986, -8.89999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -2.299999999999999, -9.99999999999998, -9.99999999999998, -7.799999999999986,\n",
      "        -6.6999999999999815, -5.6, -3.399999999999987, -9.99999999999998, -2.3000000000000016,\n",
      "        -7.799999999999985, -3.399999999999994, -2.299999999999995, -8.89999999999998,\n",
      "        -7.799999999999989, -6.699999999999982, -6.69999999999999, -9.99999999999998,\n",
      "        -6.6999999999999815, -3.399999999999993, -8.89999999999998, -6.6999999999999815,\n",
      "        -9.99999999999998, -9.99999999999998, -8.899999999999986, 7.600000000000014,\n",
      "        -5.5999999999999925, -3.399999999999983, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -5.599999999999991, -9.99999999999998,\n",
      "        -5.599999999999989, -9.99999999999998, -3.3999999999999857, -8.899999999999984,\n",
      "        -6.699999999999984, -7.799999999999985, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 36.0\n",
      "      policy2: 17.49999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.31\n",
      "      policy2: -5.830999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07450191451992529\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024749431320832425\n",
      "      mean_inference_ms: 6.542436289529703\n",
      "      mean_raw_obs_processing_ms: 0.4393425509717261\n",
      "  time_since_restore: 568.6719219684601\n",
      "  time_this_iter_s: 49.93939542770386\n",
      "  time_total_s: 568.6719219684601\n",
      "  timers:\n",
      "    learn_throughput: 109.646\n",
      "    learn_time_ms: 27360.859\n",
      "    synch_weights_time_ms: 2.792\n",
      "    training_iteration_time_ms: 49687.286\n",
      "  timestamp: 1660562558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 12\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 72000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-22-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.999999999999943\n",
      "  episode_reward_mean: 9.944999999999977\n",
      "  episode_reward_min: -17.999999999999986\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 360\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9346444010734558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018574152141809464\n",
      "          model: {}\n",
      "          policy_loss: -0.054892104119062424\n",
      "          total_loss: 7.270376205444336\n",
      "          vf_explained_var: 0.05313895642757416\n",
      "          vf_loss: 7.316910266876221\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9285067915916443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01695587858557701\n",
      "          model: {}\n",
      "          policy_loss: -0.05157144367694855\n",
      "          total_loss: 2.8638739585876465\n",
      "          vf_explained_var: 0.11961819231510162\n",
      "          vf_loss: 2.9078152179718018\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 36000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.380281690140837\n",
      "    ram_util_percent: 64.38873239436616\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 34.5\n",
      "    policy2: 6.500000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 16.645\n",
      "    policy2: -6.699999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07253054945868889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023600647763493087\n",
      "    mean_inference_ms: 6.6366342330624715\n",
      "    mean_raw_obs_processing_ms: 0.4391064981165641\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.999999999999943\n",
      "    episode_reward_mean: 9.944999999999977\n",
      "    episode_reward_min: -17.999999999999986\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [8.10000000000003, -1.799999999999971, 7.500000000000034, -8.699999999999978,\n",
      "        18.599999999999948, 17.99999999999993, 15.899999999999965, 3.299999999999968,\n",
      "        -17.999999999999986, 20.099999999999895, 5.999999999999988, 7.500000000000034,\n",
      "        8.999999999999934, 20.99999999999993, 17.099999999999916, 0.6000000000000224,\n",
      "        23.099999999999923, -4.499999999999984, 12.6, 9.29999999999998, 3.000000000000022,\n",
      "        5.400000000000002, 5.399999999999936, 18.29999999999992, 2.7283730830163222e-14,\n",
      "        6.599999999999982, 15.600000000000016, -4.20000000000001, -5.699999999999982,\n",
      "        14.699999999999916, 9.000000000000032, 9.599999999999925, 16.799999999999933,\n",
      "        1.7999999999999936, 15.899999999999972, 19.499999999999915, 9.00000000000003,\n",
      "        7.500000000000007, -0.5999999999999762, 15.600000000000017, 22.499999999999957,\n",
      "        15.000000000000012, 14.999999999999911, 7.19999999999996, 17.70000000000001,\n",
      "        29.999999999999943, 3.3000000000000114, 13.499999999999927, -3.299999999999975,\n",
      "        11.999999999999991, 8.700000000000028, 11.699999999999976, 8.099999999999998,\n",
      "        1.8000000000000167, 11.700000000000026, 20.099999999999916, 1.800000000000022,\n",
      "        -2.9999999999999916, 4.500000000000027, 7.500000000000018, 15.599999999999998,\n",
      "        10.500000000000021, 11.999999999999984, 8.999999999999979, 7.799999999999974,\n",
      "        12.60000000000001, 11.399999999999933, 5.999999999999941, 1.662558979376172e-14,\n",
      "        7.199999999999934, 13.49999999999995, -1.7999999999999856, 21.599999999999948,\n",
      "        13.199999999999992, 2.4000000000000132, 0.5999999999999829, 12.900000000000027,\n",
      "        20.999999999999947, 11.099999999999962, 16.5, 3.6000000000000294, 1.79999999999999,\n",
      "        11.100000000000023, 20.999999999999908, 15.299999999999965, 4.4999999999999485,\n",
      "        23.3999999999999, 15.600000000000023, 9.89999999999995, -4.799999999999988,\n",
      "        20.999999999999915, 12.299999999999935, 24.899999999999977, 24.899999999999935,\n",
      "        6.899999999999942, 17.699999999999896, 17.999999999999922, 9.599999999999952,\n",
      "        6.899999999999997, 14.099999999999913]\n",
      "      policy_policy1_reward: [17.0, 6.0, 17.5, -2.0, 22.0, 28.0, 21.5, 10.0, -8.0, 29.0,\n",
      "        16.0, 17.5, 19.0, 31.0, 26.0, 9.5, 32.0, -11.0, 21.5, 10.5, 13.0, 5.5, 11.0,\n",
      "        25.0, 10.0, 15.5, 24.5, 2.5, 1.0, 11.5, 19.0, 13.0, 23.5, 8.5, 16.0, 29.5, 13.5,\n",
      "        12.0, 5.0, 24.5, 32.5, 19.5, 25.0, 15.0, 25.5, 34.5, 10.0, 23.5, 4.5, 22.0,\n",
      "        16.5, 14.0, 17.0, 3.0, 19.5, 29.0, 8.5, 7.0, 9.0, 17.5, 24.5, 15.0, 22.0, 19.0,\n",
      "        14.5, 16.0, 11.5, 16.0, 10.0, 9.5, 18.0, 0.5, 30.5, 21.0, 2.5, 4.0, 18.5, 31.0,\n",
      "        20.0, 26.5, 12.5, 8.5, 20.0, 25.5, 16.5, 14.5, 29.0, 24.5, 15.5, 3.0, 31.0,\n",
      "        19.0, 30.5, 30.5, 12.5, 25.5, 22.5, 18.5, 1.5, 23.0]\n",
      "      policy_policy2_reward: [-8.89999999999998, -7.79999999999999, -9.99999999999998,\n",
      "        -6.699999999999982, -3.4000000000000057, -9.99999999999998, -5.599999999999998,\n",
      "        -6.699999999999989, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -8.89999999999998,\n",
      "        -8.899999999999986, -8.89999999999998, 6.500000000000002, -8.899999999999986,\n",
      "        -1.2000000000000008, -9.99999999999998, -0.1000000000000022, -5.599999999999994,\n",
      "        -6.699999999999987, -9.99999999999998, -8.899999999999986, -8.89999999999998,\n",
      "        -6.699999999999994, -6.699999999999994, 3.199999999999997, -9.99999999999998,\n",
      "        -3.4000000000000044, -6.6999999999999815, -6.699999999999995, -0.10000000000000175,\n",
      "        -9.99999999999998, -4.499999999999982, -4.499999999999997, -5.599999999999999,\n",
      "        -8.89999999999998, -9.99999999999998, -4.499999999999997, -9.99999999999998,\n",
      "        -7.799999999999983, -7.79999999999999, -4.499999999999987, -6.6999999999999815,\n",
      "        -9.99999999999998, -7.799999999999986, -9.99999999999998, -7.799999999999989,\n",
      "        -2.2999999999999896, -8.89999999999998, -1.2000000000000035, -7.79999999999999,\n",
      "        -8.89999999999998, -6.699999999999993, -9.99999999999998, -4.5000000000000036,\n",
      "        -9.99999999999998, -8.899999999999986, -4.499999999999991, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999986, -3.400000000000005, -0.1000000000000042,\n",
      "        -9.99999999999998, -9.99999999999998, -2.2999999999999834, -4.499999999999991,\n",
      "        -2.3000000000000043, -8.89999999999998, -7.79999999999999, -0.09999999999998366,\n",
      "        -3.400000000000005, -5.6, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999992, -8.89999999999998, -4.499999999999982,\n",
      "        -1.2000000000000028, -9.99999999999998, -5.599999999999999, -8.89999999999998,\n",
      "        -5.599999999999982, -7.799999999999986, -9.99999999999998, -6.6999999999999815,\n",
      "        -5.599999999999997, -5.599999999999999, -5.599999999999998, -7.799999999999981,\n",
      "        -4.499999999999997, -8.899999999999986, 5.4000000000000075, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.5\n",
      "      policy2: 6.500000000000002\n",
      "    policy_reward_mean:\n",
      "      policy1: 16.645\n",
      "      policy2: -6.699999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -11.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07253054945868889\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023600647763493087\n",
      "      mean_inference_ms: 6.6366342330624715\n",
      "      mean_raw_obs_processing_ms: 0.4391064981165641\n",
      "  time_since_restore: 572.6184496879578\n",
      "  time_this_iter_s: 50.28846263885498\n",
      "  time_total_s: 572.6184496879578\n",
      "  timers:\n",
      "    learn_throughput: 109.506\n",
      "    learn_time_ms: 27395.865\n",
      "    synch_weights_time_ms: 2.892\n",
      "    training_iteration_time_ms: 49888.547\n",
      "  timestamp: 1660562560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 12\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:40 (running for 00:10:15.11)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:45 (running for 00:10:20.17)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:50 (running for 00:10:25.30)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      8 |          514.427 | 32000 |    3.006 |          10.355 |          -7.349 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 72000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-22-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.299999999999912\n",
      "  episode_reward_mean: 5.778000000000002\n",
      "  episode_reward_min: -24.900000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1090837717056274\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01258448138833046\n",
      "          model: {}\n",
      "          policy_loss: -0.03754369169473648\n",
      "          total_loss: 7.177109718322754\n",
      "          vf_explained_var: 0.054440852254629135\n",
      "          vf_loss: 7.212136745452881\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1046727895736694\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012354628182947636\n",
      "          model: {}\n",
      "          policy_loss: -0.03667620196938515\n",
      "          total_loss: 2.819906711578369\n",
      "          vf_explained_var: 0.13782843947410583\n",
      "          vf_loss: 2.854112148284912\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 36000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.73333333333333\n",
      "    ram_util_percent: 64.37419354838707\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 34.0\n",
      "    policy2: 0.9999999999999983\n",
      "  policy_reward_mean:\n",
      "    policy1: 12.555\n",
      "    policy2: -6.776999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07633895811627323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024258523902248443\n",
      "    mean_inference_ms: 6.584691950244567\n",
      "    mean_raw_obs_processing_ms: 0.43076783466029395\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 27.299999999999912\n",
      "    episode_reward_mean: 5.778000000000002\n",
      "    episode_reward_min: -24.900000000000013\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [5.700000000000031, -3.8999999999999795, 3.300000000000014, -1.1999999999999738,\n",
      "        14.699999999999958, 10.799999999999969, 18.59999999999993, 6.600000000000028,\n",
      "        -7.499999999999976, -18.0, -5.099999999999978, 14.999999999999954, -7.799999999999981,\n",
      "        1.8846035843012032e-14, 4.200000000000028, 2.7000000000000237, 2.4000000000000257,\n",
      "        6.000000000000021, 11.39999999999998, 2.4000000000000203, 9.299999999999995,\n",
      "        5.40000000000001, 9.300000000000004, 5.1000000000000245, 7.200000000000024,\n",
      "        3.2999999999999674, 11.699999999999992, 0.6000000000000165, 5.7000000000000295,\n",
      "        3.900000000000025, -8.399999999999999, 13.499999999999948, 13.499999999999948,\n",
      "        9.299999999999962, 13.500000000000023, 11.099999999999977, 6.000000000000027,\n",
      "        18.900000000000002, 4.500000000000018, -0.9000000000000075, 3.300000000000015,\n",
      "        2.099999999999998, 13.499999999999952, 0.9000000000000127, -2.999999999999976,\n",
      "        6.900000000000025, 1.740274591099933e-14, 17.700000000000017, 0.6000000000000113,\n",
      "        -19.499999999999986, 21.59999999999996, 1.8000000000000238, 5.100000000000016,\n",
      "        7.5000000000000195, 8.400000000000025, -1.799999999999978, -0.6000000000000051,\n",
      "        -0.29999999999998084, -2.6999999999999766, 16.199999999999925, 12.600000000000033,\n",
      "        6.299999999999981, 6.600000000000021, 9.600000000000014, 11.40000000000001,\n",
      "        18.900000000000016, 6.0000000000000195, 5.999999999999998, 27.299999999999912,\n",
      "        -18.9, -24.900000000000013, 7.800000000000024, 11.99999999999997, 14.69999999999998,\n",
      "        15.299999999999915, 12.90000000000002, 6.300000000000033, 11.099999999999957,\n",
      "        -3.299999999999981, 0.30000000000002225, -1.4999999999999756, 6.600000000000021,\n",
      "        25.199999999999967, 9.599999999999996, -10.499999999999979, 23.399999999999935,\n",
      "        9.000000000000014, 0.29999999999996163, 5.700000000000033, -8.099999999999977,\n",
      "        10.800000000000024, 18.599999999999973, 22.79999999999996, 11.70000000000003,\n",
      "        11.099999999999966, -1.1999999999999797, 16.49999999999997, 11.699999999999958,\n",
      "        3.0000000000000164, -5.399999999999981]\n",
      "      policy_policy1_reward: [13.5, 5.0, 10.0, 5.5, 22.5, 12.0, 27.5, 15.5, 2.5, -8.0,\n",
      "        0.5, 25.0, 0.0, 10.0, 12.0, 10.5, 8.0, 16.0, 17.0, 8.0, 16.0, 11.0, 16.0, 14.0,\n",
      "        15.0, 10.0, 19.5, 9.5, 13.5, 9.5, 0.5, 18.0, 12.5, 16.0, 23.5, 20.0, 16.0, 24.5,\n",
      "        14.5, 8.0, 10.0, 5.5, 23.5, 6.5, 7.0, 7.0, 10.0, 25.5, 9.5, -9.5, 30.5, 8.5,\n",
      "        14.0, 17.5, 14.0, 0.5, 5.0, 7.5, 4.0, 24.0, 21.5, 13.0, 10.0, 18.5, 17.0, 19.0,\n",
      "        10.5, 10.5, 34.0, -10.0, -16.0, 9.0, 16.5, 22.5, 22.0, 18.5, 13.0, 20.0, 4.5,\n",
      "        7.0, -2.5, 15.5, 27.5, 13.0, -0.5, 29.0, 13.5, 7.0, 13.5, -2.5, 12.0, 22.0,\n",
      "        29.5, 19.5, 20.0, 0.0, 21.0, 19.5, 13.0, 3.5]\n",
      "      policy_policy2_reward: [-7.799999999999984, -8.899999999999986, -6.6999999999999815,\n",
      "        -6.699999999999995, -7.799999999999988, -1.2000000000000048, -8.89999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -9.99999999999998, -5.6, -9.99999999999998,\n",
      "        -7.799999999999981, -9.99999999999998, -7.799999999999981, -7.79999999999999,\n",
      "        -5.599999999999999, -9.99999999999998, -5.599999999999982, -5.599999999999995,\n",
      "        -6.6999999999999815, -5.599999999999995, -6.699999999999986, -8.89999999999998,\n",
      "        -7.799999999999986, -6.6999999999999815, -7.79999999999999, -8.899999999999983,\n",
      "        -7.799999999999981, -5.599999999999995, -8.899999999999984, -4.500000000000002,\n",
      "        0.9999999999999983, -6.6999999999999815, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -5.5999999999999925, -9.99999999999998, -8.899999999999986,\n",
      "        -6.699999999999985, -3.399999999999988, -9.99999999999998, -5.5999999999999845,\n",
      "        -9.99999999999998, -0.1000000000000022, -9.99999999999998, -7.79999999999999,\n",
      "        -8.899999999999986, -9.99999999999998, -8.89999999999998, -6.699999999999995,\n",
      "        -8.89999999999998, -9.99999999999998, -5.599999999999998, -2.300000000000002,\n",
      "        -5.599999999999989, -7.799999999999989, -6.699999999999985, -7.799999999999981,\n",
      "        -8.89999999999998, -6.6999999999999815, -3.400000000000007, -8.89999999999998,\n",
      "        -5.599999999999982, -0.10000000000000331, -4.5000000000000036, -4.499999999999996,\n",
      "        -6.699999999999986, -8.89999999999998, -8.89999999999998, -1.2000000000000044,\n",
      "        -4.499999999999992, -7.799999999999982, -6.699999999999994, -5.599999999999988,\n",
      "        -6.699999999999995, -8.89999999999998, -7.79999999999999, -6.699999999999993,\n",
      "        0.9999999999999952, -8.89999999999998, -2.299999999999998, -3.400000000000006,\n",
      "        -9.99999999999998, -5.599999999999982, -4.500000000000003, -6.699999999999991,\n",
      "        -7.799999999999981, -5.599999999999982, -1.2000000000000008, -3.400000000000005,\n",
      "        -6.699999999999982, -7.799999999999981, -8.89999999999998, -1.2000000000000033,\n",
      "        -4.499999999999984, -7.799999999999981, -9.99999999999998, -8.899999999999986]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.0\n",
      "      policy2: 0.9999999999999983\n",
      "    policy_reward_mean:\n",
      "      policy1: 12.555\n",
      "      policy2: -6.776999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -16.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07633895811627323\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.024258523902248443\n",
      "      mean_inference_ms: 6.584691950244567\n",
      "      mean_raw_obs_processing_ms: 0.43076783466029395\n",
      "  time_since_restore: 580.2699806690216\n",
      "  time_this_iter_s: 65.84284973144531\n",
      "  time_total_s: 580.2699806690216\n",
      "  timers:\n",
      "    learn_throughput: 111.751\n",
      "    learn_time_ms: 35793.984\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 64464.136\n",
      "  timestamp: 1660562575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:22:55 (running for 00:10:30.32)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:01 (running for 00:10:35.46)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:06 (running for 00:10:40.52)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:11 (running for 00:10:45.60)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:16 (running for 00:10:50.66)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:21 (running for 00:10:55.74)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:26 (running for 00:11:00.80)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=9.944999999999977 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997CE2CA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FBF880>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFEFA0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997FFE790>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B9981DC8B0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     12 |          568.672 | 36000 |    4.479 |          10.31  |          -5.831 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     12 |          572.618 | 36000 |    9.945 |          16.645 |          -6.7   |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      8 |          532.521 | 32000 |    4.116 |          10.805 |          -6.689 |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_env_steps_sampled: 39000\n",
      "    num_env_steps_trained: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-23-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.199999999999918\n",
      "  episode_reward_mean: 5.478000000000004\n",
      "  episode_reward_min: -17.399999999999977\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 390\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9776996970176697\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014508717693388462\n",
      "          model: {}\n",
      "          policy_loss: -0.041464392095804214\n",
      "          total_loss: 6.852622032165527\n",
      "          vf_explained_var: 0.20166015625\n",
      "          vf_loss: 6.891183853149414\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0056703090667725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011127601377665997\n",
      "          model: {}\n",
      "          policy_loss: -0.03441828489303589\n",
      "          total_loss: 3.595674753189087\n",
      "          vf_explained_var: 0.06080793961882591\n",
      "          vf_loss: 3.6278672218322754\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_env_steps_sampled: 39000\n",
      "    num_env_steps_trained: 39000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 78000\n",
      "  num_agent_steps_trained: 78000\n",
      "  num_env_steps_sampled: 39000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 39000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.570000000000007\n",
      "    ram_util_percent: 64.35142857142856\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 36.0\n",
      "    policy2: 24.099999999999987\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.935\n",
      "    policy2: -5.4569999999999865\n",
      "  policy_reward_min:\n",
      "    policy1: -26.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0747544641206548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0249794776711164\n",
      "    mean_inference_ms: 6.57260456432175\n",
      "    mean_raw_obs_processing_ms: 0.4394891322360616\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.199999999999918\n",
      "    episode_reward_mean: 5.478000000000004\n",
      "    episode_reward_min: -17.399999999999977\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.9000000000000283, 2.373101715136272e-14, -1.1999999999999846,\n",
      "        -0.2999999999999853, 11.700000000000017, 8.400000000000025, -1.1999999999999922,\n",
      "        13.500000000000012, 14.100000000000028, -8.999999999999986, 6.000000000000012,\n",
      "        23.999999999999936, 1.5000000000000187, -17.399999999999977, -8.699999999999985,\n",
      "        -1.4999999999999805, 6.9000000000000306, 8.10000000000001, -3.8999999999999777,\n",
      "        12.299999999999928, 7.800000000000031, 7.500000000000032, 6.900000000000032,\n",
      "        1.2000000000000246, 14.699999999999994, 16.49999999999999, 15.600000000000023,\n",
      "        8.099999999999934, 8.700000000000019, 14.699999999999958, -6.299999999999977,\n",
      "        4.500000000000025, -1.4999999999999787, 28.199999999999918, -1.1999999999999762,\n",
      "        6.900000000000025, 11.100000000000009, -4.4999999999999805, -1.7999999999999785,\n",
      "        5.700000000000017, 12.599999999999989, 7.2000000000000135, -3.8999999999999777,\n",
      "        7.200000000000008, -1.1999999999999829, 7.800000000000031, -5.999999999999989,\n",
      "        7.800000000000027, 0.6000000000000119, 3.6000000000000214, 0.3000000000000227,\n",
      "        12.000000000000021, 14.999999999999932, 6.599999999999943, 17.100000000000016,\n",
      "        -5.099999999999985, -8.399999999999975, -1.4999999999999891, 5.100000000000023,\n",
      "        10.499999999999995, -1.4999999999999956, 3.90000000000003, 2.017830347256222e-14,\n",
      "        6.900000000000032, 3.000000000000009, 6.600000000000028, -2.4000000000000052,\n",
      "        18.299999999999983, -4.799999999999977, -2.9999999999999867, 1.5000000000000262,\n",
      "        6.600000000000021, -1.4999999999999942, 20.999999999999915, 4.800000000000015,\n",
      "        6.600000000000001, 9.000000000000032, 2.0999999999999965, -2.067790383364354e-14,\n",
      "        16.499999999999915, -0.2999999999999744, 9.899999999999999, 7.500000000000032,\n",
      "        7.2000000000000295, 7.499999999999966, -2.399999999999978, 7.500000000000023,\n",
      "        10.499999999999932, 6.000000000000025, 6.300000000000025, 10.199999999999974,\n",
      "        25.799999999999915, 19.199999999999974, 5.700000000000022, 13.19999999999997,\n",
      "        14.099999999999945, 0.9000000000000209, 4.500000000000007, -9.89999999999998,\n",
      "        13.500000000000025]\n",
      "      policy_policy1_reward: [4.0, 10.0, 5.5, -9.0, -2.5, 3.0, 5.5, 23.5, 17.5, -26.5,\n",
      "        16.0, 23.0, 11.5, -8.5, -2.0, 8.5, 7.0, 17.0, 5.0, 19.0, 14.5, 17.5, 12.5, 9.0,\n",
      "        6.0, 26.5, 24.5, 17.0, 16.5, 22.5, -4.0, 14.5, 8.5, 36.0, 5.5, 12.5, 14.5, 5.5,\n",
      "        0.5, 13.5, 16.0, 9.5, 5.0, 15.0, 5.5, 14.5, 4.0, 14.5, 4.0, 12.5, 7.0, 22.0,\n",
      "        25.0, 15.5, 9.5, 0.5, -5.0, 8.5, 14.0, 20.5, 8.5, 9.5, 10.0, 12.5, 13.0, 10.0,\n",
      "        6.5, 25.0, 3.0, 7.0, 11.5, -17.5, 8.5, 25.5, 11.5, 15.5, 13.5, 11.0, 4.5, 26.5,\n",
      "        2.0, 15.5, 17.5, 15.0, 17.5, 6.5, 17.5, 15.0, 16.0, 13.0, 18.0, 21.5, 10.5,\n",
      "        13.5, 21.0, 12.0, -10.0, 14.5, -1.0, 23.5]\n",
      "      policy_policy2_reward: [-0.0999999999999841, -9.99999999999998, -6.69999999999999,\n",
      "        8.700000000000024, 14.200000000000003, 5.4000000000000075, -6.69999999999999,\n",
      "        -9.99999999999998, -3.399999999999999, 17.49999999999998, -9.99999999999998,\n",
      "        0.999999999999998, -9.99999999999998, -8.899999999999986, -6.699999999999988,\n",
      "        -9.99999999999998, -0.09999999999998677, -8.89999999999998, -8.89999999999998,\n",
      "        -6.69999999999999, -6.69999999999999, -9.99999999999998, -5.599999999999999,\n",
      "        -7.799999999999981, 8.70000000000001, -9.99999999999998, -8.899999999999986,\n",
      "        -8.89999999999998, -7.799999999999981, -7.799999999999981, -2.299999999999999,\n",
      "        -9.99999999999998, -9.99999999999998, -7.799999999999986, -6.6999999999999815,\n",
      "        -5.6, -3.399999999999987, -9.99999999999998, -2.3000000000000016, -7.799999999999985,\n",
      "        -3.399999999999994, -2.299999999999995, -8.89999999999998, -7.799999999999989,\n",
      "        -6.699999999999982, -6.69999999999999, -9.99999999999998, -6.6999999999999815,\n",
      "        -3.399999999999993, -8.89999999999998, -6.6999999999999815, -9.99999999999998,\n",
      "        -9.99999999999998, -8.899999999999986, 7.600000000000014, -5.5999999999999925,\n",
      "        -3.399999999999983, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -5.599999999999991, -9.99999999999998, -5.599999999999989,\n",
      "        -9.99999999999998, -3.3999999999999857, -8.899999999999984, -6.699999999999984,\n",
      "        -7.799999999999985, -9.99999999999998, -9.99999999999998, 24.099999999999987,\n",
      "        -9.99999999999998, -4.499999999999989, -6.6999999999999815, -8.89999999999998,\n",
      "        -4.499999999999989, -8.899999999999986, -4.500000000000003, -9.99999999999998,\n",
      "        -2.300000000000004, -5.599999999999989, -9.99999999999998, -7.799999999999983,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998, -4.499999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -7.799999999999986, 4.300000000000014,\n",
      "        8.700000000000001, -7.799999999999981, -7.799999999999981, 2.1, 10.900000000000011,\n",
      "        -9.99999999999998, -8.89999999999998, -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 36.0\n",
      "      policy2: 24.099999999999987\n",
      "    policy_reward_mean:\n",
      "      policy1: 10.935\n",
      "      policy2: -5.4569999999999865\n",
      "    policy_reward_min:\n",
      "      policy1: -26.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0747544641206548\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0249794776711164\n",
      "      mean_inference_ms: 6.57260456432175\n",
      "      mean_raw_obs_processing_ms: 0.4394891322360616\n",
      "  time_since_restore: 618.2588021755219\n",
      "  time_this_iter_s: 49.58688020706177\n",
      "  time_total_s: 618.2588021755219\n",
      "  timers:\n",
      "    learn_throughput: 109.893\n",
      "    learn_time_ms: 27299.293\n",
      "    synch_weights_time_ms: 2.693\n",
      "    training_iteration_time_ms: 49666.711\n",
      "  timestamp: 1660562608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 13\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 78000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_env_steps_sampled: 39000\n",
      "    num_env_steps_trained: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-23-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.999999999999943\n",
      "  episode_reward_mean: 11.378999999999976\n",
      "  episode_reward_min: -4.799999999999988\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 390\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9164549708366394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019783692434430122\n",
      "          model: {}\n",
      "          policy_loss: -0.05752480402588844\n",
      "          total_loss: 6.687150001525879\n",
      "          vf_explained_var: 0.2483358532190323\n",
      "          vf_loss: 6.735772132873535\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8943202495574951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017425326630473137\n",
      "          model: {}\n",
      "          policy_loss: -0.0475042462348938\n",
      "          total_loss: 2.8256492614746094\n",
      "          vf_explained_var: 0.09494423121213913\n",
      "          vf_loss: 2.865311622619629\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_env_steps_sampled: 39000\n",
      "    num_env_steps_trained: 39000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 78000\n",
      "  num_agent_steps_trained: 78000\n",
      "  num_env_steps_sampled: 39000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 39000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.52676056338029\n",
      "    ram_util_percent: 64.35352112676054\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 34.5\n",
      "    policy2: 5.4000000000000075\n",
      "  policy_reward_mean:\n",
      "    policy1: 17.815\n",
      "    policy2: -6.4359999999999875\n",
      "  policy_reward_min:\n",
      "    policy1: 0.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07291062846146068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02369628284235494\n",
      "    mean_inference_ms: 6.663732017067055\n",
      "    mean_raw_obs_processing_ms: 0.43897775669869055\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.999999999999943\n",
      "    episode_reward_mean: 11.378999999999976\n",
      "    episode_reward_min: -4.799999999999988\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [9.000000000000032, 9.599999999999925, 16.799999999999933, 1.7999999999999936,\n",
      "        15.899999999999972, 19.499999999999915, 9.00000000000003, 7.500000000000007,\n",
      "        -0.5999999999999762, 15.600000000000017, 22.499999999999957, 15.000000000000012,\n",
      "        14.999999999999911, 7.19999999999996, 17.70000000000001, 29.999999999999943,\n",
      "        3.3000000000000114, 13.499999999999927, -3.299999999999975, 11.999999999999991,\n",
      "        8.700000000000028, 11.699999999999976, 8.099999999999998, 1.8000000000000167,\n",
      "        11.700000000000026, 20.099999999999916, 1.800000000000022, -2.9999999999999916,\n",
      "        4.500000000000027, 7.500000000000018, 15.599999999999998, 10.500000000000021,\n",
      "        11.999999999999984, 8.999999999999979, 7.799999999999974, 12.60000000000001,\n",
      "        11.399999999999933, 5.999999999999941, 1.662558979376172e-14, 7.199999999999934,\n",
      "        13.49999999999995, -1.7999999999999856, 21.599999999999948, 13.199999999999992,\n",
      "        2.4000000000000132, 0.5999999999999829, 12.900000000000027, 20.999999999999947,\n",
      "        11.099999999999962, 16.5, 3.6000000000000294, 1.79999999999999, 11.100000000000023,\n",
      "        20.999999999999908, 15.299999999999965, 4.4999999999999485, 23.3999999999999,\n",
      "        15.600000000000023, 9.89999999999995, -4.799999999999988, 20.999999999999915,\n",
      "        12.299999999999935, 24.899999999999977, 24.899999999999935, 6.899999999999942,\n",
      "        17.699999999999896, 17.999999999999922, 9.599999999999952, 6.899999999999997,\n",
      "        14.099999999999913, -2.999999999999989, 5.999999999999924, 8.700000000000014,\n",
      "        16.199999999999932, 18.299999999999898, 9.600000000000014, 14.699999999999923,\n",
      "        19.49999999999995, 7.800000000000027, 9.299999999999939, 4.500000000000007,\n",
      "        10.199999999999989, 16.499999999999908, 9.900000000000018, 14.699999999999898,\n",
      "        22.199999999999918, 22.199999999999925, 10.499999999999952, 8.699999999999962,\n",
      "        0.5999999999999989, 22.799999999999944, 7.799999999999956, 5.999999999999968,\n",
      "        18.899999999999928, 18.899999999999935, 15.000000000000021, 18.29999999999996,\n",
      "        5.999999999999998, 19.19999999999994, 7.1999999999999424]\n",
      "      policy_policy1_reward: [19.0, 13.0, 23.5, 8.5, 16.0, 29.5, 13.5, 12.0, 5.0, 24.5,\n",
      "        32.5, 19.5, 25.0, 15.0, 25.5, 34.5, 10.0, 23.5, 4.5, 22.0, 16.5, 14.0, 17.0,\n",
      "        3.0, 19.5, 29.0, 8.5, 7.0, 9.0, 17.5, 24.5, 15.0, 22.0, 19.0, 14.5, 16.0, 11.5,\n",
      "        16.0, 10.0, 9.5, 18.0, 0.5, 30.5, 21.0, 2.5, 4.0, 18.5, 31.0, 20.0, 26.5, 12.5,\n",
      "        8.5, 20.0, 25.5, 16.5, 14.5, 29.0, 24.5, 15.5, 3.0, 31.0, 19.0, 30.5, 30.5,\n",
      "        12.5, 25.5, 22.5, 18.5, 1.5, 23.0, 7.0, 10.5, 11.0, 24.0, 25.0, 18.5, 11.5,\n",
      "        24.0, 14.5, 16.0, 14.5, 18.0, 21.0, 15.5, 22.5, 30.0, 30.0, 15.0, 16.5, 9.5,\n",
      "        24.0, 14.5, 16.0, 19.0, 19.0, 19.5, 25.0, 16.0, 27.0, 15.0]\n",
      "      policy_policy2_reward: [-9.99999999999998, -3.4000000000000044, -6.6999999999999815,\n",
      "        -6.699999999999995, -0.10000000000000175, -9.99999999999998, -4.499999999999982,\n",
      "        -4.499999999999997, -5.599999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -4.499999999999997, -9.99999999999998, -7.799999999999983, -7.79999999999999,\n",
      "        -4.499999999999987, -6.6999999999999815, -9.99999999999998, -7.799999999999986,\n",
      "        -9.99999999999998, -7.799999999999989, -2.2999999999999896, -8.89999999999998,\n",
      "        -1.2000000000000035, -7.79999999999999, -8.89999999999998, -6.699999999999993,\n",
      "        -9.99999999999998, -4.5000000000000036, -9.99999999999998, -8.899999999999986,\n",
      "        -4.499999999999991, -9.99999999999998, -9.99999999999998, -6.699999999999986,\n",
      "        -3.400000000000005, -0.1000000000000042, -9.99999999999998, -9.99999999999998,\n",
      "        -2.2999999999999834, -4.499999999999991, -2.3000000000000043, -8.89999999999998,\n",
      "        -7.79999999999999, -0.09999999999998366, -3.400000000000005, -5.6, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -8.89999999999998, -6.699999999999992,\n",
      "        -8.89999999999998, -4.499999999999982, -1.2000000000000028, -9.99999999999998,\n",
      "        -5.599999999999999, -8.89999999999998, -5.599999999999982, -7.799999999999986,\n",
      "        -9.99999999999998, -6.6999999999999815, -5.599999999999997, -5.599999999999999,\n",
      "        -5.599999999999998, -7.799999999999981, -4.499999999999997, -8.899999999999986,\n",
      "        5.4000000000000075, -8.899999999999986, -9.99999999999998, -4.500000000000003,\n",
      "        -2.300000000000006, -7.799999999999989, -6.69999999999999, -8.89999999999998,\n",
      "        3.2000000000000064, -4.500000000000003, -6.699999999999994, -6.699999999999986,\n",
      "        -9.99999999999998, -7.799999999999983, -4.500000000000003, -5.599999999999996,\n",
      "        -7.799999999999981, -7.79999999999999, -7.799999999999986, -4.500000000000001,\n",
      "        -7.7999999999999865, -8.899999999999986, -1.2000000000000028, -6.699999999999995,\n",
      "        -9.99999999999998, -0.1000000000000042, -0.09999999999998968, -4.499999999999982,\n",
      "        -6.6999999999999815, -9.99999999999998, -7.799999999999987, -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.5\n",
      "      policy2: 5.4000000000000075\n",
      "    policy_reward_mean:\n",
      "      policy1: 17.815\n",
      "      policy2: -6.4359999999999875\n",
      "    policy_reward_min:\n",
      "      policy1: 0.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07291062846146068\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02369628284235494\n",
      "      mean_inference_ms: 6.663732017067055\n",
      "      mean_raw_obs_processing_ms: 0.43897775669869055\n",
      "  time_since_restore: 622.7309241294861\n",
      "  time_this_iter_s: 50.11247444152832\n",
      "  time_total_s: 622.7309241294861\n",
      "  timers:\n",
      "    learn_throughput: 109.417\n",
      "    learn_time_ms: 27418.075\n",
      "    synch_weights_time_ms: 2.892\n",
      "    training_iteration_time_ms: 49943.071\n",
      "  timestamp: 1660562610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 13\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 72000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.69999999999996\n",
      "  episode_reward_mean: 6.045000000000004\n",
      "  episode_reward_min: -15.29999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1122725009918213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0172404982149601\n",
      "          model: {}\n",
      "          policy_loss: -0.0489470548927784\n",
      "          total_loss: 6.856822490692139\n",
      "          vf_explained_var: 0.14220720529556274\n",
      "          vf_loss: 6.898011207580566\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.061815857887268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021276934072375298\n",
      "          model: {}\n",
      "          policy_loss: -0.049840740859508514\n",
      "          total_loss: 2.760955810546875\n",
      "          vf_explained_var: 0.1731623262166977\n",
      "          vf_loss: 2.804413318634033\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 36000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.212499999999995\n",
      "    ram_util_percent: 64.34895833333333\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 34.5\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 12.525\n",
      "    policy2: -6.479999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07341365194754987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0223165817291309\n",
      "    mean_inference_ms: 7.181760603985529\n",
      "    mean_raw_obs_processing_ms: 0.42696520888490447\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 26.69999999999996\n",
      "    episode_reward_mean: 6.045000000000004\n",
      "    episode_reward_min: -15.29999999999998\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [6.6000000000000245, 12.000000000000025, 3.000000000000001, 9.000000000000027,\n",
      "        13.799999999999931, -1.499999999999989, 10.799999999999992, 6.600000000000023,\n",
      "        3.9000000000000283, -9.899999999999979, 11.700000000000017, -6.900000000000004,\n",
      "        -2.6999999999999953, 3.90000000000003, -14.699999999999992, 2.095545958979983e-14,\n",
      "        14.700000000000019, 16.500000000000007, 10.500000000000028, -0.8999999999999961,\n",
      "        -7.49999999999999, -2.6999999999999873, 12.000000000000032, 3.299999999999976,\n",
      "        13.499999999999982, 0.5999999999999996, 10.199999999999994, 4.2000000000000295,\n",
      "        1.5000000000000067, -9.299999999999981, 7.500000000000032, 8.100000000000033,\n",
      "        15.00000000000003, 12.00000000000003, 12.600000000000025, 9.000000000000023,\n",
      "        5.100000000000017, -11.999999999999979, -15.29999999999998, 23.09999999999996,\n",
      "        18.299999999999947, 8.100000000000021, -8.999999999999973, 1.8000000000000185,\n",
      "        -12.899999999999993, -3.2999999999999847, 2.7000000000000086, 12.30000000000002,\n",
      "        12.300000000000017, 2.400000000000026, 6.300000000000033, 2.373101715136272e-14,\n",
      "        9.000000000000021, 18.900000000000006, -8.999999999999986, 14.10000000000003,\n",
      "        -5.699999999999977, -4.4999999999999964, -1.4999999999999933, 8.699999999999983,\n",
      "        23.4, 17.699999999999974, 17.09999999999996, 15.3, 9.599999999999985, 13.499999999999902,\n",
      "        13.500000000000012, 7.499999999999986, 16.19999999999998, -9.899999999999983,\n",
      "        4.800000000000029, 7.500000000000018, 21.59999999999996, 17.100000000000016,\n",
      "        -6.299999999999976, -5.999999999999989, 16.499999999999922, 8.700000000000012,\n",
      "        12.59999999999997, 9.000000000000018, 26.69999999999996, -2.399999999999981,\n",
      "        16.799999999999976, -7.499999999999983, -2.9999999999999853, 6.000000000000032,\n",
      "        3.5999999999999943, 7.19999999999996, 21.299999999999955, -3.899999999999991,\n",
      "        10.800000000000022, 1.8000000000000278, 11.999999999999911, 12.600000000000012,\n",
      "        21.59999999999996, -5.099999999999985, 4.200000000000027, 5.999999999999998,\n",
      "        3.90000000000003, 16.199999999999932]\n",
      "      policy_policy1_reward: [10.0, 22.0, 13.0, 19.0, 20.5, 8.5, 17.5, 10.0, 9.5, -1.0,\n",
      "        19.5, -3.5, 4.0, 9.5, -8.0, 4.5, 22.5, 26.5, 9.5, 2.5, 2.5, 4.0, 16.5, 4.5,\n",
      "        18.0, 9.5, 18.0, 12.0, 11.5, -1.5, 17.5, 17.0, 19.5, 22.0, 21.5, 8.0, 14.0,\n",
      "        -2.0, -18.5, 32.0, 19.5, 17.0, 1.0, -8.0, -4.0, 4.5, 10.5, 19.0, 19.0, 2.5,\n",
      "        13.0, 4.5, 19.0, 19.0, 1.0, 17.5, 1.0, 5.5, 3.0, 16.5, 18.0, 25.5, 20.5, 16.5,\n",
      "        18.5, 23.5, 23.5, 12.0, 24.0, -1.0, 6.0, 17.5, 25.0, 20.5, 1.5, -1.5, 21.0,\n",
      "        16.5, 21.5, 19.0, 34.5, 6.5, 23.5, -3.0, 7.0, 16.0, 12.5, 15.0, 28.0, 5.0, 17.5,\n",
      "        8.5, 22.0, 21.5, 30.5, 0.5, 12.0, 10.5, 9.5, 24.0]\n",
      "      policy_policy2_reward: [-3.4000000000000026, -9.99999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, -6.6999999999999815, -9.99999999999998, -6.699999999999995,\n",
      "        -3.4000000000000035, -5.5999999999999845, -8.89999999999998, -7.799999999999981,\n",
      "        -3.4000000000000057, -6.699999999999989, -5.599999999999991, -6.699999999999995,\n",
      "        -4.5000000000000036, -7.7999999999999865, -9.99999999999998, 0.9999999999999957,\n",
      "        -3.400000000000002, -9.99999999999998, -6.699999999999995, -4.4999999999999885,\n",
      "        -1.200000000000004, -4.499999999999997, -8.89999999999998, -7.79999999999999,\n",
      "        -7.79999999999999, -9.99999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999986, -4.500000000000001, -9.99999999999998, -8.899999999999986,\n",
      "        1.0000000000000004, -8.89999999999998, -9.99999999999998, 3.2000000000000046,\n",
      "        -8.89999999999998, -1.2000000000000048, -8.899999999999983, -9.99999999999998,\n",
      "        9.800000000000006, -8.89999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -6.6999999999999815, -6.6999999999999815, -0.09999999999998954, -6.69999999999999,\n",
      "        -4.499999999999988, -9.99999999999998, -0.09999999999999593, -9.99999999999998,\n",
      "        -3.3999999999999986, -6.6999999999999815, -9.99999999999998, -4.5, -7.799999999999981,\n",
      "        5.4000000000000075, -7.79999999999999, -3.400000000000003, -1.2000000000000033,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999996,\n",
      "        -7.799999999999986, -8.89999999999998, -1.1999999999999993, -9.99999999999998,\n",
      "        -3.399999999999999, -3.3999999999999906, -7.799999999999981, -4.500000000000002,\n",
      "        -4.499999999999984, -7.799999999999987, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -6.6999999999999815, -4.5, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999989, -6.6999999999999895,\n",
      "        -8.89999999999998, -6.699999999999992, -6.699999999999994, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -5.599999999999982, -7.79999999999999,\n",
      "        -4.499999999999982, -5.5999999999999925, -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.5\n",
      "      policy2: 9.800000000000006\n",
      "    policy_reward_mean:\n",
      "      policy1: 12.525\n",
      "      policy2: -6.479999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -18.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07341365194754987\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0223165817291309\n",
      "      mean_inference_ms: 7.181760603985529\n",
      "      mean_raw_obs_processing_ms: 0.42696520888490447\n",
      "  time_since_restore: 599.8061168193817\n",
      "  time_this_iter_s: 67.28553009033203\n",
      "  time_total_s: 599.8061168193817\n",
      "  timers:\n",
      "    learn_throughput: 111.91\n",
      "    learn_time_ms: 35743.053\n",
      "    synch_weights_time_ms: 2.881\n",
      "    training_iteration_time_ms: 66639.362\n",
      "  timestamp: 1660562611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:31 (running for 00:11:05.96)\n",
      "Memory usage on this node: 20.6/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:36 (running for 00:11:11.03)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:41 (running for 00:11:16.16)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:46 (running for 00:11:21.21)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:51 (running for 00:11:26.29)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:23:56 (running for 00:11:31.33)\n",
      "Memory usage on this node: 20.5/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status   | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING  | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING  | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00002 | RUNNING  | 127.0.0.1:16440 | 5e-05  |               4000 |      9 |          580.27  | 36000 |    5.778 |          12.555 |          -6.777 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING  | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "+---------------------------------+----------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00002:\n",
      "  agent_timesteps_total: 80000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-24-01\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.299999999999912\n",
      "  episode_reward_mean: 6.677999999999998\n",
      "  episode_reward_min: -24.900000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 670766f5e4c14dc4be6edb5c79fc72a7\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0858920812606812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012725154869258404\n",
      "          model: {}\n",
      "          policy_loss: -0.03497401624917984\n",
      "          total_loss: 7.046767234802246\n",
      "          vf_explained_var: 0.1539279967546463\n",
      "          vf_loss: 7.079195976257324\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.080656886100769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013625772669911385\n",
      "          model: {}\n",
      "          policy_loss: -0.037411514669656754\n",
      "          total_loss: 2.613356590270996\n",
      "          vf_explained_var: 0.1332307606935501\n",
      "          vf_loss: 2.648043394088745\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 40000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 31.712903225806453\n",
      "    ram_util_percent: 64.3548387096774\n",
      "  pid: 16440\n",
      "  policy_reward_max:\n",
      "    policy1: 34.0\n",
      "    policy2: 6.5000000000000195\n",
      "  policy_reward_mean:\n",
      "    policy1: 12.905\n",
      "    policy2: -6.226999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.075833742497538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02402243439206358\n",
      "    mean_inference_ms: 6.631631804653052\n",
      "    mean_raw_obs_processing_ms: 0.4303197056626838\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 27.299999999999912\n",
      "    episode_reward_mean: 6.677999999999998\n",
      "    episode_reward_min: -24.900000000000013\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [3.300000000000015, 2.099999999999998, 13.499999999999952, 0.9000000000000127,\n",
      "        -2.999999999999976, 6.900000000000025, 1.740274591099933e-14, 17.700000000000017,\n",
      "        0.6000000000000113, -19.499999999999986, 21.59999999999996, 1.8000000000000238,\n",
      "        5.100000000000016, 7.5000000000000195, 8.400000000000025, -1.799999999999978,\n",
      "        -0.6000000000000051, -0.29999999999998084, -2.6999999999999766, 16.199999999999925,\n",
      "        12.600000000000033, 6.299999999999981, 6.600000000000021, 9.600000000000014,\n",
      "        11.40000000000001, 18.900000000000016, 6.0000000000000195, 5.999999999999998,\n",
      "        27.299999999999912, -18.9, -24.900000000000013, 7.800000000000024, 11.99999999999997,\n",
      "        14.69999999999998, 15.299999999999915, 12.90000000000002, 6.300000000000033,\n",
      "        11.099999999999957, -3.299999999999981, 0.30000000000002225, -1.4999999999999756,\n",
      "        6.600000000000021, 25.199999999999967, 9.599999999999996, -10.499999999999979,\n",
      "        23.399999999999935, 9.000000000000014, 0.29999999999996163, 5.700000000000033,\n",
      "        -8.099999999999977, 10.800000000000024, 18.599999999999973, 22.79999999999996,\n",
      "        11.70000000000003, 11.099999999999966, -1.1999999999999797, 16.49999999999997,\n",
      "        11.699999999999958, 3.0000000000000164, -5.399999999999981, -4.500000000000039,\n",
      "        8.700000000000014, 16.199999999999953, 2.700000000000007, 2.70000000000003,\n",
      "        0.9000000000000105, 9.000000000000018, -0.8999999999999758, 9.299999999999926,\n",
      "        -1.7999999999999745, 11.700000000000012, 10.49999999999996, -10.499999999999979,\n",
      "        -3.8999999999999777, 21.3, 0.6000000000000233, 13.19999999999994, 16.79999999999997,\n",
      "        4.800000000000008, 17.099999999999937, -7.499999999999996, 8.700000000000012,\n",
      "        3.0000000000000298, 2.100000000000021, 10.500000000000025, 15.59999999999994,\n",
      "        6.900000000000013, 2.40000000000002, 16.499999999999964, 10.200000000000005,\n",
      "        -4.1999999999999815, -0.8999999999999782, 20.999999999999975, 16.79999999999997,\n",
      "        5.10000000000001, 21.899999999999906, 12.600000000000017, 7.800000000000022,\n",
      "        10.79999999999999, 9.599999999999998]\n",
      "      policy_policy1_reward: [10.0, 5.5, 23.5, 6.5, 7.0, 7.0, 10.0, 25.5, 9.5, -9.5,\n",
      "        30.5, 8.5, 14.0, 17.5, 14.0, 0.5, 5.0, 7.5, 4.0, 24.0, 21.5, 13.0, 10.0, 18.5,\n",
      "        17.0, 19.0, 10.5, 10.5, 34.0, -10.0, -16.0, 9.0, 16.5, 22.5, 22.0, 18.5, 13.0,\n",
      "        20.0, 4.5, 7.0, -2.5, 15.5, 27.5, 13.0, -0.5, 29.0, 13.5, 7.0, 13.5, -2.5, 12.0,\n",
      "        22.0, 29.5, 19.5, 20.0, 0.0, 21.0, 19.5, 13.0, 3.5, 5.5, 16.5, 24.0, 10.5, 5.0,\n",
      "        6.5, 13.5, 8.0, 16.0, 6.0, 19.5, 15.0, -0.5, 5.0, 28.0, -1.5, 21.0, 23.5, 11.5,\n",
      "        20.5, -3.0, 16.5, 13.0, 11.0, 20.5, 24.5, 12.5, 2.5, 10.0, 18.0, 2.5, 2.5, 25.5,\n",
      "        23.5, 14.0, 27.5, 16.0, 14.5, 17.5, 13.0]\n",
      "      policy_policy2_reward: [-6.699999999999985, -3.399999999999988, -9.99999999999998,\n",
      "        -5.5999999999999845, -9.99999999999998, -0.1000000000000022, -9.99999999999998,\n",
      "        -7.79999999999999, -8.899999999999986, -9.99999999999998, -8.89999999999998,\n",
      "        -6.699999999999995, -8.89999999999998, -9.99999999999998, -5.599999999999998,\n",
      "        -2.300000000000002, -5.599999999999989, -7.799999999999989, -6.699999999999985,\n",
      "        -7.799999999999981, -8.89999999999998, -6.6999999999999815, -3.400000000000007,\n",
      "        -8.89999999999998, -5.599999999999982, -0.10000000000000331, -4.5000000000000036,\n",
      "        -4.499999999999996, -6.699999999999986, -8.89999999999998, -8.89999999999998,\n",
      "        -1.2000000000000044, -4.499999999999992, -7.799999999999982, -6.699999999999994,\n",
      "        -5.599999999999988, -6.699999999999995, -8.89999999999998, -7.79999999999999,\n",
      "        -6.699999999999993, 0.9999999999999952, -8.89999999999998, -2.299999999999998,\n",
      "        -3.400000000000006, -9.99999999999998, -5.599999999999982, -4.500000000000003,\n",
      "        -6.699999999999991, -7.799999999999981, -5.599999999999982, -1.2000000000000008,\n",
      "        -3.400000000000005, -6.699999999999982, -7.799999999999981, -8.89999999999998,\n",
      "        -1.2000000000000033, -4.499999999999984, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999986, -9.99999999999998, -7.799999999999981, -7.7999999999999865,\n",
      "        -7.799999999999989, -2.2999999999999963, -5.599999999999991, -4.499999999999988,\n",
      "        -8.89999999999998, -6.6999999999999815, -7.799999999999984, -7.799999999999981,\n",
      "        -4.500000000000002, -9.99999999999998, -8.89999999999998, -6.6999999999999815,\n",
      "        2.099999999999994, -7.799999999999988, -6.699999999999993, -6.6999999999999815,\n",
      "        -3.4000000000000035, -4.499999999999983, -7.799999999999981, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -8.899999999999986, -5.599999999999982,\n",
      "        -0.10000000000000508, 6.5000000000000195, -7.79999999999999, -6.699999999999986,\n",
      "        -3.4000000000000026, -4.5, -6.699999999999985, -8.89999999999998, -5.59999999999999,\n",
      "        -3.400000000000005, -6.699999999999986, -6.69999999999999, -3.400000000000004]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 34.0\n",
      "      policy2: 6.5000000000000195\n",
      "    policy_reward_mean:\n",
      "      policy1: 12.905\n",
      "      policy2: -6.226999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -16.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.075833742497538\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.02402243439206358\n",
      "      mean_inference_ms: 6.631631804653052\n",
      "      mean_raw_obs_processing_ms: 0.4303197056626838\n",
      "  time_since_restore: 645.5239465236664\n",
      "  time_this_iter_s: 65.25396585464478\n",
      "  time_total_s: 645.5239465236664\n",
      "  timers:\n",
      "    learn_throughput: 112.043\n",
      "    learn_time_ms: 35700.659\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 64542.421\n",
      "  timestamp: 1660562641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: '24664_00002'\n",
      "  warmup_time: 9.97730803489685\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:24:06 (running for 00:11:41.42)\n",
      "Memory usage on this node: 17.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status     | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING    | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING    | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING    | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "| PPO_MultiAgentArena_24664_00002 | TERMINATED | 127.0.0.1:16440 | 5e-05  |               4000 |     10 |          645.524 | 40000 |    6.678 |          12.905 |          -6.227 |\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:24:12 (running for 00:11:46.47)\n",
      "Memory usage on this node: 17.8/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=11.378999999999976 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99806F400>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997935E20>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809BA30>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99809B820>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B998202C10>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status     | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | RUNNING    | 127.0.0.1:16724 | 5e-05  |               3000 |     13 |          618.259 | 39000 |    5.478 |          10.935 |          -5.457 |\n",
      "| PPO_MultiAgentArena_24664_00001 | RUNNING    | 127.0.0.1:5700  | 0.0001 |               3000 |     13 |          622.731 | 39000 |   11.379 |          17.815 |          -6.436 |\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING    | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "| PPO_MultiAgentArena_24664_00002 | TERMINATED | 127.0.0.1:16440 | 5e-05  |               4000 |     10 |          645.524 | 40000 |    6.678 |          12.905 |          -6.227 |\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_env_steps_sampled: 42000\n",
      "    num_env_steps_trained: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-24-14\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.399999999999945\n",
      "  episode_reward_mean: 6.027\n",
      "  episode_reward_min: -9.89999999999998\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 420\n",
      "  experiment_id: d38982ae9cb848679d68d1a6d81645dd\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9599660038948059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012219642288982868\n",
      "          model: {}\n",
      "          policy_loss: -0.035136282444000244\n",
      "          total_loss: 6.829186916351318\n",
      "          vf_explained_var: 0.20698653161525726\n",
      "          vf_loss: 6.861879348754883\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.955750048160553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013202727772295475\n",
      "          model: {}\n",
      "          policy_loss: -0.03658846393227577\n",
      "          total_loss: 2.4863176345825195\n",
      "          vf_explained_var: 0.1992047131061554\n",
      "          vf_loss: 2.520265579223633\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_env_steps_sampled: 42000\n",
      "    num_env_steps_trained: 42000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 42000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 42000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.51076923076923\n",
      "    ram_util_percent: 62.01230769230769\n",
      "  pid: 16724\n",
      "  policy_reward_max:\n",
      "    policy1: 36.0\n",
      "    policy2: 24.099999999999987\n",
      "  policy_reward_mean:\n",
      "    policy1: 12.485\n",
      "    policy2: -6.457999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -17.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0749497638917913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.025177432667447635\n",
      "    mean_inference_ms: 6.599647187735043\n",
      "    mean_raw_obs_processing_ms: 0.43975289044302807\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.399999999999945\n",
      "    episode_reward_mean: 6.027\n",
      "    episode_reward_min: -9.89999999999998\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [-6.299999999999977, 4.500000000000025, -1.4999999999999787, 28.199999999999918,\n",
      "        -1.1999999999999762, 6.900000000000025, 11.100000000000009, -4.4999999999999805,\n",
      "        -1.7999999999999785, 5.700000000000017, 12.599999999999989, 7.2000000000000135,\n",
      "        -3.8999999999999777, 7.200000000000008, -1.1999999999999829, 7.800000000000031,\n",
      "        -5.999999999999989, 7.800000000000027, 0.6000000000000119, 3.6000000000000214,\n",
      "        0.3000000000000227, 12.000000000000021, 14.999999999999932, 6.599999999999943,\n",
      "        17.100000000000016, -5.099999999999985, -8.399999999999975, -1.4999999999999891,\n",
      "        5.100000000000023, 10.499999999999995, -1.4999999999999956, 3.90000000000003,\n",
      "        2.017830347256222e-14, 6.900000000000032, 3.000000000000009, 6.600000000000028,\n",
      "        -2.4000000000000052, 18.299999999999983, -4.799999999999977, -2.9999999999999867,\n",
      "        1.5000000000000262, 6.600000000000021, -1.4999999999999942, 20.999999999999915,\n",
      "        4.800000000000015, 6.600000000000001, 9.000000000000032, 2.0999999999999965,\n",
      "        -2.067790383364354e-14, 16.499999999999915, -0.2999999999999744, 9.899999999999999,\n",
      "        7.500000000000032, 7.2000000000000295, 7.499999999999966, -2.399999999999978,\n",
      "        7.500000000000023, 10.499999999999932, 6.000000000000025, 6.300000000000025,\n",
      "        10.199999999999974, 25.799999999999915, 19.199999999999974, 5.700000000000022,\n",
      "        13.19999999999997, 14.099999999999945, 0.9000000000000209, 4.500000000000007,\n",
      "        -9.89999999999998, 13.500000000000025, 13.500000000000025, -0.8999999999999733,\n",
      "        1.1999999999999527, 6.0, 7.800000000000022, 15.899999999999915, 10.799999999999997,\n",
      "        9.000000000000018, -3.0253577421035516e-15, -4.500000000000009, 20.999999999999936,\n",
      "        16.499999999999975, 8.400000000000015, 2.9999999999999987, -0.8999999999999994,\n",
      "        1.5000000000000298, 16.799999999999933, 8.399999999999967, 11.999999999999977,\n",
      "        6.300000000000011, 3.300000000000021, 6.000000000000026, -5.999999999999987,\n",
      "        1.2000000000000255, 17.700000000000003, 16.19999999999991, 1.499999999999989,\n",
      "        29.399999999999945, 1.4155343563970746e-15, 2.7000000000000273]\n",
      "      policy_policy1_reward: [-4.0, 14.5, 8.5, 36.0, 5.5, 12.5, 14.5, 5.5, 0.5, 13.5,\n",
      "        16.0, 9.5, 5.0, 15.0, 5.5, 14.5, 4.0, 14.5, 4.0, 12.5, 7.0, 22.0, 25.0, 15.5,\n",
      "        9.5, 0.5, -5.0, 8.5, 14.0, 20.5, 8.5, 9.5, 10.0, 12.5, 13.0, 10.0, 6.5, 25.0,\n",
      "        3.0, 7.0, 11.5, -17.5, 8.5, 25.5, 11.5, 15.5, 13.5, 11.0, 4.5, 26.5, 2.0, 15.5,\n",
      "        17.5, 15.0, 17.5, 6.5, 17.5, 15.0, 16.0, 13.0, 18.0, 21.5, 10.5, 13.5, 21.0,\n",
      "        12.0, -10.0, 14.5, -1.0, 23.5, 23.5, 8.0, 9.0, 16.0, 14.5, 21.5, 1.0, 19.0,\n",
      "        10.0, 5.5, 31.0, 26.5, 14.0, 13.0, 8.0, 11.5, 23.5, 14.0, 22.0, 13.0, 10.0,\n",
      "        16.0, 4.0, 9.0, 25.5, 24.0, 11.5, 24.0, 10.0, 10.5]\n",
      "      policy_policy2_reward: [-2.299999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        -7.799999999999986, -6.6999999999999815, -5.6, -3.399999999999987, -9.99999999999998,\n",
      "        -2.3000000000000016, -7.799999999999985, -3.399999999999994, -2.299999999999995,\n",
      "        -8.89999999999998, -7.799999999999989, -6.699999999999982, -6.69999999999999,\n",
      "        -9.99999999999998, -6.6999999999999815, -3.399999999999993, -8.89999999999998,\n",
      "        -6.6999999999999815, -9.99999999999998, -9.99999999999998, -8.899999999999986,\n",
      "        7.600000000000014, -5.5999999999999925, -3.399999999999983, -9.99999999999998,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -5.599999999999991,\n",
      "        -9.99999999999998, -5.599999999999989, -9.99999999999998, -3.3999999999999857,\n",
      "        -8.899999999999984, -6.699999999999984, -7.799999999999985, -9.99999999999998,\n",
      "        -9.99999999999998, 24.099999999999987, -9.99999999999998, -4.499999999999989,\n",
      "        -6.6999999999999815, -8.89999999999998, -4.499999999999989, -8.899999999999986,\n",
      "        -4.500000000000003, -9.99999999999998, -2.300000000000004, -5.599999999999989,\n",
      "        -9.99999999999998, -7.799999999999983, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -4.499999999999998, -9.99999999999998, -6.6999999999999815,\n",
      "        -7.799999999999986, 4.300000000000014, 8.700000000000001, -7.799999999999981,\n",
      "        -7.799999999999981, 2.1, 10.900000000000011, -9.99999999999998, -8.89999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -8.89999999999998, -7.799999999999989,\n",
      "        -9.99999999999998, -6.699999999999985, -5.6, 9.800000000000006, -9.99999999999998,\n",
      "        -9.99999999999998, -9.99999999999998, -9.99999999999998, -9.99999999999998,\n",
      "        -5.599999999999995, -9.99999999999998, -8.89999999999998, -9.99999999999998,\n",
      "        -6.6999999999999815, -5.5999999999999845, -9.99999999999998, -6.699999999999995,\n",
      "        -6.699999999999992, -9.99999999999998, -9.99999999999998, -7.799999999999981,\n",
      "        -7.799999999999981, -7.79999999999999, -9.99999999999998, 5.400000000000013,\n",
      "        -9.99999999999998, -7.799999999999981]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 36.0\n",
      "      policy2: 24.099999999999987\n",
      "    policy_reward_mean:\n",
      "      policy1: 12.485\n",
      "      policy2: -6.457999999999987\n",
      "    policy_reward_min:\n",
      "      policy1: -17.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0749497638917913\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.025177432667447635\n",
      "      mean_inference_ms: 6.599647187735043\n",
      "      mean_raw_obs_processing_ms: 0.43975289044302807\n",
      "  time_since_restore: 664.0971751213074\n",
      "  time_this_iter_s: 45.83837294578552\n",
      "  time_total_s: 664.0971751213074\n",
      "  timers:\n",
      "    learn_throughput: 111.482\n",
      "    learn_time_ms: 26910.184\n",
      "    synch_weights_time_ms: 2.593\n",
      "    training_iteration_time_ms: 49272.216\n",
      "  timestamp: 1660562654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 14\n",
      "  trial_id: '24664_00000'\n",
      "  warmup_time: 10.685415029525757\n",
      "  \n",
      "Result for PPO_MultiAgentArena_24664_00001:\n",
      "  agent_timesteps_total: 84000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_env_steps_sampled: 42000\n",
      "    num_env_steps_trained: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-24-15\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.899999999999906\n",
      "  episode_reward_mean: 12.185999999999968\n",
      "  episode_reward_min: -4.799999999999988\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 420\n",
      "  experiment_id: 348f8722bd79424ba6b398253001d82b\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.892426073551178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018450619652867317\n",
      "          model: {}\n",
      "          policy_loss: -0.05245798081159592\n",
      "          total_loss: 7.003901958465576\n",
      "          vf_explained_var: 0.16816192865371704\n",
      "          vf_loss: 7.0480570793151855\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8597659468650818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017520450055599213\n",
      "          model: {}\n",
      "          policy_loss: -0.04902741685509682\n",
      "          total_loss: 2.7394473552703857\n",
      "          vf_explained_var: 0.06624382734298706\n",
      "          vf_loss: 2.780590534210205\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_env_steps_sampled: 42000\n",
      "    num_env_steps_trained: 42000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 42000\n",
      "  num_env_steps_sampled_this_iter: 3000\n",
      "  num_env_steps_trained: 42000\n",
      "  num_env_steps_trained_this_iter: 3000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 3000\n",
      "  perf:\n",
      "    cpu_util_percent: 30.3359375\n",
      "    ram_util_percent: 61.703125\n",
      "  pid: 5700\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 6.500000000000023\n",
      "  policy_reward_mean:\n",
      "    policy1: 18.215\n",
      "    policy2: -6.028999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -1.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07304315691460792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.023754165048201967\n",
      "    mean_inference_ms: 6.688991635097284\n",
      "    mean_raw_obs_processing_ms: 0.4389140810118894\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 27.899999999999906\n",
      "    episode_reward_mean: 12.185999999999968\n",
      "    episode_reward_min: -4.799999999999988\n",
      "    episodes_this_iter: 30\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [15.599999999999998, 10.500000000000021, 11.999999999999984, 8.999999999999979,\n",
      "        7.799999999999974, 12.60000000000001, 11.399999999999933, 5.999999999999941,\n",
      "        1.662558979376172e-14, 7.199999999999934, 13.49999999999995, -1.7999999999999856,\n",
      "        21.599999999999948, 13.199999999999992, 2.4000000000000132, 0.5999999999999829,\n",
      "        12.900000000000027, 20.999999999999947, 11.099999999999962, 16.5, 3.6000000000000294,\n",
      "        1.79999999999999, 11.100000000000023, 20.999999999999908, 15.299999999999965,\n",
      "        4.4999999999999485, 23.3999999999999, 15.600000000000023, 9.89999999999995,\n",
      "        -4.799999999999988, 20.999999999999915, 12.299999999999935, 24.899999999999977,\n",
      "        24.899999999999935, 6.899999999999942, 17.699999999999896, 17.999999999999922,\n",
      "        9.599999999999952, 6.899999999999997, 14.099999999999913, -2.999999999999989,\n",
      "        5.999999999999924, 8.700000000000014, 16.199999999999932, 18.299999999999898,\n",
      "        9.600000000000014, 14.699999999999923, 19.49999999999995, 7.800000000000027,\n",
      "        9.299999999999939, 4.500000000000007, 10.199999999999989, 16.499999999999908,\n",
      "        9.900000000000018, 14.699999999999898, 22.199999999999918, 22.199999999999925,\n",
      "        10.499999999999952, 8.699999999999962, 0.5999999999999989, 22.799999999999944,\n",
      "        7.799999999999956, 5.999999999999968, 18.899999999999928, 18.899999999999935,\n",
      "        15.000000000000021, 18.29999999999996, 5.999999999999998, 19.19999999999994,\n",
      "        7.1999999999999424, 11.999999999999943, 22.199999999999925, 13.500000000000027,\n",
      "        20.699999999999903, 16.49999999999998, 15.599999999999985, 7.199999999999957,\n",
      "        26.39999999999992, 6.9000000000000234, 11.399999999999928, 27.899999999999906,\n",
      "        9.300000000000022, 18.599999999999955, -2.099999999999982, -4.499999999999982,\n",
      "        14.999999999999911, 22.4999999999999, 7.799999999999997, 3.300000000000023,\n",
      "        7.199999999999953, 16.799999999999997, -0.5999999999999774, 26.99999999999993,\n",
      "        22.49999999999996, 22.49999999999993, 8.99999999999997, 4.200000000000001, 8.70000000000003,\n",
      "        13.499999999999924, 9.600000000000003]\n",
      "      policy_policy1_reward: [24.5, 15.0, 22.0, 19.0, 14.5, 16.0, 11.5, 16.0, 10.0,\n",
      "        9.5, 18.0, 0.5, 30.5, 21.0, 2.5, 4.0, 18.5, 31.0, 20.0, 26.5, 12.5, 8.5, 20.0,\n",
      "        25.5, 16.5, 14.5, 29.0, 24.5, 15.5, 3.0, 31.0, 19.0, 30.5, 30.5, 12.5, 25.5,\n",
      "        22.5, 18.5, 1.5, 23.0, 7.0, 10.5, 11.0, 24.0, 25.0, 18.5, 11.5, 24.0, 14.5,\n",
      "        16.0, 14.5, 18.0, 21.0, 15.5, 22.5, 30.0, 30.0, 15.0, 16.5, 9.5, 24.0, 14.5,\n",
      "        16.0, 19.0, 19.0, 19.5, 25.0, 16.0, 27.0, 15.0, 22.0, 30.0, 12.5, 28.5, 26.5,\n",
      "        24.5, 15.0, 21.0, 12.5, 17.0, 33.5, 16.0, 27.5, 3.5, 0.0, 19.5, 32.5, 14.5,\n",
      "        -1.0, 15.0, 23.5, 5.0, 37.0, 16.0, 27.0, 13.5, 6.5, 16.5, 23.5, 18.5]\n",
      "      policy_policy2_reward: [-8.899999999999986, -4.499999999999991, -9.99999999999998,\n",
      "        -9.99999999999998, -6.699999999999986, -3.400000000000005, -0.1000000000000042,\n",
      "        -9.99999999999998, -9.99999999999998, -2.2999999999999834, -4.499999999999991,\n",
      "        -2.3000000000000043, -8.89999999999998, -7.79999999999999, -0.09999999999998366,\n",
      "        -3.400000000000005, -5.6, -9.99999999999998, -8.899999999999986, -9.99999999999998,\n",
      "        -8.89999999999998, -6.699999999999992, -8.89999999999998, -4.499999999999982,\n",
      "        -1.2000000000000028, -9.99999999999998, -5.599999999999999, -8.89999999999998,\n",
      "        -5.599999999999982, -7.799999999999986, -9.99999999999998, -6.6999999999999815,\n",
      "        -5.599999999999997, -5.599999999999999, -5.599999999999998, -7.799999999999981,\n",
      "        -4.499999999999997, -8.899999999999986, 5.4000000000000075, -8.899999999999986,\n",
      "        -9.99999999999998, -4.500000000000003, -2.300000000000006, -7.799999999999989,\n",
      "        -6.69999999999999, -8.89999999999998, 3.2000000000000064, -4.500000000000003,\n",
      "        -6.699999999999994, -6.699999999999986, -9.99999999999998, -7.799999999999983,\n",
      "        -4.500000000000003, -5.599999999999996, -7.799999999999981, -7.79999999999999,\n",
      "        -7.799999999999986, -4.500000000000001, -7.7999999999999865, -8.899999999999986,\n",
      "        -1.2000000000000028, -6.699999999999995, -9.99999999999998, -0.1000000000000042,\n",
      "        -0.09999999999998968, -4.499999999999982, -6.6999999999999815, -9.99999999999998,\n",
      "        -7.799999999999987, -7.799999999999981, -9.99999999999998, -7.799999999999981,\n",
      "        1.0000000000000107, -7.799999999999981, -9.99999999999998, -8.899999999999984,\n",
      "        -7.799999999999989, 5.400000000000015, -5.599999999999991, -5.599999999999984,\n",
      "        -5.59999999999999, -6.6999999999999815, -8.89999999999998, -5.599999999999998,\n",
      "        -4.500000000000002, -4.5, -9.99999999999998, -6.699999999999982, 4.299999999999997,\n",
      "        -7.799999999999985, -6.69999999999999, -5.599999999999986, -9.99999999999998,\n",
      "        6.500000000000023, -4.499999999999999, -4.499999999999996, -2.3000000000000047,\n",
      "        -7.799999999999986, -9.99999999999998, -8.89999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 37.0\n",
      "      policy2: 6.500000000000023\n",
      "    policy_reward_mean:\n",
      "      policy1: 18.215\n",
      "      policy2: -6.028999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -1.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07304315691460792\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.023754165048201967\n",
      "      mean_inference_ms: 6.688991635097284\n",
      "      mean_raw_obs_processing_ms: 0.4389140810118894\n",
      "  time_since_restore: 667.6238253116608\n",
      "  time_this_iter_s: 44.89290118217468\n",
      "  time_total_s: 667.6238253116608\n",
      "  timers:\n",
      "    learn_throughput: 111.533\n",
      "    learn_time_ms: 26897.817\n",
      "    synch_weights_time_ms: 2.992\n",
      "    training_iteration_time_ms: 49408.551\n",
      "  timestamp: 1660562655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 14\n",
      "  trial_id: '24664_00001'\n",
      "  warmup_time: 10.203230619430542\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:24:21 (running for 00:11:55.72)\n",
      "Memory usage on this node: 12.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=12.185999999999968 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E0A2E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805E3D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E201F0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EE34C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E1DA30>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E201F0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status     | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00003 | RUNNING    | 127.0.0.1:7072  | 0.0001 |               4000 |      9 |          599.806 | 36000 |    6.045 |          12.525 |          -6.48  |\n",
      "| PPO_MultiAgentArena_24664_00000 | TERMINATED | 127.0.0.1:16724 | 5e-05  |               3000 |     14 |          664.097 | 42000 |    6.027 |          12.485 |          -6.458 |\n",
      "| PPO_MultiAgentArena_24664_00001 | TERMINATED | 127.0.0.1:5700  | 0.0001 |               3000 |     14 |          667.624 | 42000 |   12.186 |          18.215 |          -6.029 |\n",
      "| PPO_MultiAgentArena_24664_00002 | TERMINATED | 127.0.0.1:16440 | 5e-05  |               4000 |     10 |          645.524 | 40000 |    6.678 |          12.905 |          -6.227 |\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Result for PPO_MultiAgentArena_24664_00003:\n",
      "  agent_timesteps_total: 80000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-15_13-24-21\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.099999999999987\n",
      "  episode_reward_mean: 7.838999999999996\n",
      "  episode_reward_min: -12.899999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 7ffa1318da8643dc83477d4ec3160033\n",
      "  hostname: DESKTOP-0LQ89AE\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0968165397644043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015119635500013828\n",
      "          model: {}\n",
      "          policy_loss: -0.045113373547792435\n",
      "          total_loss: 6.918203830718994\n",
      "          vf_explained_var: 0.20993152260780334\n",
      "          vf_loss: 6.956512928009033\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0348540544509888\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014157337136566639\n",
      "          model: {}\n",
      "          policy_loss: -0.03721832484006882\n",
      "          total_loss: 3.0938401222229004\n",
      "          vf_explained_var: 0.0853278711438179\n",
      "          vf_loss: 3.124687433242798\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 40000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 1\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  perf:\n",
      "    cpu_util_percent: 28.64714285714286\n",
      "    ram_util_percent: 59.13\n",
      "  pid: 7072\n",
      "  policy_reward_max:\n",
      "    policy1: 38.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 14.165\n",
      "    policy2: -6.325999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07339694456939398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022174916103794386\n",
      "    mean_inference_ms: 7.176587969519633\n",
      "    mean_raw_obs_processing_ms: 0.42796228060443153\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.099999999999987\n",
      "    episode_reward_mean: 7.838999999999996\n",
      "    episode_reward_min: -12.899999999999993\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "        100, 100, 100, 100, 100, 100, 100, 100]\n",
      "      episode_reward: [18.299999999999947, 8.100000000000021, -8.999999999999973, 1.8000000000000185,\n",
      "        -12.899999999999993, -3.2999999999999847, 2.7000000000000086, 12.30000000000002,\n",
      "        12.300000000000017, 2.400000000000026, 6.300000000000033, 2.373101715136272e-14,\n",
      "        9.000000000000021, 18.900000000000006, -8.999999999999986, 14.10000000000003,\n",
      "        -5.699999999999977, -4.4999999999999964, -1.4999999999999933, 8.699999999999983,\n",
      "        23.4, 17.699999999999974, 17.09999999999996, 15.3, 9.599999999999985, 13.499999999999902,\n",
      "        13.500000000000012, 7.499999999999986, 16.19999999999998, -9.899999999999983,\n",
      "        4.800000000000029, 7.500000000000018, 21.59999999999996, 17.100000000000016,\n",
      "        -6.299999999999976, -5.999999999999989, 16.499999999999922, 8.700000000000012,\n",
      "        12.59999999999997, 9.000000000000018, 26.69999999999996, -2.399999999999981,\n",
      "        16.799999999999976, -7.499999999999983, -2.9999999999999853, 6.000000000000032,\n",
      "        3.5999999999999943, 7.19999999999996, 21.299999999999955, -3.899999999999991,\n",
      "        10.800000000000022, 1.8000000000000278, 11.999999999999911, 12.600000000000012,\n",
      "        21.59999999999996, -5.099999999999985, 4.200000000000027, 5.999999999999998,\n",
      "        3.90000000000003, 16.199999999999932, 12.600000000000007, -3.8999999999999777,\n",
      "        29.099999999999987, 15.299999999999962, 3.000000000000013, 19.499999999999897,\n",
      "        13.20000000000003, 18.599999999999913, -5.999999999999984, 2.78388423424758e-14,\n",
      "        2.1000000000000068, -3.299999999999976, 15.59999999999996, 0.9000000000000262,\n",
      "        12.89999999999996, 14.399999999999984, 10.200000000000015, 13.799999999999917,\n",
      "        10.800000000000027, 22.199999999999996, -0.8999999999999859, 1.19999999999999,\n",
      "        4.50000000000003, 16.79999999999997, 7.800000000000029, 0.600000000000022, -4.200000000000031,\n",
      "        -3.899999999999979, 11.700000000000031, 20.99999999999995, 12.600000000000032,\n",
      "        18.89999999999993, 8.700000000000033, 15.000000000000014, 1.4999999999999996,\n",
      "        -5.699999999999979, 9.000000000000027, 19.79999999999995, 15.299999999999944,\n",
      "        5.999999999999929]\n",
      "      policy_policy1_reward: [19.5, 17.0, 1.0, -8.0, -4.0, 4.5, 10.5, 19.0, 19.0, 2.5,\n",
      "        13.0, 4.5, 19.0, 19.0, 1.0, 17.5, 1.0, 5.5, 3.0, 16.5, 18.0, 25.5, 20.5, 16.5,\n",
      "        18.5, 23.5, 23.5, 12.0, 24.0, -1.0, 6.0, 17.5, 25.0, 20.5, 1.5, -1.5, 21.0,\n",
      "        16.5, 21.5, 19.0, 34.5, 6.5, 23.5, -3.0, 7.0, 16.0, 12.5, 15.0, 28.0, 5.0, 17.5,\n",
      "        8.5, 22.0, 21.5, 30.5, 0.5, 12.0, 10.5, 9.5, 24.0, 21.5, 5.0, 38.0, 16.5, 13.0,\n",
      "        29.5, 21.0, 27.5, 4.0, 10.0, 0.0, 4.5, 24.5, 6.5, 13.0, 20.0, 7.0, 20.5, 12.0,\n",
      "        30.0, 8.0, 9.0, 3.5, 23.5, 9.0, 9.5, 2.5, 5.0, 19.5, 31.0, 21.5, 24.5, 16.5,\n",
      "        25.0, 11.5, -10.0, 13.5, 26.5, 16.5, 16.0]\n",
      "      policy_policy2_reward: [-1.2000000000000048, -8.899999999999983, -9.99999999999998,\n",
      "        9.800000000000006, -8.89999999999998, -7.799999999999981, -7.799999999999981,\n",
      "        -6.6999999999999815, -6.6999999999999815, -0.09999999999998954, -6.69999999999999,\n",
      "        -4.499999999999988, -9.99999999999998, -0.09999999999999593, -9.99999999999998,\n",
      "        -3.3999999999999986, -6.6999999999999815, -9.99999999999998, -4.5, -7.799999999999981,\n",
      "        5.4000000000000075, -7.79999999999999, -3.400000000000003, -1.2000000000000033,\n",
      "        -8.89999999999998, -9.99999999999998, -9.99999999999998, -4.499999999999996,\n",
      "        -7.799999999999986, -8.89999999999998, -1.1999999999999993, -9.99999999999998,\n",
      "        -3.399999999999999, -3.3999999999999906, -7.799999999999981, -4.500000000000002,\n",
      "        -4.499999999999984, -7.799999999999987, -8.89999999999998, -9.99999999999998,\n",
      "        -7.799999999999981, -8.89999999999998, -6.6999999999999815, -4.5, -9.99999999999998,\n",
      "        -9.99999999999998, -8.89999999999998, -7.799999999999989, -6.6999999999999895,\n",
      "        -8.89999999999998, -6.699999999999992, -6.699999999999994, -9.99999999999998,\n",
      "        -8.89999999999998, -8.89999999999998, -5.599999999999982, -7.79999999999999,\n",
      "        -4.499999999999982, -5.5999999999999925, -7.799999999999981, -8.899999999999986,\n",
      "        -8.89999999999998, -8.89999999999998, -1.2000000000000035, -9.99999999999998,\n",
      "        -9.99999999999998, -7.79999999999999, -8.89999999999998, -9.99999999999998,\n",
      "        -9.99999999999998, 2.0999999999999974, -7.799999999999989, -8.89999999999998,\n",
      "        -5.6, -0.1000000000000042, -5.599999999999999, 3.20000000000001, -6.699999999999987,\n",
      "        -1.2000000000000035, -7.799999999999986, -8.89999999999998, -7.7999999999999865,\n",
      "        0.9999999999999966, -6.6999999999999815, -1.2000000000000026, -8.899999999999986,\n",
      "        -6.6999999999999815, -8.89999999999998, -7.799999999999981, -9.99999999999998,\n",
      "        -8.899999999999986, -5.6, -7.79999999999999, -9.99999999999998, -9.99999999999998,\n",
      "        4.300000000000011, -4.499999999999982, -6.699999999999987, -1.2000000000000022,\n",
      "        -9.99999999999998]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max:\n",
      "      policy1: 38.0\n",
      "      policy2: 9.800000000000006\n",
      "    policy_reward_mean:\n",
      "      policy1: 14.165\n",
      "      policy2: -6.325999999999988\n",
      "    policy_reward_min:\n",
      "      policy1: -10.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07339694456939398\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.022174916103794386\n",
      "      mean_inference_ms: 7.176587969519633\n",
      "      mean_raw_obs_processing_ms: 0.42796228060443153\n",
      "  time_since_restore: 649.5732743740082\n",
      "  time_this_iter_s: 49.767157554626465\n",
      "  time_total_s: 649.5732743740082\n",
      "  timers:\n",
      "    learn_throughput: 117.214\n",
      "    learn_time_ms: 34125.742\n",
      "    synch_weights_time_ms: 2.892\n",
      "    training_iteration_time_ms: 64951.543\n",
      "  timestamp: 1660562661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: '24664_00003'\n",
      "  warmup_time: 9.841668844223022\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-15 13:24:21 (running for 00:11:55.87)\n",
      "Memory usage on this node: 12.4/31.9 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/14.5 GiB heap, 0.0/7.25 GiB objects\n",
      "Current best trial: 24664_00001 with episode_reward_mean=12.185999999999968 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 3000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E0A2E0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B99805E3D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E201F0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997EE34C0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x000002B997E1DA30>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x000002B997E201F0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n",
      "Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\n",
      "Number of trials: 4/4 (4 TERMINATED)\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "| Trial name                      | status     | loc             |     lr |   train_batch_size |   iter |   total time (s) |    ts |   reward |   agent1 return |   agent2 return |\n",
      "|---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------|\n",
      "| PPO_MultiAgentArena_24664_00000 | TERMINATED | 127.0.0.1:16724 | 5e-05  |               3000 |     14 |          664.097 | 42000 |    6.027 |          12.485 |          -6.458 |\n",
      "| PPO_MultiAgentArena_24664_00001 | TERMINATED | 127.0.0.1:5700  | 0.0001 |               3000 |     14 |          667.624 | 42000 |   12.186 |          18.215 |          -6.029 |\n",
      "| PPO_MultiAgentArena_24664_00002 | TERMINATED | 127.0.0.1:16440 | 5e-05  |               4000 |     10 |          645.524 | 40000 |    6.678 |          12.905 |          -6.227 |\n",
      "| PPO_MultiAgentArena_24664_00003 | TERMINATED | 127.0.0.1:7072  | 0.0001 |               4000 |     10 |          649.573 | 40000 |    7.839 |          14.165 |          -6.326 |\n",
      "+---------------------------------+------------+-----------------+--------+--------------------+--------+------------------+-------+----------+-----------------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 13:24:21,591\tINFO tune.py:758 -- Total run time: 716.33 seconds (715.80 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Example using Ray tune API (`tune.run()`) until some stopping condition is met.\n",
    "# This will create one (or more) Algorithms under the hood automatically w/o us having to\n",
    "# build these algos from the config.\n",
    "\n",
    "# Use a custom \"reporter\" that adds the individual policies' rewards to the output.\n",
    "reporter = CLIReporter()\n",
    "reporter.add_metric_column(\"sampler_results/policy_reward_mean/policy1\", \"agent1 return\")\n",
    "reporter.add_metric_column(\"sampler_results/policy_reward_mean/policy2\", \"agent2 return\")\n",
    "\n",
    "\n",
    "experiment_results = tune.run(\n",
    "    \"PPO\",\n",
    "\n",
    "    # training config params (translated into a python dict!)\n",
    "    config=config.to_dict(),\n",
    "\n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\n",
    "        \"timesteps_total\": 40000,  # stop if reached n sampling timesteps\n",
    "        #\"training_iteration\": 5,     # stop after n training iterations (calls to `Algorithm.train()`)\n",
    "        #\"episode_reward_mean\": 400, # stop if average (sum of) rewards in an episode is n or more\n",
    "    },  \n",
    "    progress_reporter=reporter,\n",
    "\n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=\"results\",\n",
    "         \n",
    "    # Every how many train() calls do we create a checkpoint?\n",
    "    checkpoint_freq=1,\n",
    "    # Always save last checkpoint (no matter the frequency).\n",
    "    checkpoint_at_end=True,\n",
    "\n",
    "    ###############\n",
    "    # Note about Ray Tune verbosity.\n",
    "    # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "    # 0 = silent\n",
    "    # 1 = only status updates, no logging messages\n",
    "    # 2 = status and brief trial results, includes logging messages\n",
    "    # 3 = status and detailed trial results, includes logging messages\n",
    "    # Defaults to 3.\n",
    "    ###############\n",
    "    verbose=3,\n",
    "                   \n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:  PPO_MultiAgentArena_24664_00001\n",
      "Best checkpoint from training: Checkpoint(local_path=C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\PPO\\PPO_MultiAgentArena_24664_00001_1_lr=0.0001,train_batch_size=3000_2022-08-15_13-12-42\\checkpoint_000014)\n"
     ]
    }
   ],
   "source": [
    "# Using the returned `experiment_results` object,\n",
    "# we can extract from it the best checkpoint according to some criterium, e.g. `episode_reward_mean`.\n",
    "\n",
    "# We had 4 single trials (4 Algorithm instance); return the one that performed best here.\n",
    "best_trial = experiment_results.get_best_trial()\n",
    "print(\"Best trial: \", best_trial)\n",
    "\n",
    "# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\n",
    "best_checkpoint = experiment_results.get_best_checkpoint(trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# We would expect this to be either the very last checkpoint or one close to it:\n",
    "print(f\"Best checkpoint from training: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details behind Ray RLlib resource allocation <a class=\"anchor\" id=\"resource_allocation\"></a>\n",
    "\n",
    "#### Why did we use 8 CPUs in the tune run above (2 CPUs per trial)?\n",
    "\n",
    "```\n",
    "== Status ==\n",
    "Current time: 2022-07-24 18:18:28 (running for 00:02:09.35)\n",
    "Memory usage on this node: 9.9/16.0 GiB\n",
    "Using FIFO scheduling algorithm.\n",
    "Resources requested: 8/16 CPUs, 0/0 GPUs, 0.5/3.97 GiB heap, 0.5/1.98 GiB objects\n",
    "```\n",
    "\n",
    "<img src=\"images/closer_look_at_rllib.png\" width=700 />\n",
    "\n",
    "By default, the PPO Algorithm uses 2 so called `RolloutWorkers` (you can change this via `config.rollouts(num_rollout_workers=2)`) for collecting samples from\n",
    "environments in parallel.\n",
    "We changed this setting to only 1 worker via the `config.rollouts(num_rollout_workers=1)` call in the cell above.\n",
    "\n",
    "`RolloutWorkers` are Ray Actors that have their own copies of the environment and step through episodes in parallel. Each Actor in Ray normally uses a single CPU, but besides `RolloutWorker`s, an Algorithm in RLlib also always has one local process (aka. the \"driver\" process or the \"local worker\"), which - in case of PPO -\n",
    "handles the model/policy learning updates.\n",
    "\n",
    "For our experiment above, this gives us 2 CPUs (1 rollout worker + 1 local learner) per Algorithm instance.\n",
    "\n",
    "Since our config specifies two `grid_search` with 2 different learning rates AND 2 different batch sizes, we were running 4 Algorithms in parallel above (2 learning rates x 2 batch sizes = 4 trials), hence 8 CPUs were required (4 algos x 2 CPUs each = 8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt, how to:\n",
    "\n",
    "* Use Ray Tune in combination with RLlib for hyperparameter tuning\n",
    "* How an RLlib Algorithm configuration and the Tune hyperparameter search setup determine the required computational resources for a given `tune.run()` experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 03<a ></a>\n",
    "\n",
    "#### Using the `config` that we have built so far, let's run another `tune.run()`.\n",
    "\n",
    "But this time, apply the following changes to our setup:\n",
    "\n",
    "- Setup only 1 learning rate using the `config.training(lr=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size using the `config.training(train_batch_size=...)` method call. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set the number of RolloutWorkers to 5 using the `config.rollouts(num_rollout_workers=5)` method call, which will allow us to collect more environment samples in parallel.\n",
    "- Set the `num_envs_per_worker` config parameter to 5 using the `config.rollouts(num_envs_per_worker=...)` method call. This will batch our environment on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "- Set the stop criterium to \"training_iteration=180\".\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo our tune hyperparameter search:\n",
    "config.training(\n",
    "    # ...\n",
    ")\n",
    "\n",
    "# Change the config as stated in the exercise task:\n",
    "config.rollouts(\n",
    "    # ...\n",
    ")\n",
    "\n",
    "# Run the experiment for 180 iterations:\n",
    "experiment_results = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config.to_dict(),\n",
    "    stop={\n",
    "        # ...\n",
    "    },\n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=\"results\",\n",
    "    checkpoint_freq=10,\n",
    "    checkpoint_at_end=True,\n",
    "    verbose=1,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only had a single trial (one Algorithm instance), so this should be returned here.\n",
    "best_trial = experiment_results.get_best_trial()\n",
    "print(\"Best trial: \", best_trial)\n",
    "\n",
    "# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\n",
    "best_checkpoint = experiment_results.get_best_checkpoint(trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# We would expect this to be either the very last checkpoint or one close to it:\n",
    "print(f\"Best checkpoint from training: {best_checkpoint}\")\n",
    "\n",
    "# Create a fresh algorithm and retstore its state, using our best checkploint from the experiment above.\n",
    "new_ppo = config.build()\n",
    "new_ppo.restore(best_checkpoint)\n",
    "\n",
    "# Let's see how we are doing now.\n",
    "play_one_episode(env=None, algo=new_ppo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (release resources for other notebooks to come).\n",
    "new_ppo.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    " * [Tune, Scalable Hyperparameter Tuning](https://docs.ray.io/en/latest/tune/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
