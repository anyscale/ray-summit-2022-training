{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03. Tune the hyperparameters of a RLlib Multi-Agent Model using Ray Tune\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn:\n",
    " * [How to configure Ray Tune to find solid hyperparameters more easily](#configure_ray_tune)\n",
    " * [The details behind Ray RLlib resource allocation](#resource_allocation)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to configure Ray Tune to find solid hyperparameters more easily <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "In the previous experiments, we used a single algorithm's (PPO) configuration to create\n",
    "exactly one Algorithm object and call its `train()` method manually a couple of times.\n",
    "\n",
    "A common thing to try when doing ML or RL is to look for better choices of hyperparameters, neural network architectures, or algorithm settings. This hyperparameter optimization\n",
    "problem can be tackled in a scalable fashion using Ray Tune (in combination with RLlib!).\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates, how you can setup a simple grid-search for one very important hyperparameter (the learning rate), using our already existing PPO config object and Ray Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default learning rate for PPO is: 5e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7fd722553940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena\n",
    "\n",
    "# Create a PPOConfig object (same as we did in the previous notebook):\n",
    "config = PPOConfig()\n",
    "\n",
    "# Setup our config object the exact same way as before:\n",
    "# Point to our MultiAgentArena env:\n",
    "config.environment(\n",
    "    env=MultiAgentArena,\n",
    "    env_config={\n",
    "    # If you'd like, feel free to set the size of our world differently.\n",
    "    #    \"width\": 12,\n",
    "    #    \"height\": 12,\n",
    "    },\n",
    ")\n",
    "# Multi-agent settings (same as before):\n",
    "config.multi_agent(\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")\n",
    "\n",
    "# Before setting up the learning rate hyperparam sweep,\n",
    "# let's see what the default learning rate for PPO actually is:\n",
    "print(f\"Default learning rate for PPO is: {config.lr}\")\n",
    "\n",
    "# Now let's change our existing config object and add a simple\n",
    "# grid-search over two different learning rates to it:\n",
    "config.training(\n",
    "    lr=tune.grid_search([0.00005, 0.0003]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-24 18:18:28 (running for 00:02:09.35)<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.97 GiB heap, 0.0/1.98 GiB objects<br>Current best trial: f370e_00001 with episode_reward_mean=1.659000000000009 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 2, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 0.0003, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'multi_agent_arena.multi_agent_arena.MultiAgentArena'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 2, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 0.0003, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x7f9daa3f2eb0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x7f9daa3f2c40>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f9daa4103a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'policy1': <ray.rllib.policy.policy.PolicySpec object at 0x7f9daa691eb0>, 'policy2': <ray.rllib.policy.policy.PolicySpec object at 0x7f9daa691430>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f9daa4103a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}<br>Result logdir: /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/multiagent_PPO_logs/PPO<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f370e_00000</td><td>TERMINATED</td><td>127.0.0.1:26939</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         66.1827</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -0.471</td><td style=\"text-align: right;\">                25.5</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_f370e_00001</td><td>TERMINATED</td><td>127.0.0.1:26954</td><td style=\"text-align: right;\">0.0003</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         60.1578</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.659</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               -23.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:29,265\tINFO algorithm.py:1774 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:29,265\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:29,265\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26947)\u001b[0m 2022-07-24 18:16:38,814\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26946)\u001b[0m 2022-07-24 18:16:38,814\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26946)\u001b[0m 2022-07-24 18:16:38,814\tWARNING env.py:223 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26947)\u001b[0m 2022-07-24 18:16:39,025\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26946)\u001b[0m 2022-07-24 18:16:39,027\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26947)\u001b[0m 2022-07-24 18:16:40,487\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26946)\u001b[0m 2022-07-24 18:16:40,501\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:42,043\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:42,736\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:44,950\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:47,206\tINFO trainable.py:160 -- Trainable.setup took 17.942 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:47,208\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26946)\u001b[0m 2022-07-24 18:16:48,525\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:53,697\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:53,721\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26939)\u001b[0m 2022-07-24 18:16:53,721\tWARNING deprecation.py:47 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:06,773\tINFO algorithm.py:1774 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:06,773\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:06,774\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27031)\u001b[0m 2022-07-24 18:17:23,411\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27030)\u001b[0m 2022-07-24 18:17:23,411\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27030)\u001b[0m 2022-07-24 18:17:23,412\tWARNING env.py:223 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27031)\u001b[0m 2022-07-24 18:17:23,577\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27030)\u001b[0m 2022-07-24 18:17:23,577\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27031)\u001b[0m 2022-07-24 18:17:24,773\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27030)\u001b[0m 2022-07-24 18:17:24,773\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:25,527\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:25,671\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:26,591\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f370e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.300000000000008\n",
      "  episode_reward_mean: -8.129999999999997\n",
      "  episode_reward_min: -33.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: b06a13ec4c9d44f2866d39064923d025\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.3389638662338257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.048597175627946854\n",
      "          model: {}\n",
      "          policy_loss: -0.08453787863254547\n",
      "          total_loss: 6.081151485443115\n",
      "          vf_explained_var: -0.03356152027845383\n",
      "          vf_loss: 6.155969619750977\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.3408448696136475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.046515464782714844\n",
      "          model: {}\n",
      "          policy_loss: -0.08061685413122177\n",
      "          total_loss: 1.645768642425537\n",
      "          vf_explained_var: 0.4844778776168823\n",
      "          vf_loss: 1.7170823812484741\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.80434782608695\n",
      "    ram_util_percent: 78.4608695652174\n",
      "  pid: 26939\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -4.500000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 0.6875\n",
      "    policy2: -8.817499999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2245269734403123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09076866967745986\n",
      "    mean_inference_ms: 2.4059453885117987\n",
      "    mean_raw_obs_processing_ms: 0.43106472295620984\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.300000000000008\n",
      "    episode_reward_mean: -8.129999999999997\n",
      "    episode_reward_min: -33.00000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - -0.89999999999999\n",
      "      - 2.700000000000003\n",
      "      - -19.50000000000003\n",
      "      - -10.500000000000002\n",
      "      - -10.499999999999979\n",
      "      - -25.500000000000025\n",
      "      - -9.89999999999998\n",
      "      - 1.3961054534661343e-14\n",
      "      - 11.099999999999955\n",
      "      - -8.699999999999985\n",
      "      - -33.00000000000006\n",
      "      - -15.600000000000035\n",
      "      - -12.299999999999992\n",
      "      - -25.500000000000004\n",
      "      - -6.899999999999991\n",
      "      - -2.9999999999999813\n",
      "      - 7.5000000000000195\n",
      "      - -16.499999999999986\n",
      "      - 0.6000000000000144\n",
      "      - -21.6\n",
      "      - 3.000000000000017\n",
      "      - -30.90000000000004\n",
      "      - -2.999999999999978\n",
      "      - -5.999999999999979\n",
      "      - -4.499999999999991\n",
      "      - -22.500000000000046\n",
      "      - -4.499999999999984\n",
      "      - -18.89999999999999\n",
      "      - -10.49999999999998\n",
      "      - 15.300000000000008\n",
      "      - -17.99999999999999\n",
      "      - 0.900000000000012\n",
      "      - 12.000000000000005\n",
      "      - 1.174060848541103e-14\n",
      "      - -10.799999999999974\n",
      "      - -0.8999999999999921\n",
      "      - -7.799999999999988\n",
      "      - -10.499999999999986\n",
      "      - 0.2999999999999978\n",
      "      - -9.899999999999979\n",
      "      policy_policy1_reward:\n",
      "      - 8.0\n",
      "      - 10.5\n",
      "      - -9.5\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - -15.5\n",
      "      - -1.0\n",
      "      - 10.0\n",
      "      - 20.0\n",
      "      - -2.0\n",
      "      - -23.0\n",
      "      - -10.0\n",
      "      - -4.5\n",
      "      - -15.5\n",
      "      - 2.0\n",
      "      - 7.0\n",
      "      - 17.5\n",
      "      - -6.5\n",
      "      - 9.5\n",
      "      - -16.0\n",
      "      - 13.0\n",
      "      - -22.0\n",
      "      - 7.0\n",
      "      - 4.0\n",
      "      - 5.5\n",
      "      - -12.5\n",
      "      - 5.5\n",
      "      - -10.0\n",
      "      - -6.0\n",
      "      - 22.0\n",
      "      - -8.0\n",
      "      - 6.5\n",
      "      - 22.0\n",
      "      - 10.0\n",
      "      - -3.0\n",
      "      - 8.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      policy_policy2_reward:\n",
      "      - -8.899999999999986\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999984\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.500000000000002\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999983\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -8.89999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 22.0\n",
      "      policy2: -4.500000000000002\n",
      "    policy_reward_mean:\n",
      "      policy1: 0.6875\n",
      "      policy2: -8.817499999999981\n",
      "    policy_reward_min:\n",
      "      policy1: -23.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2245269734403123\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.09076866967745986\n",
      "      mean_inference_ms: 2.4059453885117987\n",
      "      mean_raw_obs_processing_ms: 0.43106472295620984\n",
      "  time_since_restore: 16.073060750961304\n",
      "  time_this_iter_s: 16.073060750961304\n",
      "  time_total_s: 16.073060750961304\n",
      "  timers:\n",
      "    learn_throughput: 418.117\n",
      "    learn_time_ms: 9566.706\n",
      "    synch_weights_time_ms: 5.765\n",
      "    training_iteration_time_ms: 16069.097\n",
      "  timestamp: 1658679423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f370e_00000\n",
      "  warmup_time: 17.984856128692627\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:27,349\tINFO trainable.py:160 -- Trainable.setup took 20.580 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:27,350\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27030)\u001b[0m 2022-07-24 18:17:27,861\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:31,788\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:31,802\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26954)\u001b[0m 2022-07-24 18:17:31,803\tWARNING deprecation.py:47 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f370e_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-17-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.100000000000014\n",
      "  episode_reward_mean: -11.0925\n",
      "  episode_reward_min: -45.000000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 7930c8da23ce49829b3cecebb3c03c5f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3541070222854614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03301246091723442\n",
      "          model: {}\n",
      "          policy_loss: -0.06401419639587402\n",
      "          total_loss: 6.8029632568359375\n",
      "          vf_explained_var: -0.013164803385734558\n",
      "          vf_loss: 6.860374927520752\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3576741218566895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029543882235884666\n",
      "          model: {}\n",
      "          policy_loss: -0.055146731436252594\n",
      "          total_loss: 2.203115940093994\n",
      "          vf_explained_var: 0.3530558943748474\n",
      "          vf_loss: 2.2523539066314697\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.961111111111116\n",
      "    ram_util_percent: 56.388888888888886\n",
      "  pid: 26954\n",
      "  policy_reward_max:\n",
      "    policy1: 17.5\n",
      "    policy2: -4.500000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.275\n",
      "    policy2: -8.817499999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -35.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15379267296512267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06752196459219731\n",
      "    mean_inference_ms: 1.6521347099277506\n",
      "    mean_raw_obs_processing_ms: 0.29330596752252536\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 8.100000000000014\n",
      "    episode_reward_mean: -11.0925\n",
      "    episode_reward_min: -45.000000000000064\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - -45.000000000000064\n",
      "      - 6.600000000000032\n",
      "      - -7.499999999999989\n",
      "      - -1.4999999999999822\n",
      "      - -7.19999999999998\n",
      "      - -9.899999999999974\n",
      "      - -2.9999999999999987\n",
      "      - -14.999999999999973\n",
      "      - -18.899999999999988\n",
      "      - -24.600000000000037\n",
      "      - -24.000000000000007\n",
      "      - 6.300000000000008\n",
      "      - -10.199999999999976\n",
      "      - 8.100000000000014\n",
      "      - -19.500000000000007\n",
      "      - -3.299999999999986\n",
      "      - -19.500000000000007\n",
      "      - -18.300000000000043\n",
      "      - -1.4999999999999791\n",
      "      - 1.840194663316197e-14\n",
      "      - -2.4000000000000026\n",
      "      - -5.999999999999979\n",
      "      - -29.400000000000034\n",
      "      - 6.000000000000021\n",
      "      - -24.300000000000058\n",
      "      - -33.000000000000064\n",
      "      - 4.500000000000026\n",
      "      - -12.899999999999972\n",
      "      - -16.200000000000017\n",
      "      - -27.000000000000014\n",
      "      - -36.00000000000006\n",
      "      - -20.400000000000002\n",
      "      - 7.500000000000025\n",
      "      - -11.399999999999974\n",
      "      - -8.999999999999986\n",
      "      - -3.299999999999994\n",
      "      - -5.999999999999993\n",
      "      - 3.0000000000000187\n",
      "      - -19.5\n",
      "      - 1.4405143744511406e-14\n",
      "      policy_policy1_reward:\n",
      "      - -35.0\n",
      "      - 15.5\n",
      "      - 2.5\n",
      "      - 8.5\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 1.5\n",
      "      - -5.0\n",
      "      - -10.0\n",
      "      - -19.0\n",
      "      - -14.0\n",
      "      - 13.0\n",
      "      - -3.5\n",
      "      - 17.0\n",
      "      - -9.5\n",
      "      - 4.5\n",
      "      - -9.5\n",
      "      - -10.5\n",
      "      - 8.5\n",
      "      - 10.0\n",
      "      - 6.5\n",
      "      - 4.0\n",
      "      - -20.5\n",
      "      - 16.0\n",
      "      - -16.5\n",
      "      - -23.0\n",
      "      - 14.5\n",
      "      - -4.0\n",
      "      - -9.5\n",
      "      - -17.0\n",
      "      - -26.0\n",
      "      - -11.5\n",
      "      - 17.5\n",
      "      - -2.5\n",
      "      - 1.0\n",
      "      - 4.5\n",
      "      - 4.0\n",
      "      - 7.5\n",
      "      - -9.5\n",
      "      - 10.0\n",
      "      policy_policy2_reward:\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -6.699999999999985\n",
      "      - -8.89999999999998\n",
      "      - -4.500000000000001\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999895\n",
      "      - -6.6999999999999815\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999983\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.7999999999999865\n",
      "      - -9.99999999999998\n",
      "      - -4.5000000000000036\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 17.5\n",
      "      policy2: -4.500000000000001\n",
      "    policy_reward_mean:\n",
      "      policy1: -2.275\n",
      "      policy2: -8.817499999999981\n",
      "    policy_reward_min:\n",
      "      policy1: -35.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15379267296512267\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.06752196459219731\n",
      "      mean_inference_ms: 1.6521347099277506\n",
      "      mean_raw_obs_processing_ms: 0.29330596752252536\n",
      "  time_since_restore: 12.536049127578735\n",
      "  time_this_iter_s: 12.536049127578735\n",
      "  time_total_s: 12.536049127578735\n",
      "  timers:\n",
      "    learn_throughput: 494.754\n",
      "    learn_time_ms: 8084.822\n",
      "    synch_weights_time_ms: 4.155\n",
      "    training_iteration_time_ms: 12532.909\n",
      "  timestamp: 1658679459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f370e_00001\n",
      "  warmup_time: 20.60776996612549\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-17-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.300000000000008\n",
      "  episode_reward_mean: -4.762499999999993\n",
      "  episode_reward_min: -33.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: b06a13ec4c9d44f2866d39064923d025\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.2688846588134766\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06983427703380585\n",
      "          model: {}\n",
      "          policy_loss: -0.12060462683439255\n",
      "          total_loss: 5.042436599731445\n",
      "          vf_explained_var: 0.055825624614953995\n",
      "          vf_loss: 5.142090797424316\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.2690293788909912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07628258317708969\n",
      "          model: {}\n",
      "          policy_loss: -0.13342712819576263\n",
      "          total_loss: 1.014082431793213\n",
      "          vf_explained_var: 0.5728269219398499\n",
      "          vf_loss: 1.1246248483657837\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.16603773584906\n",
      "    ram_util_percent: 68.92830188679245\n",
      "  pid: 26939\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -1.1999999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.06875\n",
      "    policy2: -8.831249999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -23.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2061193797272282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08467781423377377\n",
      "    mean_inference_ms: 2.2052002600756264\n",
      "    mean_raw_obs_processing_ms: 0.3965909819967729\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.300000000000008\n",
      "    episode_reward_mean: -4.762499999999993\n",
      "    episode_reward_min: -33.00000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - -0.89999999999999\n",
      "      - 2.700000000000003\n",
      "      - -19.50000000000003\n",
      "      - -10.500000000000002\n",
      "      - -10.499999999999979\n",
      "      - -25.500000000000025\n",
      "      - -9.89999999999998\n",
      "      - 1.3961054534661343e-14\n",
      "      - 11.099999999999955\n",
      "      - -8.699999999999985\n",
      "      - -33.00000000000006\n",
      "      - -15.600000000000035\n",
      "      - -12.299999999999992\n",
      "      - -25.500000000000004\n",
      "      - -6.899999999999991\n",
      "      - -2.9999999999999813\n",
      "      - 7.5000000000000195\n",
      "      - -16.499999999999986\n",
      "      - 0.6000000000000144\n",
      "      - -21.6\n",
      "      - 3.000000000000017\n",
      "      - -30.90000000000004\n",
      "      - -2.999999999999978\n",
      "      - -5.999999999999979\n",
      "      - -4.499999999999991\n",
      "      - -22.500000000000046\n",
      "      - -4.499999999999984\n",
      "      - -18.89999999999999\n",
      "      - -10.49999999999998\n",
      "      - 15.300000000000008\n",
      "      - -17.99999999999999\n",
      "      - 0.900000000000012\n",
      "      - 12.000000000000005\n",
      "      - 1.174060848541103e-14\n",
      "      - -10.799999999999974\n",
      "      - -0.8999999999999921\n",
      "      - -7.799999999999988\n",
      "      - -10.499999999999986\n",
      "      - 0.2999999999999978\n",
      "      - -9.899999999999979\n",
      "      - 2.373101715136272e-14\n",
      "      - 0.5999999999999974\n",
      "      - -3.899999999999992\n",
      "      - 2.100000000000007\n",
      "      - 13.800000000000031\n",
      "      - 3.000000000000008\n",
      "      - 1.499999999999997\n",
      "      - -2.399999999999988\n",
      "      - 3.0000000000000187\n",
      "      - 2.858824288409778e-15\n",
      "      - -10.499999999999986\n",
      "      - -3.2999999999999963\n",
      "      - 2.9999999999999867\n",
      "      - -2.9999999999999942\n",
      "      - -8.99999999999999\n",
      "      - 7.200000000000028\n",
      "      - -10.49999999999999\n",
      "      - -8.999999999999984\n",
      "      - 4.500000000000007\n",
      "      - 8.100000000000017\n",
      "      - 6.600000000000017\n",
      "      - -5.3999999999999915\n",
      "      - 0.9000000000000127\n",
      "      - -3.300000000000004\n",
      "      - 0.6000000000000175\n",
      "      - 10.200000000000022\n",
      "      - -7.499999999999991\n",
      "      - -2.3999999999999906\n",
      "      - 2.6999999999999997\n",
      "      - -7.499999999999989\n",
      "      - -32.400000000000034\n",
      "      - 0.6000000000000083\n",
      "      - -19.499999999999993\n",
      "      - 4.500000000000009\n",
      "      - 10.199999999999994\n",
      "      - -2.699999999999987\n",
      "      - -3.299999999999987\n",
      "      - -5.399999999999973\n",
      "      - -3.899999999999987\n",
      "      - 6.000000000000025\n",
      "      policy_policy1_reward:\n",
      "      - 8.0\n",
      "      - 10.5\n",
      "      - -9.5\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - -15.5\n",
      "      - -1.0\n",
      "      - 10.0\n",
      "      - 20.0\n",
      "      - -2.0\n",
      "      - -23.0\n",
      "      - -10.0\n",
      "      - -4.5\n",
      "      - -15.5\n",
      "      - 2.0\n",
      "      - 7.0\n",
      "      - 17.5\n",
      "      - -6.5\n",
      "      - 9.5\n",
      "      - -16.0\n",
      "      - 13.0\n",
      "      - -22.0\n",
      "      - 7.0\n",
      "      - 4.0\n",
      "      - 5.5\n",
      "      - -12.5\n",
      "      - 5.5\n",
      "      - -10.0\n",
      "      - -6.0\n",
      "      - 22.0\n",
      "      - -8.0\n",
      "      - 6.5\n",
      "      - 22.0\n",
      "      - 10.0\n",
      "      - -3.0\n",
      "      - 8.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      - 10.0\n",
      "      - 9.5\n",
      "      - 5.0\n",
      "      - 11.0\n",
      "      - 20.5\n",
      "      - 13.0\n",
      "      - 11.5\n",
      "      - 6.5\n",
      "      - 13.0\n",
      "      - 10.0\n",
      "      - -0.5\n",
      "      - 4.5\n",
      "      - 13.0\n",
      "      - 7.0\n",
      "      - 1.0\n",
      "      - 15.0\n",
      "      - -0.5\n",
      "      - 1.0\n",
      "      - 14.5\n",
      "      - 17.0\n",
      "      - 15.5\n",
      "      - 3.5\n",
      "      - 6.5\n",
      "      - 4.5\n",
      "      - 9.5\n",
      "      - 18.0\n",
      "      - 2.5\n",
      "      - 6.5\n",
      "      - 10.5\n",
      "      - 2.5\n",
      "      - -23.5\n",
      "      - 9.5\n",
      "      - -9.5\n",
      "      - 14.5\n",
      "      - 18.0\n",
      "      - -1.5\n",
      "      - 4.5\n",
      "      - 3.5\n",
      "      - 5.0\n",
      "      - 16.0\n",
      "      policy_policy2_reward:\n",
      "      - -8.899999999999986\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999984\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.500000000000002\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999983\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999984\n",
      "      - -6.699999999999994\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999999\n",
      "      - -7.799999999999986\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999982\n",
      "      - -1.1999999999999986\n",
      "      - -7.79999999999999\n",
      "      - -8.899999999999986\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 22.0\n",
      "      policy2: -1.1999999999999986\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.06875\n",
      "      policy2: -8.831249999999983\n",
      "    policy_reward_min:\n",
      "      policy1: -23.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2061193797272282\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.08467781423377377\n",
      "      mean_inference_ms: 2.2052002600756264\n",
      "      mean_raw_obs_processing_ms: 0.3965909819967729\n",
      "  time_since_restore: 28.27530574798584\n",
      "  time_this_iter_s: 12.202244997024536\n",
      "  time_total_s: 28.27530574798584\n",
      "  timers:\n",
      "    learn_throughput: 457.874\n",
      "    learn_time_ms: 8736.029\n",
      "    synch_weights_time_ms: 4.32\n",
      "    training_iteration_time_ms: 14133.078\n",
      "  timestamp: 1658679460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f370e_00000\n",
      "  warmup_time: 17.984856128692627\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.300000000000008\n",
      "  episode_reward_mean: -3.8039999999999896\n",
      "  episode_reward_min: -32.400000000000034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: b06a13ec4c9d44f2866d39064923d025\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.2336210012435913\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07182275503873825\n",
      "          model: {}\n",
      "          policy_loss: -0.13330787420272827\n",
      "          total_loss: 5.375136375427246\n",
      "          vf_explained_var: 0.08007392287254333\n",
      "          vf_loss: 5.476124286651611\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.2311489582061768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07789967954158783\n",
      "          model: {}\n",
      "          policy_loss: -0.14181260764598846\n",
      "          total_loss: 1.0955662727355957\n",
      "          vf_explained_var: 0.5861777663230896\n",
      "          vf_loss: 1.2023240327835083\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.58125\n",
      "    ram_util_percent: 62.0375\n",
      "  pid: 26939\n",
      "  policy_reward_max:\n",
      "    policy1: 23.5\n",
      "    policy2: -1.199999999999989\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.92\n",
      "    policy2: -8.723999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -23.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18954799426023008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07934829705138272\n",
      "    mean_inference_ms: 2.0277113553095933\n",
      "    mean_raw_obs_processing_ms: 0.3646099691711842\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 15.300000000000008\n",
      "    episode_reward_mean: -3.8039999999999896\n",
      "    episode_reward_min: -32.400000000000034\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - 3.000000000000017\n",
      "      - -30.90000000000004\n",
      "      - -2.999999999999978\n",
      "      - -5.999999999999979\n",
      "      - -4.499999999999991\n",
      "      - -22.500000000000046\n",
      "      - -4.499999999999984\n",
      "      - -18.89999999999999\n",
      "      - -10.49999999999998\n",
      "      - 15.300000000000008\n",
      "      - -17.99999999999999\n",
      "      - 0.900000000000012\n",
      "      - 12.000000000000005\n",
      "      - 1.174060848541103e-14\n",
      "      - -10.799999999999974\n",
      "      - -0.8999999999999921\n",
      "      - -7.799999999999988\n",
      "      - -10.499999999999986\n",
      "      - 0.2999999999999978\n",
      "      - -9.899999999999979\n",
      "      - 2.373101715136272e-14\n",
      "      - 0.5999999999999974\n",
      "      - -3.899999999999992\n",
      "      - 2.100000000000007\n",
      "      - 13.800000000000031\n",
      "      - 3.000000000000008\n",
      "      - 1.499999999999997\n",
      "      - -2.399999999999988\n",
      "      - 3.0000000000000187\n",
      "      - 2.858824288409778e-15\n",
      "      - -10.499999999999986\n",
      "      - -3.2999999999999963\n",
      "      - 2.9999999999999867\n",
      "      - -2.9999999999999942\n",
      "      - -8.99999999999999\n",
      "      - 7.200000000000028\n",
      "      - -10.49999999999999\n",
      "      - -8.999999999999984\n",
      "      - 4.500000000000007\n",
      "      - 8.100000000000017\n",
      "      - 6.600000000000017\n",
      "      - -5.3999999999999915\n",
      "      - 0.9000000000000127\n",
      "      - -3.300000000000004\n",
      "      - 0.6000000000000175\n",
      "      - 10.200000000000022\n",
      "      - -7.499999999999991\n",
      "      - -2.3999999999999906\n",
      "      - 2.6999999999999997\n",
      "      - -7.499999999999989\n",
      "      - -32.400000000000034\n",
      "      - 0.6000000000000083\n",
      "      - -19.499999999999993\n",
      "      - 4.500000000000009\n",
      "      - 10.199999999999994\n",
      "      - -2.699999999999987\n",
      "      - -3.299999999999987\n",
      "      - -5.399999999999973\n",
      "      - -3.899999999999987\n",
      "      - 6.000000000000025\n",
      "      - -7.499999999999988\n",
      "      - -12.599999999999973\n",
      "      - 6.600000000000026\n",
      "      - 4.500000000000018\n",
      "      - 10.200000000000028\n",
      "      - -2.999999999999983\n",
      "      - -30.000000000000025\n",
      "      - 1.5959455978986625e-14\n",
      "      - -17.399999999999984\n",
      "      - -0.29999999999998606\n",
      "      - -7.799999999999983\n",
      "      - -10.499999999999979\n",
      "      - -5.399999999999988\n",
      "      - -13.199999999999994\n",
      "      - -11.99999999999999\n",
      "      - 2.017830347256222e-14\n",
      "      - -16.799999999999997\n",
      "      - -0.5999999999999955\n",
      "      - 1.5000000000000222\n",
      "      - -11.999999999999975\n",
      "      - -16.5\n",
      "      - 4.500000000000009\n",
      "      - -2.9999999999999862\n",
      "      - -11.399999999999975\n",
      "      - -1.1999999999999797\n",
      "      - -10.199999999999978\n",
      "      - 2.7000000000000246\n",
      "      - -11.99999999999997\n",
      "      - -16.199999999999996\n",
      "      - 5.100000000000005\n",
      "      - -21.000000000000007\n",
      "      - -5.399999999999988\n",
      "      - 7.200000000000024\n",
      "      - -0.8999999999999915\n",
      "      - 2.100000000000028\n",
      "      - 2.999999999999976\n",
      "      - 2.628453010800058e-14\n",
      "      - 13.499999999999995\n",
      "      - -6.9\n",
      "      - -4.499999999999991\n",
      "      policy_policy1_reward:\n",
      "      - 13.0\n",
      "      - -22.0\n",
      "      - 7.0\n",
      "      - 4.0\n",
      "      - 5.5\n",
      "      - -12.5\n",
      "      - 5.5\n",
      "      - -10.0\n",
      "      - -6.0\n",
      "      - 22.0\n",
      "      - -8.0\n",
      "      - 6.5\n",
      "      - 22.0\n",
      "      - 10.0\n",
      "      - -3.0\n",
      "      - 8.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      - 10.0\n",
      "      - 9.5\n",
      "      - 5.0\n",
      "      - 11.0\n",
      "      - 20.5\n",
      "      - 13.0\n",
      "      - 11.5\n",
      "      - 6.5\n",
      "      - 13.0\n",
      "      - 10.0\n",
      "      - -0.5\n",
      "      - 4.5\n",
      "      - 13.0\n",
      "      - 7.0\n",
      "      - 1.0\n",
      "      - 15.0\n",
      "      - -0.5\n",
      "      - 1.0\n",
      "      - 14.5\n",
      "      - 17.0\n",
      "      - 15.5\n",
      "      - 3.5\n",
      "      - 6.5\n",
      "      - 4.5\n",
      "      - 9.5\n",
      "      - 18.0\n",
      "      - 2.5\n",
      "      - 6.5\n",
      "      - 10.5\n",
      "      - 2.5\n",
      "      - -23.5\n",
      "      - 9.5\n",
      "      - -9.5\n",
      "      - 14.5\n",
      "      - 18.0\n",
      "      - -1.5\n",
      "      - 4.5\n",
      "      - 3.5\n",
      "      - 5.0\n",
      "      - 16.0\n",
      "      - 2.5\n",
      "      - -7.0\n",
      "      - 15.5\n",
      "      - 14.5\n",
      "      - 18.0\n",
      "      - 7.0\n",
      "      - -20.0\n",
      "      - 10.0\n",
      "      - -8.5\n",
      "      - 7.5\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 3.5\n",
      "      - -12.0\n",
      "      - -2.0\n",
      "      - 10.0\n",
      "      - -9.0\n",
      "      - 5.0\n",
      "      - 11.5\n",
      "      - -2.0\n",
      "      - -6.5\n",
      "      - 14.5\n",
      "      - 7.0\n",
      "      - -2.5\n",
      "      - 5.5\n",
      "      - -3.5\n",
      "      - 10.5\n",
      "      - -2.0\n",
      "      - -9.5\n",
      "      - 8.5\n",
      "      - -11.0\n",
      "      - 3.5\n",
      "      - 15.0\n",
      "      - 8.0\n",
      "      - 11.0\n",
      "      - 13.0\n",
      "      - 10.0\n",
      "      - 23.5\n",
      "      - 2.0\n",
      "      - 5.5\n",
      "      policy_policy2_reward:\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.500000000000002\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999983\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999984\n",
      "      - -6.699999999999994\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999999\n",
      "      - -7.799999999999986\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999982\n",
      "      - -1.1999999999999986\n",
      "      - -7.79999999999999\n",
      "      - -8.899999999999986\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999982\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -7.799999999999981\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -1.199999999999989\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -5.6\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -6.6999999999999815\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -3.399999999999983\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -7.799999999999981\n",
      "      - -8.899999999999984\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 23.5\n",
      "      policy2: -1.199999999999989\n",
      "    policy_reward_mean:\n",
      "      policy1: 4.92\n",
      "      policy2: -8.723999999999984\n",
      "    policy_reward_min:\n",
      "      policy1: -23.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.18954799426023008\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.07934829705138272\n",
      "      mean_inference_ms: 2.0277113553095933\n",
      "      mean_raw_obs_processing_ms: 0.3646099691711842\n",
      "  time_since_restore: 39.792874813079834\n",
      "  time_this_iter_s: 11.517569065093994\n",
      "  time_total_s: 39.792874813079834\n",
      "  timers:\n",
      "    learn_throughput: 484.488\n",
      "    learn_time_ms: 8256.138\n",
      "    synch_weights_time_ms: 3.769\n",
      "    training_iteration_time_ms: 13259.548\n",
      "  timestamp: 1658679471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f370e_00000\n",
      "  warmup_time: 17.984856128692627\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999966\n",
      "  episode_reward_mean: -6.7162499999999925\n",
      "  episode_reward_min: -45.000000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 7930c8da23ce49829b3cecebb3c03c5f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.305110216140747\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028486724942922592\n",
      "          model: {}\n",
      "          policy_loss: -0.0594126358628273\n",
      "          total_loss: 6.84129524230957\n",
      "          vf_explained_var: -0.03111620806157589\n",
      "          vf_loss: 6.892162799835205\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3046021461486816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02951633743941784\n",
      "          model: {}\n",
      "          policy_loss: -0.060261648148298264\n",
      "          total_loss: 1.9316262006759644\n",
      "          vf_explained_var: 0.3391050100326538\n",
      "          vf_loss: 1.9830329418182373\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.42941176470588\n",
      "    ram_util_percent: 61.9764705882353\n",
      "  pid: 26954\n",
      "  policy_reward_max:\n",
      "    policy1: 30.5\n",
      "    policy2: 3.199999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.95\n",
      "    policy2: -8.666249999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -35.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1526779142500076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06729485935444483\n",
      "    mean_inference_ms: 1.6349752181346109\n",
      "    mean_raw_obs_processing_ms: 0.29051517747835165\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 21.599999999999966\n",
      "    episode_reward_mean: -6.7162499999999925\n",
      "    episode_reward_min: -45.000000000000064\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - -45.000000000000064\n",
      "      - 6.600000000000032\n",
      "      - -7.499999999999989\n",
      "      - -1.4999999999999822\n",
      "      - -7.19999999999998\n",
      "      - -9.899999999999974\n",
      "      - -2.9999999999999987\n",
      "      - -14.999999999999973\n",
      "      - -18.899999999999988\n",
      "      - -24.600000000000037\n",
      "      - -24.000000000000007\n",
      "      - 6.300000000000008\n",
      "      - -10.199999999999976\n",
      "      - 8.100000000000014\n",
      "      - -19.500000000000007\n",
      "      - -3.299999999999986\n",
      "      - -19.500000000000007\n",
      "      - -18.300000000000043\n",
      "      - -1.4999999999999791\n",
      "      - 1.840194663316197e-14\n",
      "      - -2.4000000000000026\n",
      "      - -5.999999999999979\n",
      "      - -29.400000000000034\n",
      "      - 6.000000000000021\n",
      "      - -24.300000000000058\n",
      "      - -33.000000000000064\n",
      "      - 4.500000000000026\n",
      "      - -12.899999999999972\n",
      "      - -16.200000000000017\n",
      "      - -27.000000000000014\n",
      "      - -36.00000000000006\n",
      "      - -20.400000000000002\n",
      "      - 7.500000000000025\n",
      "      - -11.399999999999974\n",
      "      - -8.999999999999986\n",
      "      - -3.299999999999994\n",
      "      - -5.999999999999993\n",
      "      - 3.0000000000000187\n",
      "      - -19.5\n",
      "      - 1.4405143744511406e-14\n",
      "      - -17.999999999999993\n",
      "      - 21.599999999999966\n",
      "      - -1.499999999999981\n",
      "      - 3.600000000000025\n",
      "      - 11.99999999999998\n",
      "      - 5.100000000000032\n",
      "      - -6.900000000000002\n",
      "      - -15.899999999999972\n",
      "      - 9.000000000000032\n",
      "      - -11.999999999999988\n",
      "      - -9.599999999999973\n",
      "      - -1.4999999999999822\n",
      "      - 8.100000000000026\n",
      "      - 1.5000000000000209\n",
      "      - -6.299999999999981\n",
      "      - 1.8000000000000278\n",
      "      - -1.4999999999999951\n",
      "      - 10.500000000000014\n",
      "      - -18.900000000000027\n",
      "      - -7.499999999999979\n",
      "      - 11.700000000000028\n",
      "      - -8.699999999999985\n",
      "      - 2.100000000000003\n",
      "      - 1.8000000000000203\n",
      "      - -3.2999999999999785\n",
      "      - 10.20000000000003\n",
      "      - -14.999999999999975\n",
      "      - -19.499999999999993\n",
      "      - -1.7999999999999874\n",
      "      - 8.700000000000033\n",
      "      - -14.999999999999979\n",
      "      - -1.7999999999999834\n",
      "      - 7.500000000000018\n",
      "      - 13.500000000000005\n",
      "      - -9.299999999999994\n",
      "      - -7.499999999999979\n",
      "      - -3.899999999999986\n",
      "      - 2.100000000000011\n",
      "      - -25.500000000000025\n",
      "      - -13.499999999999993\n",
      "      policy_policy1_reward:\n",
      "      - -35.0\n",
      "      - 15.5\n",
      "      - 2.5\n",
      "      - 8.5\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 1.5\n",
      "      - -5.0\n",
      "      - -10.0\n",
      "      - -19.0\n",
      "      - -14.0\n",
      "      - 13.0\n",
      "      - -3.5\n",
      "      - 17.0\n",
      "      - -9.5\n",
      "      - 4.5\n",
      "      - -9.5\n",
      "      - -10.5\n",
      "      - 8.5\n",
      "      - 10.0\n",
      "      - 6.5\n",
      "      - 4.0\n",
      "      - -20.5\n",
      "      - 16.0\n",
      "      - -16.5\n",
      "      - -23.0\n",
      "      - 14.5\n",
      "      - -4.0\n",
      "      - -9.5\n",
      "      - -17.0\n",
      "      - -26.0\n",
      "      - -11.5\n",
      "      - 17.5\n",
      "      - -2.5\n",
      "      - 1.0\n",
      "      - 4.5\n",
      "      - 4.0\n",
      "      - 7.5\n",
      "      - -9.5\n",
      "      - 10.0\n",
      "      - -8.0\n",
      "      - 30.5\n",
      "      - 8.5\n",
      "      - 12.5\n",
      "      - 22.0\n",
      "      - 14.0\n",
      "      - 2.0\n",
      "      - -7.0\n",
      "      - 19.0\n",
      "      - -2.0\n",
      "      - -4.0\n",
      "      - 8.5\n",
      "      - 17.0\n",
      "      - 11.5\n",
      "      - 1.5\n",
      "      - 8.5\n",
      "      - 8.5\n",
      "      - 20.5\n",
      "      - -10.0\n",
      "      - 2.5\n",
      "      - 14.0\n",
      "      - -2.0\n",
      "      - 11.0\n",
      "      - 8.5\n",
      "      - 4.5\n",
      "      - 18.0\n",
      "      - -5.0\n",
      "      - -9.5\n",
      "      - -5.0\n",
      "      - 16.5\n",
      "      - -5.0\n",
      "      - 6.0\n",
      "      - 17.5\n",
      "      - 23.5\n",
      "      - -1.5\n",
      "      - 2.5\n",
      "      - 5.0\n",
      "      - 11.0\n",
      "      - -15.5\n",
      "      - -3.5\n",
      "      policy_policy2_reward:\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -6.699999999999985\n",
      "      - -8.89999999999998\n",
      "      - -4.500000000000001\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999895\n",
      "      - -6.6999999999999815\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999983\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.7999999999999865\n",
      "      - -9.99999999999998\n",
      "      - -4.5000000000000036\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999984\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -7.7999999999999865\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -2.3000000000000043\n",
      "      - -6.699999999999995\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -7.79999999999999\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - 3.199999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 30.5\n",
      "      policy2: 3.199999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 1.95\n",
      "      policy2: -8.666249999999982\n",
      "    policy_reward_min:\n",
      "      policy1: -35.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1526779142500076\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.06729485935444483\n",
      "      mean_inference_ms: 1.6349752181346109\n",
      "      mean_raw_obs_processing_ms: 0.29051517747835165\n",
      "  time_since_restore: 24.212697982788086\n",
      "  time_this_iter_s: 11.67664885520935\n",
      "  time_total_s: 24.212697982788086\n",
      "  timers:\n",
      "    learn_throughput: 515.631\n",
      "    learn_time_ms: 7757.483\n",
      "    synch_weights_time_ms: 3.592\n",
      "    training_iteration_time_ms: 12102.435\n",
      "  timestamp: 1658679471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f370e_00001\n",
      "  warmup_time: 20.60776996612549\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-18-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.49999999999995\n",
      "  episode_reward_mean: -1.8869999999999953\n",
      "  episode_reward_min: -32.400000000000034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: b06a13ec4c9d44f2866d39064923d025\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.209573745727539\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06191510707139969\n",
      "          model: {}\n",
      "          policy_loss: -0.12124519050121307\n",
      "          total_loss: 5.230431079864502\n",
      "          vf_explained_var: 0.15485931932926178\n",
      "          vf_loss: 5.3098835945129395\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.2096030712127686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06190652400255203\n",
      "          model: {}\n",
      "          policy_loss: -0.1266329139471054\n",
      "          total_loss: 1.1399942636489868\n",
      "          vf_explained_var: 0.5589136481285095\n",
      "          vf_loss: 1.2248402833938599\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.91052631578947\n",
      "    ram_util_percent: 62.5578947368421\n",
      "  pid: 26939\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: -1.199999999999989\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.595\n",
      "    policy2: -8.481999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -23.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1735127459337548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07387244230349216\n",
      "    mean_inference_ms: 1.8532553224339705\n",
      "    mean_raw_obs_processing_ms: 0.3347621228883842\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 25.49999999999995\n",
      "    episode_reward_mean: -1.8869999999999953\n",
      "    episode_reward_min: -32.400000000000034\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - 6.600000000000017\n",
      "      - -5.3999999999999915\n",
      "      - 0.9000000000000127\n",
      "      - -3.300000000000004\n",
      "      - 0.6000000000000175\n",
      "      - 10.200000000000022\n",
      "      - -7.499999999999991\n",
      "      - -2.3999999999999906\n",
      "      - 2.6999999999999997\n",
      "      - -7.499999999999989\n",
      "      - -32.400000000000034\n",
      "      - 0.6000000000000083\n",
      "      - -19.499999999999993\n",
      "      - 4.500000000000009\n",
      "      - 10.199999999999994\n",
      "      - -2.699999999999987\n",
      "      - -3.299999999999987\n",
      "      - -5.399999999999973\n",
      "      - -3.899999999999987\n",
      "      - 6.000000000000025\n",
      "      - -7.499999999999988\n",
      "      - -12.599999999999973\n",
      "      - 6.600000000000026\n",
      "      - 4.500000000000018\n",
      "      - 10.200000000000028\n",
      "      - -2.999999999999983\n",
      "      - -30.000000000000025\n",
      "      - 1.5959455978986625e-14\n",
      "      - -17.399999999999984\n",
      "      - -0.29999999999998606\n",
      "      - -7.799999999999983\n",
      "      - -10.499999999999979\n",
      "      - -5.399999999999988\n",
      "      - -13.199999999999994\n",
      "      - -11.99999999999999\n",
      "      - 2.017830347256222e-14\n",
      "      - -16.799999999999997\n",
      "      - -0.5999999999999955\n",
      "      - 1.5000000000000222\n",
      "      - -11.999999999999975\n",
      "      - -16.5\n",
      "      - 4.500000000000009\n",
      "      - -2.9999999999999862\n",
      "      - -11.399999999999975\n",
      "      - -1.1999999999999797\n",
      "      - -10.199999999999978\n",
      "      - 2.7000000000000246\n",
      "      - -11.99999999999997\n",
      "      - -16.199999999999996\n",
      "      - 5.100000000000005\n",
      "      - -21.000000000000007\n",
      "      - -5.399999999999988\n",
      "      - 7.200000000000024\n",
      "      - -0.8999999999999915\n",
      "      - 2.100000000000028\n",
      "      - 2.999999999999976\n",
      "      - 2.628453010800058e-14\n",
      "      - 13.499999999999995\n",
      "      - -6.9\n",
      "      - -4.499999999999991\n",
      "      - -18.29999999999999\n",
      "      - 2.70000000000003\n",
      "      - 20.99999999999993\n",
      "      - 3.6000000000000125\n",
      "      - -15.299999999999986\n",
      "      - 6.000000000000028\n",
      "      - 11.999999999999995\n",
      "      - 3.000000000000008\n",
      "      - -6.899999999999986\n",
      "      - -0.8999999999999946\n",
      "      - 25.49999999999995\n",
      "      - -0.5999999999999829\n",
      "      - -22.50000000000005\n",
      "      - 12.899999999999949\n",
      "      - -7.4999999999999885\n",
      "      - 18.599999999999987\n",
      "      - -9.29999999999998\n",
      "      - -5.999999999999982\n",
      "      - 10.500000000000007\n",
      "      - 14.099999999999945\n",
      "      - 2.5951463200613034e-14\n",
      "      - -4.499999999999973\n",
      "      - -1.4999999999999791\n",
      "      - 15.599999999999918\n",
      "      - 16.499999999999904\n",
      "      - 7.200000000000031\n",
      "      - 8.100000000000014\n",
      "      - -6.899999999999981\n",
      "      - 9.599999999999964\n",
      "      - -1.5000000000000004\n",
      "      - -14.999999999999998\n",
      "      - -1.1999999999999926\n",
      "      - -16.499999999999993\n",
      "      - 3.0000000000000258\n",
      "      - 8.100000000000016\n",
      "      - 15.59999999999994\n",
      "      - 7.500000000000005\n",
      "      - -2.9999999999999947\n",
      "      - 1.484923295436147e-14\n",
      "      - -24.00000000000007\n",
      "      policy_policy1_reward:\n",
      "      - 15.5\n",
      "      - 3.5\n",
      "      - 6.5\n",
      "      - 4.5\n",
      "      - 9.5\n",
      "      - 18.0\n",
      "      - 2.5\n",
      "      - 6.5\n",
      "      - 10.5\n",
      "      - 2.5\n",
      "      - -23.5\n",
      "      - 9.5\n",
      "      - -9.5\n",
      "      - 14.5\n",
      "      - 18.0\n",
      "      - -1.5\n",
      "      - 4.5\n",
      "      - 3.5\n",
      "      - 5.0\n",
      "      - 16.0\n",
      "      - 2.5\n",
      "      - -7.0\n",
      "      - 15.5\n",
      "      - 14.5\n",
      "      - 18.0\n",
      "      - 7.0\n",
      "      - -20.0\n",
      "      - 10.0\n",
      "      - -8.5\n",
      "      - 7.5\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 3.5\n",
      "      - -12.0\n",
      "      - -2.0\n",
      "      - 10.0\n",
      "      - -9.0\n",
      "      - 5.0\n",
      "      - 11.5\n",
      "      - -2.0\n",
      "      - -6.5\n",
      "      - 14.5\n",
      "      - 7.0\n",
      "      - -2.5\n",
      "      - 5.5\n",
      "      - -3.5\n",
      "      - 10.5\n",
      "      - -2.0\n",
      "      - -9.5\n",
      "      - 8.5\n",
      "      - -11.0\n",
      "      - 3.5\n",
      "      - 15.0\n",
      "      - 8.0\n",
      "      - 11.0\n",
      "      - 13.0\n",
      "      - 10.0\n",
      "      - 23.5\n",
      "      - 2.0\n",
      "      - 5.5\n",
      "      - -10.5\n",
      "      - 10.5\n",
      "      - 31.0\n",
      "      - 12.5\n",
      "      - -13.0\n",
      "      - 16.0\n",
      "      - 22.0\n",
      "      - 13.0\n",
      "      - 2.0\n",
      "      - 8.0\n",
      "      - 30.0\n",
      "      - 5.0\n",
      "      - -12.5\n",
      "      - 18.5\n",
      "      - 2.5\n",
      "      - 27.5\n",
      "      - -1.5\n",
      "      - 4.0\n",
      "      - 20.5\n",
      "      - 23.0\n",
      "      - 10.0\n",
      "      - 0.0\n",
      "      - 8.5\n",
      "      - 24.5\n",
      "      - 26.5\n",
      "      - 15.0\n",
      "      - 11.5\n",
      "      - 2.0\n",
      "      - 18.5\n",
      "      - 8.5\n",
      "      - -5.0\n",
      "      - 5.5\n",
      "      - -6.5\n",
      "      - 13.0\n",
      "      - 17.0\n",
      "      - 24.5\n",
      "      - 12.0\n",
      "      - 7.0\n",
      "      - 10.0\n",
      "      - -14.0\n",
      "      policy_policy2_reward:\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999999\n",
      "      - -7.799999999999986\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999982\n",
      "      - -1.1999999999999986\n",
      "      - -7.79999999999999\n",
      "      - -8.899999999999986\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999982\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -7.799999999999981\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -1.199999999999989\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -5.6\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -6.6999999999999815\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -3.399999999999983\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -7.799999999999981\n",
      "      - -8.899999999999984\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -7.799999999999989\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -2.299999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999983\n",
      "      - -4.499999999999998\n",
      "      - -5.6\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999991\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000001\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999984\n",
      "      - -3.4000000000000017\n",
      "      - -8.899999999999983\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.49999999999999\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 31.0\n",
      "      policy2: -1.199999999999989\n",
      "    policy_reward_mean:\n",
      "      policy1: 6.595\n",
      "      policy2: -8.481999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -23.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1735127459337548\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.07387244230349216\n",
      "      mean_inference_ms: 1.8532553224339705\n",
      "      mean_raw_obs_processing_ms: 0.3347621228883842\n",
      "  time_since_restore: 52.72836351394653\n",
      "  time_this_iter_s: 12.9354887008667\n",
      "  time_total_s: 52.72836351394653\n",
      "  timers:\n",
      "    learn_throughput: 474.831\n",
      "    learn_time_ms: 8424.045\n",
      "    synch_weights_time_ms: 5.171\n",
      "    training_iteration_time_ms: 13174.476\n",
      "  timestamp: 1658679484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f370e_00000\n",
      "  warmup_time: 17.984856128692627\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-18-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999966\n",
      "  episode_reward_mean: -3.4379999999999926\n",
      "  episode_reward_min: -36.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 7930c8da23ce49829b3cecebb3c03c5f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2751227617263794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0248244721442461\n",
      "          model: {}\n",
      "          policy_loss: -0.054494041949510574\n",
      "          total_loss: 7.224046230316162\n",
      "          vf_explained_var: -0.08909265697002411\n",
      "          vf_loss: 7.267369747161865\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2752472162246704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0255158469080925\n",
      "          model: {}\n",
      "          policy_loss: -0.06100752204656601\n",
      "          total_loss: 2.2302846908569336\n",
      "          vf_explained_var: 0.286397248506546\n",
      "          vf_loss: 2.2798101902008057\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.78947368421053\n",
      "    ram_util_percent: 62.55263157894737\n",
      "  pid: 26954\n",
      "  policy_reward_max:\n",
      "    policy1: 30.5\n",
      "    policy2: 3.199999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.33\n",
      "    policy2: -8.767999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -26.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15108737543440562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06667021707533413\n",
      "    mean_inference_ms: 1.6103192693273116\n",
      "    mean_raw_obs_processing_ms: 0.2869102268007526\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 21.599999999999966\n",
      "    episode_reward_mean: -3.4379999999999926\n",
      "    episode_reward_min: -36.00000000000006\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - -2.4000000000000026\n",
      "      - -5.999999999999979\n",
      "      - -29.400000000000034\n",
      "      - 6.000000000000021\n",
      "      - -24.300000000000058\n",
      "      - -33.000000000000064\n",
      "      - 4.500000000000026\n",
      "      - -12.899999999999972\n",
      "      - -16.200000000000017\n",
      "      - -27.000000000000014\n",
      "      - -36.00000000000006\n",
      "      - -20.400000000000002\n",
      "      - 7.500000000000025\n",
      "      - -11.399999999999974\n",
      "      - -8.999999999999986\n",
      "      - -3.299999999999994\n",
      "      - -5.999999999999993\n",
      "      - 3.0000000000000187\n",
      "      - -19.5\n",
      "      - 1.4405143744511406e-14\n",
      "      - -17.999999999999993\n",
      "      - 21.599999999999966\n",
      "      - -1.499999999999981\n",
      "      - 3.600000000000025\n",
      "      - 11.99999999999998\n",
      "      - 5.100000000000032\n",
      "      - -6.900000000000002\n",
      "      - -15.899999999999972\n",
      "      - 9.000000000000032\n",
      "      - -11.999999999999988\n",
      "      - -9.599999999999973\n",
      "      - -1.4999999999999822\n",
      "      - 8.100000000000026\n",
      "      - 1.5000000000000209\n",
      "      - -6.299999999999981\n",
      "      - 1.8000000000000278\n",
      "      - -1.4999999999999951\n",
      "      - 10.500000000000014\n",
      "      - -18.900000000000027\n",
      "      - -7.499999999999979\n",
      "      - 11.700000000000028\n",
      "      - -8.699999999999985\n",
      "      - 2.100000000000003\n",
      "      - 1.8000000000000203\n",
      "      - -3.2999999999999785\n",
      "      - 10.20000000000003\n",
      "      - -14.999999999999975\n",
      "      - -19.499999999999993\n",
      "      - -1.7999999999999874\n",
      "      - 8.700000000000033\n",
      "      - -14.999999999999979\n",
      "      - -1.7999999999999834\n",
      "      - 7.500000000000018\n",
      "      - 13.500000000000005\n",
      "      - -9.299999999999994\n",
      "      - -7.499999999999979\n",
      "      - -3.899999999999986\n",
      "      - 2.100000000000011\n",
      "      - -25.500000000000025\n",
      "      - -13.499999999999993\n",
      "      - -15.899999999999974\n",
      "      - -7.499999999999977\n",
      "      - -2.999999999999989\n",
      "      - 1.5000000000000142\n",
      "      - 3.0000000000000133\n",
      "      - 5.999999999999957\n",
      "      - 12.000000000000034\n",
      "      - 1.662558979376172e-14\n",
      "      - 12.600000000000025\n",
      "      - 5.9999999999999485\n",
      "      - 3.0000000000000107\n",
      "      - 0.8999999999999605\n",
      "      - 14.999999999999918\n",
      "      - -16.499999999999986\n",
      "      - 11.100000000000032\n",
      "      - -7.799999999999985\n",
      "      - 9.600000000000032\n",
      "      - 9.000000000000034\n",
      "      - -11.99999999999998\n",
      "      - -2.3999999999999813\n",
      "      - -13.49999999999998\n",
      "      - -2.999999999999989\n",
      "      - -9.29999999999998\n",
      "      - -1.4999999999999865\n",
      "      - -10.199999999999985\n",
      "      - -23.700000000000014\n",
      "      - 14.100000000000001\n",
      "      - 6.000000000000018\n",
      "      - -2.9999999999999787\n",
      "      - -1.7999999999999776\n",
      "      - 3.900000000000023\n",
      "      - -5.3999999999999755\n",
      "      - -21.000000000000007\n",
      "      - 1.500000000000007\n",
      "      - -14.999999999999972\n",
      "      - -2.999999999999982\n",
      "      - 6.000000000000021\n",
      "      - 13.499999999999988\n",
      "      - 12.599999999999946\n",
      "      - 13.799999999999976\n",
      "      policy_policy1_reward:\n",
      "      - 6.5\n",
      "      - 4.0\n",
      "      - -20.5\n",
      "      - 16.0\n",
      "      - -16.5\n",
      "      - -23.0\n",
      "      - 14.5\n",
      "      - -4.0\n",
      "      - -9.5\n",
      "      - -17.0\n",
      "      - -26.0\n",
      "      - -11.5\n",
      "      - 17.5\n",
      "      - -2.5\n",
      "      - 1.0\n",
      "      - 4.5\n",
      "      - 4.0\n",
      "      - 7.5\n",
      "      - -9.5\n",
      "      - 10.0\n",
      "      - -8.0\n",
      "      - 30.5\n",
      "      - 8.5\n",
      "      - 12.5\n",
      "      - 22.0\n",
      "      - 14.0\n",
      "      - 2.0\n",
      "      - -7.0\n",
      "      - 19.0\n",
      "      - -2.0\n",
      "      - -4.0\n",
      "      - 8.5\n",
      "      - 17.0\n",
      "      - 11.5\n",
      "      - 1.5\n",
      "      - 8.5\n",
      "      - 8.5\n",
      "      - 20.5\n",
      "      - -10.0\n",
      "      - 2.5\n",
      "      - 14.0\n",
      "      - -2.0\n",
      "      - 11.0\n",
      "      - 8.5\n",
      "      - 4.5\n",
      "      - 18.0\n",
      "      - -5.0\n",
      "      - -9.5\n",
      "      - -5.0\n",
      "      - 16.5\n",
      "      - -5.0\n",
      "      - 6.0\n",
      "      - 17.5\n",
      "      - 23.5\n",
      "      - -1.5\n",
      "      - 2.5\n",
      "      - 5.0\n",
      "      - 11.0\n",
      "      - -15.5\n",
      "      - -3.5\n",
      "      - -7.0\n",
      "      - 2.5\n",
      "      - 7.0\n",
      "      - 11.5\n",
      "      - 13.0\n",
      "      - 16.0\n",
      "      - 22.0\n",
      "      - 4.5\n",
      "      - 21.5\n",
      "      - 16.0\n",
      "      - 13.0\n",
      "      - 6.5\n",
      "      - 25.0\n",
      "      - -6.5\n",
      "      - 20.0\n",
      "      - 0.0\n",
      "      - 18.5\n",
      "      - 19.0\n",
      "      - -2.0\n",
      "      - 6.5\n",
      "      - -3.5\n",
      "      - 7.0\n",
      "      - -1.5\n",
      "      - 8.5\n",
      "      - -9.0\n",
      "      - -17.0\n",
      "      - 23.0\n",
      "      - 16.0\n",
      "      - 7.0\n",
      "      - 6.0\n",
      "      - 9.5\n",
      "      - 3.5\n",
      "      - -11.0\n",
      "      - 11.5\n",
      "      - -5.0\n",
      "      - 7.0\n",
      "      - 16.0\n",
      "      - 23.5\n",
      "      - 21.5\n",
      "      - 20.5\n",
      "      policy_policy2_reward:\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.7999999999999865\n",
      "      - -9.99999999999998\n",
      "      - -4.5000000000000036\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999984\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -7.7999999999999865\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -2.3000000000000043\n",
      "      - -6.699999999999995\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -7.79999999999999\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - 3.199999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000002\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999984\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -7.7999999999999865\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -1.2000000000000026\n",
      "      - -6.69999999999999\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -5.5999999999999925\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -6.699999999999995\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 30.5\n",
      "      policy2: 3.199999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 5.33\n",
      "      policy2: -8.767999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -26.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15108737543440562\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.06667021707533413\n",
      "      mean_inference_ms: 1.6103192693273116\n",
      "      mean_raw_obs_processing_ms: 0.2869102268007526\n",
      "  time_since_restore: 37.3873827457428\n",
      "  time_this_iter_s: 13.174684762954712\n",
      "  time_total_s: 37.3873827457428\n",
      "  timers:\n",
      "    learn_throughput: 487.776\n",
      "    learn_time_ms: 8200.487\n",
      "    synch_weights_time_ms: 3.816\n",
      "    training_iteration_time_ms: 12457.36\n",
      "  timestamp: 1658679485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f370e_00001\n",
      "  warmup_time: 20.60776996612549\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-18-18\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.49999999999995\n",
      "  episode_reward_mean: -0.4709999999999945\n",
      "  episode_reward_min: -25.500000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: b06a13ec4c9d44f2866d39064923d025\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.193349838256836\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0501253679394722\n",
      "          model: {}\n",
      "          policy_loss: -0.11139199882745743\n",
      "          total_loss: 5.739889621734619\n",
      "          vf_explained_var: 0.11192943900823593\n",
      "          vf_loss: 5.800529956817627\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 1.1927940845489502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.051160939037799835\n",
      "          model: {}\n",
      "          policy_loss: -0.11676229536533356\n",
      "          total_loss: 1.5815024375915527\n",
      "          vf_explained_var: 0.4954671263694763\n",
      "          vf_loss: 1.6464641094207764\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.00526315789473\n",
      "    ram_util_percent: 62.247368421052634\n",
      "  pid: 26939\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: -0.09999999999999842\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.78\n",
      "    policy2: -8.250999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16808905866166113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07209508331667307\n",
      "    mean_inference_ms: 1.7932501935732938\n",
      "    mean_raw_obs_processing_ms: 0.32479407154435436\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 25.49999999999995\n",
      "    episode_reward_mean: -0.4709999999999945\n",
      "    episode_reward_min: -25.500000000000007\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - -16.5\n",
      "      - 4.500000000000009\n",
      "      - -2.9999999999999862\n",
      "      - -11.399999999999975\n",
      "      - -1.1999999999999797\n",
      "      - -10.199999999999978\n",
      "      - 2.7000000000000246\n",
      "      - -11.99999999999997\n",
      "      - -16.199999999999996\n",
      "      - 5.100000000000005\n",
      "      - -21.000000000000007\n",
      "      - -5.399999999999988\n",
      "      - 7.200000000000024\n",
      "      - -0.8999999999999915\n",
      "      - 2.100000000000028\n",
      "      - 2.999999999999976\n",
      "      - 2.628453010800058e-14\n",
      "      - 13.499999999999995\n",
      "      - -6.9\n",
      "      - -4.499999999999991\n",
      "      - -18.29999999999999\n",
      "      - 2.70000000000003\n",
      "      - 20.99999999999993\n",
      "      - 3.6000000000000125\n",
      "      - -15.299999999999986\n",
      "      - 6.000000000000028\n",
      "      - 11.999999999999995\n",
      "      - 3.000000000000008\n",
      "      - -6.899999999999986\n",
      "      - -0.8999999999999946\n",
      "      - 25.49999999999995\n",
      "      - -0.5999999999999829\n",
      "      - -22.50000000000005\n",
      "      - 12.899999999999949\n",
      "      - -7.4999999999999885\n",
      "      - 18.599999999999987\n",
      "      - -9.29999999999998\n",
      "      - -5.999999999999982\n",
      "      - 10.500000000000007\n",
      "      - 14.099999999999945\n",
      "      - 2.5951463200613034e-14\n",
      "      - -4.499999999999973\n",
      "      - -1.4999999999999791\n",
      "      - 15.599999999999918\n",
      "      - 16.499999999999904\n",
      "      - 7.200000000000031\n",
      "      - 8.100000000000014\n",
      "      - -6.899999999999981\n",
      "      - 9.599999999999964\n",
      "      - -1.5000000000000004\n",
      "      - -14.999999999999998\n",
      "      - -1.1999999999999926\n",
      "      - -16.499999999999993\n",
      "      - 3.0000000000000258\n",
      "      - 8.100000000000016\n",
      "      - 15.59999999999994\n",
      "      - 7.500000000000005\n",
      "      - -2.9999999999999947\n",
      "      - 1.484923295436147e-14\n",
      "      - -24.00000000000007\n",
      "      - -5.999999999999986\n",
      "      - 9.000000000000023\n",
      "      - -11.99999999999998\n",
      "      - -0.9000000000000282\n",
      "      - -8.399999999999977\n",
      "      - 0.600000000000006\n",
      "      - 10.500000000000028\n",
      "      - 10.500000000000025\n",
      "      - -8.999999999999982\n",
      "      - 1.3850032232198828e-14\n",
      "      - -3.8999999999999946\n",
      "      - 2.1000000000000174\n",
      "      - -25.500000000000007\n",
      "      - 2.095545958979983e-14\n",
      "      - -7.499999999999979\n",
      "      - 5.10000000000003\n",
      "      - 11.40000000000002\n",
      "      - 15.000000000000018\n",
      "      - 2.9999999999999964\n",
      "      - -4.499999999999982\n",
      "      - 13.500000000000016\n",
      "      - 3.900000000000025\n",
      "      - -19.499999999999986\n",
      "      - 22.499999999999932\n",
      "      - -5.099999999999978\n",
      "      - -5.999999999999988\n",
      "      - 0.3000000000000209\n",
      "      - 2.4000000000000115\n",
      "      - -12.899999999999979\n",
      "      - -20.999999999999996\n",
      "      - 8.700000000000014\n",
      "      - -1.500000000000004\n",
      "      - 9.000000000000021\n",
      "      - 5.10000000000003\n",
      "      - -1.4999999999999756\n",
      "      - -10.799999999999983\n",
      "      - -14.999999999999986\n",
      "      - -5.999999999999987\n",
      "      - 4.200000000000015\n",
      "      - 4.5000000000000195\n",
      "      policy_policy1_reward:\n",
      "      - -6.5\n",
      "      - 14.5\n",
      "      - 7.0\n",
      "      - -2.5\n",
      "      - 5.5\n",
      "      - -3.5\n",
      "      - 10.5\n",
      "      - -2.0\n",
      "      - -9.5\n",
      "      - 8.5\n",
      "      - -11.0\n",
      "      - 3.5\n",
      "      - 15.0\n",
      "      - 8.0\n",
      "      - 11.0\n",
      "      - 13.0\n",
      "      - 10.0\n",
      "      - 23.5\n",
      "      - 2.0\n",
      "      - 5.5\n",
      "      - -10.5\n",
      "      - 10.5\n",
      "      - 31.0\n",
      "      - 12.5\n",
      "      - -13.0\n",
      "      - 16.0\n",
      "      - 22.0\n",
      "      - 13.0\n",
      "      - 2.0\n",
      "      - 8.0\n",
      "      - 30.0\n",
      "      - 5.0\n",
      "      - -12.5\n",
      "      - 18.5\n",
      "      - 2.5\n",
      "      - 27.5\n",
      "      - -1.5\n",
      "      - 4.0\n",
      "      - 20.5\n",
      "      - 23.0\n",
      "      - 10.0\n",
      "      - 0.0\n",
      "      - 8.5\n",
      "      - 24.5\n",
      "      - 26.5\n",
      "      - 15.0\n",
      "      - 11.5\n",
      "      - 2.0\n",
      "      - 18.5\n",
      "      - 8.5\n",
      "      - -5.0\n",
      "      - 5.5\n",
      "      - -6.5\n",
      "      - 13.0\n",
      "      - 17.0\n",
      "      - 24.5\n",
      "      - 12.0\n",
      "      - 7.0\n",
      "      - 10.0\n",
      "      - -14.0\n",
      "      - 4.0\n",
      "      - 19.0\n",
      "      - -2.0\n",
      "      - 8.0\n",
      "      - 0.5\n",
      "      - 4.0\n",
      "      - 20.5\n",
      "      - 15.0\n",
      "      - -4.5\n",
      "      - 10.0\n",
      "      - 5.0\n",
      "      - 11.0\n",
      "      - -15.5\n",
      "      - 4.5\n",
      "      - 2.5\n",
      "      - 14.0\n",
      "      - 17.0\n",
      "      - 19.5\n",
      "      - 13.0\n",
      "      - 5.5\n",
      "      - 23.5\n",
      "      - 4.0\n",
      "      - -9.5\n",
      "      - 27.0\n",
      "      - 0.5\n",
      "      - 4.0\n",
      "      - 1.5\n",
      "      - 8.0\n",
      "      - -4.0\n",
      "      - -11.0\n",
      "      - 16.5\n",
      "      - 8.5\n",
      "      - 19.0\n",
      "      - 14.0\n",
      "      - 8.5\n",
      "      - -8.5\n",
      "      - -5.0\n",
      "      - 4.0\n",
      "      - 12.0\n",
      "      - 14.5\n",
      "      policy_policy2_reward:\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -6.6999999999999815\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -3.399999999999983\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -7.799999999999981\n",
      "      - -8.899999999999984\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -7.799999999999989\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -2.299999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999983\n",
      "      - -4.499999999999998\n",
      "      - -5.6\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999991\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000001\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999984\n",
      "      - -3.4000000000000017\n",
      "      - -8.899999999999983\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.49999999999999\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -3.4000000000000004\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000004\n",
      "      - -4.499999999999986\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -4.499999999999989\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -5.599999999999982\n",
      "      - -4.5000000000000036\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -0.09999999999999842\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000003\n",
      "      - -5.6\n",
      "      - -9.99999999999998\n",
      "      - -1.1999999999999973\n",
      "      - -5.599999999999986\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -2.300000000000004\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 31.0\n",
      "      policy2: -0.09999999999999842\n",
      "    policy_reward_mean:\n",
      "      policy1: 7.78\n",
      "      policy2: -8.250999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -15.5\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.16808905866166113\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.07209508331667307\n",
      "      mean_inference_ms: 1.7932501935732938\n",
      "      mean_raw_obs_processing_ms: 0.32479407154435436\n",
      "  time_since_restore: 66.1827187538147\n",
      "  time_this_iter_s: 13.454355239868164\n",
      "  time_total_s: 66.1827187538147\n",
      "  timers:\n",
      "    learn_throughput: 473.677\n",
      "    learn_time_ms: 8444.565\n",
      "    synch_weights_time_ms: 4.921\n",
      "    training_iteration_time_ms: 13229.151\n",
      "  timestamp: 1658679498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f370e_00000\n",
      "  warmup_time: 17.984856128692627\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-18-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.49999999999996\n",
      "  episode_reward_mean: 0.09000000000000907\n",
      "  episode_reward_min: -25.500000000000025\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 7930c8da23ce49829b3cecebb3c03c5f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2465245723724365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020220668986439705\n",
      "          model: {}\n",
      "          policy_loss: -0.0520910881459713\n",
      "          total_loss: 6.320826053619385\n",
      "          vf_explained_var: 0.11014227569103241\n",
      "          vf_loss: 6.3592681884765625\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.240596055984497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019733281806111336\n",
      "          model: {}\n",
      "          policy_loss: -0.05178985744714737\n",
      "          total_loss: 2.2816340923309326\n",
      "          vf_explained_var: 0.24780772626399994\n",
      "          vf_loss: 2.320103883743286\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.078947368421055\n",
      "    ram_util_percent: 62.231578947368426\n",
      "  pid: 26954\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 3.199999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.55\n",
      "    policy2: -8.459999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1523011802678452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06712996931537121\n",
      "    mean_inference_ms: 1.6142894372272958\n",
      "    mean_raw_obs_processing_ms: 0.28959553952079603\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.49999999999996\n",
      "    episode_reward_mean: 0.09000000000000907\n",
      "    episode_reward_min: -25.500000000000025\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - 11.700000000000028\n",
      "      - -8.699999999999985\n",
      "      - 2.100000000000003\n",
      "      - 1.8000000000000203\n",
      "      - -3.2999999999999785\n",
      "      - 10.20000000000003\n",
      "      - -14.999999999999975\n",
      "      - -19.499999999999993\n",
      "      - -1.7999999999999874\n",
      "      - 8.700000000000033\n",
      "      - -14.999999999999979\n",
      "      - -1.7999999999999834\n",
      "      - 7.500000000000018\n",
      "      - 13.500000000000005\n",
      "      - -9.299999999999994\n",
      "      - -7.499999999999979\n",
      "      - -3.899999999999986\n",
      "      - 2.100000000000011\n",
      "      - -25.500000000000025\n",
      "      - -13.499999999999993\n",
      "      - -15.899999999999974\n",
      "      - -7.499999999999977\n",
      "      - -2.999999999999989\n",
      "      - 1.5000000000000142\n",
      "      - 3.0000000000000133\n",
      "      - 5.999999999999957\n",
      "      - 12.000000000000034\n",
      "      - 1.662558979376172e-14\n",
      "      - 12.600000000000025\n",
      "      - 5.9999999999999485\n",
      "      - 3.0000000000000107\n",
      "      - 0.8999999999999605\n",
      "      - 14.999999999999918\n",
      "      - -16.499999999999986\n",
      "      - 11.100000000000032\n",
      "      - -7.799999999999985\n",
      "      - 9.600000000000032\n",
      "      - 9.000000000000034\n",
      "      - -11.99999999999998\n",
      "      - -2.3999999999999813\n",
      "      - -13.49999999999998\n",
      "      - -2.999999999999989\n",
      "      - -9.29999999999998\n",
      "      - -1.4999999999999865\n",
      "      - -10.199999999999985\n",
      "      - -23.700000000000014\n",
      "      - 14.100000000000001\n",
      "      - 6.000000000000018\n",
      "      - -2.9999999999999787\n",
      "      - -1.7999999999999776\n",
      "      - 3.900000000000023\n",
      "      - -5.3999999999999755\n",
      "      - -21.000000000000007\n",
      "      - 1.500000000000007\n",
      "      - -14.999999999999972\n",
      "      - -2.999999999999982\n",
      "      - 6.000000000000021\n",
      "      - 13.499999999999988\n",
      "      - 12.599999999999946\n",
      "      - 13.799999999999976\n",
      "      - -8.399999999999988\n",
      "      - 0.2999999999999996\n",
      "      - 2.373101715136272e-14\n",
      "      - -8.099999999999982\n",
      "      - 19.49999999999996\n",
      "      - -10.799999999999992\n",
      "      - 11.399999999999965\n",
      "      - 3.000000000000027\n",
      "      - 14.999999999999993\n",
      "      - -3.2999999999999967\n",
      "      - -3.8999999999999724\n",
      "      - 4.50000000000003\n",
      "      - -5.999999999999976\n",
      "      - 1.2000000000000097\n",
      "      - 5.999999999999986\n",
      "      - 7.499999999999993\n",
      "      - 5.700000000000028\n",
      "      - 2.1000000000000156\n",
      "      - -2.999999999999979\n",
      "      - 7.199999999999967\n",
      "      - 6.00000000000002\n",
      "      - 3.000000000000012\n",
      "      - -5.399999999999988\n",
      "      - 6.00000000000003\n",
      "      - -10.199999999999974\n",
      "      - 5.700000000000003\n",
      "      - 1.5000000000000213\n",
      "      - 8.699999999999966\n",
      "      - 13.800000000000002\n",
      "      - 5.0999999999999925\n",
      "      - 8.39999999999996\n",
      "      - -12.89999999999998\n",
      "      - 1.840194663316197e-14\n",
      "      - -2.399999999999978\n",
      "      - 3.000000000000015\n",
      "      - 5.7000000000000295\n",
      "      - -1.4999999999999796\n",
      "      - -11.399999999999974\n",
      "      - 12.600000000000028\n",
      "      - 15.00000000000001\n",
      "      policy_policy1_reward:\n",
      "      - 14.0\n",
      "      - -2.0\n",
      "      - 11.0\n",
      "      - 8.5\n",
      "      - 4.5\n",
      "      - 18.0\n",
      "      - -5.0\n",
      "      - -9.5\n",
      "      - -5.0\n",
      "      - 16.5\n",
      "      - -5.0\n",
      "      - 6.0\n",
      "      - 17.5\n",
      "      - 23.5\n",
      "      - -1.5\n",
      "      - 2.5\n",
      "      - 5.0\n",
      "      - 11.0\n",
      "      - -15.5\n",
      "      - -3.5\n",
      "      - -7.0\n",
      "      - 2.5\n",
      "      - 7.0\n",
      "      - 11.5\n",
      "      - 13.0\n",
      "      - 16.0\n",
      "      - 22.0\n",
      "      - 4.5\n",
      "      - 21.5\n",
      "      - 16.0\n",
      "      - 13.0\n",
      "      - 6.5\n",
      "      - 25.0\n",
      "      - -6.5\n",
      "      - 20.0\n",
      "      - 0.0\n",
      "      - 18.5\n",
      "      - 19.0\n",
      "      - -2.0\n",
      "      - 6.5\n",
      "      - -3.5\n",
      "      - 7.0\n",
      "      - -1.5\n",
      "      - 8.5\n",
      "      - -9.0\n",
      "      - -17.0\n",
      "      - 23.0\n",
      "      - 16.0\n",
      "      - 7.0\n",
      "      - 6.0\n",
      "      - 9.5\n",
      "      - 3.5\n",
      "      - -11.0\n",
      "      - 11.5\n",
      "      - -5.0\n",
      "      - 7.0\n",
      "      - 16.0\n",
      "      - 23.5\n",
      "      - 21.5\n",
      "      - 20.5\n",
      "      - 0.5\n",
      "      - 7.0\n",
      "      - 10.0\n",
      "      - -2.5\n",
      "      - 29.5\n",
      "      - -3.0\n",
      "      - 17.0\n",
      "      - 13.0\n",
      "      - 19.5\n",
      "      - -1.0\n",
      "      - 5.0\n",
      "      - 14.5\n",
      "      - 4.0\n",
      "      - 9.0\n",
      "      - 16.0\n",
      "      - 17.5\n",
      "      - 13.5\n",
      "      - 11.0\n",
      "      - 7.0\n",
      "      - 15.0\n",
      "      - 16.0\n",
      "      - 13.0\n",
      "      - 3.5\n",
      "      - 16.0\n",
      "      - -3.5\n",
      "      - 13.5\n",
      "      - 11.5\n",
      "      - 16.5\n",
      "      - 20.5\n",
      "      - 14.0\n",
      "      - 14.0\n",
      "      - -4.0\n",
      "      - 10.0\n",
      "      - 6.5\n",
      "      - 7.5\n",
      "      - 13.5\n",
      "      - 8.5\n",
      "      - -2.5\n",
      "      - 21.5\n",
      "      - 25.0\n",
      "      policy_policy2_reward:\n",
      "      - -2.3000000000000043\n",
      "      - -6.699999999999995\n",
      "      - -8.899999999999986\n",
      "      - -6.6999999999999815\n",
      "      - -7.79999999999999\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - 3.199999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000002\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999984\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -7.7999999999999865\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -1.2000000000000026\n",
      "      - -6.69999999999999\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -5.5999999999999925\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -6.699999999999995\n",
      "      - -8.89999999999998\n",
      "      - -6.699999999999988\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999994\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -5.599999999999986\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000002\n",
      "      - -2.2999999999999887\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -6.6999999999999815\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999996\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.499999999999995\n",
      "      - -7.799999999999983\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 3.199999999999998\n",
      "    policy_reward_mean:\n",
      "      policy1: 8.55\n",
      "      policy2: -8.459999999999985\n",
      "    policy_reward_min:\n",
      "      policy1: -17.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1523011802678452\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.06712996931537121\n",
      "      mean_inference_ms: 1.6142894372272958\n",
      "      mean_raw_obs_processing_ms: 0.28959553952079603\n",
      "  time_since_restore: 50.73679065704346\n",
      "  time_this_iter_s: 13.34940791130066\n",
      "  time_total_s: 50.73679065704346\n",
      "  timers:\n",
      "    learn_throughput: 484.268\n",
      "    learn_time_ms: 8259.886\n",
      "    synch_weights_time_ms: 3.565\n",
      "    training_iteration_time_ms: 12678.89\n",
      "  timestamp: 1658679498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f370e_00001\n",
      "  warmup_time: 20.60776996612549\n",
      "  \n",
      "Result for PPO_MultiAgentArena_f370e_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-24_18-18-28\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.49999999999996\n",
      "  episode_reward_mean: 1.659000000000009\n",
      "  episode_reward_min: -23.700000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 7930c8da23ce49829b3cecebb3c03c5f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2369275093078613\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015599277801811695\n",
      "          model: {}\n",
      "          policy_loss: -0.0497160404920578\n",
      "          total_loss: 6.410096168518066\n",
      "          vf_explained_var: 0.11773829907178879\n",
      "          vf_loss: 6.44401741027832\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2108319997787476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019512496888637543\n",
      "          model: {}\n",
      "          policy_loss: -0.050665903836488724\n",
      "          total_loss: 2.230023145675659\n",
      "          vf_explained_var: 0.24603311717510223\n",
      "          vf_loss: 2.2675180435180664\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  num_steps_trained_this_iter: 4000\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.392857142857146\n",
      "    ram_util_percent: 61.50714285714285\n",
      "  pid: 26954\n",
      "  policy_reward_max:\n",
      "    policy1: 29.5\n",
      "    policy2: 0.9999999999999961\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.745\n",
      "    policy2: -8.085999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1519751199493472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06696337719064735\n",
      "    mean_inference_ms: 1.6037970002680704\n",
      "    mean_raw_obs_processing_ms: 0.289711623652398\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 100.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 19.49999999999996\n",
      "    episode_reward_mean: 1.659000000000009\n",
      "    episode_reward_min: -23.700000000000014\n",
      "    episodes_this_iter: 40\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      - 100\n",
      "      episode_reward:\n",
      "      - -13.49999999999998\n",
      "      - -2.999999999999989\n",
      "      - -9.29999999999998\n",
      "      - -1.4999999999999865\n",
      "      - -10.199999999999985\n",
      "      - -23.700000000000014\n",
      "      - 14.100000000000001\n",
      "      - 6.000000000000018\n",
      "      - -2.9999999999999787\n",
      "      - -1.7999999999999776\n",
      "      - 3.900000000000023\n",
      "      - -5.3999999999999755\n",
      "      - -21.000000000000007\n",
      "      - 1.500000000000007\n",
      "      - -14.999999999999972\n",
      "      - -2.999999999999982\n",
      "      - 6.000000000000021\n",
      "      - 13.499999999999988\n",
      "      - 12.599999999999946\n",
      "      - 13.799999999999976\n",
      "      - -8.399999999999988\n",
      "      - 0.2999999999999996\n",
      "      - 2.373101715136272e-14\n",
      "      - -8.099999999999982\n",
      "      - 19.49999999999996\n",
      "      - -10.799999999999992\n",
      "      - 11.399999999999965\n",
      "      - 3.000000000000027\n",
      "      - 14.999999999999993\n",
      "      - -3.2999999999999967\n",
      "      - -3.8999999999999724\n",
      "      - 4.50000000000003\n",
      "      - -5.999999999999976\n",
      "      - 1.2000000000000097\n",
      "      - 5.999999999999986\n",
      "      - 7.499999999999993\n",
      "      - 5.700000000000028\n",
      "      - 2.1000000000000156\n",
      "      - -2.999999999999979\n",
      "      - 7.199999999999967\n",
      "      - 6.00000000000002\n",
      "      - 3.000000000000012\n",
      "      - -5.399999999999988\n",
      "      - 6.00000000000003\n",
      "      - -10.199999999999974\n",
      "      - 5.700000000000003\n",
      "      - 1.5000000000000213\n",
      "      - 8.699999999999966\n",
      "      - 13.800000000000002\n",
      "      - 5.0999999999999925\n",
      "      - 8.39999999999996\n",
      "      - -12.89999999999998\n",
      "      - 1.840194663316197e-14\n",
      "      - -2.399999999999978\n",
      "      - 3.000000000000015\n",
      "      - 5.7000000000000295\n",
      "      - -1.4999999999999796\n",
      "      - -11.399999999999974\n",
      "      - 12.600000000000028\n",
      "      - 15.00000000000001\n",
      "      - 4.500000000000025\n",
      "      - -0.29999999999997196\n",
      "      - 4.200000000000022\n",
      "      - -6.599999999999987\n",
      "      - 4.4999999999999805\n",
      "      - 4.200000000000023\n",
      "      - -10.799999999999995\n",
      "      - 14.099999999999984\n",
      "      - 12.000000000000032\n",
      "      - 9.900000000000011\n",
      "      - 5.399999999999951\n",
      "      - -8.99999999999998\n",
      "      - 15.000000000000018\n",
      "      - 11.10000000000003\n",
      "      - 12.000000000000012\n",
      "      - 7.500000000000028\n",
      "      - -2.99999999999998\n",
      "      - -6.8999999999999755\n",
      "      - 0.3000000000000298\n",
      "      - -2.099999999999996\n",
      "      - 4.800000000000017\n",
      "      - 9.600000000000026\n",
      "      - 3.9000000000000257\n",
      "      - 4.200000000000026\n",
      "      - 17.099999999999923\n",
      "      - 13.49999999999998\n",
      "      - 2.0999999999999917\n",
      "      - -8.099999999999982\n",
      "      - -5.400000000000002\n",
      "      - -2.099999999999976\n",
      "      - -7.199999999999989\n",
      "      - 13.499999999999947\n",
      "      - 3.600000000000015\n",
      "      - 4.500000000000025\n",
      "      - 5.400000000000029\n",
      "      - -8.999999999999995\n",
      "      - 2.100000000000022\n",
      "      - -1.4999999999999747\n",
      "      - 7.500000000000009\n",
      "      - -10.199999999999976\n",
      "      policy_policy1_reward:\n",
      "      - -3.5\n",
      "      - 7.0\n",
      "      - -1.5\n",
      "      - 8.5\n",
      "      - -9.0\n",
      "      - -17.0\n",
      "      - 23.0\n",
      "      - 16.0\n",
      "      - 7.0\n",
      "      - 6.0\n",
      "      - 9.5\n",
      "      - 3.5\n",
      "      - -11.0\n",
      "      - 11.5\n",
      "      - -5.0\n",
      "      - 7.0\n",
      "      - 16.0\n",
      "      - 23.5\n",
      "      - 21.5\n",
      "      - 20.5\n",
      "      - 0.5\n",
      "      - 7.0\n",
      "      - 10.0\n",
      "      - -2.5\n",
      "      - 29.5\n",
      "      - -3.0\n",
      "      - 17.0\n",
      "      - 13.0\n",
      "      - 19.5\n",
      "      - -1.0\n",
      "      - 5.0\n",
      "      - 14.5\n",
      "      - 4.0\n",
      "      - 9.0\n",
      "      - 16.0\n",
      "      - 17.5\n",
      "      - 13.5\n",
      "      - 11.0\n",
      "      - 7.0\n",
      "      - 15.0\n",
      "      - 16.0\n",
      "      - 13.0\n",
      "      - 3.5\n",
      "      - 16.0\n",
      "      - -3.5\n",
      "      - 13.5\n",
      "      - 11.5\n",
      "      - 16.5\n",
      "      - 20.5\n",
      "      - 14.0\n",
      "      - 14.0\n",
      "      - -4.0\n",
      "      - 10.0\n",
      "      - 6.5\n",
      "      - 7.5\n",
      "      - 13.5\n",
      "      - 8.5\n",
      "      - -2.5\n",
      "      - 21.5\n",
      "      - 25.0\n",
      "      - 14.5\n",
      "      - 7.5\n",
      "      - 12.0\n",
      "      - -1.0\n",
      "      - 3.5\n",
      "      - 6.5\n",
      "      - -3.0\n",
      "      - 23.0\n",
      "      - 22.0\n",
      "      - 15.5\n",
      "      - 11.0\n",
      "      - 1.0\n",
      "      - 19.5\n",
      "      - 20.0\n",
      "      - 22.0\n",
      "      - 17.5\n",
      "      - 7.0\n",
      "      - 2.0\n",
      "      - 7.0\n",
      "      - 3.5\n",
      "      - 11.5\n",
      "      - 18.5\n",
      "      - 9.5\n",
      "      - 12.0\n",
      "      - 26.0\n",
      "      - 23.5\n",
      "      - 11.0\n",
      "      - -2.5\n",
      "      - 3.5\n",
      "      - 3.5\n",
      "      - -0.5\n",
      "      - 23.5\n",
      "      - 12.5\n",
      "      - 9.0\n",
      "      - 11.0\n",
      "      - 1.0\n",
      "      - 11.0\n",
      "      - 8.5\n",
      "      - 17.5\n",
      "      - -3.5\n",
      "      policy_policy2_reward:\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -1.2000000000000026\n",
      "      - -6.69999999999999\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -5.5999999999999925\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999984\n",
      "      - -6.699999999999995\n",
      "      - -8.89999999999998\n",
      "      - -6.699999999999988\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999994\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -5.599999999999986\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000002\n",
      "      - -2.2999999999999887\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999815\n",
      "      - -7.799999999999981\n",
      "      - -9.99999999999998\n",
      "      - -7.799999999999981\n",
      "      - -6.6999999999999815\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999996\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.499999999999995\n",
      "      - -7.799999999999983\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -8.899999999999986\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -7.79999999999999\n",
      "      - -7.79999999999999\n",
      "      - -5.599999999999982\n",
      "      - 0.9999999999999961\n",
      "      - -2.2999999999999954\n",
      "      - -7.79999999999999\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -5.599999999999994\n",
      "      - -5.6\n",
      "      - -9.99999999999998\n",
      "      - -4.500000000000002\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -8.899999999999986\n",
      "      - -6.69999999999999\n",
      "      - -5.599999999999984\n",
      "      - -6.699999999999986\n",
      "      - -8.899999999999983\n",
      "      - -5.599999999999989\n",
      "      - -7.79999999999999\n",
      "      - -8.899999999999983\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999994\n",
      "      - -8.89999999999998\n",
      "      - -5.599999999999988\n",
      "      - -6.699999999999986\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -4.5\n",
      "      - -5.599999999999982\n",
      "      - -9.99999999999998\n",
      "      - -8.89999999999998\n",
      "      - -9.99999999999998\n",
      "      - -9.99999999999998\n",
      "      - -6.6999999999999895\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      policy1: 29.5\n",
      "      policy2: 0.9999999999999961\n",
      "    policy_reward_mean:\n",
      "      policy1: 9.745\n",
      "      policy2: -8.085999999999986\n",
      "    policy_reward_min:\n",
      "      policy1: -17.0\n",
      "      policy2: -9.99999999999998\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1519751199493472\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.06696337719064735\n",
      "      mean_inference_ms: 1.6037970002680704\n",
      "      mean_raw_obs_processing_ms: 0.289711623652398\n",
      "  time_since_restore: 60.157792806625366\n",
      "  time_this_iter_s: 9.42100214958191\n",
      "  time_total_s: 60.157792806625366\n",
      "  timers:\n",
      "    learn_throughput: 513.138\n",
      "    learn_time_ms: 7795.167\n",
      "    synch_weights_time_ms: 3.514\n",
      "    training_iteration_time_ms: 12026.009\n",
      "  timestamp: 1658679508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f370e_00001\n",
      "  warmup_time: 20.60776996612549\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 18:18:28,810\tINFO tune.py:737 -- Total run time: 129.61 seconds (129.23 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n",
      "Best checkpoint:  <ray.air.checkpoint.Checkpoint object at 0x7f9daa29adc0>\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RAY TUNE API .run() UNTIL STOP CONDITION\n",
    "#\n",
    "# Note about Ray Tune verbosity.\n",
    "# Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "# 0 = silent\n",
    "# 1 = only status updates, no logging messages\n",
    "# 2 = status and brief trial results, includes logging messages\n",
    "# 3 = status and detailed trial results, includes logging messages\n",
    "# Defaults to 3.\n",
    "###############\n",
    "\n",
    "\n",
    "experiment_results = tune.run(\n",
    "    # Registered Algo appreviation.\n",
    "    \"PPO\",\n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\n",
    "        \"training_iteration\": 3,     # stop after 3 training iterations (calls to `Algorithm.train()`)\n",
    "        #\"episode_reward_mean\": 400, # stop if average (sum of) rewards in an episode is 400 or more\n",
    "        #\"timesteps_total\": 100000,  # stop if reached 100,000 sampling timesteps\n",
    "    },\n",
    "    # training config params (translated into a python dict!)\n",
    "    config=config.to_dict(),              \n",
    "    # redirect logs instead of default ~/ray_results/\n",
    "    local_dir=\"results\",\n",
    "    # Set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True,\n",
    "    # Reduce logging messages.\n",
    "    verbose=3,\n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "print(\"Best checkpoint: \", experiment_results.best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details behind Ray RLlib resource allocation <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "When running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HomeWork\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises <a ></a>\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    " * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
