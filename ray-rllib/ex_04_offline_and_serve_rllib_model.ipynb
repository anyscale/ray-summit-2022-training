{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04. (Take-home) Advanced Topic: Introduction to Offline RL and Serving your RLlib Model using Ray Serve API\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn:\n",
    " * [What's offline RL (aka \"batch RL\")?](#offline_rl)\n",
    " * [How to configure RLlib for offline RL](#offline_rl_with_rllib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's offline RL (aka \"batch RL\")? <a class=\"anchor\" id=\"offline_rl\"></a>\n",
    "\n",
    "So far, we have dealt with a so-called \"online\" setting for RL: We had direct influence over a live-running environment (a simulator). We were able to send arbitrary actions to this simulator and collect its responses (rewards and observations), thereby learning \"as we go\". This setup is called \"online\" RL:\n",
    "\n",
    "<img src=\"images/online_rl.png\" width=\"80%\"></img>\n",
    "\n",
    "However, often and especially in real-life industry settings, we are faced with the problem of not having a simulator at hand.\n",
    "In this case, we need to fall back to offline RL:\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=\"70%\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Due to the dynamic nature of adversarial multi-agent scenarios, we will cover the topic of\n",
    "of offline RL here only for the single-agent case.\n",
    "Research on multi-agent offline RL is bleeding edge and not well explored by RLlib thus far (see references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offline RL comes in two flavours:\n",
    "* Pure imitation learning (the agent will try to imitate 100% the actions/behavior that it finds in the offline data). This setup is nothing else but supervised learning with a `-log(p)` loss function.\n",
    "* Imitation learning plus improvement over the recorded, offline behavior.\n",
    "\n",
    "In fact, modern offline RL algorithms are capable of performing imitation plus improvement, such that they can learn to perfectly play e.g. the CartPole environment, when only behavioral data from a randomly acting agent is available! We'll explore this right now using RLlib's new CRR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.crr.crr.CRRConfig at 0x7fcb52549fa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning a decent policy using offline RL requires specialized RL algorithms.\n",
    "# Examples of offline RL algos are RLlib's \"CRR\", \"MARWIL\", or \"CQL\".\n",
    "# For this example, we'll use the \"Pendulum-v0\" environment and have the \"CRR\"\n",
    "# (critic regularized regression) algorithm learn how to solve this environment, purely from\n",
    "# data recorded from a random/beginner agent.\n",
    "\n",
    "\n",
    "# Import the config class of the algorithm, we would like to train with: CRR.\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "\n",
    "# Create a defaut CRR config:\n",
    "config = CRRConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "# NOTE: We said above that we wouldn't really have an environment available (so how can\n",
    "# we set one up here??).\n",
    "# The following is only to tell the algorithm\n",
    "config.environment(env=\"Pendulum-v1\")\n",
    "\n",
    "#################################################\n",
    "# This is the most important piece of code \n",
    "# in this notebook:\n",
    "# It explains how to point your \n",
    "# algorithm to the correct offline data file\n",
    "# (instead of a live-environment).\n",
    "#################################################\n",
    "config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        \"paths\": \"offline_rl_data/pendulum.zip\",\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "# RLlib's CRR is a very new algorithm (since 1.13) and only supports\n",
    "# the PyTorch framework thus far. We'll provide a tf version in the near future.\n",
    "config.framework(\"torch\")\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "config.evaluation(\n",
    "    # Run evaluation once per `train()` call (by default, RLlib will evaluate 10 episodes).\n",
    "    evaluation_interval=1,\n",
    "    # Use a separate resource (\"RLlib rollout worker\")\n",
    "    evaluation_num_workers=1,\n",
    "    # Run evaluation parallel to training.\n",
    "    evaluation_parallel_to_training=True,\n",
    "    # Use a slightly different config for the evaluation:\n",
    "    evaluation_config={\n",
    "        # - Use a real environment (so we can fully trust the evaluation results, rewards, etc..)\n",
    "        \"input\": \"sampler\",\n",
    "        # - Switch off exploration for better (less stochastic) action computations.\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Keep configuring our CRR algorithm by calling the config object's `training()` method and passing the following settings into that call:\n",
    "\n",
    "```\n",
    "gamma: 0.99\n",
    "train_batch_size: 1024\n",
    "critic_lr: 0.0003\n",
    "actor_lr: 0.0003\n",
    "target_network_update_freq: 1\n",
    "tau: 0.0001\n",
    "weight_type: \"exp\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.crr.crr.CRRConfig at 0x7fcb52108130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the `training()` call on your config here in this cell:\n",
    "config.training()  # <- fill this out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use `tune.run()` to kick off the experiment, similar to how we did it in the previous notebook. Let's see how fast CRR can learn to play pendulum from scratch (from beginner's data)! As stopping criteria, use `timesteps_total=2000000` and `evaluation/episode_reward_mean=-300`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 17:27:37,953\tINFO services.py:1477 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268\u001b[39m\u001b[22m\n",
      "2022-07-24 17:27:41,123\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n",
      "2022-07-24 17:27:41,517\tINFO plugin_schema_manager.py:51 -- Loading the default runtime env schemas: ['/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n",
      "2022-07-24 17:27:51,606\tERROR trial_runner.py:920 -- Trial CRR_282d2_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 989, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py\", line 2195, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::CRR.__init__()\u001b[39m (pid=26118, ip=127.0.0.1, repr=CRR)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 291, in __init__\n",
      "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 163, in setup\n",
      "    super().setup(config)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 402, in setup\n",
      "    policy_class=self.get_default_policy_class(self.config),\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 185, in get_default_policy_class\n",
      "    raise ValueError(\"Non-torch frameworks are not supported yet!\")\n",
      "ValueError: Non-torch frameworks are not supported yet!\n",
      "\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tINFO algorithm.py:1774 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,589\tERROR worker.py:754 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::CRR.__init__()\u001b[39m (pid=26118, ip=127.0.0.1, repr=CRR)\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 291, in __init__\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 163, in setup\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     super().setup(config)\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 402, in setup\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     policy_class=self.get_default_policy_class(self.config),\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 185, in get_default_policy_class\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     raise ValueError(\"Non-torch frameworks are not supported yet!\")\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m ValueError: Non-torch frameworks are not supported yet!\n",
      "2022-07-24 17:27:51,624\tERROR ray_trial_executor.py:103 -- An exception occurred when trying to stop the Ray actor:Traceback (most recent call last):\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 94, in post_stop_cleanup\n",
      "    ray.get(future, timeout=0)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py\", line 2195, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::CRR.__init__()\u001b[39m (pid=26118, ip=127.0.0.1, repr=CRR)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 291, in __init__\n",
      "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 163, in setup\n",
      "    super().setup(config)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 402, in setup\n",
      "    policy_class=self.get_default_policy_class(self.config),\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 185, in get_default_policy_class\n",
      "    raise ValueError(\"Non-torch frameworks are not supported yet!\")\n",
      "ValueError: Non-torch frameworks are not supported yet!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-07-24 17:27:51 (running for 00:00:10.36)\n",
      "Memory usage on this node: 11.9/16.0 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5.0/16 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/sven/ray_results/CRR\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------+----------+-------+\n",
      "| Trial name      | status   | loc   |\n",
      "|-----------------+----------+-------|\n",
      "| CRR_282d2_00000 | RUNNING  |       |\n",
      "+-----------------+----------+-------+\n",
      "\n",
      "\n",
      "Result for CRR_282d2_00000:\n",
      "  trial_id: 282d2_00000\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-24 17:27:51 (running for 00:00:10.38)\n",
      "Memory usage on this node: 11.9/16.0 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/sven/ray_results/CRR\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+-----------------+----------+-------+\n",
      "| Trial name      | status   | loc   |\n",
      "|-----------------+----------+-------|\n",
      "| CRR_282d2_00000 | ERROR    |       |\n",
      "+-----------------+----------+-------+\n",
      "Number of errored trials: 1\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "| Trial name      |   # failures | error file                                                                  |\n",
      "|-----------------+--------------+-----------------------------------------------------------------------------|\n",
      "| CRR_282d2_00000 |            1 | /Users/sven/ray_results/CRR/CRR_282d2_00000_0_2022-07-24_17-27-41/error.txt |\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-07-24 17:27:51 (running for 00:00:10.39)\n",
      "Memory usage on this node: 11.9/16.0 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/sven/ray_results/CRR\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+-----------------+----------+-------+\n",
      "| Trial name      | status   | loc   |\n",
      "|-----------------+----------+-------|\n",
      "| CRR_282d2_00000 | ERROR    |       |\n",
      "+-----------------+----------+-------+\n",
      "Number of errored trials: 1\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "| Trial name      |   # failures | error file                                                                  |\n",
      "|-----------------+--------------+-----------------------------------------------------------------------------|\n",
      "| CRR_282d2_00000 |            1 | /Users/sven/ray_results/CRR/CRR_282d2_00000_0_2022-07-24_17-27-41/error.txt |\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [CRR_282d2_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform the `tune.run()` call here:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# config=...  <- check out the previous notebook on how to use tune.run() with an RLlib config object\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ...\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/tune.py:731\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [CRR_282d2_00000])"
     ]
    }
   ],
   "source": [
    "# Perform the `tune.run()` call here:\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "tune.run(\n",
    "    \"CRR\",\n",
    "    # config=...  <- check out the previous notebook on how to use tune.run() with an RLlib config object\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
