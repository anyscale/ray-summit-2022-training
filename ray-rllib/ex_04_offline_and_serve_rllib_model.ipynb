{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04. (Take-home) Advanced Topic: Introduction to Offline RL and Serving your RLlib Model using Ray Serve API\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn:\n",
    " * [What's offline RL (aka \"batch RL\")?](#offline_rl)\n",
    " * [How to configure RLlib for offline RL](#offline_rl_with_rllib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's offline RL (aka \"batch RL\")? <a class=\"anchor\" id=\"offline_rl\"></a>\n",
    "\n",
    "So far, we have dealt with a so-called \"online\" setting for RL, in which we had direct control over a live environment (or a simulator). We were able to send arbitrary actions to this simulator and collect its responses (rewards and observations), thereby learning \"as we go\". This setup is called \"online\" RL:\n",
    "\n",
    "<img src=\"images/online_rl.png\" width=\"80%\"></img>\n",
    "\n",
    "However, often and especially in real-life industry settings, we are faced with the problem of not having a simulator at hand.\n",
    "In this case, we need to fall back to offline RL:\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=\"70%\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Due to the dynamic nature of adversarial multi-agent scenarios, we will cover the topic of\n",
    "of offline RL here only for the single-agent case.\n",
    "Research on multi-agent offline RL is bleeding edge and not well explored by RLlib thus far (see references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL comes in two flavours:\n",
    "\n",
    "#### Pure imitation learning\n",
    "\n",
    "The agent will try to imitate 100% the actions/behavior that it finds in the offline data).\n",
    "This setup is nothing else but supervised learning with a `-log(p)` loss function.\n",
    "\n",
    "#### Imitation learning plus improvement over the recorded behavior\n",
    "\n",
    "The agent will partly imitate the offline, recorded behavior, but also try to improve over it, learning a policy that will\n",
    "perform better in the actual environment. This is achieved by focusing on those actions within the distribution that seem more \n",
    "promising, e.g. via weighting based on the received rewards.\n",
    "\n",
    "\n",
    "### Example\n",
    "In fact, modern offline RL algorithms are capable learning to perfectly play e.g. the Pendulum environment, when only behavioral data from a randomly acting agent is available! We'll explore this right now using RLlib's new CRR algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"images/pendulum.gif\", width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.crr.crr.CRRConfig at 0x7fbf3b52eee0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Learning a decent policy using offline RL requires specialized RL algorithms.\n",
    "# Examples of offline RL algos are RLlib's \"CRR\", \"MARWIL\", or \"CQL\".\n",
    "# For this example, we'll use the \"Pendulum-v0\" environment and have the \"CRR\"\n",
    "# (critic regularized regression) algorithm learn how to solve this environment, purely from\n",
    "# data recorded from a random/beginner agent.\n",
    "\n",
    "\n",
    "# Import the config class of the algorithm, we would like to train with: CRR.\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "\n",
    "# Create a defaut CRR config:\n",
    "config = CRRConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "# NOTE: We said above that we wouldn't really have an environment available (so how can\n",
    "# we set one up here??).\n",
    "# The following is only to tell the algorithm\n",
    "config.environment(env=\"Pendulum-v1\")\n",
    "\n",
    "#################################################\n",
    "# This is the most important piece of code \n",
    "# in this notebook:\n",
    "# It explains how to point your \n",
    "# algorithm to the correct offline data file\n",
    "# (instead of a live-environment).\n",
    "#################################################\n",
    "config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/pendulum_replay_v1.1.0.zip\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    # The (continuous) actions in our input files are already normalized\n",
    "    # (meaning between -1.0 and 1.0) -> We don't have to do anything with them prior to\n",
    "    # computing losses.\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "# RLlib's CRR is a very new algorithm (since 1.13) and only supports\n",
    "# the PyTorch framework thus far. We'll provide a tf version in the near future.\n",
    "config.framework(\"torch\")\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "config.evaluation(\n",
    "    # Run evaluation once per `train()` call (by default, RLlib will evaluate 10 episodes).\n",
    "    evaluation_interval=1,\n",
    "    # Use a separate resource (\"RLlib rollout worker\")\n",
    "    evaluation_num_workers=1,\n",
    "    # Run evaluation parallel to training.\n",
    "    evaluation_parallel_to_training=True,\n",
    "    # Use a slightly different config for the evaluation:\n",
    "    evaluation_config={\n",
    "        # - Use a real environment (so we can fully trust the evaluation results, rewards, etc..)\n",
    "        \"input\": \"sampler\",\n",
    "        # - Switch off exploration for better (less stochastic) action computations.\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### 1) Finish CRR configuration\n",
    "\n",
    "Keep configuring our CRR algorithm by calling the config object's `training()` method and passing the following settings into that call:\n",
    "\n",
    "```\n",
    "gamma: 0.99\n",
    "train_batch_size: 1024\n",
    "target_network_update_freq: 1\n",
    "tau: 0.0001\n",
    "weight_type: \"exp\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.crr.crr.CRRConfig at 0x7fbf3b52eee0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the `training()` call on your config here in this cell:\n",
    "config.training(\n",
    "    gamma=0.99,\n",
    "    # <- complete the other arguments to configure our CRR algo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use `tune.run()` to kick off the experiment\n",
    "\n",
    "Similar to how we did it in the previous notebook, use `tune.run()` to kick off our offline RL learning experiment.\n",
    "Let's see how fast CRR can learn to play pendulum from scratch (from beginner's data)!\n",
    "\n",
    "- As stopping criteria, use `timesteps_total=2000000` and `evaluation/episode_reward_mean=-300`.\n",
    "- Also, make sure checkpoints are created every iteration (`checkpoint_freq=1`).\n",
    "- Set the output directory (`local_dir` arg) to \"results\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 17:27:37,953\tINFO services.py:1477 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268\u001b[39m\u001b[22m\n",
      "2022-07-24 17:27:41,123\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n",
      "2022-07-24 17:27:41,517\tINFO plugin_schema_manager.py:51 -- Loading the default runtime env schemas: ['/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n",
      "2022-07-24 17:27:51,606\tERROR trial_runner.py:920 -- Trial CRR_282d2_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 989, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py\", line 2195, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::CRR.__init__()\u001b[39m (pid=26118, ip=127.0.0.1, repr=CRR)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 291, in __init__\n",
      "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 163, in setup\n",
      "    super().setup(config)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 402, in setup\n",
      "    policy_class=self.get_default_policy_class(self.config),\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 185, in get_default_policy_class\n",
      "    raise ValueError(\"Non-torch frameworks are not supported yet!\")\n",
      "ValueError: Non-torch frameworks are not supported yet!\n",
      "\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tINFO algorithm.py:1774 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,582\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m 2022-07-24 17:27:51,589\tERROR worker.py:754 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::CRR.__init__()\u001b[39m (pid=26118, ip=127.0.0.1, repr=CRR)\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 291, in __init__\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 163, in setup\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     super().setup(config)\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 402, in setup\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     policy_class=self.get_default_policy_class(self.config),\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 185, in get_default_policy_class\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m     raise ValueError(\"Non-torch frameworks are not supported yet!\")\n",
      "\u001b[2m\u001b[36m(CRR pid=26118)\u001b[0m ValueError: Non-torch frameworks are not supported yet!\n",
      "2022-07-24 17:27:51,624\tERROR ray_trial_executor.py:103 -- An exception occurred when trying to stop the Ray actor:Traceback (most recent call last):\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 94, in post_stop_cleanup\n",
      "    ray.get(future, timeout=0)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py\", line 2195, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::CRR.__init__()\u001b[39m (pid=26118, ip=127.0.0.1, repr=CRR)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 291, in __init__\n",
      "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 163, in setup\n",
      "    super().setup(config)\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 402, in setup\n",
      "    policy_class=self.get_default_policy_class(self.config),\n",
      "  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/crr.py\", line 185, in get_default_policy_class\n",
      "    raise ValueError(\"Non-torch frameworks are not supported yet!\")\n",
      "ValueError: Non-torch frameworks are not supported yet!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-07-24 17:27:51 (running for 00:00:10.36)\n",
      "Memory usage on this node: 11.9/16.0 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5.0/16 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/sven/ray_results/CRR\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------+----------+-------+\n",
      "| Trial name      | status   | loc   |\n",
      "|-----------------+----------+-------|\n",
      "| CRR_282d2_00000 | RUNNING  |       |\n",
      "+-----------------+----------+-------+\n",
      "\n",
      "\n",
      "Result for CRR_282d2_00000:\n",
      "  trial_id: 282d2_00000\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-24 17:27:51 (running for 00:00:10.38)\n",
      "Memory usage on this node: 11.9/16.0 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/sven/ray_results/CRR\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+-----------------+----------+-------+\n",
      "| Trial name      | status   | loc   |\n",
      "|-----------------+----------+-------|\n",
      "| CRR_282d2_00000 | ERROR    |       |\n",
      "+-----------------+----------+-------+\n",
      "Number of errored trials: 1\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "| Trial name      |   # failures | error file                                                                  |\n",
      "|-----------------+--------------+-----------------------------------------------------------------------------|\n",
      "| CRR_282d2_00000 |            1 | /Users/sven/ray_results/CRR/CRR_282d2_00000_0_2022-07-24_17-27-41/error.txt |\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-07-24 17:27:51 (running for 00:00:10.39)\n",
      "Memory usage on this node: 11.9/16.0 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/sven/ray_results/CRR\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+-----------------+----------+-------+\n",
      "| Trial name      | status   | loc   |\n",
      "|-----------------+----------+-------|\n",
      "| CRR_282d2_00000 | ERROR    |       |\n",
      "+-----------------+----------+-------+\n",
      "Number of errored trials: 1\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "| Trial name      |   # failures | error file                                                                  |\n",
      "|-----------------+--------------+-----------------------------------------------------------------------------|\n",
      "| CRR_282d2_00000 |            1 | /Users/sven/ray_results/CRR/CRR_282d2_00000_0_2022-07-24_17-27-41/error.txt |\n",
      "+-----------------+--------------+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [CRR_282d2_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform the `tune.run()` call here:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# config=...  <- check out the previous notebook on how to use tune.run() with an RLlib config object\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ...\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/tune.py:731\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [CRR_282d2_00000])"
     ]
    }
   ],
   "source": [
    "# Perform the `tune.run()` call here:\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "tune.run(\n",
    "    \"CRR\",\n",
    "    # config=...  <- check out the previous notebook on how to use tune.run() with an RLlib config object\n",
    "    # ...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-26 12:06:53 (running for 00:04:19.11)<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 7.0/16 CPUs, 0/0 GPUs, 0.0/5.07 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/results/CRR<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:02:45,067\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:02:45,086\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:02:45,086\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m Checking /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/pendulum_replay_v1.1.0.zip ...\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m fpath=/Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/pendulum_replay_v1.1.0.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:02:47,593\tWARNING read_api.py:260 -- The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m [dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=40790)\u001b[0m 2022-07-26 12:03:01,597\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40792)\u001b[0m 2022-07-26 12:03:01,597\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40793)\u001b[0m 2022-07-26 12:03:01,597\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40791)\u001b[0m 2022-07-26 12:03:01,597\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40791)\u001b[0m 2022-07-26 12:03:01,607\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40790)\u001b[0m 2022-07-26 12:03:01,607\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40792)\u001b[0m 2022-07-26 12:03:01,607\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40793)\u001b[0m 2022-07-26 12:03:01,607\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=40791)\u001b[0m DatasetReader 2 has 24875, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40793)\u001b[0m DatasetReader 4 has 24875, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40792)\u001b[0m DatasetReader 3 has 24875, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40790)\u001b[0m DatasetReader 1 has 24875, samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:03:01,826\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:03:01,828\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:03:01,842\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:03:01,842\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:03:10,608\tINFO trainable.py:160 -- Trainable.setup took 25.543 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:03:10,609\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40795)\u001b[0m 2022-07-26 12:03:10,593\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40795)\u001b[0m 2022-07-26 12:03:10,597\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m 2022-07-26 12:03:10,789\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/crr/torch/crr_torch_policy.py:363: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "\u001b[2m\u001b[36m(CRR pid=40772)\u001b[0m   torch.from_numpy(self.action_space.low).to(target_a_next),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40795)\u001b[0m 2022-07-26 12:03:10,918\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "2022-07-26 12:06:55,044\tWARNING tune.py:665 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform the `tune.run()` call here:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/tune.py:701\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    694\u001b[0m progress_reporter\u001b[38;5;241m.\u001b[39msetup(\n\u001b[1;32m    695\u001b[0m     start_time\u001b[38;5;241m=\u001b[39mtune_start,\n\u001b[1;32m    696\u001b[0m     total_samples\u001b[38;5;241m=\u001b[39msearch_alg\u001b[38;5;241m.\u001b[39mtotal_samples,\n\u001b[1;32m    697\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    698\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    699\u001b[0m )\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 701\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n\u001b[1;32m    703\u001b[0m         _report_progress(runner, progress_reporter)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:812\u001b[0m, in \u001b[0;36mTrialRunner.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_trial:\n\u001b[1;32m    810\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot new trial to run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_trial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 812\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_and_handle_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_experiment_if_needed()\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:755\u001b[0m, in \u001b[0;36mTrialRunner._wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_and_handle_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, next_trial: Optional[Trial]):\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    754\u001b[0m         \u001b[38;5;66;03m# Single wait of entire tune loop.\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m         event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_executor_event\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_live_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_trial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m _ExecutorEventType\u001b[38;5;241m.\u001b[39mPG_READY:\n\u001b[1;32m    759\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_pg_ready(next_trial)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py:945\u001b[0m, in \u001b[0;36mRayTrialExecutor.get_next_executor_event\u001b[0;34m(self, live_trials, next_trial_exists)\u001b[0m\n\u001b[1;32m    936\u001b[0m     futures_to_wait \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pg_manager\u001b[38;5;241m.\u001b[39mget_staging_future_list() \u001b[38;5;241m+\u001b[39m futures_to_wait\n\u001b[1;32m    938\u001b[0m     )\n\u001b[1;32m    939\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_next_executor_event before wait with futures \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfutures_to_wait\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_trial_exists=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_trial_exists\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m )\n\u001b[0;32m--> 945\u001b[0m ready_futures, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures_to_wait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next_event_wait\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m###################################################################\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;66;03m# Dealing with no future returned case.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m###################################################################\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ready_futures) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py:2379\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2377\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2378\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2379\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1422\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:173\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform the `tune.run()` call here:\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "tune.run(\n",
    "    \"CRR\",\n",
    "    config=config.to_dict(),\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True,\n",
    "    local_dir=\"results\",\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Let's record our trained algorithm on a live Pendulum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/crr_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "2022-07-26 12:27:50.841 python[41299:1745453] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fbf2c05dc00>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-07-26 12:27:50.842 python[41299:1745453] Warning: Expected min height of view: (<NSButton: 0x7fbf2843fa40>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-07-26 12:27:50.844 python[41299:1745453] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fbf28442730>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-07-26 12:27:50.846 python[41299:1745453] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fbf28448bd0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-07-26 12:27:51,137\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "2022-07-26 12:27:51,140\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/pendulum_replay_v1.1.0.zip ...\n",
      "fpath=/Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/pendulum_replay_v1.1.0.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 12:27:54,074\tINFO services.py:1477 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2022-07-26 12:27:56,095\tWARNING read_api.py:260 -- The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=41349)\u001b[0m 2022-07-26 12:28:06,895\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41349)\u001b[0m 2022-07-26 12:28:06,902\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41351)\u001b[0m 2022-07-26 12:28:06,895\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41351)\u001b[0m 2022-07-26 12:28:06,902\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41350)\u001b[0m 2022-07-26 12:28:06,896\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41350)\u001b[0m 2022-07-26 12:28:06,902\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41352)\u001b[0m 2022-07-26 12:28:06,895\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41352)\u001b[0m 2022-07-26 12:28:06,902\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-26 12:28:07,024\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-26 12:28:07,026\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-26 12:28:07,041\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-07-26 12:28:07,042\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=41351)\u001b[0m DatasetReader 3 has 24875, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41349)\u001b[0m DatasetReader 1 has 24875, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41352)\u001b[0m DatasetReader 4 has 24875, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41350)\u001b[0m DatasetReader 2 has 24875, samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 12:28:14,442\tINFO trainable.py:160 -- Trainable.setup took 23.308 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-07-26 12:28:14,443\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-07-26 12:28:14,463\tINFO trainable.py:654 -- Restored on 127.0.0.1 from checkpoint: results/CRR/CRR_Pendulum-v1_11bef_00000_0_2022-07-26_12-02-34/checkpoint_000018\n",
      "2022-07-26 12:28:14,464\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 18, '_timesteps_total': None, '_time_total': 181.0962312221527, '_episodes_total': 0}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41354)\u001b[0m 2022-07-26 12:28:14,425\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41354)\u001b[0m 2022-07-26 12:28:14,429\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"crr_video/rl-video-episode-0.mp4\" controls  width=\"500\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "\n",
    "env = RecordVideo(gym.make(\"Pendulum-v1\"), \"crr_video\")\n",
    "obs = env.reset()\n",
    "\n",
    "crr = config.build()\n",
    "crr.restore(\"results/CRR/CRR_Pendulum-v1_11bef_00000_0_2022-07-26_12-02-34/checkpoint_000018/checkpoint-18\")\n",
    "\n",
    "while True:\n",
    "    a = crr.compute_single_action(observation=obs)\n",
    "    obs, reward, done, _ = env.step(a)\n",
    "    # Is the episode `done`? -> Quit.\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "Video(\"crr_video/rl-video-episode-0.mp4\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
