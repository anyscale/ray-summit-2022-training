{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05. (Take-home) Advanced Topic: Adding an in-game Recommender using RLlib\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn how to:\n",
    "\n",
    " * [Intro RecSys with RL](#recsys_rl)\n",
    " * [Create a RecSys RL environment](#recsys_env)\n",
    " * [Train a Contextual Bandit on the environment](#cb)\n",
    " * [Train using a RL Online algorithm on the environment](#online)\n",
    " * [Train using a RL Offline algorithm on the environment](#offline)\n",
    " \n",
    " \n",
    " find RLlib algos to train policy models on environments.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro RecSys with RL <a class=\"anchor\" id=\"recsys_rl\"></a>\n",
    "\n",
    "A Recommender System <b>(RecSys)</b> suggests items that are most pertinent to a particular user.  Examples of recommender systems include:\n",
    "<ul>\n",
    "    <li>Video recommendations (e.g. YouTube, Netflix)</li>\n",
    "    <li>Online shopping recommendations (e.g. Amazon)</li>\n",
    "    <li>Advertisements on a website</li>\n",
    "</ul>\n",
    "\n",
    "<b>Two main approaches to training algorithms</b> for RecSys are: \n",
    "<ol>\n",
    "    <li>Traditional Machine Learning <b>(ML)</b></li>\n",
    "    <li>Reinforcement Learning <b>(RL)</b></li>\n",
    "    </ol>\n",
    "\n",
    "<b>In traditional ML</b>, data is gathered about users and products (features or X's), and the views or actions by users of those products (dependent variable or y's). A ranking algorithm is trained on all the data at once as if all the actions occurred in one time step (e.g. collaborative filtering).  Such a <b><i>static</i> model</b> is useful when there are millions of items and users, since learning from all data at once is efficient.\n",
    "\n",
    "<b>In RL</b>, users interact with offers repeatedly over time.  Per iteration, we recommend items to a user, observe the user's behaviour, and receive rewards based on the user's actions.  The <b><i>dynamic</i> model</b> is iteratively trained based on \n",
    "the last observation of recommendation, action, reward.  One caveat with RL, since a recommendation needs to be calculated at every time step in RL, only a pre-selected handful of top candidate items per user (from the traditional ML ranking model) is presented in the simulation environment.  \n",
    "\n",
    "<b>Offline RL is particularly relevant in a RecSys context.</b>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "    <b>üí° Online vs Offline RL, when algorithm learning from an environment is: </b> <br><br>\n",
    "    ‚úîÔ∏è in a live fashion (typically gaming platforms or complex systems simulations), this is called <b>online RL</b> and evaluation during training is <b>on-policy</b>. <br><br>\n",
    "    ‚úîÔ∏è gathered from log files (RecSys: of user offers and actions), this is called <b>offline RL</b> and evaluation during training is <b>off-policy</b>, because the policy (RL word for model) used to log the data is different from the policy used to explore the data. </b> \n",
    "</div>\n",
    "\n",
    "Through the log files of historic user offers and user actions, offline RL in a RecSys context implicitly explores the last Recommender model put into production.  ‚ÄúSerendipitous‚Äù aspects of user experience can be explored through offline RL, since random actions the user did not historically take can be tried in the simulation.\n",
    "\n",
    "This additional offline RL step after logging is an important part of modern Recommender Systems, to ensure current models do not propagate errors or bias.\n",
    "\n",
    "TODO create an overall picture of RecSys system with offline RL.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a RecSys RL Environment <a class=\"anchor\" id=\"recsys_env\"></a>\n",
    "\n",
    "As we learned in the first 2 lessons, the first step to training a RL RecSys policy model is to create a live <b>environment</b> that can interact with a RL Algorithm to train a recommender agent. \n",
    "\n",
    "In this notebook, we will use <b><a href=\"https://github.com/google-research/recsim\">Google's RecSim environment</a></b>, which was developed for the YouTube recommendation problem.  The environment is <i>Timelimit-based</i>, meaning the termination condition for an episode will be after a fixed number (60) of videos are watched. The RecSim environment consists of:\n",
    "\n",
    "<img src=\"./images/recsim_environment.png\" width=\"90%\" />\n",
    "\n",
    "* <b>Document Model</b>, in the range [0, 1].  \n",
    "<ul>\n",
    "    <li>On the 0-end of the scale, <b>\"sweet\"</b> documents lead to large amounts of <b>\"click bait\"</b> or immediate engagement. Sweetness values are drawn from ln Normal(Œºsweet, œÉsweet).</li>\n",
    "    <li>On the 1-end of the scale, documents termed <b>kale</b>, are less click-bait, but tend to <b>increase user long-term satisfaction</b>. Kale values are drawn from ln Normal(Œºkale, œÉkale)</li>\n",
    "    <li>Mixed doc values are drawn from linear interpolation between parameters of the two distributions in proportion to their kaleness.</li>\n",
    "    </ul>\n",
    "* <b>User Model</b>, simulated as having: \n",
    "<ul>\n",
    "    <li><i>evolving, unknown contexts</i> (interests, preferences, satisfaction, activity, mood)</li>\n",
    "    <li><i>unobservable events</i> that could impact user behavior (personalized promotions, interuptions that cause turning off a video such as because someone rang their doorbell)</li>\n",
    "    </ul>\n",
    "* <b>Rewards</b>, or user satisfaction after their choice, modeled in the range [0, 1] that stochastically (and slowly) increases or decreases with the consumption of different types of content; kale or sweetness.  \n",
    "\n",
    "\n",
    "<b>RLlib comes with 3 RecSim environments</b>  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "üëâ <b>Long Term Satisfaction</b> (used in this tutorial) <br>\n",
    "- Interest Evolution <br>\n",
    "- Interest Exploration <br>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n",
      "tensorflow: 2.7.0\n",
      "gym: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from scipy.stats import linregress, sem\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# silence the many tensorflow warnings\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import recsim\n",
    "\n",
    "print(f\"tensorflow: {tf.__version__}\")\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RecSim instance using the following config parameters \n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # The number of possible documents/videos/candidates that we can recommend\n",
    "    \"slate_size\": 1, # The number of recommendations that we will be making\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# # What are our spaces?\n",
    "# pretty_print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "# pretty_print(f\"action space = {lts_10_1_env.action_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"doc:\\n  '0':\\n  - 0.54881352186203\\n  '1':\\n  - 0.7151893377304077\\n  '2':\\n  - 0.6027633547782898\\n  '3':\\n  - 0.5448831915855408\\n  '4':\\n  - 0.42365479469299316\\n  '5':\\n  - 0.6458941102027893\\n  '6':\\n  - 0.4375872015953064\\n  '7':\\n  - 0.891772985458374\\n  '8':\\n  - 0.9636627435684204\\n  '9':\\n  - 0.3834415078163147\\nresponse:\\n- click: 1\\n  engagement: 32.88135528564453\\nuser: []\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pretty_print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 1.80; done = False\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 3  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pretty_print(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified-lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified-lts' to be used in RLlib configs (see below)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     action \u001b[38;5;241m=\u001b[39m action_kaleiest\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#action = np.random.choice([action_kaleiest, action_sweetest])\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_lts_10_1_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Create linear model of rewards over time.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/core.py:324\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m, reward, done, info\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mLTSWithStrongerDissatisfactionEffect.observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobservation\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mspaces:\n\u001b[0;32m---> 33\u001b[0m         observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241m.\u001b[39m_user_model\u001b[38;5;241m.\u001b[39m_user_state\u001b[38;5;241m.\u001b[39msatisfaction])\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengagement\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/core.py:238\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattempted to get missing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)\n\u001b[1;32m    237\u001b[0m     )\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/core.py:234\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattempted to get missing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 9.47; done = True\n"
     ]
    }
   ],
   "source": [
    "# Inspect the modified (1-slate back into the env) using the env's `step()` method.\n",
    "action = 4  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pretty_print(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO uncomment later - this takes too much time for now\n",
    "\n",
    "# # Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# # We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
    "# lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "#     \"num_candidates\": 20,\n",
    "#     \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "#     \"resample_documents\": True,\n",
    "#     # Convert to Discrete action space.\n",
    "#     \"convert_to_discrete_action_space\": True,\n",
    "#     # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "#     \"wrap_for_bandits\": True,\n",
    "# }))\n",
    "\n",
    "# lts_20_2_env_mean_random_reward, _ = \\\n",
    "#     measure_random_performance_for_env(lts_20_2_env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Contextual Bandit on the environment <a class=\"anchor\" id=\"cb\"></a>\n",
    "\n",
    "A Bandit session is one where we have an opportunity to recommend the user an item and observe their behaviour. We receive a reward if they click.\n",
    "\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>Bandits</b></i></li>\n",
    "    <li>On the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#bandits\">algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select RLlib Bandit algorithm w/Upper Confidence Bound (UCB) exploration\n",
    "# and find that algorithm's config class\n",
    "\n",
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.bandit import BanditLinUCBConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig at 0x7f82c03f8310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not sure how to pass in all these params...\n",
    "\n",
    "BanditLinUCBConfig()\\\n",
    "    .environment(env_config={\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,})\n",
    "\n",
    "\n",
    "# bandit_config = {\n",
    "#     \"env\": \"modified_lts\",\n",
    "#     \"env_config\": {\n",
    "#         \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "#         \"slate_size\": 2,\n",
    "#         \"resample_documents\": True,\n",
    "\n",
    "#         # Bandit-specific flags:\n",
    "#         \"convert_to_discrete_action_space\": True,\n",
    "#         # Convert \"doc\" key into \"item\" key.\n",
    "#         \"wrap_for_bandits\": True,\n",
    "#         # Use consistent seeds for the environment ...\n",
    "#         \"seed\": 0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig'>\n"
     ]
    }
   ],
   "source": [
    "# # uncomment below to see the long list of specifically PPO default config values\n",
    "# print(f\"Bandit's default config is:\")\n",
    "# print(pretty_print(BanditLinUCBConfig().to_dict()))\n",
    "\n",
    "# Choose your config settings and instantiate a config object with those settings\n",
    "# Define algorithm config values\n",
    "env_name = \"modified-lts\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"my_LinUCB_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "bandit_config = (\n",
    "    BanditLinUCBConfig()\n",
    "    .framework(framework='torch')\n",
    "    # .environment(env=env_name, disable_env_checking=False)\n",
    "    .environment(\n",
    "        env=env_name, \n",
    "        env_config={\n",
    "            \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "            \"slate_size\": 2,\n",
    "            \"resample_documents\": True,\n",
    "            \"convert_to_discrete_action_space\": True,\n",
    "            # Convert \"doc\" key into \"item\" key.\n",
    "            \"wrap_for_bandits\": True,})\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # do not override defaults\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    ")\n",
    "\n",
    "print(type(bandit_config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m 2022-08-01 22:18:24,018\tERROR worker.py:754 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5450, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8119541fd0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 128, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m     spec = self.spec(path)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 151, in spec\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m     raise error.Error(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m gym.error.Error: Attempted to look up malformed environment ID: b'modified-lts'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5450, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8119541fd0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 514, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('modified-lts') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5450)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m 2022-08-01 22:18:24,018\tERROR worker.py:754 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5453, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbf1f621fd0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 128, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m     spec = self.spec(path)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 151, in spec\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m     raise error.Error(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m gym.error.Error: Attempted to look up malformed environment ID: b'modified-lts'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5453, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbf1f621fd0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 514, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('modified-lts') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5453)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m 2022-08-01 22:18:24,018\tERROR worker.py:754 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5452, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe592d11f70>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 128, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m     spec = self.spec(path)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 151, in spec\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m     raise error.Error(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m gym.error.Error: Attempted to look up malformed environment ID: b'modified-lts'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5452, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe592d11f70>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 514, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('modified-lts') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5452)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m 2022-08-01 22:18:24,018\tERROR worker.py:754 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5451, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd95c542fa0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 128, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m     spec = self.spec(path)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 151, in spec\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m     raise error.Error(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m gym.error.Error: Attempted to look up malformed environment ID: b'modified-lts'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5451, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd95c542fa0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 514, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('modified-lts') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5451)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    },
    {
     "ename": "EnvError",
     "evalue": "The env string you provided ('modified-lts') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:399\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:175\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate_workers_after_construction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:318\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:440\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    439\u001b[0m     local_result \u001b[38;5;241m=\u001b[39m [func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_worker())]\n\u001b[0;32m--> 440\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_result \u001b[38;5;241m+\u001b[39m remote_results\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py:2195\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2194\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2195\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5452, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe592d11f70>)\n  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make\n    return registry.make(id, **kwargs)\n  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 128, in make\n    spec = self.spec(path)\n  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/gym/envs/registration.py\", line 151, in spec\n    raise error.Error(\ngym.error.Error: Attempted to look up malformed environment ID: b'modified-lts'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=5452, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe592d11f70>)\n  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 514, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n  File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\nray.rllib.utils.error.EnvError: The env string you provided ('modified-lts') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEnvError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     ray\u001b[38;5;241m.\u001b[39mshutdown()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# # Use the config object's `build()` method for generating\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # an RLlib Algorithm instance that we can then train.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m linucb_algo \u001b[38;5;241m=\u001b[39m \u001b[43mbandit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithm type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(linucb_algo)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train the Bandit Algorithm instance\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm_config.py:304\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger_creator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_creator \u001b[38;5;241m=\u001b[39m logger_creator\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:291\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m     }\n\u001b[1;32m    289\u001b[0m }\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:424\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# errors.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mactor_init_failed:\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;66;03m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;66;03m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;66;03m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;66;03m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mEnvError\u001b[0m: The env string you provided ('modified-lts') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    }
   ],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "linucb_algo = bandit_config.build()\n",
    "print(f\"Algorithm type: {type(linucb_algo)}\")\n",
    "\n",
    "# train the Bandit Algorithm instance\n",
    "for i in range(300):\n",
    "    # Call its `train()` method\n",
    "    result = linucb_algo.train()\n",
    "    print(f\"Iteration={i}, Mean Reward={result['episode_reward_mean']}\")\n",
    "\n",
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "linucb_algo.stop()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
