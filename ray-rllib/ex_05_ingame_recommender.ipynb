{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05. (Take-home) Advanced Topic: Adding an in-game Recommender using RLlib\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn how to:\n",
    "\n",
    " * [Intro RecSys with RL](#recsys_rl)\n",
    " * [Create a RecSys RL environment](#recsys_env)\n",
    " * [Train a Contextual Bandit on the environment](#cb)\n",
    " * [Train using a RL Online algorithm on the environment](#online)\n",
    " * [Train using a RL Offline algorithm on the environment](#offline)\n",
    " \n",
    " \n",
    " find RLlib algos to train policy models on environments.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro RecSys with RL <a class=\"anchor\" id=\"recsys_rl\"></a>\n",
    "\n",
    "A Recommender System <b>(RecSys)</b> suggests items that are most pertinent to a particular user.  Examples of recommender systems include:\n",
    "<ul>\n",
    "    <li>Video recommendations (e.g. YouTube, Netflix)</li>\n",
    "    <li>Online shopping recommendations (e.g. Amazon)</li>\n",
    "    <li>Advertisements on a website</li>\n",
    "</ul>\n",
    "\n",
    "<b>Two main approaches to training algorithms</b> for RecSys are: \n",
    "<ol>\n",
    "    <li>Traditional Machine Learning <b>(ML)</b></li>\n",
    "    <li>Reinforcement Learning <b>(RL)</b></li>\n",
    "    </ol>\n",
    "\n",
    "<b>In traditional ML</b>, data is gathered about users and products (features or X's), and the views or actions by users of those products (dependent variable or y's). A ranking algorithm is trained on all the data at once as if all the actions occurred in one time step (e.g. collaborative filtering).  Such a <b><i>static</i> model</b> is useful when there are millions of items and users, since learning from all data at once is efficient.\n",
    "\n",
    "<b>In RL</b>, users interact with offers repeatedly over time.  Per iteration, we recommend items to a user, observe the user's behaviour, and receive rewards based on the user's actions.  The <b><i>dynamic</i> model</b> is iteratively trained based on \n",
    "the last observation of recommendation, action, reward.  One caveat with RL, since a recommendation needs to be calculated at every time step in RL, only a pre-selected handful of top candidate items per user (from the traditional ML ranking model) is presented in the simulation environment.  \n",
    "\n",
    "<b>Offline RL is particularly relevant in a RecSys context.</b>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "    <b>üí° Online vs Offline RL, when algorithm learning from an environment is: </b> <br><br>\n",
    "    ‚úîÔ∏è in a live fashion (typically gaming platforms or complex systems simulations), this is called <b>online RL</b> and evaluation during training is <b>on-policy</b>. <br><br>\n",
    "    ‚úîÔ∏è gathered from log files (RecSys: of user offers and actions), this is called <b>offline RL</b> and evaluation during training is <b>off-policy</b>, because the policy (RL word for model) used to log the data is different from the policy used to explore the data. </b> \n",
    "</div>\n",
    "\n",
    "Through the log files of historic user offers and user actions, offline RL in a RecSys context implicitly explores the last Recommender model put into production.  ‚ÄúSerendipitous‚Äù aspects of user experience can be explored through offline RL, since random actions the user did not historically take can be tried in the simulation.\n",
    "\n",
    "This additional offline RL step after logging is an important part of modern Recommender Systems, to ensure current models do not propagate errors or bias.\n",
    "\n",
    "TODO create an overall picture of RecSys system with offline RL.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a RecSys RL Environment <a class=\"anchor\" id=\"recsys_env\"></a>\n",
    "\n",
    "As we learned in the first 2 lessons, the first step to training a RL RecSys policy model is to create a live <b>environment</b> that can interact with a RL Algorithm to train a recommender agent. \n",
    "\n",
    "In this notebook, we will use <b><a href=\"https://github.com/google-research/recsim\">Google's RecSim environment</a></b>, which was developed for the YouTube recommendation problem.  The environment is <i>Timelimit-based</i>, meaning the termination condition for an episode will be after a fixed number (60) of videos are watched. The RecSim environment consists of:\n",
    "\n",
    "<img src=\"./images/recsim_environment.png\" width=\"90%\" />\n",
    "\n",
    "* <b>Document Model</b>, in the range [0, 1].  \n",
    "<ul>\n",
    "    <li>On the 0-end of the scale, <b>\"sweet\"</b> documents lead to large amounts of <b>\"click bait\"</b> or immediate engagement. Sweetness values are drawn from ln Normal(Œºsweet, œÉsweet).</li>\n",
    "    <li>On the 1-end of the scale, documents termed <b>kale</b>, are less click-bait, but tend to <b>increase user long-term satisfaction</b>. Kale values are drawn from ln Normal(Œºkale, œÉkale)</li>\n",
    "    <li>Mixed doc values are drawn from linear interpolation between parameters of the two distributions in proportion to their kaleness.</li>\n",
    "    </ul>\n",
    "* <b>User Model</b>, simulated as having: \n",
    "<ul>\n",
    "    <li><i>evolving, unknown contexts</i> (interests, preferences, satisfaction, activity, mood)</li>\n",
    "    <li><i>unobservable events</i> that could impact user behavior (personalized promotions, interuptions that cause turning off a video such as because someone rang their doorbell)</li>\n",
    "    </ul>\n",
    "* <b>Rewards</b>, or user satisfaction after their choice, modeled in the range [0, 1] that stochastically (and slowly) increases or decreases with the consumption of different types of content; kale or sweetness.  \n",
    "\n",
    "\n",
    "<b>RLlib comes with 3 RecSim environments</b>  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "üëâ <b>Long Term Satisfaction</b> (used in this tutorial) <br>\n",
    "- Interest Evolution <br>\n",
    "- Interest Exploration <br>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n",
      "tensorflow: 2.6.0\n",
      "gym: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from scipy.stats import linregress, sem\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# silence the many tensorflow warnings\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import recsim\n",
    "\n",
    "print(f\"tensorflow: {tf.__version__}\")\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RecSim instance using the following config parameters \n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # The number of possible documents/videos/candidates that we can recommend\n",
    "    \"slate_size\": 1, # The number of recommendations that we will be making\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# # What are our spaces?\n",
    "# pretty_print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "# pretty_print(f\"action space = {lts_10_1_env.action_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"doc:\\n  '0':\\n  - 0.54881352186203\\n  '1':\\n  - 0.7151893377304077\\n  '2':\\n  - 0.6027633547782898\\n  '3':\\n  - 0.5448831915855408\\n  '4':\\n  - 0.42365479469299316\\n  '5':\\n  - 0.6458941102027893\\n  '6':\\n  - 0.4375872015953064\\n  '7':\\n  - 0.891772985458374\\n  '8':\\n  - 0.9636627435684204\\n  '9':\\n  - 0.3834415078163147\\nresponse:\\n- click: 0\\n  engagement: 81.6318588256836\\nuser: []\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pretty_print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 40.13; done = False\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 3  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pretty_print(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified-lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified-lts' to be used in RLlib configs (see below)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000639093375221753\n"
     ]
    }
   ],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 9.47; done = True\n"
     ]
    }
   ],
   "source": [
    "# Inspect the modified (1-slate back into the env) using the env's `step()` method.\n",
    "action = 4  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pretty_print(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO uncomment later - this takes too much time for now\n",
    "\n",
    "# # Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# # We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
    "# lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "#     \"num_candidates\": 20,\n",
    "#     \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "#     \"resample_documents\": True,\n",
    "#     # Convert to Discrete action space.\n",
    "#     \"convert_to_discrete_action_space\": True,\n",
    "#     # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "#     \"wrap_for_bandits\": True,\n",
    "# }))\n",
    "\n",
    "# lts_20_2_env_mean_random_reward, _ = \\\n",
    "#     measure_random_performance_for_env(lts_20_2_env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Contextual Bandit on the environment <a class=\"anchor\" id=\"cb\"></a>\n",
    "\n",
    "A Bandit session is one where we have an opportunity to recommend the user an item and observe their behaviour. We receive a reward if they click.\n",
    "\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>Bandits</b></i></li>\n",
    "    <li>On the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#bandits\">algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select RLlib Bandit algorithm w/Upper Confidence Bound (UCB) exploration\n",
    "# and find that algorithm's config class\n",
    "\n",
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.bandit import BanditLinUCBConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig at 0x17a6c4cd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not sure how to pass in all these params...\n",
    "\n",
    "BanditLinUCBConfig()\\\n",
    "    .environment(env_config={\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,})\n",
    "\n",
    "\n",
    "# bandit_config = {\n",
    "#     \"env\": \"modified_lts\",\n",
    "#     \"env_config\": {\n",
    "#         \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "#         \"slate_size\": 2,\n",
    "#         \"resample_documents\": True,\n",
    "\n",
    "#         # Bandit-specific flags:\n",
    "#         \"convert_to_discrete_action_space\": True,\n",
    "#         # Convert \"doc\" key into \"item\" key.\n",
    "#         \"wrap_for_bandits\": True,\n",
    "#         # Use consistent seeds for the environment ...\n",
    "#         \"seed\": 0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig'>\n"
     ]
    }
   ],
   "source": [
    "# # uncomment below to see the long list of specifically PPO default config values\n",
    "# print(f\"Bandit's default config is:\")\n",
    "# print(pretty_print(BanditLinUCBConfig().to_dict()))\n",
    "\n",
    "# Choose your config settings and instantiate a config object with those settings\n",
    "# Define algorithm config values\n",
    "env_name = \"modified-lts\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"my_LinUCB_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "bandit_config = (\n",
    "    BanditLinUCBConfig()\n",
    "    .framework(framework='torch')\n",
    "    # .environment(env=env_name, disable_env_checking=False)\n",
    "    .environment(\n",
    "        env=env_name, \n",
    "        env_config={\n",
    "            \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "            \"slate_size\": 2,\n",
    "            \"resample_documents\": True,\n",
    "            \"convert_to_discrete_action_space\": True,\n",
    "            # Convert \"doc\" key into \"item\" key.\n",
    "            \"wrap_for_bandits\": True,})\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # do not override defaults\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    ")\n",
    "\n",
    "print(type(bandit_config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 20:56:07,276\tERROR services.py:1376 -- Failed to start the dashboard: Failed to start the dashboard, return code 1\n",
      " The last 10 lines of /tmp/ray/session_2022-07-30_20-56-05_382623_51105/logs/dashboard.log:\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1200, in add_routes\n",
      "    registered_routes.extend(route_def.register(self))\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_routedef.py\", line 98, in register\n",
      "    resource = router.add_static(self.prefix, self.path, **self.kwargs)\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1124, in add_static\n",
      "    resource = StaticResource(\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 559, in __init__\n",
      "    raise ValueError(f\"No directory exists at '{directory}'\") from error\n",
      "ValueError: No directory exists at '/Users/christy/Documents/ray/dashboard/client/build/static'\n",
      "2022-07-30 20:56:07,278\tERROR services.py:1377 -- Failed to start the dashboard, return code 1\n",
      " The last 10 lines of /tmp/ray/session_2022-07-30_20-56-05_382623_51105/logs/dashboard.log:\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1200, in add_routes\n",
      "    registered_routes.extend(route_def.register(self))\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_routedef.py\", line 98, in register\n",
      "    resource = router.add_static(self.prefix, self.path, **self.kwargs)\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1124, in add_static\n",
      "    resource = StaticResource(\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 559, in __init__\n",
      "    raise ValueError(f\"No directory exists at '{directory}'\") from error\n",
      "ValueError: No directory exists at '/Users/christy/Documents/ray/dashboard/client/build/static'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/christy/Documents/ray/python/ray/_private/services.py\", line 1362, in start_api_server\n",
      "    raise Exception(err_msg + last_log_str)\n",
      "Exception: Failed to start the dashboard, return code 1\n",
      " The last 10 lines of /tmp/ray/session_2022-07-30_20-56-05_382623_51105/logs/dashboard.log:\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1200, in add_routes\n",
      "    registered_routes.extend(route_def.register(self))\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_routedef.py\", line 98, in register\n",
      "    resource = router.add_static(self.prefix, self.path, **self.kwargs)\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 1124, in add_static\n",
      "    resource = StaticResource(\n",
      "  File \"/Users/christy/miniforge3/envs/rllib2/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py\", line 559, in __init__\n",
      "    raise ValueError(f\"No directory exists at '{directory}'\") from error\n",
      "ValueError: No directory exists at '/Users/christy/Documents/ray/dashboard/client/build/static'\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/christy/Documents/ray/python/ray/dashboard/modules/reporter/reporter_agent.py:46: UserWarning: `gpustat` package is not installed. GPU monitoring is not available. To have full functionality of the dashboard please install `pip install ray[default]`.)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm type: <class 'ray.rllib.algorithms.bandit.bandit.BanditLinUCB'>\n",
      "Iteration=0, Mean Reward=nan\n",
      "Iteration=1, Mean Reward=nan\n",
      "Iteration=2, Mean Reward=nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "linucb_algo = bandit_config.build()\n",
    "print(f\"Algorithm type: {type(linucb_algo)}\")\n",
    "\n",
    "# train the Bandit Algorithm instance\n",
    "for i in range(3):\n",
    "    # Call its `train()` method\n",
    "    result = linucb_algo.train()\n",
    "    print(f\"Iteration={i}, Mean Reward={result['episode_reward_mean']}\")\n",
    "\n",
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "linucb_algo.stop()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û°Ô∏è [Next notebook](./ex_06_rllib_end_to_end_demo.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_04_offline_rl_with_rllib.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
