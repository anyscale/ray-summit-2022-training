{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710f04f4",
   "metadata": {},
   "source": [
    "# Exercise 01. Introduction to the OpenAI Gym Environment and RLlib Algorithm top-level APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved<br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "‚û°Ô∏è [Link to next notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, we will learn about:\n",
    " * [What is an Environment in Reinforcement Learning (RL)?](#intro_env)\n",
    " * [Overview of RL terminology](#intro_rl)\n",
    " * [Introduction to OpenAI Gym environments](#intro_gym)\n",
    " * [High-level OpenAI Gym API calls](#intro_gym_api)\n",
    " * [Overview of RLlib](#intro_rllib)\n",
    " * [Train a RL model using an algorithm from RLlib](#intro_rllib_api)\n",
    " * [Evaluate a RLlib model](#eval_rllib)\n",
    " * [Reload RLlib model from checkpoint and run inference](#reload_rllib)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aca4e8",
   "metadata": {},
   "source": [
    "## What is an environment in RL? <a class=\"anchor\" id=\"intro_env\"></a>\n",
    "\n",
    "Solving a problem in RL begins with an **environment**. In the simplest definition of RL:\n",
    "\n",
    "> An **agent** interacts with an **environment** and receives a reward.\n",
    "\n",
    "An environment in RL is the agent's world, it is a simulation of the problem to be solved. \n",
    "\n",
    "<img align=\"center\" src=\"images/env_key_concept1.png\" width=\"50%\" />\n",
    "\n",
    "The **environment** simulator might be of a:\n",
    "<ul>\n",
    "    <li>real, physical machine such as a gas turbine or autonomous vehicle</li>\n",
    "    <li>real, abstract system such as user behavior on a website or the stock market</li>\n",
    "    <li>virtual sytem on a computer such as a board game or a video game</li>\n",
    "    </ul>\n",
    "    \n",
    "The **agent** represents what is triggering the actions.  For example it could be:\n",
    "<ul>\n",
    "    <li>a software system that is triggering actions for machines</li>\n",
    "    <li>a type of user or investor</li>\n",
    "    <li>a game player or game system that is competing against real players </li>\n",
    "    </ul> \n",
    "    \n",
    "\n",
    "<i>Comparison of RL to supervised learning</i> <br>\n",
    "<ul>\n",
    "    <li><i>Data</i>.  In supervised learning, you start with a labeled dataset.  In contrast, the <b>data in RL is not given up front; the environment acts as a data generator</b>.  One can also do RL on a pre-collected dataset (called offline RL), we will touch on offline RL later. </li>\n",
    "    <li><i>Training</i>.  Traditional supervised learning views the world as more of a one-shot training, not as action -> fedback -> improved action -> repeat. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Why bother with an Agent, Environment, and RL?</b>  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "<b>üí° Reinforcement learning (RL) is useful when you have sequential decisions that need to be optimized over time.</b> \n",
    "</div>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf844e4",
   "metadata": {},
   "source": [
    "## Overview of RL terminology <a class=\"anchor\" id=\"intro_rl\"></a>\n",
    "\n",
    "An RL environment consists of: \n",
    "\n",
    "1. all possible actions (**action space**)\n",
    "2. a complete description of the environment, nothing hidden (**state space**)\n",
    "3. an observation by the agent of certain parts of the state (**observation space**)\n",
    "4. **reward**, which is the only feedback the agent receives after each action.\n",
    "\n",
    "The model that tries to maximize the expected sum over all future rewards is called a **policy**. The policy is a function mapping the environment's observations to an action to take, usually written **œÄ** (s(t)) -> a(t).  <i>In deep reinforcement learning, this function is a neural network</i>.\n",
    "\n",
    "Below is a high-level image of how the Agent and Environment work together in a RL simulation feedback loop in RLlib.\n",
    "\n",
    "<img src=\"images/env_key_concept2.png\" width=\"98%\" />\n",
    "\n",
    "The **RL simulation feedback loop** repeatedly collects data, for one (single-agent case) or multiple (multi-agent case) policies, trains the policies on these collected data, and makes sure the policies' weights are kept in synch. \n",
    "\n",
    "During simulation loops, the environment collects observations, taken actions, receives rewards and so-called **done** flags, indicating the boundaries of different episodes the agents play through in the simulation.\n",
    "\n",
    "Each simulation iteration is called a <b>time step</b>.  The simulation iterations of action -> reward -> next state -> train -> repeat, until the end state, is called an **episode**, or in RLlib, a **rollout**.  \n",
    "> üëâ Each episode consists of one or many time steps.\n",
    "\n",
    "<b>Per episode</b> (or between **done** flag == True), the RL simulation feedback loop repeats up to some specified end state (termination state or timesteps). Examples of termination are:\n",
    "<ul>\n",
    "    <li>the end of a maze (termination state)</li>  \n",
    "    <li>the player died in a game (termination state)</li>\n",
    "    <li>after 60 videos watched in a recommender system (timesteps).</li>\n",
    "    </ul>\n",
    "    \n",
    "<b>Why train for many episodes?</b>  When you are doing machine learning, you do not just do something once and report the result.  You do it many times, to make sure you did not just get \"lucky\" one time, and you report typically the average result over all the trials.  RL is similar.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>üí° In RL, the policy is trained by repeating trials, or episodes (or rollouts), then reporting the calculated reward typically as an average of all achieved rewards.</b> \n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07193fc6",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym example: frozen lake <a class=\"anchor\" id=\"intro_gym\"></a>\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) is a well-known reference library of RL environments. \n",
    "\n",
    "#### 1. import gym\n",
    "\n",
    "Below is how you would import gym and view all available environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742e5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "Num Gym Environments: 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[EnvSpec(FrozenLake-v1), EnvSpec(FrozenLake8x8-v1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import gym\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# List all available gym environments\n",
    "all_env  =  list(gym.envs.registry.all())\n",
    "print(f'Num Gym Environments: {len(all_env)}')\n",
    "\n",
    "# You could loop through and list all environments if you wanted\n",
    "# [print(e) for e in all_env]\n",
    "envs_starting_with_f = [e for e in all_env if str(e).startswith(\"EnvSpec(Frozen\")]\n",
    "envs_starting_with_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddd364",
   "metadata": {},
   "source": [
    "#### 2. Instatiate your Gym object\n",
    "\n",
    "The way you instantiate a Gym environment is with the **make()** function.\n",
    "\n",
    "The .make() function takes arguments:\n",
    "- **name of the Gym environment**, type: str, Required.\n",
    "- **runtime parameter values**, Optional.\n",
    "\n",
    "For the required string argument, you need to know the Gym name.  You can find the Gym name in the Gym documentation for environments, either:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "    \n",
    "Below is an example of how to create a basic Gym environment, [frozen lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/).  We can see below that the termination condition of an episode will be time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00d01a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n",
      "env_spec: EnvSpec(FrozenLake-v1)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"FrozenLake-v1\"\n",
    "\n",
    "# Instantiate gym env object with a runtime parameter value (is_slippery).\n",
    "# is_slippery specifies if the environment is deterministic or stochastic\n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    is_slippery=False,  # whether the grid-world behaves deterministically or not\n",
    ")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c1e21",
   "metadata": {},
   "source": [
    "#### 3. Inspect the environment action and observations spaces\n",
    "\n",
    "Gym Environments can be deterministic or stochastic.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Deterministic</b> if the current state + selected action determines the next state of the environment.  <i>Chess is an example of a deterministic environment</i>, since all possible states/action combinations can be described as a discrete set of rules with states bounded by the pieces and size of the board.</li>\n",
    "    <li>\n",
    "        <b>Stochastic</b> if the policy output action is a probability distribution over a set of possible actions at time step t. In this case, the agent needs to compute its action from the policy in two steps. i) sample actions from the policy according to the probability distribution, ii) compute log likelihoods of the actions. <i>Random visitors to a website is an example of a stochastic environment</i>. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Gym actions.</b> The action_space describes the numerical structure of the legitimate actions that can be applied to the environment. \n",
    "\n",
    "For example, if we have 4 possible discrete actions, we could encode them as:\n",
    "<ul>\n",
    "    <li>0: LEFT</li>\n",
    "    <li>1: DOWN</li>\n",
    "    <li>2: RIGHT</li>\n",
    "    <li>3: UP</li>\n",
    "</ul>\n",
    "\n",
    "<b>Gym observations.</b>  The observation_space defines the structure as well as the legitimate values for the observation of a state of the environment.  \n",
    "\n",
    "For example, if we have a 4x4 grid, we could encode them as {0,1,2,3, 4, ‚Ä¶ ,15} for grid positions ((0,0), (0,1), (0,2), (0,3), ‚Ä¶. (3,3)).\n",
    "\n",
    "\n",
    "From the Gym [documentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) about the frozen lake environment, we see: <br>\n",
    "\n",
    "|Frozen Lake      | Gym space   |\n",
    "|---------------- | ----------- |\n",
    "|Action Space     | Discrete(4) |\n",
    "|Observation Space| Discrete(16)|\n",
    "\n",
    "\n",
    " \n",
    "<b><a href=\"https://github.com/openai/gym/tree/master/gym/spaces\">Gym spaces</a></b> are gym data types.  The main types are `Discrete` for discrete numbers and `Box` for continuous numbers.  \n",
    "\n",
    "Gym Space `Discrete` elements are Python type `int`, and Gym Space `Box` are Python type `float32`.\n",
    "\n",
    "Below is an example how to inspect the environment action and observations spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78620a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a gym environment.\n",
      "\n",
      "gym action space: Discrete(4)\n",
      "gym observation space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# check if it is a gym instance\n",
    "if isinstance(env, gym.Env):\n",
    "    print(\"This is a gym environment.\")\n",
    "    print()\n",
    "\n",
    "    # print gym Spaces\n",
    "    if isinstance(env.action_space, gym.spaces.Space):\n",
    "        print(f\"gym action space: {env.action_space}\")\n",
    "    if isinstance(env.observation_space, gym.spaces.Space):\n",
    "        print(f\"gym observation space: {env.observation_space}\") \n",
    "        \n",
    "# Note: the action space is discrete with 4 possible actions.\n",
    "# Note: the observation space is 4x4 and thus runs from 0 to 15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad91e6",
   "metadata": {},
   "source": [
    "#### 4. Inspect gym environment default & runtime parameters\n",
    "\n",
    "Gym environments contain 2 sets of parameters that are set after the environment object is instantiated.\n",
    "<ul>\n",
    "    <li><b>Default parameters</b> are fixed in the Gym environment code itself.</li>\n",
    "    <li><b>Runtime parameters</b> are passed into the make() function as **kwargs.</li>\n",
    "    </ul>\n",
    "\n",
    "Below is an example of how to inspect the environment parameters.  Notice we can tell from the parameters that our frozen lake environment is: <br>\n",
    "1) <i>Deterministic</i>, and <br>\n",
    "2) Episode terminates with time step condition <i>max_episode_steps</i> = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4dc0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default spec params...\n",
      "id: FrozenLake-v1\n",
      "reward_threshold: 0.7\n",
      "nondeterministic: False\n",
      "max_episode_steps: 100\n",
      "order_enforce: True\n",
      "\n",
      "Runtime spec params...\n",
      "map_name: 4x4\n",
      "is_slippery: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect env.spec parameters\n",
    " \n",
    "# View default env spec params that are hard-coded in Gym code itself\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "# rewards above this value considered \"success\"\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "# env is deterministic or stochastic\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "# number of time steps per episode\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "# must reset before step or render\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\") \n",
    "\n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print()\n",
    "print(\"Runtime spec params...\")\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    "\n",
    "# Note:  We can tell that our frozen lake environment is: \n",
    "# 1) Success criteria is rewards >= 0.7\n",
    "# 2) Deterministic\n",
    "# 3) Episode terminates when number time_steps = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd5020",
   "metadata": {},
   "source": [
    "#### 5. Perform some basic Gym API calls <a class=\"anchor\" id=\"intro_gym_api\"></a>\n",
    "\n",
    "The most basic Gym API methods are:\n",
    "<ul>\n",
    "    <li><b>env.reset()</b> <br>Reset the environment to an initial state.  You should call this method every time at the start of a new episode.</li>\n",
    "    <li><b>env.render()</b>  <br>Visually inspect the environment. This is for human/debugging purposes; it is not seen by the agent/algorithm.  Note you cannot inspect an environment before it has been initialized with env.reset().</li>\n",
    "    <li><b>env.step(action)</b> <br>Take an action from the possible action space values.  It takes an action as input, computes the state of the environment after applying that action and returns the 4-tuple (observation, reward, done, info).</li>\n",
    "    <li><b>env.close()</b> <br>Close an environment.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd2bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Print the starting observation.  \n",
    "# Recall possible observations are between 4x4 grid.\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee81cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 1, reward: 0.0, done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Take an action\n",
    "# Recall the possible actions are: 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "\n",
    "new_obs, reward, done, _ = env.step(2) #Right\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()\n",
    "new_obs, reward, done, _ = env.step(1) #Down\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec105",
   "metadata": {},
   "source": [
    "We can also try to run an action in the frozen lake environment which is outside the defined number range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0620a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this cell if you want whole notebook to run without errors\n",
    "\n",
    "# Try to take an invalid action\n",
    "\n",
    "#env.step(4) # invalid\n",
    "\n",
    "# should see KeyError below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9c090-0c94-47b7-8e7d-a54af12c446f",
   "metadata": {},
   "source": [
    "To test out your environment, typically you will loop through a few episodes to make sure it works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8a6854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96726776ff9a49e5a8343ac390868d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# The following three lines are for rendering purposes only.\n",
    "# They allow us to render the env frame-by-frame in-place\n",
    "# (w/o creating a huge output which we would then have to scroll through).\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with out:\n",
    "\n",
    "    # Putting the simple API methods together.\n",
    "    # Here is a pattern for running a bunch of episodes.\n",
    "    num_episodes = 5 # Number of episodes you want to run the agent\n",
    "    total_reward = 0.0  # Initialize reward to 0\n",
    "\n",
    "    # Loop through episodes\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        # Reset the environment at the start of each episode\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Loop through time steps per episode\n",
    "        while True:\n",
    "            # take random action, but you can also do something more intelligent \n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # If the epsiode is up, then start another one\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Render the env (in place).\n",
    "            time.sleep(0.3)\n",
    "            out.clear_output(wait=True)\n",
    "            print(f\"episode: {ep}\")\n",
    "            print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "            env.render()\n",
    "\n",
    "# Close the env\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f1156",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview of RLlib <a class=\"anchor\" id=\"intro_rllib\"></a>\n",
    "\n",
    "<img width=\"7%\" src=\"images/rllib-logo.png\"> is the most comprehensive open-source Reinforcement Learning framework. **[RLlib](https://github.com/ray-project/ray/tree/master/rllib)** is <b>distributed by default</b> since it is built on top of **[Ray](https://docs.ray.io/en/latest/)**, an easy-to-use, open-source, distributed computing framework for Python that can handle complex, heterogeneous applications. Ray and RLlib run on compute clusters on any cloud without vendor lock.\n",
    "\n",
    "RLlib includes <b>25+ available [algorithms](https://docs.ray.io/en/master/rllib/rllib-algorithms.html)</b>, converted to both <img width=\"3%\" src=\"images/tensorflow-logo.png\">_TensorFlow_ and <img width=\"3%\" src=\"images/pytorch-logo.png\">_PyTorch_, covering different sub-categories of RL: _model-free_, _offline RL_, _model-based_, and _gradient-free_.¬†Almost any RLlib algorithm can learn in a <b>multi-agent</b> setting.¬†Many algorithms support <b>RNNs</b> and <b>LSTMs</b>.\n",
    "\n",
    "On a very high level, RLlib is organized by **environments**, **algorithms**, **examples**, **tuned_examples**, and **models**.  \n",
    "\n",
    "    ray\n",
    "    |- rllib\n",
    "    |  |- env \n",
    "    |  |- algorithms \n",
    "    |  |  |- alpha_zero \n",
    "    |  |  |- appo \n",
    "    |  |  |- ppo \n",
    "    |  |  |- ... \n",
    "    |  |- examples \n",
    "    |  |- tuned_examples\n",
    "    |  |- models\n",
    "\n",
    "Within **_env_** you will find different [base classes](https://docs.ray.io/en/latest/rllib/package_ref/env.html) that you can inherit from to make it easy to implement your environment. RLlib supports environments created using the **OpenAI Gym API** (which supports most user cases). The base classes in the env directory allow for users to implement environments that are not covered by OpenAI Gym, such as multi agent environments or environments that have strict performance or hosting requirements. In the next notebook, we will use the **RLlib MultiAgentEnv** base class to train a **multi agent** RL model.\n",
    "\n",
    "Within **_examples_** you will find some examples of common custom rllib use cases.  \n",
    "\n",
    "Within **_tuned\\_examples_**, you will find, sorted by algorithm, suggested hyperparameter value choices within .yaml files. Ray¬†RLlib team ran simulations/benchmarks to find suggested hyperparameter value choices.¬†¬†These¬†files are used¬†for daily testing, and weekly hard-task testing to make sure they all run at speed,¬†for both TF and Torch.¬†Helps give you a leg-up with initial parameter choices!\n",
    "\n",
    "Within **_models_**, you will find advanced building blocks for building deep neural networks in either <img width=\"3%\" src=\"images/tensorflow-logo.png\">_TensorFlow_ or <img width=\"3%\" src=\"images/pytorch-logo.png\">_PyTorch_.  For example, the building blocks for DNN, CNN, RNN, LSTM are here.\n",
    "\n",
    "In this tutorial, we will mainly focus on the **_algorithms_** package, where we will find RLlib algos to train policy models on environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a23493-80b1-4032-a98f-f14827026501",
   "metadata": {},
   "source": [
    "## Train a RL model using an algorithm from RLlib <a class=\"anchor\" id=\"intro_rllib_api\"></a>\n",
    "\n",
    "Once you have an environment, next you need to decide which RL algorithm to use.\n",
    "\n",
    "#### Step 1.  Import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5655b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package pickle5 becomes unnecessary in Python 3.8 and above. Its presence may confuse libraries including Ray. Please uninstall the package.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d2983-90fd-48bd-b61a-1ee79b62770f",
   "metadata": {},
   "source": [
    "#### Step 2. Check environment for errors   \n",
    "\n",
    "Before you start training, it is a good idea to check the environment for errors.  RLlib provides a convenient [Environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) for this.  It checks that the environment is compatible with OpenAI Gym and RLlib (and outputs a warning if necessary).\n",
    "\n",
    "We will start with a new environment, Cart-Pole.  Take a look at the Gym documentation:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "\n",
    "Below, we start a new Cart Pole environment, then in the next cell, check it for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ddfdb23-2554-48cf-aeed-cdc1b19f02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<CartPoleEnv<CartPole-v1>>>\n",
      "env_spec: EnvSpec(CartPole-v1)\n",
      "\n",
      "Environment parameters...\n",
      "[('id', 'CartPole-v1'),\n",
      " ('entry_point', 'gym.envs.classic_control:CartPoleEnv'),\n",
      " ('reward_threshold', 475.0),\n",
      " ('nondeterministic', False),\n",
      " ('max_episode_steps', 500),\n",
      " ('order_enforce', True),\n",
      " ('_kwargs', {}),\n",
      " ('_env_name', 'CartPole')]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Instantiate gym env object with a runtime parameter value\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# inspect env.spec parameters\n",
    "print()\n",
    "print(\"Environment parameters...\")\n",
    "pprint.pprint(list(vars(env_spec).items()), sort_dicts=False)\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps\n",
    "# Note:  We can tell that our CartPole environment is: \n",
    "# 1) Success criteria is rewards >= 475\n",
    "# 2) Deterministic\n",
    "# 3) Episode terminates when number time_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b4730c-ad46-42c9-ae70-ef1f551883b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "\n",
    "# How to check you do not have any environment errors\n",
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53918a3d-3c4f-4d89-8d22-4ff449cef6c7",
   "metadata": {},
   "source": [
    "Let's run through the environment, without rendering, and record the mean reward.  The purpose of this is to obtain a baseline before training a RLlib algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° If you are doing benchmarks, this random model is often called a <b>\"baseline\".</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de621b2d-387d-4366-90e5-d47736d1587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************\n",
      "Baseline mean_reward: 22.29 out of success: 475.0 after 60000 episodes\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "# Putting the simple API methods together.\n",
    "# Here is a pattern for running a bunch of episodes.\n",
    "num_episodes = 60000 # Number of episodes you want to run the agent\n",
    "total_reward = 0.0  # Initialize reward to 0\n",
    "\n",
    "# Loop through episodes\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Loop through time steps per episode\n",
    "    while True:\n",
    "        # take random action, but you can also do something more intelligent \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # If the epsiode is up, then start another one\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# calculate mean_reward\n",
    "print()\n",
    "print(\"**************\")\n",
    "mean_reward = total_reward / num_episodes\n",
    "print(f\"Baseline mean_reward: {mean_reward:.2f} out of success: {env_spec.reward_threshold} after {num_episodes} episodes\")\n",
    "print(\"**************\")\n",
    "        \n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a757e7",
   "metadata": {},
   "source": [
    "#### Step 3.  Select an algorithm and find that algorithm's config class  \n",
    "\n",
    "There are many factors to consider when selecting which algorithm to use on your environment.  Following are some high-level best practices.\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>The first distinction comes from your action space</b>, i.e., do you have discrete (e.g. LEFT, RIGHT, ‚Ä¶) or continuous actions (ex: go to a certain speed)? To check high-level if an algorithm will work, look at the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">RLlib algorithms doc page</a>.  <i>Algorithms are listed according to whether or not they support Discrete action spaces vs Continuous action spaces or both.</i> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Choose a stable algorithm.</b>  When you look at the cumulative rewards per time step, they should rise steadily.  You do not want an algorithm where reward jumps up and down a lot.\n",
    "    </li>\n",
    "    <li><b>Choose the most sample-efficient algorithm that works for your environment</b>. <i>PPO is extremely sample-efficient.  SAC is much less sample-efficient.</i>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Once you have selected the algorithm, <b>look up that algorithm's config class</b>.\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61b7804-1332-45fe-a5bc-4b20364c02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1cb28-1f27-4257-9521-6c7157d0e3ab",
   "metadata": {},
   "source": [
    "#### Step 4. Choose your config settings and instantiate a config object with those settings\n",
    "\n",
    "As of Ray 1.13, RLlib configs been converted from primitive dictionaries into Objects. This makes them harder to print, but easier to set/pass.\n",
    "\n",
    "**Note about RLlib config precedence**\n",
    "<ol>\n",
    "    <li>Highest precedence are <b>trainer instantiation settings</b>, these override any other config settings</li>\n",
    "    <li>RLlib <b>specific algorithm config</b> (see config class description above)</li>\n",
    "    <li>RLlib <b><a href\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py#L58\">general config</a></b> settings have the lowest precedence</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422a7baf-cbd8-4141-a9d7-974239231c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common RLlib General config (for all algorithms)\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "config = AlgorithmConfig()\n",
    "\n",
    "# # Uncomment for long list of parameters\n",
    "# print(f\"RLlib Trainer's general default config is:\")\n",
    "# config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bb6a2",
   "metadata": {},
   "source": [
    "**Note about num_workers**\n",
    "\n",
    "Number of Ray workers is the number of parallel workers or actors for rollouts.  Actual num_workers will be what you specifiy+1 for head node.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° <b>For num_workers, use ONE LESS than the number of cores you want to use</b> (or omit this argument and let Ray automatically use all cores)!\n",
    "</div>\n",
    "\n",
    "\n",
    "Below, num_workers = 7  # means actual number workers=8 including head node; where 8 is #cpu on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99524e70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>\n"
     ]
    }
   ],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "# Define algorithm config values\n",
    "env_name = \"CartPole-v1\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"my_PPO_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "config_PPO = (\n",
    "    PPOConfig()\n",
    "    .framework(framework='torch')\n",
    "    .environment(env=env_name, disable_env_checking=False)\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    ")\n",
    "\n",
    "print(type(config_PPO))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb86ff8",
   "metadata": {},
   "source": [
    "#### Step 5. Instantiate a Trainer from the environment and config objects\n",
    "\n",
    "**Two ways to train RLlib models***\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/package_ref/index.html\">RLlib API.</a> The main methods are:</li>\n",
    "    <ul>\n",
    "        <li>train()</li>\n",
    "        <li>evaluate()</li>\n",
    "        <li>save()</li>\n",
    "        <li><b>restore()</b></li>\n",
    "        <li><b>compute_single_action()</b></li>\n",
    "    </ul>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/tune/api_docs/overview.html\">Ray Tune API.</a>  The main methods are:</li>\n",
    "        <ul>\n",
    "            <li><b>run()</b></li>\n",
    "    </ul>\n",
    "    </ol>\n",
    "    \n",
    "*RLlib CLI from command line using .yml file also exists, but the .yml file is undocumented: <i>rllib train -f [myfile_name].yml</i><br>\n",
    "\n",
    "üëâ RLlib API <b>.train()</b> will train for 1 episode only.  Good for debugging since every single output will be shown for the 1 episode of training.  \n",
    "\n",
    "üëâ However for usual purposes, Ray Tune API <b>.run()</b> is more convenient since with 1 function call you get experiment management: save, checkpoint, evaluate, and train up to a stopping criteria.\n",
    "\n",
    "‚úîÔ∏è Both methods will run the RLlib [environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) you saw earlier in this notebook (Step 4. Check environment).\n",
    "\n",
    "üëç You have to use RLlib API method <b>.restore()</b> to reload a checkpointed RLlib model for Serving and Offline learning.  Tune API methods will not work.\n",
    "\n",
    "üëç After a model is trained, it can be used for inference.  The RLlib API method <b>compute_single_action()</b> will use the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference).  You will see this method used at the end of this notebook. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>In summary: <br>\n",
    "    üí° If you are training a RLlib model, train it using Ray Tune API method .run()!!  <br>\n",
    "    üëâ  If you are developing or debugging a RLlib model, train it using RLlib API method .train()!! <br>\n",
    "    üëâ  If you need to restore a RLlib model, use RLlib API method .restore()!!</b>\n",
    "</div>\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a6107f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###############\n",
    "# # EXAMPLE USING RLLIB API .train() FOR 1 EPISODE\n",
    "# # For completeness, here is how to train RLlib using RLlib API's .train() method\n",
    "# # The code below instantiates a trainer and trains for 1 single episode. \n",
    "# # To train for N number of episodes, you would put _.train()_ into a loop, \n",
    "# # similar to the way we ran the Gym _env.step()_ in a loop.\n",
    "# ###############\n",
    "\n",
    "# # To start fresh, restart Ray in case it is already running\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "    \n",
    "# # Use .build() similar to how gym environments are passed to the gym .make() method.\n",
    "# rllib_algo = config_train.build(env=env_name)\n",
    "# print(type(rllib_algo))\n",
    "\n",
    "# # run the trainer for 1 episode\n",
    "# for i in range(10):\n",
    "#   result = rllib_algo.train()\n",
    "#   print(f\"Iteration={i} Episode R={result['episode_reward_mean']}\")\n",
    "# # Release Algorithm's resources.\n",
    "# rllib_algo.stop()\n",
    "\n",
    "# # Below, you will see the output evaluation_interval times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62252115",
   "metadata": {},
   "source": [
    "From the above cell, you can see how to train a RLlib algorithm 1 episode at a time.  But it is more practical to train RLlib algorithms using Ray Tune, since many more options are available.\n",
    "\n",
    "**Instantiate a trainer using Ray Tune API**\n",
    "\n",
    "Ray Tune offers experiment management in a single call <b>.run()</b>.  In the code below, we <b>specify a stopping criteria</b> to train until a certain Reward is achieved.  In case the desired training reward level is never reached, backup stop criteria can be given.  Tune will stop training whenever the earliest stop criteria is met.  However, best practice starting out is to only have 1 criteria, so you can be sure what is going to happen.\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e34aa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# EXAMPLE USING RAY TUNE API .run() IN A LOOP UNTIL STOP CONDITION\n",
    "# Note about Ray Tune verbosity.\n",
    "# Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "# 0 = silent\n",
    "# 1 = only status updates, no logging messages\n",
    "# 2 = status and brief trial results, includes logging messages\n",
    "# 3 = status and detailed trial results, includes logging messages\n",
    "# Defaults to 3.\n",
    "###############\n",
    "\n",
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "evaluation_interval = 100   #100, num training episodes to run between eval steps\n",
    "verbosity = 2 # Tune screen verbosity\n",
    "\n",
    "experiment_results = tune.run(\"PPO\", \n",
    "                    \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\n",
    "          # \"training_iteration\": 200,  # stop if achieved 200 episodes\n",
    "          # \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\n",
    "          },  \n",
    "              \n",
    "    # training config params\n",
    "    config = config_PPO.to_dict(),\n",
    "                    \n",
    "    #redirect logs instead of default ~/ray_results/\n",
    "    local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq = checkpoint_freq,\n",
    "    checkpoint_at_end=True,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    verbose = verbosity,\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc331b91-dcb8-4225-8e30-94415c6b0c11",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "Scroll through the output in the cell above.  Look for a single table row that looks like this: <br>\n",
    "\n",
    "<img src=\"images/ppo_cartpole_tune_output.png\"></img>\n",
    "\n",
    "What this telling you is that Tune ran your experiment.  It was terminated when Reward reached 409.14, which satisified the stopping criteria of Reward >= 400, out of an environment max reward possible of 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd024c",
   "metadata": {},
   "source": [
    "## Evaluate a RLlib model <a class=\"anchor\" id=\"eval_rllib\"></a>\n",
    "\n",
    "RLlib trainers can be evaluated by:\n",
    "<ul>\n",
    "    <li>Calling RLlib API method .evaluate()*</li>\n",
    "    <li>Examining <b>Ray Tune</b> trainer object </li>\n",
    "    <li>Examining <b>Ray Tune</b> experiment trial results </li>\n",
    "    <li>Visualizing training progress in <b>TensorBoard</b></li>\n",
    "    </ul>\n",
    "\n",
    "*RLlib trainer objects can be examined manually using <i>rllib_trainer.evaluate()</i>, but it gives the same info you already saw in the single episode .train() output.\n",
    "\n",
    "**Ray Tune trainer object** <br>\n",
    "First, let's start by looking at the trainer object. How long did training take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb3c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  52.10 seconds,    0.87 minutes\n"
     ]
    }
   ],
   "source": [
    "# Get RLlib default stats\n",
    "stats = experiment_results.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')\n",
    "\n",
    "# Typically takes about 67 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cad71",
   "metadata": {},
   "source": [
    "**Ray Tune experiment trial results**\n",
    "\n",
    "Read all the experiment trials into a single pandas dataframe.  The dataframe will have 1 row per trial.\n",
    "\n",
    "Below, we see a dataframe with only 1 row because Tune only ran 1 trial.  (Because we did not specify a hyperparameter space to search for tuning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99483c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (1, 413)\n",
      "Index(['episode_reward_max', 'episode_reward_min', 'episode_reward_mean',\n",
      "       'episode_len_mean', 'episodes_this_iter', 'num_healthy_workers',\n",
      "       'num_agent_steps_sampled', 'num_agent_steps_trained',\n",
      "       'num_env_steps_sampled', 'num_env_steps_trained',\n",
      "       ...\n",
      "       'info/learner/default_policy/learner_stats/total_loss',\n",
      "       'info/learner/default_policy/learner_stats/policy_loss',\n",
      "       'info/learner/default_policy/learner_stats/vf_loss',\n",
      "       'info/learner/default_policy/learner_stats/vf_explained_var',\n",
      "       'info/learner/default_policy/learner_stats/kl',\n",
      "       'info/learner/default_policy/learner_stats/entropy',\n",
      "       'info/learner/default_policy/learner_stats/entropy_coeff',\n",
      "       'config/evaluation_config/tf_session_args/gpu_options/allow_growth',\n",
      "       'config/evaluation_config/tf_session_args/device_count/CPU',\n",
      "       'config/evaluation_config/multiagent/policies/default_policy'],\n",
      "      dtype='object', length=413)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>num_agent_steps_sampled</th>\n",
       "      <th>num_agent_steps_trained</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86469_00000</th>\n",
       "      <td>500.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>406.21</td>\n",
       "      <td>406.21</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "trial_id                                                                   \n",
       "86469_00000               500.0                74.0               406.21   \n",
       "\n",
       "             episode_len_mean  episodes_this_iter  num_healthy_workers  \\\n",
       "trial_id                                                                 \n",
       "86469_00000            406.21                   8                    4   \n",
       "\n",
       "             num_agent_steps_sampled  num_agent_steps_trained  \n",
       "trial_id                                                       \n",
       "86469_00000                    60000                    60000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read trainer results in a pandas dataframe\n",
    "df = experiment_results.results_df\n",
    "\n",
    "print(f\"df.shape: {df.shape}\")  #Only 1 trial\n",
    "print(df.columns)\n",
    "df.iloc[:,0:8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd6b1e",
   "metadata": {},
   "source": [
    "For how many episodes did training run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15cdff25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of episodes for the 1st trial\n",
    "df.iloc[0,:].episodes_this_iter  \n",
    "\n",
    "# Answer is 9 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed7e6a",
   "metadata": {},
   "source": [
    "What were the best parameter values?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53a815bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out, the following cell output is long because there are many parameters!\n",
    "\n",
    "# trainer.get_best_config(metric=\"episode_reward_mean\", mode=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd62d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize the training progress in TensorBoard\n",
    "\n",
    "RLlib automatically creates logs for your trained RLlib models that can be visualized in TensorBoard.  To visualize the performance of your RL model:\n",
    "\n",
    "<ol>\n",
    "    <li>Open a terminal</li>\n",
    "    <li><i><b>cd</b></i> into the logdir path from the above cell's output.</li>\n",
    "    <li><i><b>ls</b></i></li>\n",
    "    <li>You should see files that look like: checkpoint_NNNNNN</li>\n",
    "    <li>To be able to compare all your experiments, cd one dir level up.\n",
    "    <li><i><b>cd ..</b></i>  \n",
    "    <li><i><b>tensorboard --logdir . </b></i></li>\n",
    "    <li>Look at the url in the message, and open it in a browser</li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a04eff",
   "metadata": {},
   "source": [
    "#### Screenshot of Tensorboard\n",
    "\n",
    "TensorBoard will give you many pages of charts.  Below displaying just Train/Eval mean and min rewards.\n",
    "\n",
    "The charts below are showing \"sample efficiency\", the number of training steps it took to achieve a certain level of performance.\n",
    "\n",
    "<b>Train Performance:</b> <br>\n",
    "\n",
    "---\n",
    "<img src=\"images/ppo_cartpole_training_rewards.png\" width=\"80%\" />\n",
    "\n",
    "<b>Eval Performance:</b> <br>\n",
    "<img src=\"images/ppo_cartpole_eval_rewards.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf198368",
   "metadata": {},
   "source": [
    "## Reload RLlib model from checkpoint and run inference <a class=\"anchor\" id=\"reload_rllib\"></a>\n",
    "\n",
    "We want to reload the desired RLlib model from checkpoint file and then run the model inference mode on the environment it was trained on.  \n",
    "\n",
    "You will need:\n",
    "<ul>\n",
    "    <li>Your <b>algorithm's config class</b> and exact same <a href=\"#intro_rllib_api\">config settings you used to train your model.</a></li>\n",
    "    <li>Name of the <b>environment</b> you used to train the model.</li>\n",
    "    <li>Path to the desired <b>checkpoint</b> file you want to use to restore the model.</li>\n",
    "    </ul>\n",
    "\n",
    "#### Step 1. Find the best model checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82b94d2c-9c28-411a-ad9d-ab46357b5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_86469_00000_0_2022-07-26_09-53-51\n",
      "<ray.air.checkpoint.Checkpoint object at 0x17769fee0>\n"
     ]
    }
   ],
   "source": [
    "# Get best checkpoint path\n",
    "logdir = experiment_results.get_best_logdir(metric=\"evaluation_reward_mean\", mode=\"max\")\n",
    "print(logdir)\n",
    "\n",
    "# Get last checkpoint path\n",
    "checkpoint = experiment_results.get_last_checkpoint()\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f109b-8618-4f2a-8484-2a0b62890d6e",
   "metadata": {},
   "source": [
    "#### Step 2. Re-initialize an already-trained algorithm trainer object from the checkpoint file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b933a724-7840-4fb7-bf0c-0b64e627c183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 09:54:45,367\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-07-26 09:54:45,368\tINFO algorithm.py:340 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "Install gputil for GPU system monitoring.\n",
      "Restored on 127.0.0.1 from checkpoint: /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_86469_00000_0_2022-07-26_09-53-51/checkpoint_000015\n",
      "Current state after restoring: {'_iteration': 15, '_timesteps_total': None, '_time_total': 47.47431993484497, '_episodes_total': 423}\n"
     ]
    }
   ],
   "source": [
    "# Create new Algorithm and restore its state from the last checkpoint.\n",
    "\n",
    "# create an empty Algorithm\n",
    "algo = config_PPO.build(env=env_name)\n",
    "\n",
    "# restore the agent from the checkpoint\n",
    "algo.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e4a55-fbe1-439e-9a5f-76818a2289e5",
   "metadata": {},
   "source": [
    "#### Step 3. Play and render the game as a video\n",
    "\n",
    "Now we want to record a video of the trained model doing inference in the environment it was trained on.\n",
    "\n",
    "Gym includes video recording and saving capability in its wrapper <i>`gym.wrappers.RecordVideo`</i>, so we will import and use that method to wrap the <i>`gym.make()`</i> method.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üëç During inference, call the RLlib API method <b>compute_single_action()</b>, which uses the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference). \n",
    "</div>\n",
    "\n",
    "Execute the cell below, and you will see a video of the rollouts, so you can verify visually that the agent is starting from a near perfect score.\n",
    "\n",
    "Note that the CartPole environment's perfect score is 500.  Since we trained our model to around 400, the restored agent should appear very stable. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03f9f3bb-9ea2-471d-98d2-19632a79ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done: reward = 500.0\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "#############\n",
    "## Create the env to do inference on\n",
    "#############\n",
    "# Try this first to make sure it is working\n",
    "# env = gym.make(env_name)\n",
    "# Once you have confirmed it works, wrap the method\n",
    "# RecordVideo() takes as input a path name where to store the video\n",
    "# RecordVideo() includes its own render step, records, and saves the video.\n",
    "env = RecordVideo(gym.make(env_name), \"ppo_video\", )\n",
    "obs = env.reset()\n",
    "\n",
    "#############\n",
    "## Use the restored model and run it in inference mode\n",
    "## You will see a pop-up video rendering for about 10 seconds\n",
    "#############\n",
    "num_episodes_during_inference = 1\n",
    "num_episodes = 0\n",
    "episode_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while num_episodes < num_episodes_during_inference:\n",
    "    # Compute an action (`a`).\n",
    "    a = algo.compute_single_action(observation=obs)\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, _ = env.step(a)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    # Is the episode `done`? -> Reset.\n",
    "    if done:\n",
    "        print(f\"Episode done: reward = {episode_reward}\")\n",
    "        obs = env.reset()\n",
    "        num_episodes += 1\n",
    "        episode_reward = 0.0\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# The restored agent manages to achieve a perfect score during the 1 episode rollout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb7940-bb3c-4899-a7fa-66a676ba30bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Play and share your video .mp4 file with others if you want.</b>\n",
    "<br><br>\n",
    "\n",
    "Show a video player in the notebook, and play the video you just recorded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b3aa827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"ppo_video/rl-video-episode-0.mp4\" controls  width=\"500\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_video='ppo_video/rl-video-episode-0.mp4'\n",
    "Video(cart_pole_video, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40b3830e-d0ae-4794-85b2-4b245faef04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray if you are done\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fbd96",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb74fe",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. How would you choose another algorithm to train Cart Pole?  Hint:  Look at the [RLlib algorithm doc page](https://docs.ray.io/en/master/rllib/rllib-algorithms.html).  How would you change the choice of RLlib algorithm from <b>PPO to DQN</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511893b",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa00b0e-9136-4d52-9977-b1845064b60e",
   "metadata": {},
   "source": [
    "‚û°Ô∏è [Link to next notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a924cb-af94-43dd-9e4e-4fab1b648302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
