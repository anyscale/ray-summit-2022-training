{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710f04f4",
   "metadata": {},
   "source": [
    "# Exercise 01. Introduction to the OpenAI Gym Environment and RLlib Algorithm top-level APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, we will learn about:\n",
    " * [What is an Environment in RL?](#intro_env)\n",
    " * [Overview of RL terminology](#intro_rl)\n",
    " * [Introduction to OpenAI Gym environments](#intro_gym)\n",
    " * [High-level OpenAI Gym API calls](#intro_gym_api)\n",
    " * [Overview of RLlib](#intro_rllib)\n",
    " * [Train a RL model using an algorithm from RLlib](#intro_rllib_api)\n",
    " * [Evaluate a RLlib model](#eval_rllib)\n",
    " * [Reload RLlib model from checkpoint and run inference](#reload_rllib)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aca4e8",
   "metadata": {},
   "source": [
    "## What is an environment in RL? <a class=\"anchor\" id=\"intro_env\"></a>\n",
    "\n",
    "Solving a problem in RL begins with an **environment**. In the simplest definition of RL:\n",
    "\n",
    "> An **agent** lives inside an **environment** and tries to act smart.\n",
    "\n",
    "An environment in RL is the agent's world, it is a simulation of the problem to be solved. \n",
    "\n",
    "<img src=\"images/agent_in_env.png\" width=\"80%\" />\n",
    "\n",
    "The environment simulator might be a:\n",
    "<ul>\n",
    "    <li>real, physical situation such as a gas turbine</li>\n",
    "    <li>virtual sytem on a computer such as a board game or video game</li>\n",
    "    </ul>\n",
    "Why bother with an Agent and Environment?  \n",
    "\n",
    "> RL is useful when you have sequential decisions that need to be optimized over time. \n",
    "\n",
    "Traditional supervised learning views the world as more of a one-shot training, not as action -> fedback -> improved action -> repeat.\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf844e4",
   "metadata": {},
   "source": [
    "## Overview of RL terminology <a class=\"anchor\" id=\"intro_rl\"></a>\n",
    "\n",
    "An RL environment consists of: \n",
    "\n",
    "1. all possible actions (**action space**)\n",
    "2. a complete description of the environment, nothing hidden (**state space**)\n",
    "3. an observation by the agent of certain parts of the state (**observation space**)\n",
    "4. **reward**, which is the only feedback the agent receives after each action.\n",
    "\n",
    "The model that tries to maximize the expected sum over all future rewards is called a **policy**. The policy is a function mapping the environment's observations to an action to take, usually written **œÄ** (s(t)) -> a(t).\n",
    "\n",
    "Below is a high-level image of how the Agent and Environment work together in a RL simulation feedback loop in RLlib.\n",
    "\n",
    "<img src=\"images/agent_env_inference_loop.png\" width=\"98%\" />\n",
    "\n",
    "The **RL simulation feedback loop** repeatedly collects data, for one (single-agent case) or multiple (multi-agent case) policies, trains the policies on these collected data, and makes sure the policies' weights are kept in synch. Thereby, the collected environment data contains observations, taken actions, received rewards and so-called **done** flags, indicating the boundaries of different episodes the agents play through in the simulation.\n",
    "\n",
    "The simulation iterations of action -> reward -> next state -> train -> repeat, until the end state, is called an **episode**, or in RLlib, a **rollout**\n",
    "\n",
    "<b>Per episode</b> (or between **done** flag == True), the RL simulation feedback loop repeats up to some specified end state (termination state or timesteps). Examples of termination could be:\n",
    "<ul>\n",
    "    <li>the end of a maze (termination state)</li>  \n",
    "    <li>the player died in a game (termination state)</li>\n",
    "    <li>after 60 videos watched in a recommender system (timesteps).</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07193fc6",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym example: frozen lake <a class=\"anchor\" id=\"intro_gym\"></a>\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) is a well-known reference library of RL environments. \n",
    "\n",
    "#### 1. import gym\n",
    "\n",
    "Below is how you would import gym and view all available environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "742e5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "Num Gym Environments: 849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[EnvSpec(FrozenLake-v1), EnvSpec(FrozenLake8x8-v1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import gym\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# List all available gym environments\n",
    "all_env  =  list(gym.envs.registry.all())\n",
    "print(f'Num Gym Environments: {len(all_env)}')\n",
    "\n",
    "# You could loop through and list all environments if you wanted\n",
    "envs_starting_with_f = [e for e in all_env if str(e).startswith(\"EnvSpec(Frozen\")]\n",
    "envs_starting_with_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddd364",
   "metadata": {},
   "source": [
    "#### 2. Instatiate your Gym object\n",
    "\n",
    "The way you instantiate a Gym environment is with the **make()** function.\n",
    "\n",
    "The .make() function takes arguments:\n",
    "- **name of the Gym environment**, type: str, Required.\n",
    "- **runtime parameter values**, Optional.\n",
    "\n",
    "For the required string argument, you need to know the Gym name.  You can find the Gym name in the Gym documentation for environments, either:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "    \n",
    "Below is an example of how to create a basic Gym environment, [frozen lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/).  We can see below that the termination condition of an episode will be time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00d01a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n",
      "env_spec: EnvSpec(FrozenLake-v1)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"FrozenLake-v1\"\n",
    "\n",
    "# Instantiate gym env object with a runtime parameter value (is_slippery).\n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    is_slippery=False,  # whether the grid-world behaves deterministically or not\n",
    ")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c1e21",
   "metadata": {},
   "source": [
    "#### 3. Inspect the environment action and observations spaces\n",
    "\n",
    "Gym Environments can be deterministic or stochastic.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Deterministic</b> if the current state + selected action determines the next state of the environment.  Chess is a deterministic environment, since all possible states/action combinations can be described as a discrete set of rules with states bounded by the pieces and size of the board.</li>\n",
    "    <li>\n",
    "        <b>Stochastic</b> if the policy output action is a probability distribution over a set of possible actions at time step t. In this case the agent needs to compute its action from the policy in two steps. i) sample actions from the policy according to the probability distribution, ii) compute log likelihoods of the actions. Stochastic environments are random in nature.  Random visitors to a website is an example of a stochastic environment. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Gym actions.</b> The action_space describes the numerical structure of the legitimate actions that can be applied to the environment. \n",
    "\n",
    "For example, if we have 4 possible discrete actions, we could encode them as:\n",
    "<ul>\n",
    "    <li>0: LEFT</li>\n",
    "    <li>1: DOWN</li>\n",
    "    <li>2: RIGHT</li>\n",
    "    <li>3: UP</li>\n",
    "</ul>\n",
    "\n",
    "<b>Gym observations.</b>  The observation_space defines the structure as well as the legitimate values for the observation of the state of the environment.  \n",
    "\n",
    "For example, if we have a 4x4 grid, we could encode them as {0,1,2,3, 4, ‚Ä¶ ,16} for grid positions ((0,0), (0,1), (0,2), (0,3), ‚Ä¶. (3,3)).\n",
    "\n",
    "\n",
    "From the Gym [documentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) about the frozen lake environment, we see: <br>\n",
    "\n",
    "|Frozen Lake      | Gym space   |\n",
    "|---------------- | ----------- |\n",
    "|Action Space     | Discrete(4) |\n",
    "|Observation Space| Discrete(16)|\n",
    "\n",
    "\n",
    " \n",
    "<b><a href=\"https://github.com/openai/gym/tree/master/gym/spaces\">Gym spaces</a></b> are gym data types.  The main types are `Discrete` for discrete numbers and `Box` for continuous numbers.  \n",
    "\n",
    "Gym Space `Discrete` elements are Python type `int`, and Gym Space `Box` are Python type `float32`.\n",
    "\n",
    "Below is an example how to inspect the environment action and observations spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78620a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a gym environment.\n",
      "\n",
      "gym action space: Discrete(4)\n",
      "gym observation space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# check if it is a gym instance\n",
    "if isinstance(env, gym.Env):\n",
    "    print(\"This is a gym environment.\")\n",
    "    print()\n",
    "\n",
    "    # print gym Spaces\n",
    "    if isinstance(env.action_space, gym.spaces.Space):\n",
    "        print(f\"gym action space: {env.action_space}\")\n",
    "    if isinstance(env.observation_space, gym.spaces.Space):\n",
    "        print(f\"gym observation space: {env.observation_space}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad91e6",
   "metadata": {},
   "source": [
    "#### 4. Inspect gym environment parameters\n",
    "\n",
    "Gym environments contain 2 sets of configuration parameters that are set after the environment object is instantiated.\n",
    "<ul>\n",
    "    <li><b>Runtime parameters</b> are passed into the make() function as **kwargs.</li>\n",
    "    <li><b>Default parameters</b> are fixed in the Gym environment code.</li>\n",
    "    </ul>\n",
    "\n",
    "Below is an example of how to inspect the environment parameters.  \n",
    "\n",
    "Notice we can tell from the parameters that our frozen lake environment is: \n",
    "1) Deterministic, and \n",
    "2) Episode terminates with time step condition max_episode_steps = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4dc0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime spec params...\n",
      "map_name: 4x4\n",
      "is_slippery: False\n",
      "\n",
      "Default spec params...\n",
      "id: FrozenLake-v1\n",
      "entry_point: gym.envs.toy_text:FrozenLakeEnv\n",
      "reward_threshold: 0.7\n",
      "nondeterministic: False\n",
      "max_episode_steps: 100\n",
      "order_enforce: True\n"
     ]
    }
   ],
   "source": [
    "# inspect env.spec parameters\n",
    " \n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print(\"Runtime spec params...\")\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    " \n",
    "# View default env spec params\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "print(f\"entry_point: {env_spec.entry_point}\")\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\")\n",
    "\n",
    "# We can tell that our frozen lake environment is: \n",
    "# 1) Deterministic, and \n",
    "# 2) Episode terminates with condition max_episode_steps = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd5020",
   "metadata": {},
   "source": [
    "#### 5. Perform some basic Gym API calls <a class=\"anchor\" id=\"intro_gym_api\"></a>\n",
    "\n",
    "The most basic Gym API methods are:\n",
    "<ul>\n",
    "    <li><b>env.reset()</b> <br>Reset the environment to an initial state, this is how you initialize an environment so you can run a simulation on it.  You should call this method every time to initiate a new episode.</li>\n",
    "    <li><b>env.render()</b>  <br>Visually inspect the environment anytime. Note you cannot inspect an environment before it has been initialized with env.reset().</li>\n",
    "    <li><b>env.step(action)</b> <br>Take an action from the possible action space values.  It accepts an action, computes the state of the environment after applying that action and returns the 4-tuple (observation, reward, done, info).</li>\n",
    "    <li><b>env.close()</b> <br>Close an environment.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd2bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Print the starting observation.  Recall possible observations are between 0-16.\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee81cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 1, reward: 0.0, done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Take an action\n",
    "# Recall the possible actions are: 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "\n",
    "new_obs, reward, done, _ = env.step(2) #Right\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()\n",
    "new_obs, reward, done, _ = env.step(1) #Down\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec105",
   "metadata": {},
   "source": [
    "We can also try to run an action in the frozen lake environment which is outside the defined number range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0620a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this cell if you want whole notebook to run without errors\n",
    "\n",
    "# Try to take an invalid action\n",
    "\n",
    "#env.step(4) # invalid\n",
    "\n",
    "# should see KeyError below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8a6854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ae297afe914f58b59516fc9831dc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# The following three lines are for rendering purposes only.\n",
    "# They allow us to render the env frame-by-frame in-place\n",
    "# (w/o creating a huge output which we would then have to scroll through).\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with out:\n",
    "\n",
    "    # Putting the simple API methods together.\n",
    "    # Here is a pattern for running a bunch of episodes.\n",
    "    num_episodes = 5 # Number of episodes you want to run the agent\n",
    "    total_reward = 0.0  # Initialize reward to 0\n",
    "\n",
    "    # Loop through episodes\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        # Reset the environment at the start of each episode\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Loop through time steps per episode\n",
    "        while True:\n",
    "            # take random action, but you can also do something more intelligent \n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # If the epsiode is up, then start another one\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Render the env (in place).\n",
    "            time.sleep(0.3)\n",
    "            out.clear_output(wait=True)\n",
    "            print(f\"episode: {ep}\")\n",
    "            print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "            env.render()\n",
    "\n",
    "            \n",
    "# Close the env\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f1156",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview of RLlib <a class=\"anchor\" id=\"intro_rllib\"></a>\n",
    "\n",
    "<img width=\"7%\" src=\"images/rllib-logo.png\"> is the most comprehensive open-source Reinforcement Learning framework. **[RLlib](https://github.com/ray-project/ray/tree/master/rllib)** is <b>distributed by default</b> since it is built on top of **[Ray](https://docs.ray.io/en/latest/)**, an easy-to-use, open-source, distributed computing framework for Python that can handle complex, heterogeneous applications. Ray and RLlib run on compute clusters on any cloud without vendor lock.\n",
    "\n",
    "RLlib includes <b>25+ available [algorithms](https://docs.ray.io/en/master/rllib/rllib-algorithms.html)</b>, converted to both <img width=\"3%\" src=\"images/tensorflow-logo.png\">_TensorFlow_ and <img width=\"3%\" src=\"images/pytorch-logo.png\">_PyTorch_, covering different sub-categories of RL: _model-free_, _offline RL_, _model-based_, and _gradient-free_.¬†Almost any RLlib algorithm can learn in a <b>multi-agent</b> setting.¬†Many algorithms support <b>RNNs</b> and <b>LSTMs</b>.\n",
    "\n",
    "On a very high level, RLlib is organized by **environments**, **algorithms**, **examples**, and **tuned_examples**.  \n",
    "\n",
    "    ray\n",
    "    |- rllib\n",
    "    |  |- algorithms \n",
    "    |  |  |- alpha_zero \n",
    "    |  |  |- appo \n",
    "    |  |  |- crr\n",
    "    |  |  |- ppo \n",
    "    |  |  |- ... \n",
    "    |  |- env \n",
    "    |  |- examples \n",
    "    |  |- tuned_examples\n",
    "\n",
    "Within **_env_** you will find different [base classes](https://docs.ray.io/en/latest/rllib/package_ref/env.html) that you can inherit from to make it easy to implement your environment. RLlib supports environments created using the **OpenAI Gym API** (which supports most user cases). The base classes in the env directory allow for users to implement environments that don't fall into common use cases such as multi agent environments and environments that have strict performance or hosting requirements. In the next notebook, you will see we're using the **RLlib MultiAgentEnv** base class to train a **multi agent** RL model.\n",
    "\n",
    "Within **_examples_** you will find some examples of common custom rllib use cases..  \n",
    "\n",
    "Within **_tuned_examples_**, you will find, sorted by algorithm, suggested hyperparameter value choices within .yaml files. Ray¬†RLlib team ran simulations/benchmarks to find suggested hyperparameter value choices.¬†¬†These¬†files used¬†for daily testing, and weekly hard-task testing to make sure they all run at speed,¬†for both TF and Torch.¬†Helps you with leg-up with parameter choices!\n",
    "\n",
    "\n",
    "In this tutorial, we will mainly focus on the **_algorithms_** package, where we will find RLlib algos to train policy models on environments.\n",
    "\n",
    "\n",
    "#### Step 1.  Import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5655b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d2983-90fd-48bd-b61a-1ee79b62770f",
   "metadata": {},
   "source": [
    "#### Check environment for errors\n",
    "\n",
    "Before you start training, it is a good idea to check the environment for errors.  RLlib provides a convenient [Environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) for this.  \n",
    "    \n",
    "Below, we start with a new environment, [Cart-Pole](https://www.gymlibrary.ml/environments/classic_control/cart_pole/), then in the next cell, check it for errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ddfdb23-2554-48cf-aeed-cdc1b19f02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<CartPoleEnv<CartPole-v1>>>\n",
      "env_spec: EnvSpec(CartPole-v1)\n",
      "Runtime spec params...\n",
      "\n",
      "Default spec params...\n",
      "id: CartPole-v1\n",
      "entry_point: gym.envs.classic_control:CartPoleEnv\n",
      "reward_threshold: 475.0\n",
      "nondeterministic: False\n",
      "max_episode_steps: 500\n",
      "order_enforce: True\n"
     ]
    }
   ],
   "source": [
    "# Instantiate gym env object with a runtime parameter value\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    " \n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print(\"Runtime spec params...\")\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    " \n",
    "# View default env spec params\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "print(f\"entry_point: {env_spec.entry_point}\")\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99b4730c-ad46-42c9-ae70-ef1f551883b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "\n",
    "# How to check you do not have any environment errors\n",
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a757e7",
   "metadata": {},
   "source": [
    "## Train a RL model using an algorithm from RLlib <a class=\"anchor\" id=\"intro_rllib_api\"></a>\n",
    "\n",
    "#### Step 2.  Select an algorithm and instantiate a config object using that algorithm's config class  \n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">Open RLlib docs</a></li>\n",
    "    <li>Scroll down and click url of algo you're searching for, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0de2e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common RLlib General config (for all algorithms)\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "config = AlgorithmConfig()\n",
    "\n",
    "# # Uncomment for long list of parameters\n",
    "# print(f\"RLlib Trainer's general default config is:\")\n",
    "# config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bb6a2",
   "metadata": {},
   "source": [
    "#### Step 3. Choose your config settings   \n",
    "\n",
    "As of Ray 1.13, RLlib configs been converted from primitive dictionaries into Objects. This makes them harder to print, but easier to set/pass.\n",
    "\n",
    "**Note about RLlib config precedence**\n",
    "<ol>\n",
    "    <li>Highest precedence are <b>trainer instantiation settings</b>, these override any other config settings</li>\n",
    "    <li>RLlib <b>specific algorithm config</b> (see config class description above)</li>\n",
    "    <li>RLlib <b><a href\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py#L58\">general config</a></b> settings have the lowest precedence</li>\n",
    "    </ol>\n",
    "\n",
    "\n",
    "**Note about num_workers**\n",
    "\n",
    "Number of Ray workers is the number of parallel workers or actors for rollouts.  Actual num_workers will be what you specifiy+1 for head node.\n",
    "\n",
    "<b>Use ONE LESS than the number of cores you want to use</b> (or omit this argument and let Ray automatically use all cores)! <br>\n",
    "\n",
    "\n",
    "Below, num_workers = 7  # means actual number workers=8 including head node; where 8 is #cpu on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99524e70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>\n"
     ]
    }
   ],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "# Define algorithm config values\n",
    "env_name = \"CartPole-v1\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"my_PPO_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "config_train = (\n",
    "    PPOConfig()\n",
    "    .framework(framework='torch')\n",
    "    .environment(env=env_name, disable_env_checking=False)\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    ")\n",
    "\n",
    "print(type(config_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb86ff8",
   "metadata": {},
   "source": [
    "#### Step 4. Instantiate a Trainer from the config object\n",
    "\n",
    "**Two ways to train RLlib models***\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/package_ref/index.html\">RLlib API.</a> The main methods are:</li>\n",
    "    <ul>\n",
    "        <li>train()</li>\n",
    "        <li>evaluate()</li>\n",
    "        <li>save()</li>\n",
    "        <li><b>restore()</b></li>\n",
    "        <li><b>compute_single_action()</b></li>\n",
    "    </ul>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/tune/api_docs/overview.html\">Ray Tune API.</a>  The main methods are:</li>\n",
    "        <ul>\n",
    "            <li><b>run()</b></li>\n",
    "    </ul>\n",
    "    </ol>\n",
    "    \n",
    "*RLlib CLI from command line using .yml file also exists, but the .yml file is undocumented: <i>rllib train -f [myfile_name].yml</i><br>\n",
    "\n",
    "üëâ RLlib API <b>.train()</b> will train for 1 episode only.  Good for debugging since every single output will be shown for the 1 episode of training.  \n",
    "\n",
    "üëâ However for usual purposes, Ray Tune API <b>.run()</b> is more convenient since with 1 function call you get experiment management: save, checkpoint, evaluate, and train subject to stopping criteria.\n",
    "\n",
    "‚úîÔ∏è Both methods will run the RLlib [environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) you saw earlier in this notebook (cells just after Step 1. Import ray).\n",
    "\n",
    "üëç You have to use RLlib API method <b>.restore()</b> to reload a checkpointed RLlib model for Serving and Offline learning.  Tune API methods will not work.\n",
    "\n",
    "üëç After a model is trained, it can be used in inference mode.  The RLlib API method <b>compute_single_action()</b> will use the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference).  You will see this method used at the end of this notebook.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>In summary, if you are going to train a RLlib model, train it use Ray Tune API method .run()!!  <br>\n",
    "    If you need to restore a RLlib model, use RLlib API method .restore()!!</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a6107f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###############\n",
    "# # EXAMPLE USING RLLIB API .train() FOR 1 EPISODE\n",
    "# # For completeness, here is how to train RLlib using RLlib API's .train() method\n",
    "# # The code below instantiates a trainer and trains for 1 single episode. \n",
    "# # To train for N number of episodes, you would put _.train()_ into a loop, \n",
    "# # similar to the way we ran the Gym _env.step()_ in a loop.\n",
    "# ###############\n",
    "\n",
    "# # To start fresh, restart Ray in case it is already running\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "    \n",
    "# # Use .build() similar to how gym environments are passed to the gym .make() method.\n",
    "# rllib_algo = config_train.build(env=env_name)\n",
    "# print(type(rllib_algo))\n",
    "\n",
    "# # run the trainer for 1 episode\n",
    "# for i in range(10):\n",
    "#   result = rllib_algo.train()\n",
    "#   print(f\"Iteration={i} Episode R={result['episode_reward_mean']}\")\n",
    "# # Release Algorithm's resources.\n",
    "# rllib_algo.stop()\n",
    "\n",
    "# # Below, you will see the output evaluation_interval times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62252115",
   "metadata": {},
   "source": [
    "From the above cell, you can see how to train a RLlib algorithm 1 episode at a time.  But it is more practical to train RLlib algorithms using Ray Tune, since many more options are available.\n",
    "\n",
    "**Instantiate a trainer using Ray Tune API**\n",
    "\n",
    "Ray Tune offers experiment management in a single call <b>.run()</b>.  In the code below, we <b>specify a stopping criteria</b> to train until a certain Reward is achieved.  In case the desired training reward level is never reached, backup stop criteria can be given.  Tune will stop training whenever the earliest stop criteria is met.  However, best practice starting out is to only have 1 criteria, so you can be sure what is going to happen.\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e34aa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 18:02:08,576\tINFO services.py:1477 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2022-07-15 18:02:11,419\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-15 18:06:08 (running for 00:03:57.20)<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/5.31 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_7c74f_00000</td><td>TERMINATED</td><td>127.0.0.1:6351</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         213.113</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  406.41</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  58</td><td style=\"text-align: right;\">            406.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 18:02:11,824\tINFO plugin_schema_manager.py:51 -- Loading the default runtime env schemas: ['/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n",
      "\u001b[2m\u001b[36m(PPO pid=6351)\u001b[0m 2022-07-15 18:02:23,244\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=6351)\u001b[0m 2022-07-15 18:02:23,244\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6351)\u001b[0m 2022-07-15 18:02:23,244\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=6351)\u001b[0m 2022-07-15 18:02:34,945\tINFO trainable.py:160 -- Trainable.setup took 11.706 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=6351)\u001b[0m 2022-07-15 18:02:34,945\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.71256862404526, 'policy_loss': -0.039127241408512474, 'vf_loss': 8.74584205381332, 'vf_explained_var': 0.00914683021524901, 'kl': 0.029268890468861712, 'entropy': 0.6648874534073697, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000},sampler_results={'episode_reward_max': 72.0, 'episode_reward_min': 9.0, 'episode_reward_mean': 21.281767955801104, 'episode_len_mean': 21.281767955801104, 'episode_media': {}, 'episodes_this_iter': 181, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [29.0, 11.0, 34.0, 11.0, 35.0, 18.0, 15.0, 17.0, 34.0, 24.0, 32.0, 43.0, 16.0, 21.0, 20.0, 15.0, 19.0, 15.0, 27.0, 24.0, 15.0, 34.0, 12.0, 19.0, 15.0, 12.0, 41.0, 18.0, 20.0, 31.0, 29.0, 10.0, 23.0, 26.0, 20.0, 16.0, 15.0, 11.0, 13.0, 21.0, 12.0, 27.0, 11.0, 11.0, 30.0, 11.0, 10.0, 39.0, 22.0, 38.0, 15.0, 14.0, 11.0, 14.0, 20.0, 27.0, 17.0, 12.0, 11.0, 22.0, 17.0, 13.0, 19.0, 13.0, 18.0, 11.0, 25.0, 40.0, 16.0, 14.0, 12.0, 24.0, 18.0, 20.0, 23.0, 18.0, 21.0, 13.0, 40.0, 56.0, 11.0, 27.0, 14.0, 14.0, 21.0, 21.0, 12.0, 20.0, 23.0, 29.0, 15.0, 14.0, 10.0, 24.0, 22.0, 10.0, 10.0, 12.0, 11.0, 12.0, 23.0, 21.0, 13.0, 17.0, 16.0, 35.0, 25.0, 40.0, 19.0, 28.0, 11.0, 19.0, 19.0, 40.0, 21.0, 42.0, 23.0, 36.0, 15.0, 36.0, 10.0, 16.0, 13.0, 15.0, 26.0, 66.0, 42.0, 20.0, 13.0, 15.0, 13.0, 20.0, 16.0, 18.0, 13.0, 72.0, 12.0, 11.0, 13.0, 15.0, 26.0, 14.0, 28.0, 26.0, 14.0, 51.0, 15.0, 12.0, 16.0, 33.0, 24.0, 15.0, 11.0, 31.0, 14.0, 12.0, 46.0, 13.0, 19.0, 12.0, 11.0, 64.0, 50.0, 27.0, 16.0, 12.0, 31.0, 13.0, 9.0, 12.0, 10.0, 13.0, 23.0, 19.0, 27.0, 33.0, 14.0, 31.0, 27.0, 27.0, 15.0], 'episode_lengths': [29, 11, 34, 11, 35, 18, 15, 17, 34, 24, 32, 43, 16, 21, 20, 15, 19, 15, 27, 24, 15, 34, 12, 19, 15, 12, 41, 18, 20, 31, 29, 10, 23, 26, 20, 16, 15, 11, 13, 21, 12, 27, 11, 11, 30, 11, 10, 39, 22, 38, 15, 14, 11, 14, 20, 27, 17, 12, 11, 22, 17, 13, 19, 13, 18, 11, 25, 40, 16, 14, 12, 24, 18, 20, 23, 18, 21, 13, 40, 56, 11, 27, 14, 14, 21, 21, 12, 20, 23, 29, 15, 14, 10, 24, 22, 10, 10, 12, 11, 12, 23, 21, 13, 17, 16, 35, 25, 40, 19, 28, 11, 19, 19, 40, 21, 42, 23, 36, 15, 36, 10, 16, 13, 15, 26, 66, 42, 20, 13, 15, 13, 20, 16, 18, 13, 72, 12, 11, 13, 15, 26, 14, 28, 26, 14, 51, 15, 12, 16, 33, 24, 15, 11, 31, 14, 12, 46, 13, 19, 12, 11, 64, 50, 27, 16, 12, 31, 13, 9, 12, 10, 13, 23, 19, 27, 33, 14, 31, 27, 27, 15]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.11760136657698336, 'mean_inference_ms': 0.862859120137015, 'mean_action_processing_ms': 0.06467016680146769, 'mean_env_wait_ms': 0.06236825194153991, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=72.0,episode_reward_min=9.0,episode_reward_mean=21.281767955801104,episode_len_mean=21.281767955801104,episodes_this_iter=181,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.11760136657698336, 'mean_inference_ms': 0.862859120137015, 'mean_action_processing_ms': 0.06467016680146769, 'mean_env_wait_ms': 0.06236825194153991, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=4000,num_agent_steps_trained=4000,num_env_steps_sampled=4000,num_env_steps_trained=4000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=4000,timers={'training_iteration_time_ms': 6776.675, 'load_time_ms': 0.392, 'load_throughput': 10205119.221, 'learn_time_ms': 5630.751, 'learn_throughput': 710.385, 'synch_weights_time_ms': 1.599},counters={'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000},perf={'cpu_util_percent': 22.06, 'ram_util_percent': 73.80999999999999} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 233.0, 'episode_reward_min': 17.0, 'episode_reward_mean': 120.15, 'episode_len_mean': 120.15, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [196.0, 35.0, 233.0, 89.0, 195.0, 135.0, 131.0, 61.0, 166.0, 136.0, 38.0, 231.0, 76.0, 17.0, 141.0, 79.0, 36.0, 46.0, 210.0, 152.0], 'episode_lengths': [196, 35, 233, 89, 195, 135, 131, 61, 166, 136, 38, 231, 76, 17, 141, 79, 36, 46, 210, 152]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.0689812190520783, 'mean_inference_ms': 0.6925233588639195, 'mean_action_processing_ms': 0.05192179449782793, 'mean_env_wait_ms': 0.04740027143633903, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 2403, 'num_env_steps_sampled_this_iter': 2403, 'timesteps_this_iter': 2403, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.3, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.636595105099422, 'policy_loss': -0.030342744296837237, 'vf_loss': 8.66186943105472, 'vf_explained_var': 0.060554905924745786, 'kl': 0.01689484724049233, 'entropy': 0.6161398415924401, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000},sampler_results={'episode_reward_max': 136.0, 'episode_reward_min': 9.0, 'episode_reward_mean': 42.62, 'episode_len_mean': 42.62, 'episode_media': {}, 'episodes_this_iter': 83, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [16.0, 12.0, 31.0, 13.0, 9.0, 12.0, 10.0, 13.0, 23.0, 19.0, 27.0, 33.0, 14.0, 31.0, 27.0, 27.0, 15.0, 136.0, 35.0, 49.0, 96.0, 21.0, 26.0, 20.0, 11.0, 45.0, 53.0, 13.0, 30.0, 23.0, 44.0, 90.0, 71.0, 57.0, 23.0, 50.0, 31.0, 11.0, 47.0, 54.0, 12.0, 123.0, 50.0, 64.0, 54.0, 29.0, 71.0, 42.0, 55.0, 52.0, 9.0, 40.0, 17.0, 65.0, 15.0, 34.0, 21.0, 12.0, 96.0, 29.0, 44.0, 43.0, 30.0, 76.0, 46.0, 29.0, 41.0, 128.0, 136.0, 59.0, 29.0, 74.0, 46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0], 'episode_lengths': [16, 12, 31, 13, 9, 12, 10, 13, 23, 19, 27, 33, 14, 31, 27, 27, 15, 136, 35, 49, 96, 21, 26, 20, 11, 45, 53, 13, 30, 23, 44, 90, 71, 57, 23, 50, 31, 11, 47, 54, 12, 123, 50, 64, 54, 29, 71, 42, 55, 52, 9, 40, 17, 65, 15, 34, 21, 12, 96, 29, 44, 43, 30, 76, 46, 29, 41, 128, 136, 59, 29, 74, 46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.12411989776877673, 'mean_inference_ms': 0.9584169918203829, 'mean_action_processing_ms': 0.07130312274029978, 'mean_env_wait_ms': 0.06967929233315172, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=136.0,episode_reward_min=9.0,episode_reward_mean=42.62,episode_len_mean=42.62,episodes_this_iter=83,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.12411989776877673, 'mean_inference_ms': 0.9584169918203829, 'mean_action_processing_ms': 0.07130312274029978, 'mean_env_wait_ms': 0.06967929233315172, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=8000,num_agent_steps_trained=8000,num_env_steps_sampled=8000,num_env_steps_trained=8000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=8000,timers={'training_iteration_time_ms': 6913.39, 'load_time_ms': 0.435, 'load_throughput': 9198035.088, 'learn_time_ms': 5626.568, 'learn_throughput': 710.913, 'synch_weights_time_ms': 1.69},counters={'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000},perf={'cpu_util_percent': 20.83076923076923, 'ram_util_percent': 73.79999999999998} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.3, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.30877770967381, 'policy_loss': -0.021009840103246833, 'vf_loss': 9.327130740175965, 'vf_explained_var': 0.09925423796458911, 'kl': 0.008856009830368331, 'entropy': 0.5828838939307839, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 12000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 12000},sampler_results={'episode_reward_max': 335.0, 'episode_reward_min': 9.0, 'episode_reward_mean': 70.97, 'episode_len_mean': 70.97, 'episode_media': {}, 'episodes_this_iter': 31, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [90.0, 71.0, 57.0, 23.0, 50.0, 31.0, 11.0, 47.0, 54.0, 12.0, 123.0, 50.0, 64.0, 54.0, 29.0, 71.0, 42.0, 55.0, 52.0, 9.0, 40.0, 17.0, 65.0, 15.0, 34.0, 21.0, 12.0, 96.0, 29.0, 44.0, 43.0, 30.0, 76.0, 46.0, 29.0, 41.0, 128.0, 136.0, 59.0, 29.0, 74.0, 46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0], 'episode_lengths': [90, 71, 57, 23, 50, 31, 11, 47, 54, 12, 123, 50, 64, 54, 29, 71, 42, 55, 52, 9, 40, 17, 65, 15, 34, 21, 12, 96, 29, 44, 43, 30, 76, 46, 29, 41, 128, 136, 59, 29, 74, 46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.12190035914949277, 'mean_inference_ms': 0.9574463925059397, 'mean_action_processing_ms': 0.07096220581628547, 'mean_env_wait_ms': 0.06964452676231846, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=335.0,episode_reward_min=9.0,episode_reward_mean=70.97,episode_len_mean=70.97,episodes_this_iter=31,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.12190035914949277, 'mean_inference_ms': 0.9574463925059397, 'mean_action_processing_ms': 0.07096220581628547, 'mean_env_wait_ms': 0.06964452676231846, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=12000,num_agent_steps_trained=12000,num_env_steps_sampled=12000,num_env_steps_trained=12000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=12000,timers={'training_iteration_time_ms': 6720.312, 'load_time_ms': 0.437, 'load_throughput': 9156203.02, 'learn_time_ms': 5526.129, 'learn_throughput': 723.834, 'synch_weights_time_ms': 1.651},counters={'num_env_steps_sampled': 12000, 'num_env_steps_trained': 12000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 12000},perf={'cpu_util_percent': 19.833333333333332, 'ram_util_percent': 73.65555555555557} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 223.0, 'episode_reward_mean': 356.1, 'episode_len_mean': 356.1, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [440.0, 419.0, 273.0, 384.0, 363.0, 284.0, 296.0, 245.0, 299.0, 393.0, 500.0, 487.0, 223.0, 380.0, 279.0, 411.0, 319.0, 318.0, 500.0, 309.0], 'episode_lengths': [440, 419, 273, 384, 363, 284, 296, 245, 299, 393, 500, 487, 223, 380, 279, 411, 319, 318, 500, 309]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.0665097734193804, 'mean_inference_ms': 0.6948196660213402, 'mean_action_processing_ms': 0.05161884875700999, 'mean_env_wait_ms': 0.047627302270313046, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 7122, 'num_env_steps_sampled_this_iter': 7122, 'timesteps_this_iter': 7122, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.3, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.52878723247077, 'policy_loss': -0.015254268881374149, 'vf_loss': 9.542681080807922, 'vf_explained_var': 0.08181779474340459, 'kl': 0.004534667909946376, 'entropy': 0.5641433158869384, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 16000, 'num_env_steps_trained': 16000, 'num_agent_steps_sampled': 16000, 'num_agent_steps_trained': 16000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 9.0, 'episode_reward_mean': 100.87, 'episode_len_mean': 100.87, 'episode_media': {}, 'episodes_this_iter': 19, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [9.0, 40.0, 17.0, 65.0, 15.0, 34.0, 21.0, 12.0, 96.0, 29.0, 44.0, 43.0, 30.0, 76.0, 46.0, 29.0, 41.0, 128.0, 136.0, 59.0, 29.0, 74.0, 46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0], 'episode_lengths': [9, 40, 17, 65, 15, 34, 21, 12, 96, 29, 44, 43, 30, 76, 46, 29, 41, 128, 136, 59, 29, 74, 46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.11852192337386443, 'mean_inference_ms': 0.9399033970904824, 'mean_action_processing_ms': 0.06969546953820766, 'mean_env_wait_ms': 0.06823692225991146, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=9.0,episode_reward_mean=100.87,episode_len_mean=100.87,episodes_this_iter=19,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.11852192337386443, 'mean_inference_ms': 0.9399033970904824, 'mean_action_processing_ms': 0.06969546953820766, 'mean_env_wait_ms': 0.06823692225991146, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=16000,num_agent_steps_trained=16000,num_env_steps_sampled=16000,num_env_steps_trained=16000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=16000,timers={'training_iteration_time_ms': 6584.442, 'load_time_ms': 0.43, 'load_throughput': 9311622.589, 'learn_time_ms': 5426.143, 'learn_throughput': 737.172, 'synch_weights_time_ms': 1.635},counters={'num_env_steps_sampled': 16000, 'num_env_steps_trained': 16000, 'num_agent_steps_sampled': 16000, 'num_agent_steps_trained': 16000},perf={'cpu_util_percent': 16.33333333333333, 'ram_util_percent': 73.01111111111112} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.15, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.620804404186947, 'policy_loss': -0.013570404084779882, 'vf_loss': 9.633693033649076, 'vf_explained_var': 0.039515253484890024, 'kl': 0.0045453134315362196, 'entropy': 0.5404751148595605, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 20000, 'num_env_steps_trained': 20000, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 133.54, 'episode_len_mean': 133.54, 'episode_media': {}, 'episodes_this_iter': 12, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [30.0, 76.0, 46.0, 29.0, 41.0, 128.0, 136.0, 59.0, 29.0, 74.0, 46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0], 'episode_lengths': [30, 76, 46, 29, 41, 128, 136, 59, 29, 74, 46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.11592581421787555, 'mean_inference_ms': 0.9268874749647226, 'mean_action_processing_ms': 0.06876905161819975, 'mean_env_wait_ms': 0.0671933667436809, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=133.54,episode_len_mean=133.54,episodes_this_iter=12,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.11592581421787555, 'mean_inference_ms': 0.9268874749647226, 'mean_action_processing_ms': 0.06876905161819975, 'mean_env_wait_ms': 0.0671933667436809, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=20000,num_agent_steps_trained=20000,num_env_steps_sampled=20000,num_env_steps_trained=20000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=20000,timers={'training_iteration_time_ms': 6704.001, 'load_time_ms': 0.428, 'load_throughput': 9349763.709, 'learn_time_ms': 5573.527, 'learn_throughput': 717.678, 'synch_weights_time_ms': 1.622},counters={'num_env_steps_sampled': 20000, 'num_env_steps_trained': 20000, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000},perf={'cpu_util_percent': 23.39, 'ram_util_percent': 73.15} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 218.0, 'episode_reward_mean': 348.2, 'episode_len_mean': 348.2, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [310.0, 284.0, 319.0, 376.0, 239.0, 251.0, 389.0, 500.0, 500.0, 218.0, 329.0, 305.0, 225.0, 500.0, 497.0, 427.0, 345.0, 301.0, 347.0, 302.0], 'episode_lengths': [310, 284, 319, 376, 239, 251, 389, 500, 500, 218, 329, 305, 225, 500, 497, 427, 345, 301, 347, 302]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06679178657786204, 'mean_inference_ms': 0.7025356217541068, 'mean_action_processing_ms': 0.05219971649280962, 'mean_env_wait_ms': 0.04829965699000962, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 6964, 'num_env_steps_sampled_this_iter': 6964, 'timesteps_this_iter': 6964, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.075, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.61737614088161, 'policy_loss': -0.011717201763343427, 'vf_loss': 9.62890136370095, 'vf_explained_var': 0.05241622585122303, 'kl': 0.0025596711704815576, 'entropy': 0.5673649104051692, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 24000, 'num_env_steps_trained': 24000, 'num_agent_steps_sampled': 24000, 'num_agent_steps_trained': 24000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 170.07, 'episode_len_mean': 170.07, 'episode_media': {}, 'episodes_this_iter': 10, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0], 'episode_lengths': [46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.11399039916213567, 'mean_inference_ms': 0.9166958089432051, 'mean_action_processing_ms': 0.06780737387925238, 'mean_env_wait_ms': 0.06652545328648818, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=170.07,episode_len_mean=170.07,episodes_this_iter=10,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.11399039916213567, 'mean_inference_ms': 0.9166958089432051, 'mean_action_processing_ms': 0.06780737387925238, 'mean_env_wait_ms': 0.06652545328648818, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=24000,num_agent_steps_trained=24000,num_env_steps_sampled=24000,num_env_steps_trained=24000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=24000,timers={'training_iteration_time_ms': 6747.879, 'load_time_ms': 0.438, 'load_throughput': 9122183.598, 'learn_time_ms': 5608.217, 'learn_throughput': 713.239, 'synch_weights_time_ms': 1.629},counters={'num_env_steps_sampled': 24000, 'num_env_steps_trained': 24000, 'num_agent_steps_sampled': 24000, 'num_agent_steps_trained': 24000},perf={'cpu_util_percent': 19.084210526315786, 'ram_util_percent': 73.22105263157896} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.572376381453648, 'policy_loss': -0.013931177220036907, 'vf_loss': 9.586031277974447, 'vf_explained_var': 0.08762666243378835, 'kl': 0.0073657950539655495, 'entropy': 0.5655713231332841, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 28000, 'num_env_steps_trained': 28000, 'num_agent_steps_sampled': 28000, 'num_agent_steps_trained': 28000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 203.86, 'episode_len_mean': 203.86, 'episode_media': {}, 'episodes_this_iter': 11, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0], 'episode_lengths': [41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.11154296649469153, 'mean_inference_ms': 0.9046294819078394, 'mean_action_processing_ms': 0.06674670353465437, 'mean_env_wait_ms': 0.06566355996433577, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=203.86,episode_len_mean=203.86,episodes_this_iter=11,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.11154296649469153, 'mean_inference_ms': 0.9046294819078394, 'mean_action_processing_ms': 0.06674670353465437, 'mean_env_wait_ms': 0.06566355996433577, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=28000,num_agent_steps_trained=28000,num_env_steps_sampled=28000,num_env_steps_trained=28000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=28000,timers={'training_iteration_time_ms': 6767.224, 'load_time_ms': 0.437, 'load_throughput': 9159297.457, 'learn_time_ms': 5642.85, 'learn_throughput': 708.862, 'synch_weights_time_ms': 1.628},counters={'num_env_steps_sampled': 28000, 'num_env_steps_trained': 28000, 'num_agent_steps_sampled': 28000, 'num_agent_steps_trained': 28000},perf={'cpu_util_percent': 21.84, 'ram_util_percent': 73.38000000000001} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 233.0, 'episode_reward_mean': 403.35, 'episode_len_mean': 403.35, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 283.0, 500.0, 363.0, 500.0, 254.0, 266.0, 500.0, 278.0, 500.0, 500.0, 233.0, 302.0, 500.0, 288.0, 467.0, 500.0, 440.0, 393.0, 500.0], 'episode_lengths': [500, 283, 500, 363, 500, 254, 266, 500, 278, 500, 500, 233, 302, 500, 288, 467, 500, 440, 393, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06842363704497006, 'mean_inference_ms': 0.7207955493500593, 'mean_action_processing_ms': 0.05352031564695049, 'mean_env_wait_ms': 0.04992464852118505, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 8067, 'num_env_steps_sampled_this_iter': 8067, 'timesteps_this_iter': 8067, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.58939728070331, 'policy_loss': -0.013702172360893699, 'vf_loss': 9.602893292006627, 'vf_explained_var': 0.05286624527746631, 'kl': 0.0054979774167862976, 'entropy': 0.5632843732833862, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 32000, 'num_env_steps_trained': 32000, 'num_agent_steps_sampled': 32000, 'num_agent_steps_trained': 32000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 236.97, 'episode_len_mean': 236.97, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0], 'episode_lengths': [19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10924252270792927, 'mean_inference_ms': 0.8940688402338977, 'mean_action_processing_ms': 0.06588580951107675, 'mean_env_wait_ms': 0.0648376810240726, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=236.97,episode_len_mean=236.97,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10924252270792927, 'mean_inference_ms': 0.8940688402338977, 'mean_action_processing_ms': 0.06588580951107675, 'mean_env_wait_ms': 0.0648376810240726, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=32000,num_agent_steps_trained=32000,num_env_steps_sampled=32000,num_env_steps_trained=32000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=32000,timers={'training_iteration_time_ms': 6740.18, 'load_time_ms': 0.436, 'load_throughput': 9176653.084, 'learn_time_ms': 5632.038, 'learn_throughput': 710.223, 'synch_weights_time_ms': 1.633},counters={'num_env_steps_sampled': 32000, 'num_env_steps_trained': 32000, 'num_agent_steps_sampled': 32000, 'num_agent_steps_trained': 32000},perf={'cpu_util_percent': 17.764999999999997, 'ram_util_percent': 73.20499999999998} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.609002396368211, 'policy_loss': -0.012169052180784044, 'vf_loss': 9.62096002537717, 'vf_explained_var': 0.00598917725265667, 'kl': 0.005637865676712509, 'entropy': 0.5510125609495307, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 36000, 'num_env_steps_trained': 36000, 'num_agent_steps_sampled': 36000, 'num_agent_steps_trained': 36000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 270.85, 'episode_len_mean': 270.85, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0], 'episode_lengths': [148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10729584691754887, 'mean_inference_ms': 0.8866824204434299, 'mean_action_processing_ms': 0.06528376678451575, 'mean_env_wait_ms': 0.06425199672534132, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=270.85,episode_len_mean=270.85,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10729584691754887, 'mean_inference_ms': 0.8866824204434299, 'mean_action_processing_ms': 0.06528376678451575, 'mean_env_wait_ms': 0.06425199672534132, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=36000,num_agent_steps_trained=36000,num_env_steps_sampled=36000,num_env_steps_trained=36000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=36000,timers={'training_iteration_time_ms': 6813.047, 'load_time_ms': 0.446, 'load_throughput': 8965913.188, 'learn_time_ms': 5667.671, 'learn_throughput': 705.757, 'synch_weights_time_ms': 1.637},counters={'num_env_steps_sampled': 36000, 'num_env_steps_trained': 36000, 'num_agent_steps_sampled': 36000, 'num_agent_steps_trained': 36000},perf={'cpu_util_percent': 24.970000000000002, 'ram_util_percent': 73.26000000000002} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 200.0, 'episode_reward_mean': 350.75, 'episode_len_mean': 350.75, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [418.0, 298.0, 239.0, 321.0, 443.0, 231.0, 376.0, 260.0, 346.0, 500.0, 311.0, 340.0, 500.0, 411.0, 363.0, 200.0, 228.0, 437.0, 472.0, 321.0], 'episode_lengths': [418, 298, 239, 321, 443, 231, 376, 260, 346, 500, 311, 340, 500, 411, 363, 200, 228, 437, 472, 321]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06949027107496852, 'mean_inference_ms': 0.732580390279169, 'mean_action_processing_ms': 0.0543354522074853, 'mean_env_wait_ms': 0.05112256324745834, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 7015, 'num_env_steps_sampled_this_iter': 7015, 'timesteps_this_iter': 7015, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.587440517384518, 'policy_loss': -0.012225248161903632, 'vf_loss': 9.59946260862453, 'vf_explained_var': 0.13061305521636882, 'kl': 0.0054170842830765405, 'entropy': 0.5636245527575093, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 40000, 'num_env_steps_trained': 40000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 301.52, 'episode_len_mean': 301.52, 'episode_media': {}, 'episodes_this_iter': 10, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0], 'episode_lengths': [70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10605828522181683, 'mean_inference_ms': 0.8829336408438431, 'mean_action_processing_ms': 0.06497927637710026, 'mean_env_wait_ms': 0.06391389914912288, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=301.52,episode_len_mean=301.52,episodes_this_iter=10,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10605828522181683, 'mean_inference_ms': 0.8829336408438431, 'mean_action_processing_ms': 0.06497927637710026, 'mean_env_wait_ms': 0.06391389914912288, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=40000,num_agent_steps_trained=40000,num_env_steps_sampled=40000,num_env_steps_trained=40000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=40000,timers={'training_iteration_time_ms': 6885.782, 'load_time_ms': 0.448, 'load_throughput': 8934030.566, 'learn_time_ms': 5743.424, 'learn_throughput': 696.449, 'synch_weights_time_ms': 1.656},counters={'num_env_steps_sampled': 40000, 'num_env_steps_trained': 40000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000},perf={'cpu_util_percent': 21.74285714285714, 'ram_util_percent': 73.34761904761906} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.355498544631466, 'policy_loss': -0.01305214251361547, 'vf_loss': 9.368252878291633, 'vf_explained_var': 0.25000524245282657, 'kl': 0.007941887115554032, 'entropy': 0.541136517704174, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 44000, 'num_env_steps_trained': 44000, 'num_agent_steps_sampled': 44000, 'num_agent_steps_trained': 44000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 21.0, 'episode_reward_mean': 321.06, 'episode_len_mean': 321.06, 'episode_media': {}, 'episodes_this_iter': 17, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0], 'episode_lengths': [92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10398463024143798, 'mean_inference_ms': 0.876651921791401, 'mean_action_processing_ms': 0.0644372380495568, 'mean_env_wait_ms': 0.06342163902158854, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=21.0,episode_reward_mean=321.06,episode_len_mean=321.06,episodes_this_iter=17,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10398463024143798, 'mean_inference_ms': 0.876651921791401, 'mean_action_processing_ms': 0.0644372380495568, 'mean_env_wait_ms': 0.06342163902158854, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=44000,num_agent_steps_trained=44000,num_env_steps_sampled=44000,num_env_steps_trained=44000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=44000,timers={'training_iteration_time_ms': 6876.802, 'load_time_ms': 0.463, 'load_throughput': 8648049.485, 'learn_time_ms': 5746.75, 'learn_throughput': 696.046, 'synch_weights_time_ms': 1.659},counters={'num_env_steps_sampled': 44000, 'num_env_steps_trained': 44000, 'num_agent_steps_sampled': 44000, 'num_agent_steps_trained': 44000},perf={'cpu_util_percent': 21.3, 'ram_util_percent': 73.6888888888889} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 290.0, 'episode_reward_min': 152.0, 'episode_reward_mean': 215.8, 'episode_len_mean': 215.8, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [173.0, 264.0, 244.0, 228.0, 205.0, 180.0, 184.0, 252.0, 159.0, 237.0, 261.0, 290.0, 190.0, 231.0, 152.0, 189.0, 179.0, 222.0, 230.0, 246.0], 'episode_lengths': [173, 264, 244, 228, 205, 180, 184, 252, 159, 237, 261, 290, 190, 231, 152, 189, 179, 222, 230, 246]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06944360315294815, 'mean_inference_ms': 0.7308798605875941, 'mean_action_processing_ms': 0.05422043603709167, 'mean_env_wait_ms': 0.05098469173382077, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 4316, 'num_env_steps_sampled_this_iter': 4316, 'timesteps_this_iter': 4316, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.415102524911203, 'policy_loss': -0.019376018050537316, 'vf_loss': 9.434113785528368, 'vf_explained_var': 0.19129753772930433, 'kl': 0.009726393465639716, 'entropy': 0.5450151594095333, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 48000, 'num_env_steps_trained': 48000, 'num_agent_steps_sampled': 48000, 'num_agent_steps_trained': 48000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 21.0, 'episode_reward_mean': 341.24, 'episode_len_mean': 341.24, 'episode_media': {}, 'episodes_this_iter': 12, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0], 'episode_lengths': [53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10280563141997906, 'mean_inference_ms': 0.8733737028112258, 'mean_action_processing_ms': 0.06419760225979043, 'mean_env_wait_ms': 0.06312049271929798, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=21.0,episode_reward_mean=341.24,episode_len_mean=341.24,episodes_this_iter=12,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10280563141997906, 'mean_inference_ms': 0.8733737028112258, 'mean_action_processing_ms': 0.06419760225979043, 'mean_env_wait_ms': 0.06312049271929798, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=48000,num_agent_steps_trained=48000,num_env_steps_sampled=48000,num_env_steps_trained=48000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=48000,timers={'training_iteration_time_ms': 6846.276, 'load_time_ms': 0.459, 'load_throughput': 8715889.657, 'learn_time_ms': 5754.121, 'learn_throughput': 695.154, 'synch_weights_time_ms': 1.648},counters={'num_env_steps_sampled': 48000, 'num_env_steps_trained': 48000, 'num_agent_steps_sampled': 48000, 'num_agent_steps_trained': 48000},perf={'cpu_util_percent': 19.40666666666667, 'ram_util_percent': 73.69333333333334} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.293730189210626, 'policy_loss': -0.006062260696724538, 'vf_loss': 9.299556821392429, 'vf_explained_var': 0.33705648831141893, 'kl': 0.00628290909315946, 'entropy': 0.5396269858844819, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 52000, 'num_env_steps_trained': 52000, 'num_agent_steps_sampled': 52000, 'num_agent_steps_trained': 52000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 335.74, 'episode_len_mean': 335.74, 'episode_media': {}, 'episodes_this_iter': 18, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0], 'episode_lengths': [207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10181068602635623, 'mean_inference_ms': 0.8728382033182369, 'mean_action_processing_ms': 0.06410865162737789, 'mean_env_wait_ms': 0.06305300237250519, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=335.74,episode_len_mean=335.74,episodes_this_iter=18,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10181068602635623, 'mean_inference_ms': 0.8728382033182369, 'mean_action_processing_ms': 0.06410865162737789, 'mean_env_wait_ms': 0.06305300237250519, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=52000,num_agent_steps_trained=52000,num_env_steps_sampled=52000,num_env_steps_trained=52000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=52000,timers={'training_iteration_time_ms': 6905.779, 'load_time_ms': 0.462, 'load_throughput': 8649387.019, 'learn_time_ms': 5789.962, 'learn_throughput': 690.851, 'synch_weights_time_ms': 1.659},counters={'num_env_steps_sampled': 52000, 'num_env_steps_trained': 52000, 'num_agent_steps_sampled': 52000, 'num_agent_steps_trained': 52000},perf={'cpu_util_percent': 22.98, 'ram_util_percent': 73.04999999999998} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 194.0, 'episode_reward_mean': 447.4, 'episode_len_mean': 447.4, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 496.0, 352.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 262.0, 500.0, 500.0, 500.0, 318.0, 500.0, 368.0, 500.0, 458.0, 500.0, 194.0], 'episode_lengths': [500, 496, 352, 500, 500, 500, 500, 500, 500, 262, 500, 500, 500, 318, 500, 368, 500, 458, 500, 194]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.07037190559748632, 'mean_inference_ms': 0.7411247380699331, 'mean_action_processing_ms': 0.05498020386693735, 'mean_env_wait_ms': 0.05188595228716738, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 8948, 'num_env_steps_sampled_this_iter': 8948, 'timesteps_this_iter': 8948, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.388293426267563, 'policy_loss': -0.01007595468632957, 'vf_loss': 9.398232944037325, 'vf_explained_var': 0.16214527122436032, 'kl': 0.003638541375525363, 'entropy': 0.5258719867596062, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 56000, 'num_env_steps_trained': 56000, 'num_agent_steps_sampled': 56000, 'num_agent_steps_trained': 56000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 339.2, 'episode_len_mean': 339.2, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0], 'episode_lengths': [500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10164170120704168, 'mean_inference_ms': 0.8746276921426056, 'mean_action_processing_ms': 0.06425573849038622, 'mean_env_wait_ms': 0.06318001588790287, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=339.2,episode_len_mean=339.2,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10164170120704168, 'mean_inference_ms': 0.8746276921426056, 'mean_action_processing_ms': 0.06425573849038622, 'mean_env_wait_ms': 0.06318001588790287, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=56000,num_agent_steps_trained=56000,num_env_steps_sampled=56000,num_env_steps_trained=56000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=56000,timers={'training_iteration_time_ms': 7095.2, 'load_time_ms': 0.48, 'load_throughput': 8337333.4, 'learn_time_ms': 5942.523, 'learn_throughput': 673.115, 'synch_weights_time_ms': 1.66},counters={'num_env_steps_sampled': 56000, 'num_env_steps_trained': 56000, 'num_agent_steps_sampled': 56000, 'num_agent_steps_trained': 56000},perf={'cpu_util_percent': 21.287499999999998, 'ram_util_percent': 72.91666666666667} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.01875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.416324819544311, 'policy_loss': -0.007287045033468354, 'vf_loss': 9.423532508521951, 'vf_explained_var': 0.005176296285403672, 'kl': 0.004233596965905569, 'entropy': 0.5301542523086712, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 60000, 'num_env_steps_trained': 60000, 'num_agent_steps_sampled': 60000, 'num_agent_steps_trained': 60000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 344.4, 'episode_len_mean': 344.4, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10160728997613216, 'mean_inference_ms': 0.8764078428014733, 'mean_action_processing_ms': 0.06433840193201576, 'mean_env_wait_ms': 0.06332192418500028, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=344.4,episode_len_mean=344.4,episodes_this_iter=8,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10160728997613216, 'mean_inference_ms': 0.8764078428014733, 'mean_action_processing_ms': 0.06433840193201576, 'mean_env_wait_ms': 0.06332192418500028, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=60000,num_agent_steps_trained=60000,num_env_steps_sampled=60000,num_env_steps_trained=60000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=60000,timers={'training_iteration_time_ms': 7158.571, 'load_time_ms': 0.491, 'load_throughput': 8151798.261, 'learn_time_ms': 5986.685, 'learn_throughput': 668.149, 'synch_weights_time_ms': 1.67},counters={'num_env_steps_sampled': 60000, 'num_env_steps_trained': 60000, 'num_agent_steps_sampled': 60000, 'num_agent_steps_trained': 60000},perf={'cpu_util_percent': 26.572727272727274, 'ram_util_percent': 73.12727272727271} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 500.0, 'episode_reward_mean': 500.0, 'episode_len_mean': 500.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06985320452918928, 'mean_inference_ms': 0.7393122093275258, 'mean_action_processing_ms': 0.05465018965759879, 'mean_env_wait_ms': 0.05144956501649032, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 10000, 'num_env_steps_sampled_this_iter': 10000, 'timesteps_this_iter': 10000, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.009375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.414241000144713, 'policy_loss': -0.005806554881955988, 'vf_loss': 9.420016736881708, 'vf_explained_var': -0.007781352791734921, 'kl': 0.003289392649692521, 'entropy': 0.5241313477036773, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 64000, 'num_env_steps_trained': 64000, 'num_agent_steps_sampled': 64000, 'num_agent_steps_trained': 64000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 356.26, 'episode_len_mean': 356.26, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 249.0, 500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [469, 500, 500, 463, 500, 500, 154, 249, 500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10178767756300836, 'mean_inference_ms': 0.8800895369580297, 'mean_action_processing_ms': 0.06458156607119436, 'mean_env_wait_ms': 0.06360912131525409, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=356.26,episode_len_mean=356.26,episodes_this_iter=8,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10178767756300836, 'mean_inference_ms': 0.8800895369580297, 'mean_action_processing_ms': 0.06458156607119436, 'mean_env_wait_ms': 0.06360912131525409, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=64000,num_agent_steps_trained=64000,num_env_steps_sampled=64000,num_env_steps_trained=64000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=64000,timers={'training_iteration_time_ms': 7198.008, 'load_time_ms': 0.541, 'load_throughput': 7387914.924, 'learn_time_ms': 5997.233, 'learn_throughput': 666.974, 'synch_weights_time_ms': 1.744},counters={'num_env_steps_sampled': 64000, 'num_env_steps_trained': 64000, 'num_agent_steps_sampled': 64000, 'num_agent_steps_trained': 64000},perf={'cpu_util_percent': 19.720833333333335, 'ram_util_percent': 73.27083333333333} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0046875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.354560178326022, 'policy_loss': -0.006925364447537289, 'vf_loss': 9.361454539145193, 'vf_explained_var': 0.005178798783210016, 'kl': 0.006617996746644971, 'entropy': 0.4953269959777914, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 68000, 'num_env_steps_trained': 68000, 'num_agent_steps_sampled': 68000, 'num_agent_steps_trained': 68000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 362.91, 'episode_len_mean': 362.91, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 500.0, 474.0, 500.0, 500.0, 483.0, 75.0, 500.0, 474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [500, 500, 474, 500, 500, 483, 75, 500, 474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10206354804268063, 'mean_inference_ms': 0.8842657959321586, 'mean_action_processing_ms': 0.06488308953977304, 'mean_env_wait_ms': 0.0639152309454302, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=362.91,episode_len_mean=362.91,episodes_this_iter=8,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10206354804268063, 'mean_inference_ms': 0.8842657959321586, 'mean_action_processing_ms': 0.06488308953977304, 'mean_env_wait_ms': 0.0639152309454302, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=68000,num_agent_steps_trained=68000,num_env_steps_sampled=68000,num_env_steps_trained=68000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=68000,timers={'training_iteration_time_ms': 7189.021, 'load_time_ms': 0.568, 'load_throughput': 7039489.783, 'learn_time_ms': 5982.094, 'learn_throughput': 668.662, 'synch_weights_time_ms': 1.749},counters={'num_env_steps_sampled': 68000, 'num_env_steps_trained': 68000, 'num_agent_steps_sampled': 68000, 'num_agent_steps_trained': 68000},perf={'cpu_util_percent': 22.62222222222222, 'ram_util_percent': 73.32222222222222} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 500.0, 'episode_reward_mean': 500.0, 'episode_len_mean': 500.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06926904756361393, 'mean_inference_ms': 0.7335958206364562, 'mean_action_processing_ms': 0.05424061285750018, 'mean_env_wait_ms': 0.05098115552824205, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 10000, 'num_env_steps_sampled_this_iter': 10000, 'timesteps_this_iter': 10000, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0046875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.302484333899713, 'policy_loss': -0.004020367162201994, 'vf_loss': 9.306473575099822, 'vf_explained_var': -0.0019477573133284045, 'kl': 0.006631236409160502, 'entropy': 0.5020913974572253, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 72000, 'num_env_steps_trained': 72000, 'num_agent_steps_sampled': 72000, 'num_agent_steps_trained': 72000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 367.59, 'episode_len_mean': 367.59, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [474.0, 191.0, 500.0, 400.0, 432.0, 413.0, 457.0, 212.0, 269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [474, 191, 500, 400, 432, 413, 457, 212, 269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10213575911218697, 'mean_inference_ms': 0.886338565576095, 'mean_action_processing_ms': 0.06500926073494369, 'mean_env_wait_ms': 0.06406110255648953, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=367.59,episode_len_mean=367.59,episodes_this_iter=8,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10213575911218697, 'mean_inference_ms': 0.886338565576095, 'mean_action_processing_ms': 0.06500926073494369, 'mean_env_wait_ms': 0.06406110255648953, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=72000,num_agent_steps_trained=72000,num_env_steps_sampled=72000,num_env_steps_trained=72000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=72000,timers={'training_iteration_time_ms': 7180.936, 'load_time_ms': 0.577, 'load_throughput': 6938181.217, 'learn_time_ms': 5957.657, 'learn_throughput': 671.405, 'synch_weights_time_ms': 1.758},counters={'num_env_steps_sampled': 72000, 'num_env_steps_trained': 72000, 'num_agent_steps_sampled': 72000, 'num_agent_steps_trained': 72000},perf={'cpu_util_percent': 17.181818181818183, 'ram_util_percent': 73.07272727272728} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0046875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.25364603432276, 'policy_loss': -0.003631837810239484, 'vf_loss': 9.257251063726281, 'vf_explained_var': -0.005884651535300798, 'kl': 0.005709939280192382, 'entropy': 0.4695009417431329, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 76000, 'num_env_steps_trained': 76000, 'num_agent_steps_sampled': 76000, 'num_agent_steps_trained': 76000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 376.8, 'episode_len_mean': 376.8, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [269.0, 500.0, 500.0, 333.0, 307.0, 226.0, 202.0, 278.0, 163.0, 500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [269, 500, 500, 333, 307, 226, 202, 278, 163, 500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10218619149145876, 'mean_inference_ms': 0.8881154344709811, 'mean_action_processing_ms': 0.06513934323688192, 'mean_env_wait_ms': 0.06418645444418739, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=376.8,episode_len_mean=376.8,episodes_this_iter=8,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10218619149145876, 'mean_inference_ms': 0.8881154344709811, 'mean_action_processing_ms': 0.06513934323688192, 'mean_env_wait_ms': 0.06418645444418739, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=76000,num_agent_steps_trained=76000,num_env_steps_sampled=76000,num_env_steps_trained=76000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=76000,timers={'training_iteration_time_ms': 7085.955, 'load_time_ms': 0.568, 'load_throughput': 7048361.971, 'learn_time_ms': 5893.536, 'learn_throughput': 678.71, 'synch_weights_time_ms': 1.76},counters={'num_env_steps_sampled': 76000, 'num_env_steps_trained': 76000, 'num_agent_steps_sampled': 76000, 'num_agent_steps_trained': 76000},perf={'cpu_util_percent': 20.66666666666667, 'ram_util_percent': 72.82222222222224} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 500.0, 'episode_reward_mean': 500.0, 'episode_len_mean': 500.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06906825604459937, 'mean_inference_ms': 0.7316843244685247, 'mean_action_processing_ms': 0.054127236295468964, 'mean_env_wait_ms': 0.05081624272123384, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 10000, 'num_env_steps_sampled_this_iter': 10000, 'timesteps_this_iter': 10000, 'num_healthy_workers': 0},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0046875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.156851075285225, 'policy_loss': -0.00400612549355594, 'vf_loss': 9.160832289213776, 'vf_explained_var': 0.006919014325705908, 'kl': 0.005316633865020051, 'entropy': 0.4831878565331941, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 80000, 'num_env_steps_trained': 80000, 'num_agent_steps_sampled': 80000, 'num_agent_steps_trained': 80000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 390.34, 'episode_len_mean': 390.34, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 188.0, 220.0, 369.0, 287.0, 279.0, 285.0, 265.0, 224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 132.0], 'episode_lengths': [500, 188, 220, 369, 287, 279, 285, 265, 224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 132]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10223426888506093, 'mean_inference_ms': 0.8897800598691268, 'mean_action_processing_ms': 0.06527561678333073, 'mean_env_wait_ms': 0.0643020782271634, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=390.34,episode_len_mean=390.34,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.10223426888506093, 'mean_inference_ms': 0.8897800598691268, 'mean_action_processing_ms': 0.06527561678333073, 'mean_env_wait_ms': 0.0643020782271634, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=80000,num_agent_steps_trained=80000,num_env_steps_sampled=80000,num_env_steps_trained=80000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=80000,timers={'training_iteration_time_ms': 6951.105, 'load_time_ms': 0.566, 'load_throughput': 7064685.868, 'learn_time_ms': 5770.935, 'learn_throughput': 693.129, 'synch_weights_time_ms': 1.747},counters={'num_env_steps_sampled': 80000, 'num_env_steps_trained': 80000, 'num_agent_steps_sampled': 80000, 'num_agent_steps_trained': 80000},perf={'cpu_util_percent': 17.531818181818185, 'ram_util_percent': 73.06818181818181} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_7c74f_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0046875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.030955660727717, 'policy_loss': 5.5034210284550984e-05, 'vf_loss': 9.030898360283144, 'vf_explained_var': 0.002467831680851598, 'kl': 0.00048548778544067343, 'entropy': 0.4908732631193694, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 84000, 'num_env_steps_trained': 84000, 'num_agent_steps_sampled': 84000, 'num_agent_steps_trained': 84000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 58.0, 'episode_reward_mean': 406.41, 'episode_len_mean': 406.41, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [224.0, 269.0, 58.0, 307.0, 200.0, 500.0, 388.0, 280.0, 309.0, 291.0, 201.0, 336.0, 367.0, 386.0, 249.0, 288.0, 366.0, 205.0, 205.0, 268.0, 199.0, 277.0, 215.0, 216.0, 226.0, 364.0, 192.0, 236.0, 187.0, 230.0, 294.0, 168.0, 176.0, 198.0, 202.0, 381.0, 277.0, 500.0, 500.0, 425.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 132.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [224, 269, 58, 307, 200, 500, 388, 280, 309, 291, 201, 336, 367, 386, 249, 288, 366, 205, 205, 268, 199, 277, 215, 216, 226, 364, 192, 236, 187, 230, 294, 168, 176, 198, 202, 381, 277, 500, 500, 425, 349, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 132, 500, 500, 500, 500, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.1023314661363376, 'mean_inference_ms': 0.8919928086840155, 'mean_action_processing_ms': 0.06540553896661812, 'mean_env_wait_ms': 0.06445124470132933, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=58.0,episode_reward_mean=406.41,episode_len_mean=406.41,episodes_this_iter=8,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.1023314661363376, 'mean_inference_ms': 0.8919928086840155, 'mean_action_processing_ms': 0.06540553896661812, 'mean_env_wait_ms': 0.06445124470132933, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=84000,num_agent_steps_trained=84000,num_env_steps_sampled=84000,num_env_steps_trained=84000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=84000,timers={'training_iteration_time_ms': 6944.924, 'load_time_ms': 0.559, 'load_throughput': 7153242.944, 'learn_time_ms': 5747.001, 'learn_throughput': 696.015, 'synch_weights_time_ms': 1.73},counters={'num_env_steps_sampled': 84000, 'num_env_steps_trained': 84000, 'num_agent_steps_sampled': 84000, 'num_agent_steps_trained': 84000},perf={'cpu_util_percent': 22.61111111111111, 'ram_util_percent': 73.9} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=6355)\u001b[0m E0715 18:06:08.883694000 123145563291648 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6354)\u001b[0m E0715 18:06:08.883759000 123145502986240 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m E0715 18:06:08.883879000 123145486372864 chttp2_transport.cc:1103]     Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m 2022-07-15 18:06:08,899\tERROR worker.py:754 -- Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"python/ray/_raylet.pyx\", line 812, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"python/ray/_raylet.pyx\", line 623, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"python/ray/_raylet.pyx\", line 663, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"python/ray/_raylet.pyx\", line 670, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"python/ray/_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"python/ray/_raylet.pyx\", line 621, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/actor.py\", line 1244, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/actor.py\", line 1296, in exit_actor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     ray._private.worker.disconnect()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py\", line 2038, in disconnect\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     worker.gcs_log_subscriber.close()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/gcs_pubsub.py\", line 281, in close\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     self._stub.GcsSubscriberCommandBatch(req, timeout=5)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/grpc/_channel.py\", line 944, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     state, call, = self._blocking(request, timeout, metadata, credentials,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/grpc/_channel.py\", line 933, in _blocking\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     event = call.next_event()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 338, in grpc._cython.cygrpc.SegregatedCall.next_event\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 171, in grpc._cython.cygrpc._next_call_event\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/threading.py\", line 256, in __enter__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     def __enter__(self):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/_private/worker.py\", line 751, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6356)\u001b[0m SystemExit: 1\n",
      "2022-07-15 18:06:09,001\tINFO tune.py:737 -- Total run time: 237.68 seconds (237.15 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RAY TUNE API .run() IN A LOOP UNTIL STOP CONDITION\n",
    "# Note about Ray Tune verbosity.\n",
    "# Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "# 0 = silent\n",
    "# 1 = only status updates, no logging messages\n",
    "# 2 = status and brief trial results, includes logging messages\n",
    "# 3 = status and detailed trial results, includes logging messages\n",
    "# Defaults to 3.\n",
    "###############\n",
    "\n",
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "evaluation_interval = 100   #100, num training episodes to run between eval steps\n",
    "verbosity = 2 # Tune screen verbosity\n",
    "\n",
    "experiment_results = tune.run(\"PPO\", \n",
    "                    \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\n",
    "          # \"training_iteration\": 200,  # stop if achieved 200 episodes\n",
    "          # \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\n",
    "          },  \n",
    "              \n",
    "    # training config params\n",
    "    config = config_train.to_dict(),\n",
    "                    \n",
    "    #redirect logs instead of default ~/ray_results/\n",
    "    local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq = checkpoint_freq,\n",
    "    checkpoint_at_end=True,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    verbose = verbosity,\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc331b91-dcb8-4225-8e30-94415c6b0c11",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Scroll through the output in the cell above.  Look for a single table row that looks like this: <br>\n",
    "\n",
    "<img src=\"images/ppo_cartpole_tune_output.png\"></img>\n",
    "\n",
    "What this telling you is that Tune ran your experiment.  It was terminated when Reward reached 409.14, which satisified the stopping criteria of Reward >= 400, out of an environment max reward possible of 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd024c",
   "metadata": {},
   "source": [
    "## Evaluate a RLlib model <a class=\"anchor\" id=\"eval_rllib\"></a>\n",
    "\n",
    "RLlib trainers can be evaluated by*:\n",
    "<ul>\n",
    "    <li>Examining <b>Ray Tune</b> trainer object </li>\n",
    "    <li>Examining <b>Ray Tune</b> experiment trial results </li>\n",
    "    <li>Visualizing training progress in <b>TensorBoard</b></li>\n",
    "    </ul>\n",
    "\n",
    "*RLlib trainer objects can also be examined manually, but it gives the same info you already saw in the single episode .train() output: <i>rllib_trainer.evaluate()</i>\n",
    "\n",
    "**Ray Tune trainer object** <br>\n",
    "First, let's start by looking at the trainer object. How long did training take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb3c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  66.01 seconds,    1.10 minutes\n"
     ]
    }
   ],
   "source": [
    "# Get RLlib default stats\n",
    "stats = experiment_results.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')\n",
    "\n",
    "# Typically takes about 67 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cad71",
   "metadata": {},
   "source": [
    "**Ray Tune experiment trial results**\n",
    "\n",
    "Read all the experiment trials into a single pandas dataframe.  The dataframe will have 1 row per trial.\n",
    "\n",
    "Below, we see a dataframe with only 1 row because Tune only ran 1 trial.  (Because we did not specify a hyperparameter space to search for tuning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99483c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (1, 422)\n",
      "Index(['episode_reward_max', 'episode_reward_min', 'episode_reward_mean',\n",
      "       'episode_len_mean', 'episodes_this_iter', 'num_healthy_workers',\n",
      "       'num_agent_steps_sampled', 'num_agent_steps_trained',\n",
      "       'num_env_steps_sampled', 'num_env_steps_trained',\n",
      "       ...\n",
      "       'info/learner/default_policy/learner_stats/total_loss',\n",
      "       'info/learner/default_policy/learner_stats/policy_loss',\n",
      "       'info/learner/default_policy/learner_stats/vf_loss',\n",
      "       'info/learner/default_policy/learner_stats/vf_explained_var',\n",
      "       'info/learner/default_policy/learner_stats/kl',\n",
      "       'info/learner/default_policy/learner_stats/entropy',\n",
      "       'info/learner/default_policy/learner_stats/entropy_coeff',\n",
      "       'config/evaluation_config/tf_session_args/gpu_options/allow_growth',\n",
      "       'config/evaluation_config/tf_session_args/device_count/CPU',\n",
      "       'config/evaluation_config/multiagent/policies/default_policy'],\n",
      "      dtype='object', length=422)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>num_agent_steps_sampled</th>\n",
       "      <th>num_agent_steps_trained</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89997_00000</th>\n",
       "      <td>500.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>409.14</td>\n",
       "      <td>409.14</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>56000</td>\n",
       "      <td>56000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "trial_id                                                                   \n",
       "89997_00000               500.0                21.0               409.14   \n",
       "\n",
       "             episode_len_mean  episodes_this_iter  num_healthy_workers  \\\n",
       "trial_id                                                                 \n",
       "89997_00000            409.14                   9                    4   \n",
       "\n",
       "             num_agent_steps_sampled  num_agent_steps_trained  \n",
       "trial_id                                                       \n",
       "89997_00000                    56000                    56000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read trainer results in a pandas dataframe\n",
    "df = experiment_results.results_df\n",
    "\n",
    "print(f\"df.shape: {df.shape}\")  #Only 1 trial\n",
    "print(df.columns)\n",
    "df.iloc[:,0:8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd6b1e",
   "metadata": {},
   "source": [
    "For how many episodes did training run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15cdff25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of episodes for the 1st trial\n",
    "df.iloc[0,:].episodes_this_iter  \n",
    "\n",
    "# Answer is 9 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed7e6a",
   "metadata": {},
   "source": [
    "What were the best parameter values?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53a815bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out, the following cell output is long because there are many parameters!\n",
    "\n",
    "# trainer.get_best_config(metric=\"episode_reward_mean\", mode=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd62d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize the training progress in TensorBoard\n",
    "\n",
    "RLlib automatically creates logs for your trained RLlib models that can be visualized in TensorBoard.  To visualize the performance of your RL model:\n",
    "\n",
    "<ol>\n",
    "    <li>Open a terminal</li>\n",
    "    <li><i><b>cd</b></i> into the logdir path from the above cell's output.</li>\n",
    "    <li><i><b>ls</b></i></li>\n",
    "    <li>You should see files that look like: checkpoint_NNNNNN</li>\n",
    "    <li>To be able to compare all your experiments, cd one dir level up.\n",
    "    <li><i><b>cd ..</b></i>  \n",
    "    <li><i><b>tensorboard --logdir . </b></i></li>\n",
    "    <li>Look at the url in the message, and open it in a browser</li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a04eff",
   "metadata": {},
   "source": [
    "#### Screenshot of Tensorboard\n",
    "\n",
    "TensorBoard will give you many pages of charts.  Below displaying just Train/Eval mean and min rewards.\n",
    "\n",
    "The charts below are showing \"sample efficiency\", the number of training steps it took to achieve a certain level of performance.\n",
    "\n",
    "<b>Train Performance:</b> <br>\n",
    "\n",
    "---\n",
    "<img src=\"images/ppo_cartpole_training_rewards.png\" width=\"80%\" />\n",
    "\n",
    "<b>Eval Performance:</b> <br>\n",
    "<img src=\"images/ppo_cartpole_eval_rewards.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf198368",
   "metadata": {},
   "source": [
    "## Play and render the game as a video <a class=\"anchor\" id=\"reload_rllib\"></a>\n",
    "\n",
    "To do this, we need to reload the desired RLlib model from checkpoint and then run the model inference mode on the environment it was trained on.  \n",
    "\n",
    "Where is the best model checkpoint file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b94d2c-9c28-411a-ad9d-ab46357b5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_89997_00000_0_2022-07-10_18-26-50\n",
      "/Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_89997_00000_0_2022-07-10_18-26-50/checkpoint_000014/checkpoint-14\n"
     ]
    }
   ],
   "source": [
    "# Get best checkpoint path\n",
    "logdir = experiment_results.get_best_logdir(metric=\"evaluation_reward_mean\", mode=\"max\")\n",
    "print(logdir)\n",
    "\n",
    "# Get last checkpoint path\n",
    "checkpoint = experiment_results.get_last_checkpoint()\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683354a3-bd3f-42be-badf-70a05ad0071d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Restore the desired, already-trained RLlib model from checkpoint file.</b>  \n",
    "\n",
    "You will need:\n",
    "<ul>\n",
    "    <li>Your <b>algorithm's config class</b> and exact same <a href=\"#intro_rllib_api\">config settings you used to train your model.</a></li>\n",
    "    <li>Name of the <b>environment</b> you used to train the model.</li>\n",
    "    <li>Path to the desired <b>checkpoint</b> file you want to use to restore the model.</li>\n",
    "    </ul>\n",
    "\n",
    "See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b933a724-7840-4fb7-bf0c-0b64e627c183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 18:27:57,896\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-07-10 18:27:57,897\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-07-10 18:28:01,293\tINFO trainable.py:590 -- Restored on 127.0.0.1 from checkpoint: /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_89997_00000_0_2022-07-10_18-26-50/checkpoint_000014/checkpoint-14\n",
      "2022-07-10 18:28:01,298\tINFO trainable.py:599 -- Current state after restoring: {'_iteration': 14, '_timesteps_total': None, '_time_total': 60.11512041091919, '_episodes_total': 409}\n"
     ]
    }
   ],
   "source": [
    "# Create new Algorithm and restore its state from the last checkpoint.\n",
    "\n",
    "# create an empty Algorithm\n",
    "algo = config_train.build(env=env_name)\n",
    "\n",
    "# restore the agent from the checkpoint\n",
    "algo.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e4a55-fbe1-439e-9a5f-76818a2289e5",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Record a video of the trained model doing inference in the environment it was trained on.</b>\n",
    "<br><br>\n",
    "\n",
    "Gym includes video recording and saving capability in its wrapper <i>`gym.wrappers.RecordVideo`</i>, so we will import and use that method to wrap the <i>`gym.make()`</i> method.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üëç During inference, call the RLlib API method <b>compute_single_action()</b>, which uses the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference). \n",
    "</div>\n",
    "\n",
    "Execute the cell below, and you will see a video of the rollouts, so you can verify visually that the agent is starting from a near perfect score.\n",
    "\n",
    "Note that the CartPole environment's perfect score is 500.  Since we trained our model to around 400, the restored agent should appear very stable. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03f9f3bb-9ea2-471d-98d2-19632a79ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/miniforge3/envs/ray_dev/lib/python3.8/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/ppo_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done: reward = 500.0\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "#############\n",
    "## Create the env to do inference on\n",
    "#############\n",
    "# Try this first to make sure it is working\n",
    "# env = gym.make(env_name)\n",
    "# Once you have confirmed it works, wrap the method\n",
    "# RecordVideo() takes as input a path name where to store the video\n",
    "# RecordVideo() includes its own render step, records, and saves the video.\n",
    "env = RecordVideo(gym.make(env_name), \"ppo_video\", )\n",
    "obs = env.reset()\n",
    "\n",
    "#############\n",
    "## Use the restored model and run it in inference mode\n",
    "## You will see a pop-up video rendering for about 10 seconds\n",
    "#############\n",
    "num_episodes_during_inference = 1\n",
    "num_episodes = 0\n",
    "episode_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while num_episodes < num_episodes_during_inference:\n",
    "    # Compute an action (`a`).\n",
    "    a = algo.compute_single_action(observation=obs)\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, _ = env.step(a)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    # Is the episode `done`? -> Reset.\n",
    "    if done:\n",
    "        print(f\"Episode done: reward = {episode_reward}\")\n",
    "        obs = env.reset()\n",
    "        num_episodes += 1\n",
    "        episode_reward = 0.0\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# The restored agent manages to achieve a perfect score during the 1 episode rollout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb7940-bb3c-4899-a7fa-66a676ba30bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Play and share your video .mp4 file with others if you want.</b>\n",
    "<br><br>\n",
    "\n",
    "Show a video player in the notebook, and play the video you just recorded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b3aa827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"ppo_video/rl-video-episode-0.mp4\" controls  width=\"500\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_video='ppo_video/rl-video-episode-0.mp4'\n",
    "Video(cart_pole_video, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40b3830e-d0ae-4794-85b2-4b245faef04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray if you are done\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fbd96",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb74fe",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. How would choose another algorithm to train Cart Pole?  Hint:  Look at the [RLlib algorithm doc page](https://docs.ray.io/en/master/rllib/rllib-algorithms.html).  How would you change the choice of RLlib algorithm from <b>PPO to TD3</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511893b",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
