{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710f04f4",
   "metadata": {},
   "source": [
    "# Exercise 01. Introduction to the OpenAI Gym Environment and RLlib Algorithm top-level APIs\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, we will learn about:\n",
    " * [What is an Environment in RL?](#intro_env)\n",
    " * [Overview of RL terminology](#intro_rl)\n",
    " * [Introduction to OpenAI Gym environments](#intro_gym)\n",
    " * [High-level OpenAI Gym API calls](#intro_gym_api)\n",
    " * [Overview of RLlib](#intro_rllib)\n",
    " * [Train a RL model using an algorithm from RLlib](#intro_rllib_api)\n",
    " * [Evaluate a RLlib model](#eval_rllib)\n",
    " * [Reload RLlib model from checkpoint and run inference](#reload_rllib)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aca4e8",
   "metadata": {},
   "source": [
    "## What is an environment in RL? <a class=\"anchor\" id=\"intro_env\"></a>\n",
    "\n",
    "Solving a problem in RL begins with an **environment**. In the simplest definition of RL:\n",
    "\n",
    "> An **agent** interacts with an **environment** and receives a reward.\n",
    "\n",
    "An environment in RL is the agent's world, it is a simulation of the problem to be solved. \n",
    "\n",
    "<img src=\"images/env_key_concept1.png\" width=\"50%\" />\n",
    "\n",
    "The environment simulator might be a:\n",
    "<ul>\n",
    "    <li>real, physical situation such as a gas turbine</li>\n",
    "    <li>virtual sytem on a computer such as a board game or video game</li>\n",
    "    </ul>\n",
    "Why bother with an Agent and Environment?  \n",
    "\n",
    "> RL is useful when you have sequential decisions that need to be optimized over time. \n",
    "\n",
    "Traditional supervised learning views the world as more of a one-shot training, not as action -> fedback -> improved action -> repeat.\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf844e4",
   "metadata": {},
   "source": [
    "## Overview of RL terminology <a class=\"anchor\" id=\"intro_rl\"></a>\n",
    "\n",
    "An RL environment consists of: \n",
    "\n",
    "1. all possible actions (**action space**)\n",
    "2. a complete description of the environment, nothing hidden (**state space**)\n",
    "3. an observation by the agent of certain parts of the state (**observation space**)\n",
    "4. **reward**, which is the only feedback the agent receives per action.\n",
    "\n",
    "The model that tries to maximize the expected sum over all future rewards is called a **policy**. The policy is a function mapping the environment's observations to an action to take, usually written **π** (s(t)) -> a(t).\n",
    "\n",
    "Below is a high-level image of how the Agent and Environment work together in a RL simulation feedback loop in RLlib.\n",
    "\n",
    "<img src=\"images/env_key_concept2.png\" width=\"98%\" />\n",
    "\n",
    "The **RL simulation feedback loop** repeatedly collects data, for one (single-agent case) or multiple (multi-agent case) policies, trains the policies on these collected data, and makes sure the policies' weights are kept in synch. Thereby, the collected environment data contains observations, taken actions, received rewards and so-called **done** flags, indicating the boundaries of different episodes the agents play through in the simulation.\n",
    "\n",
    "The simulation iterations of action -> reward -> next state -> train -> repeat, until the end state, is called an **episode**, or in RLlib, a **rollout**\n",
    "\n",
    "<b>Per episode</b> (or between **done** flag == True), the RL simulation feedback loop repeats up to some specified end state (termination state or timesteps). Examples of termination could be:\n",
    "<ul>\n",
    "    <li>the end of a maze (termination state)</li>  \n",
    "    <li>the player died in a game (termination state)</li>\n",
    "    <li>after 60 videos watched in a recommender system (timesteps).</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07193fc6",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym example: frozen lake <a class=\"anchor\" id=\"intro_gym\"></a>\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) is a well-known reference library of RL environments. \n",
    "\n",
    "#### 1. import gym\n",
    "\n",
    "Below is how you would import gym and view all available environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742e5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "Num Gym Environments: 103\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import gym\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# List all available gym environments\n",
    "all_env  =  list(gym.envs.registry.all())\n",
    "print(f'Num Gym Environments: {len(all_env)}')\n",
    "\n",
    "# # You could loop through and list all environments if you wanted\n",
    "# [print(e) for e in all_env]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddd364",
   "metadata": {},
   "source": [
    "#### 2. Instatiate your Gym object\n",
    "\n",
    "The way you instantiate a Gym environment is with the **make()** function.\n",
    "\n",
    "The .make() function takes arguments:\n",
    "- **name of the Gym environment**, type: str, Required.\n",
    "- **runtime parameter values**, Optional.\n",
    "\n",
    "For the required string argument, you need to know the Gym name.  You can find the Gym name in the Gym documentation for environments, either:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "    \n",
    "Below is an example of how to create a basic Gym environment, [frozen lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/).  We can see below that the termination condition of an episode will be time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00d01a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n",
      "env_spec: EnvSpec(FrozenLake-v1)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"FrozenLake-v1\"\n",
    "env_runtime_param_value = False\n",
    "\n",
    "# Instantiate gym env object with a runtime parameter value\n",
    "env = gym.make(\n",
    "        env_name, \n",
    "        is_slippery=env_runtime_param_value)\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c1e21",
   "metadata": {},
   "source": [
    "#### 3. Inspect the environment action and observations spaces\n",
    "\n",
    "Gym Environments can be deterministic or stochastic.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Deterministic</b> if the current state + selected action determines the next state of the environment.  Chess is a deterministic environment, since all possible states/action combinations can be described as a discrete set of rules with states bounded by the pieces and size of the board.</li>\n",
    "    <li>\n",
    "        <b>Stochastic</b> if the policy output action is a probability distribution over a set of possible actions at time step t. In this case the agent needs to compute its action from the policy in two steps. i) sample actions from the policy according to the probability distribution, ii) compute log likelihoods of the actions. Stochastic environments are random in nature.  Random visitors to a website is an example of a stochastic environment. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Gym actions.</b> The action_space describes the numerical structure of the legitimate actions that can be applied to the environment. \n",
    "\n",
    "For example, if we have 4 possible discrete actions, we could encode them as:\n",
    "<ul>\n",
    "    <li>0: LEFT</li>\n",
    "    <li>1: DOWN</li>\n",
    "    <li>2: RIGHT</li>\n",
    "    <li>3: UP</li>\n",
    "</ul>\n",
    "\n",
    "<b>Gym observations.</b>  The observation_space defines the structure as well as the legitimate values for the observation of the state of the environment.  \n",
    "\n",
    "For example, if we have a 4x4 grid, we could encode them as {0,1,2,3, 4, … ,16} for grid positions ((0,0), (0,1), (0,2), (0,3), …. (3,3)).\n",
    "\n",
    "\n",
    "From the Gym [documentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) about the frozen lake environment, we see: <br>\n",
    "\n",
    "|Frozen Lake      | Gym space   |\n",
    "|---------------- | ----------- |\n",
    "|Action Space     | Discrete(4) |\n",
    "|Observation Space| Discrete(16)|\n",
    "\n",
    "\n",
    " \n",
    "<b><a href=\"https://github.com/openai/gym/tree/master/gym/spaces\">Gym spaces</a></b> are gym data types.  The main types are `Discrete` for discrete numbers and `Box` for continuous numbers.  \n",
    "\n",
    "Gym Space `Discrete` elements are Python type `int`, and Gym Space `Box` are Python type `float32`.\n",
    "\n",
    "Below is an example how to inspect the environment action and observations spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78620a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a gym environment.\n",
      "\n",
      "gym action space: Discrete(4)\n",
      "gym observation space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# check if it is a gym instance\n",
    "if isinstance(env, gym.Env):\n",
    "    print(\"This is a gym environment.\")\n",
    "    print()\n",
    "\n",
    "    # print gym Spaces\n",
    "    if isinstance(env.action_space, gym.spaces.Space):\n",
    "        print(f\"gym action space: {env.action_space}\")\n",
    "    if isinstance(env.observation_space, gym.spaces.Space):\n",
    "        print(f\"gym observation space: {env.observation_space}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d87d06-fd7a-469d-8310-15f983a972b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comment this cell if you want whole notebook to run without errors\n",
    "\n",
    "# # Try to take an invalid action\n",
    "\n",
    "# env.step(4) # invalid\n",
    "\n",
    "# # should see KeyError below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad91e6",
   "metadata": {},
   "source": [
    "#### 4. Inspect gym environment parameters\n",
    "\n",
    "Gym environments contain 2 sets of configuration parameters that are set after the environment object is instantiated.\n",
    "<ul>\n",
    "    <li><b>Runtime parameters</b> are passed into the make() function as **kwargs.</li>\n",
    "    <li><b>Default parameters</b> are fixed in the Gym environment code.</li>\n",
    "    </ul>\n",
    "\n",
    "Below is an example of how to inspect the environment parameters.  \n",
    "\n",
    "Notice we can tell from the parameters that our frozen lake environment is: \n",
    "1) Deterministic, and \n",
    "2) Episode terminates with time step condition max_episode_steps = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dc0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime spec params...\n",
      "map_name: 4x4\n",
      "is_slippery: False\n",
      "\n",
      "Default spec params...\n",
      "id: FrozenLake-v1\n",
      "entry_point: gym.envs.toy_text:FrozenLakeEnv\n",
      "reward_threshold: 0.7\n",
      "nondeterministic: False\n",
      "max_episode_steps: 100\n",
      "order_enforce: True\n"
     ]
    }
   ],
   "source": [
    "# inspect env.spec parameters\n",
    " \n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print(\"Runtime spec params...\")\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    " \n",
    "# View default env spec params\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "print(f\"entry_point: {env_spec.entry_point}\")\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\")\n",
    "\n",
    "# We can tell that our frozen lake environment is: \n",
    "# 1) Deterministic, and \n",
    "# 2) Episode terminates with condition max_episode_steps = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd5020",
   "metadata": {},
   "source": [
    "#### 5. Perform some basic Gym API calls <a class=\"anchor\" id=\"intro_gym_api\"></a>\n",
    "\n",
    "The most basic Gym API methods are:\n",
    "<ul>\n",
    "    <li><b>env.reset()</b> <br>Reset the environment to an initial state, this is how you initialize an environment so you can run a simulation on it.  You should call this method every time to initiate a new episode.</li>\n",
    "    <li><b>env.render()</b>  <br>Visually inspect the environment anytime. Note you cannot inspect an environment before it has been initialized with env.reset().</li>\n",
    "    <li><b>env.step(action)</b> <br>Take an action from the possible action space values.  It accepts an action, computes the state of the environment after applying that action and returns the 4-tuple (observation, reward, done, info).</li>\n",
    "    <li><b>env.close()</b> <br>Close an environment.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd2bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Print the starting observation.  Recall possible observations are between 0-16.\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee81cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 1, reward: 0.0, done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Take an action\n",
    "# Recall the possible actions are: 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "\n",
    "new_obs, reward, done, _ = env.step(2) #Right\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()\n",
    "new_obs, reward, done, _ = env.step(1) #Down\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec105",
   "metadata": {},
   "source": [
    "We can also try to run an action in the frozen lake environment which is outside the defined number range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0620a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comment this cell if you want whole notebook to run without errors\n",
    "\n",
    "# # Try to take an invalid action\n",
    "\n",
    "# env.step(4) # invalid\n",
    "\n",
    "# # should see KeyError below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8a6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 200\n",
      "obs: 5, reward: 1.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 400\n",
      "obs: 5, reward: 1.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 600\n",
      "obs: 5, reward: 2.0, done: True\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "episode: 800\n",
      "obs: 12, reward: 5.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n"
     ]
    }
   ],
   "source": [
    "# Putting the simple API methods together.\n",
    "# Here is a pattern for running a bunch of episodes.\n",
    " \n",
    "num_episodes = 1000 # Number of episodes you want to run the agent\n",
    "render_freq = 200  # Render every X number of episodes \n",
    "total_reward = 0  # Initialize reward to 0\n",
    "\n",
    "# Loop through episodes\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Loop through time steps per episode\n",
    "    while True:\n",
    "        # take random action, but you can also do something more intelligent \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # If the epsiode is up, then start another one\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Render the env only every render_freq episodes\n",
    "    if ep % render_freq == 0:\n",
    "        print(f\"episode: {ep}\")\n",
    "        print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "        env.render()\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f1156",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview of RLlib <a class=\"anchor\" id=\"intro_rllib\"></a>\n",
    "\n",
    "<img width=\"7%\" src=\"images/rllib-logo.png\"> is the most comprehensive open-source Reinforcement Learning framework. **[RLlib](https://github.com/ray-project/ray/tree/master/rllib)** is <b>distributed by default</b> since it is built on top of **[Ray](https://docs.ray.io/en/latest/)**, an easy-to-use, open-source, distributed computing framework for Python that can handle complex, heterogeneous applications. Ray and RLlib run on compute clusters on any cloud without vendor lock.\n",
    "\n",
    "RLlib includes <b>25+ available [algorithms](https://docs.ray.io/en/master/rllib/rllib-algorithms.html)</b>, converted to both <img width=\"3%\" src=\"images/tensorflow-logo.png\">_TensorFlow_ and <img width=\"3%\" src=\"images/pytorch-logo.png\">_PyTorch_, covering different sub-categories of RL: _model-based_, _model-free_, and _Offline RL_. Almost any RLlib algorithm can learn in a <b>multi-agent</b> setting. Many algorithms support <b>RNNs and LSTMs</b>.\n",
    "\n",
    "Roughly, RLlib is organized by **environments**, **algorithms**, **examples**, and **tuned_examples**.  \n",
    "\n",
    "    ray\n",
    "    |- rllib\n",
    "    |  |- env \n",
    "    |  |- algorithms \n",
    "    |  |  |- alpha_zero \n",
    "    |  |  |- appo \n",
    "    |  |  |- ppo \n",
    "    |  |  |- ... \n",
    "    |  |- examples \n",
    "    |  |- tuned_examples\n",
    "\n",
    "Within **_env_** you will find different [base classes](https://docs.ray.io/en/latest/rllib/package_ref/env.html) that you can inherit from to make it easy to implement your environment. RLlib supports environments created using the **OpenAI Gym API** (which supports most user cases). The base classes in the env directory allow for users to implement environments that don't fall into common use cases such as multi agent environments and environments that have strict performance or hosting requirements. In the next notebook, you will see we're using the **RLlib MultiAgentEnv** base class to train a **multi agent** RL model.\n",
    "\n",
    "Within **_examples_** you will find some examples of common custom rllib use cases..  \n",
    "\n",
    "Within **_tuned_examples_**, you will find, sorted by algorithm, suggested hyperparameter value choices within .yaml files. Ray RLlib team ran simulations/benchmarks to find suggested hyperparameter value choices.  These files used for daily testing, and weekly hard-task testing to make sure they all run at speed, for both TF and Torch. Helps you with leg-up with parameter choices!\n",
    "\n",
    "\n",
    "In this tutorial, we will mainly focus on **_algorithms_**, where we will find RLlib algorithms to train RLlib models on environments.\n",
    "\n",
    "\n",
    "#### Step 1.  Import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5655b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package pickle5 becomes unnecessary in Python 3.8 and above. Its presence may confuse libraries including Ray. Please uninstall the package.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d2983-90fd-48bd-b61a-1ee79b62770f",
   "metadata": {},
   "source": [
    "#### Check environment for errors\n",
    "\n",
    "Before you start training, it is a good idea to check the environment for errors.  RLlib provides a convenient [Environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) for this.  \n",
    "    \n",
    "Below, we start with a new environment, [Cart-Pole](https://www.gymlibrary.ml/environments/classic_control/cart_pole/), then in the next cell, check it for errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ddfdb23-2554-48cf-aeed-cdc1b19f02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<CartPoleEnv<CartPole-v1>>>\n",
      "env_spec: EnvSpec(CartPole-v1)\n",
      "Runtime spec params...\n",
      "\n",
      "Default spec params...\n",
      "id: CartPole-v1\n",
      "entry_point: gym.envs.classic_control:CartPoleEnv\n",
      "reward_threshold: 475.0\n",
      "nondeterministic: False\n",
      "max_episode_steps: 500\n",
      "order_enforce: True\n"
     ]
    }
   ],
   "source": [
    "# Instantiate gym env object with a runtime parameter value\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    " \n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print(\"Runtime spec params...\")\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    " \n",
    "# View default env spec params\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "print(f\"entry_point: {env_spec.entry_point}\")\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99b4730c-ad46-42c9-ae70-ef1f551883b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "\n",
    "# How to check you do not have any environment errors\n",
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    ray.rllib.utils.pre_checks.env.check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a757e7",
   "metadata": {},
   "source": [
    "## Train a RL model using an algorithm from RLlib <a class=\"anchor\" id=\"intro_rllib_api\"></a>\n",
    "\n",
    "#### Step 2.  Select an algorithm and instantiate a config object using that algorithm's config class  \n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">Open RLlib docs</a></li>\n",
    "    <li>Scroll down and click url of algo you're searching for, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0de2e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common RLlib General config (for all algorithms)\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "config = AlgorithmConfig()\n",
    "\n",
    "# # Uncomment for long list of parameters\n",
    "# print(f\"RLlib Trainer's general default config is:\")\n",
    "# config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bb6a2",
   "metadata": {},
   "source": [
    "#### Step 3. Choose your config settings   \n",
    "\n",
    "As of Ray 1.13, RLlib configs been converted from primitive dictionaries into Objects. This makes them harder to print, but easier to set/pass.\n",
    "\n",
    "**Note about RLlib config precedence**\n",
    "<ol>\n",
    "    <li>Highest precedence are <b>trainer instantiation settings</b>, these override any other config settings</li>\n",
    "    <li>RLlib <b>specific algorithm config</b> (see config class description above)</li>\n",
    "    <li>RLlib <b><a href\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py#L58\">general config</a></b> settings have the lowest precedence</li>\n",
    "    </ol>\n",
    "\n",
    "\n",
    "**Note about num_workers**\n",
    "\n",
    "Number of Ray workers is the number of parallel workers or actors for rollouts.  Actual num_workers will be what you specifiy+1 for head node.\n",
    "\n",
    "<b>Use ONE LESS than the number of cores you want to use</b> (or omit this argument and let Ray automatically use all cores)! <br>\n",
    "\n",
    "\n",
    "Below, num_workers = 7  # means actual number workers=8 including head node; where 8 is #cpu on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99524e70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>\n"
     ]
    }
   ],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "# Define algorithm config values\n",
    "env_name = \"CartPole-v1\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"my_PPO_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "config_train = (\n",
    "    PPOConfig()\n",
    "    .framework(framework='torch')\n",
    "    .environment(env=env_name, disable_env_checking=False)\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    ")\n",
    "\n",
    "print(type(config_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb86ff8",
   "metadata": {},
   "source": [
    "#### Step 4. Instantiate a Trainer from the config object\n",
    "\n",
    "**Two ways to train RLlib models***\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/package_ref/index.html\">RLlib API.</a> The main methods are:</li>\n",
    "    <ul>\n",
    "        <li>train()</li>\n",
    "        <li>evaluate()</li>\n",
    "        <li>save()</li>\n",
    "        <li><b>restore()</b></li>\n",
    "        <li><b>compute_single_action()</b></li>\n",
    "    </ul>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/tune/api_docs/overview.html\">Ray Tune API.</a>  The main methods are:</li>\n",
    "        <ul>\n",
    "            <li><b>run()</b></li>\n",
    "    </ul>\n",
    "    </ol>\n",
    "    \n",
    "*RLlib CLI from command line using .yml file also exists, but the .yml file is undocumented: <i>rllib train -f [myfile_name].yml</i><br>\n",
    "\n",
    "👉 RLlib API <b>.train()</b> will train for 1 episode only.  Good for debugging since every single output will be shown for the 1 episode of training.  \n",
    "\n",
    "👉 However for usual purposes, Ray Tune API <b>.run()</b> is more convenient since with 1 function call you get experiment management: save, checkpoint, evaluate, and train subject to stopping criteria.\n",
    "\n",
    "✔️ Both methods will run the RLlib [environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) you saw earlier in this notebook (cells just after Step 1. Import ray).\n",
    "\n",
    "👍 You have to use RLlib API method <b>.restore()</b> to reload a checkpointed RLlib model for Serving and Offline learning.  Tune API methods will not work.\n",
    "\n",
    "👍 After a model is trained, it can be used in inference mode.  The RLlib API method <b>compute_single_action()</b> will use the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference).  You will see this method used at the end of this notebook.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>In summary, if you are going to train a RLlib model, train it use Ray Tune API method .run()!!  <br>\n",
    "    If you need to restore a RLlib model, use RLlib API method .restore()!!</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "💡 <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a6107f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###############\n",
    "# # EXAMPLE USING RLLIB API .train() FOR 1 EPISODE\n",
    "# # For completeness, here is how to train RLlib using RLlib API's .train() method\n",
    "# # The code below instantiates a trainer and trains for 1 single episode. \n",
    "# # To train for N number of episodes, you would put _.train()_ into a loop, \n",
    "# # similar to the way we ran the Gym _env.step()_ in a loop.\n",
    "# ###############\n",
    "\n",
    "# # To start fresh, restart Ray in case it is already running\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "    \n",
    "# # Use .build() similar to how gym environments are passed to the gym .make() method.\n",
    "# rllib_trainer = config_train.build(env=env_name)\n",
    "# print(type(rllib_trainer))\n",
    "\n",
    "# # run the trainer for 1 episode\n",
    "# rllib_trainer.train()\n",
    "\n",
    "# # Below, you will see the output evaluation_interval times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62252115",
   "metadata": {},
   "source": [
    "From the above cell, you can see how to train a RLlib algorithm 1 episode at a time.  But it is more practical to train RLlib algorithms using Ray Tune, since many more options are available.\n",
    "\n",
    "**Instantiate a trainer using Ray Tune API**\n",
    "\n",
    "Ray Tune offers experiment management in a single call <b>.run()</b>.  In the code below, we <b>specify a stopping criteria</b> to train until a certain Reward is achieved.  In case the desired training reward level is never reached, backup stop criteria can be given.  Tune will stop training whenever the earliest stop criteria is met.  However, best practice starting out is to only have 1 criteria, so you can be sure what is going to happen.\n",
    "\n",
    "💡 <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24e34aa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 18:26:49,027\tERROR services.py:1494 -- Failed to start the dashboard: Failed to start the dashboard, return code 0\n",
      " The last 10 lines of /tmp/ray/session_2022-07-10_18-26-47_099252_67268/logs/dashboard.log:\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/head.py\", line 105, in _configure_http_server\n",
      "    http_server = HttpServerDashboardHead(\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 69, in __init__\n",
      "    raise ex\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 60, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 31, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm install && npm ci && npm run build): '/Users/christy/Documents/ray/python/ray/dashboard/client/build'\n",
      "2022-07-10 18:26:49,028\tERROR services.py:1495 -- Failed to start the dashboard, return code 0\n",
      " The last 10 lines of /tmp/ray/session_2022-07-10_18-26-47_099252_67268/logs/dashboard.log:\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/head.py\", line 105, in _configure_http_server\n",
      "    http_server = HttpServerDashboardHead(\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 69, in __init__\n",
      "    raise ex\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 60, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 31, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm install && npm ci && npm run build): '/Users/christy/Documents/ray/python/ray/dashboard/client/build'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/christy/Documents/ray/python/ray/_private/services.py\", line 1472, in start_dashboard\n",
      "    raise Exception(err_msg + last_log_str)\n",
      "Exception: Failed to start the dashboard, return code 0\n",
      " The last 10 lines of /tmp/ray/session_2022-07-10_18-26-47_099252_67268/logs/dashboard.log:\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/head.py\", line 105, in _configure_http_server\n",
      "    http_server = HttpServerDashboardHead(\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 69, in __init__\n",
      "    raise ex\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 60, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/Users/christy/Documents/ray/python/ray/dashboard/http_server_head.py\", line 31, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm install && npm ci && npm run build): '/Users/christy/Documents/ray/python/ray/dashboard/client/build'\n",
      "2022-07-10 18:26:50,055\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-10 18:27:57 (running for 00:01:06.93)<br>Memory usage on this node: 12.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.91 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_89997_00000</td><td>TERMINATED</td><td>127.0.0.1:67303</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         60.1151</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  409.14</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            409.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=67303)\u001b[0m 2022-07-10 18:26:53,481\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=67303)\u001b[0m 2022-07-10 18:26:53,481\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_CartPole-v1_89997_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.712568479455927, 'policy_loss': -0.039127241758008795, 'vf_loss': 8.745841918965821, 'vf_explained_var': 0.00914683085615917, 'kl': 0.029268890223067345, 'entropy': 0.6648874541123708, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000},sampler_results={'episode_reward_max': 72.0, 'episode_reward_min': 9.0, 'episode_reward_mean': 21.281767955801104, 'episode_len_mean': 21.281767955801104, 'episode_media': {}, 'episodes_this_iter': 181, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [29.0, 11.0, 34.0, 11.0, 35.0, 18.0, 15.0, 17.0, 34.0, 24.0, 32.0, 43.0, 16.0, 21.0, 20.0, 15.0, 19.0, 15.0, 27.0, 24.0, 15.0, 34.0, 12.0, 19.0, 15.0, 12.0, 41.0, 18.0, 20.0, 31.0, 29.0, 10.0, 23.0, 26.0, 20.0, 16.0, 15.0, 11.0, 13.0, 21.0, 12.0, 27.0, 11.0, 11.0, 30.0, 11.0, 10.0, 39.0, 22.0, 38.0, 15.0, 14.0, 11.0, 14.0, 20.0, 27.0, 17.0, 12.0, 11.0, 22.0, 17.0, 13.0, 19.0, 13.0, 18.0, 11.0, 25.0, 40.0, 16.0, 14.0, 12.0, 24.0, 18.0, 20.0, 23.0, 18.0, 21.0, 13.0, 40.0, 56.0, 11.0, 27.0, 14.0, 14.0, 21.0, 21.0, 12.0, 20.0, 23.0, 29.0, 15.0, 14.0, 10.0, 24.0, 22.0, 10.0, 10.0, 12.0, 11.0, 12.0, 23.0, 21.0, 13.0, 17.0, 16.0, 35.0, 25.0, 40.0, 19.0, 28.0, 11.0, 19.0, 19.0, 40.0, 21.0, 42.0, 23.0, 36.0, 15.0, 36.0, 10.0, 16.0, 13.0, 15.0, 26.0, 66.0, 42.0, 20.0, 13.0, 15.0, 13.0, 20.0, 16.0, 18.0, 13.0, 72.0, 12.0, 11.0, 13.0, 15.0, 26.0, 14.0, 28.0, 26.0, 14.0, 51.0, 15.0, 12.0, 16.0, 33.0, 24.0, 15.0, 11.0, 31.0, 14.0, 12.0, 46.0, 13.0, 19.0, 12.0, 11.0, 64.0, 50.0, 27.0, 16.0, 12.0, 31.0, 13.0, 9.0, 12.0, 10.0, 13.0, 23.0, 19.0, 27.0, 33.0, 14.0, 31.0, 27.0, 27.0, 15.0], 'episode_lengths': [29, 11, 34, 11, 35, 18, 15, 17, 34, 24, 32, 43, 16, 21, 20, 15, 19, 15, 27, 24, 15, 34, 12, 19, 15, 12, 41, 18, 20, 31, 29, 10, 23, 26, 20, 16, 15, 11, 13, 21, 12, 27, 11, 11, 30, 11, 10, 39, 22, 38, 15, 14, 11, 14, 20, 27, 17, 12, 11, 22, 17, 13, 19, 13, 18, 11, 25, 40, 16, 14, 12, 24, 18, 20, 23, 18, 21, 13, 40, 56, 11, 27, 14, 14, 21, 21, 12, 20, 23, 29, 15, 14, 10, 24, 22, 10, 10, 12, 11, 12, 23, 21, 13, 17, 16, 35, 25, 40, 19, 28, 11, 19, 19, 40, 21, 42, 23, 36, 15, 36, 10, 16, 13, 15, 26, 66, 42, 20, 13, 15, 13, 20, 16, 18, 13, 72, 12, 11, 13, 15, 26, 14, 28, 26, 14, 51, 15, 12, 16, 33, 24, 15, 11, 31, 14, 12, 46, 13, 19, 12, 11, 64, 50, 27, 16, 12, 31, 13, 9, 12, 10, 13, 23, 19, 27, 33, 14, 31, 27, 27, 15]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.039250576927941386, 'mean_inference_ms': 0.2869319062006608, 'mean_action_processing_ms': 0.01810642653649441, 'mean_env_wait_ms': 0.01718810161263156, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=72.0,episode_reward_min=9.0,episode_reward_mean=21.281767955801104,episode_len_mean=21.281767955801104,episodes_this_iter=181,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.039250576927941386, 'mean_inference_ms': 0.2869319062006608, 'mean_action_processing_ms': 0.01810642653649441, 'mean_env_wait_ms': 0.01718810161263156, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=4000,num_agent_steps_trained=4000,num_env_steps_sampled=4000,num_env_steps_trained=4000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=4000,timers={'training_iteration_time_ms': 3124.883, 'load_time_ms': 0.351, 'load_throughput': 11389827.563, 'learn_time_ms': 2736.103, 'learn_throughput': 1461.933, 'synch_weights_time_ms': 1.593},counters={'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000},perf={'cpu_util_percent': 29.339999999999996, 'ram_util_percent': 77.52000000000001} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_89997_00000 reported custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.3, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.30877769583015, 'policy_loss': -0.021009808580481237, 'vf_loss': 9.327130681724958, 'vf_explained_var': 0.09925421905773942, 'kl': 0.008856010656157813, 'entropy': 0.5828838929053276, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 12000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 12000},sampler_results={'episode_reward_max': 335.0, 'episode_reward_min': 9.0, 'episode_reward_mean': 70.97, 'episode_len_mean': 70.97, 'episode_media': {}, 'episodes_this_iter': 31, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [90.0, 71.0, 57.0, 23.0, 50.0, 31.0, 11.0, 47.0, 54.0, 12.0, 123.0, 50.0, 64.0, 54.0, 29.0, 71.0, 42.0, 55.0, 52.0, 9.0, 40.0, 17.0, 65.0, 15.0, 34.0, 21.0, 12.0, 96.0, 29.0, 44.0, 43.0, 30.0, 76.0, 46.0, 29.0, 41.0, 128.0, 136.0, 59.0, 29.0, 74.0, 46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0], 'episode_lengths': [90, 71, 57, 23, 50, 31, 11, 47, 54, 12, 123, 50, 64, 54, 29, 71, 42, 55, 52, 9, 40, 17, 65, 15, 34, 21, 12, 96, 29, 44, 43, 30, 76, 46, 29, 41, 128, 136, 59, 29, 74, 46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.03662473294636633, 'mean_inference_ms': 0.2859663123215109, 'mean_action_processing_ms': 0.017734192687303984, 'mean_env_wait_ms': 0.01709935162050027, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=335.0,episode_reward_min=9.0,episode_reward_mean=70.97,episode_len_mean=70.97,episodes_this_iter=31,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.03662473294636633, 'mean_inference_ms': 0.2859663123215109, 'mean_action_processing_ms': 0.017734192687303984, 'mean_env_wait_ms': 0.01709935162050027, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=12000,num_agent_steps_trained=12000,num_env_steps_sampled=12000,num_env_steps_trained=12000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=12000,timers={'training_iteration_time_ms': 3177.248, 'load_time_ms': 0.26, 'load_throughput': 15359062.557, 'learn_time_ms': 2797.428, 'learn_throughput': 1429.885, 'synch_weights_time_ms': 1.509},counters={'num_env_steps_sampled': 12000, 'num_env_steps_trained': 12000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 12000},perf={'cpu_util_percent': 27.839999999999996, 'ram_util_percent': 78.32} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_89997_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 223.0, 'episode_reward_mean': 356.1, 'episode_len_mean': 356.1, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [440.0, 419.0, 273.0, 384.0, 363.0, 284.0, 296.0, 245.0, 299.0, 393.0, 500.0, 487.0, 223.0, 380.0, 279.0, 411.0, 319.0, 318.0, 500.0, 309.0], 'episode_lengths': [440, 419, 273, 384, 363, 284, 296, 245, 299, 393, 500, 487, 223, 380, 279, 411, 319, 318, 500, 309]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.02410948439203038, 'mean_inference_ms': 0.2515861373875738, 'mean_action_processing_ms': 0.015200122975011648, 'mean_env_wait_ms': 0.014148463277838799, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 7122, 'num_env_steps_sampled_this_iter': 7122, 'timesteps_this_iter': 7122},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.3, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.528787276565387, 'policy_loss': -0.015254237240440742, 'vf_loss': 9.542681082858834, 'vf_explained_var': 0.0818177862833905, 'kl': 0.00453466727955443, 'entropy': 0.5641433151819373, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 16000, 'num_env_steps_trained': 16000, 'num_agent_steps_sampled': 16000, 'num_agent_steps_trained': 16000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 9.0, 'episode_reward_mean': 100.87, 'episode_len_mean': 100.87, 'episode_media': {}, 'episodes_this_iter': 19, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [9.0, 40.0, 17.0, 65.0, 15.0, 34.0, 21.0, 12.0, 96.0, 29.0, 44.0, 43.0, 30.0, 76.0, 46.0, 29.0, 41.0, 128.0, 136.0, 59.0, 29.0, 74.0, 46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0], 'episode_lengths': [9, 40, 17, 65, 15, 34, 21, 12, 96, 29, 44, 43, 30, 76, 46, 29, 41, 128, 136, 59, 29, 74, 46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.03604860928357085, 'mean_inference_ms': 0.2848454299599664, 'mean_action_processing_ms': 0.017683699269928875, 'mean_env_wait_ms': 0.0170567144996309, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=9.0,episode_reward_mean=100.87,episode_len_mean=100.87,episodes_this_iter=19,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.03604860928357085, 'mean_inference_ms': 0.2848454299599664, 'mean_action_processing_ms': 0.017683699269928875, 'mean_env_wait_ms': 0.0170567144996309, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=16000,num_agent_steps_trained=16000,num_env_steps_sampled=16000,num_env_steps_trained=16000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=16000,timers={'training_iteration_time_ms': 3168.664, 'load_time_ms': 0.238, 'load_throughput': 16819264.16, 'learn_time_ms': 2791.545, 'learn_throughput': 1432.898, 'synch_weights_time_ms': 1.517},counters={'num_env_steps_sampled': 16000, 'num_env_steps_trained': 16000, 'num_agent_steps_sampled': 16000, 'num_agent_steps_trained': 16000},perf={'cpu_util_percent': 20.4, 'ram_util_percent': 78.25714285714285} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_89997_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 218.0, 'episode_reward_mean': 348.2, 'episode_len_mean': 348.2, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [310.0, 284.0, 319.0, 376.0, 239.0, 251.0, 389.0, 500.0, 500.0, 218.0, 329.0, 305.0, 225.0, 500.0, 497.0, 427.0, 345.0, 301.0, 347.0, 302.0], 'episode_lengths': [310, 284, 319, 376, 239, 251, 389, 500, 500, 218, 329, 305, 225, 500, 497, 427, 345, 301, 347, 302]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.024009054541370957, 'mean_inference_ms': 0.24818888283122872, 'mean_action_processing_ms': 0.015306010110368145, 'mean_env_wait_ms': 0.014131137137705082, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 6964, 'num_env_steps_sampled_this_iter': 6964, 'timesteps_this_iter': 6964},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.075, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.617491657503189, 'policy_loss': -0.011717470511994375, 'vf_loss': 9.629017138737504, 'vf_explained_var': 0.05320862948253591, 'kl': 0.002559508053341673, 'entropy': 0.5673648748346555, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 24000, 'num_env_steps_trained': 24000, 'num_agent_steps_sampled': 24000, 'num_agent_steps_trained': 24000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 170.07, 'episode_len_mean': 170.07, 'episode_media': {}, 'episodes_this_iter': 10, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [46.0, 19.0, 11.0, 44.0, 75.0, 103.0, 25.0, 86.0, 13.0, 34.0, 84.0, 41.0, 80.0, 57.0, 37.0, 31.0, 30.0, 20.0, 122.0, 106.0, 19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0], 'episode_lengths': [46, 19, 11, 44, 75, 103, 25, 86, 13, 34, 84, 41, 80, 57, 37, 31, 30, 20, 122, 106, 19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.03532342106364097, 'mean_inference_ms': 0.284860319583276, 'mean_action_processing_ms': 0.01762558566239516, 'mean_env_wait_ms': 0.017014055882056435, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=170.07,episode_len_mean=170.07,episodes_this_iter=10,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.03532342106364097, 'mean_inference_ms': 0.284860319583276, 'mean_action_processing_ms': 0.01762558566239516, 'mean_env_wait_ms': 0.017014055882056435, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=24000,num_agent_steps_trained=24000,num_env_steps_sampled=24000,num_env_steps_trained=24000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=24000,timers={'training_iteration_time_ms': 3144.416, 'load_time_ms': 0.231, 'load_throughput': 17316926.888, 'learn_time_ms': 2769.793, 'learn_throughput': 1444.151, 'synch_weights_time_ms': 1.476},counters={'num_env_steps_sampled': 24000, 'num_env_steps_trained': 24000, 'num_agent_steps_sampled': 24000, 'num_agent_steps_trained': 24000},perf={'cpu_util_percent': 19.45714285714286, 'ram_util_percent': 77.82857142857142} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_89997_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 177.0, 'episode_reward_mean': 378.2, 'episode_len_mean': 378.2, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 278.0, 500.0, 369.0, 500.0, 300.0, 399.0, 177.0, 373.0, 500.0, 500.0, 243.0, 344.0, 179.0, 399.0, 500.0, 500.0, 223.0, 500.0, 280.0], 'episode_lengths': [500, 278, 500, 369, 500, 300, 399, 177, 373, 500, 500, 243, 344, 179, 399, 500, 500, 223, 500, 280]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.02399673978913858, 'mean_inference_ms': 0.2477921144696873, 'mean_action_processing_ms': 0.015315514327661647, 'mean_env_wait_ms': 0.014215948140223719, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 7564, 'num_env_steps_sampled_this_iter': 7564, 'timesteps_this_iter': 7564},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0375, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.584355620927708, 'policy_loss': -0.012539551568327732, 'vf_loss': 9.596720795990318, 'vf_explained_var': 0.0652694062520099, 'kl': 0.004648902748239314, 'entropy': 0.5640272016486814, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 32000, 'num_env_steps_trained': 32000, 'num_agent_steps_sampled': 32000, 'num_agent_steps_trained': 32000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 236.99, 'episode_len_mean': 236.99, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [19.0, 18.0, 36.0, 25.0, 11.0, 59.0, 22.0, 15.0, 104.0, 148.0, 181.0, 71.0, 99.0, 50.0, 40.0, 140.0, 158.0, 49.0, 13.0, 70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 251.0, 500.0], 'episode_lengths': [19, 18, 36, 25, 11, 59, 22, 15, 104, 148, 181, 71, 99, 50, 40, 140, 158, 49, 13, 70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 251, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.034533426789586485, 'mean_inference_ms': 0.28378780262410114, 'mean_action_processing_ms': 0.01752167394464257, 'mean_env_wait_ms': 0.01693224906206447, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=236.99,episode_len_mean=236.99,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.034533426789586485, 'mean_inference_ms': 0.28378780262410114, 'mean_action_processing_ms': 0.01752167394464257, 'mean_env_wait_ms': 0.01693224906206447, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=32000,num_agent_steps_trained=32000,num_env_steps_sampled=32000,num_env_steps_trained=32000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=32000,timers={'training_iteration_time_ms': 3132.314, 'load_time_ms': 0.246, 'load_throughput': 16256992.248, 'learn_time_ms': 2759.465, 'learn_throughput': 1449.557, 'synch_weights_time_ms': 1.501},counters={'num_env_steps_sampled': 32000, 'num_env_steps_trained': 32000, 'num_agent_steps_sampled': 32000, 'num_agent_steps_trained': 32000},perf={'cpu_util_percent': 29.387500000000003, 'ram_util_percent': 77.9875} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_89997_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 264.0, 'episode_reward_mean': 464.8, 'episode_len_mean': 464.8, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 500.0, 500.0, 500.0, 349.0, 500.0, 500.0, 500.0, 500.0, 500.0, 264.0, 500.0, 500.0, 287.0, 500.0, 500.0, 500.0, 500.0, 500.0, 396.0], 'episode_lengths': [500, 500, 500, 500, 349, 500, 500, 500, 500, 500, 264, 500, 500, 287, 500, 500, 500, 500, 500, 396]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.02398923419226056, 'mean_inference_ms': 0.24771447303234365, 'mean_action_processing_ms': 0.01542516019212074, 'mean_env_wait_ms': 0.01424574959224489, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 9296, 'num_env_steps_sampled_this_iter': 9296, 'timesteps_this_iter': 9296},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.01875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.570250658835134, 'policy_loss': -0.021297379993703416, 'vf_loss': 9.591454705884379, 'vf_explained_var': 0.051694108273393366, 'kl': 0.004980524489019567, 'entropy': 0.5466910907658198, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 40000, 'num_env_steps_trained': 40000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 306.21, 'episode_len_mean': 306.21, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [70.0, 109.0, 16.0, 297.0, 215.0, 97.0, 49.0, 39.0, 134.0, 323.0, 192.0, 106.0, 152.0, 161.0, 335.0, 167.0, 11.0, 92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 251.0, 500.0, 500.0, 444.0, 500.0, 374.0, 401.0, 156.0, 410.0, 332.0, 440.0, 383.0, 498.0, 500.0, 500.0, 500.0, 242.0, 500.0, 500.0, 500.0, 500.0], 'episode_lengths': [70, 109, 16, 297, 215, 97, 49, 39, 134, 323, 192, 106, 152, 161, 335, 167, 11, 92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 251, 500, 500, 444, 500, 374, 401, 156, 410, 332, 440, 383, 498, 500, 500, 500, 242, 500, 500, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.03379211213143341, 'mean_inference_ms': 0.28331431984958905, 'mean_action_processing_ms': 0.017447722892685192, 'mean_env_wait_ms': 0.016907270967717464, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=11.0,episode_reward_mean=306.21,episode_len_mean=306.21,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.03379211213143341, 'mean_inference_ms': 0.28331431984958905, 'mean_action_processing_ms': 0.017447722892685192, 'mean_env_wait_ms': 0.016907270967717464, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=40000,num_agent_steps_trained=40000,num_env_steps_sampled=40000,num_env_steps_trained=40000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=40000,timers={'training_iteration_time_ms': 3161.84, 'load_time_ms': 0.248, 'load_throughput': 16121087.729, 'learn_time_ms': 2788.065, 'learn_throughput': 1434.687, 'synch_weights_time_ms': 1.447},counters={'num_env_steps_sampled': 40000, 'num_env_steps_trained': 40000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000},perf={'cpu_util_percent': 25.48888888888889, 'ram_util_percent': 78.04444444444444} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_89997_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 485.0, 'episode_reward_mean': 499.25, 'episode_len_mean': 499.25, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 485.0, 500.0], 'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 485, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.023928835164500693, 'mean_inference_ms': 0.24709744451779656, 'mean_action_processing_ms': 0.015369093760605626, 'mean_env_wait_ms': 0.014237575194298416, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 9985, 'num_env_steps_sampled_this_iter': 9985, 'timesteps_this_iter': 9985},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0046875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.49340570921539, 'policy_loss': -0.013890837514472584, 'vf_loss': 9.507261759235012, 'vf_explained_var': 0.04985133024954027, 'kl': 0.00742107697099916, 'entropy': 0.5605686643431264, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 48000, 'num_env_steps_trained': 48000, 'num_agent_steps_sampled': 48000, 'num_agent_steps_trained': 48000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 21.0, 'episode_reward_mean': 360.07, 'episode_len_mean': 360.07, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [92.0, 28.0, 123.0, 225.0, 136.0, 134.0, 286.0, 35.0, 182.0, 142.0, 101.0, 293.0, 53.0, 102.0, 405.0, 286.0, 365.0, 112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 251.0, 500.0, 500.0, 444.0, 500.0, 374.0, 401.0, 156.0, 410.0, 332.0, 440.0, 383.0, 498.0, 500.0, 500.0, 500.0, 242.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 247.0, 500.0, 307.0, 500.0, 480.0, 500.0, 500.0, 500.0, 325.0], 'episode_lengths': [92, 28, 123, 225, 136, 134, 286, 35, 182, 142, 101, 293, 53, 102, 405, 286, 365, 112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 251, 500, 500, 444, 500, 374, 401, 156, 410, 332, 440, 383, 498, 500, 500, 500, 242, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 247, 500, 307, 500, 480, 500, 500, 500, 325]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.033289501791646546, 'mean_inference_ms': 0.28276485168749144, 'mean_action_processing_ms': 0.017436543291101666, 'mean_env_wait_ms': 0.0169022941775384, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=21.0,episode_reward_mean=360.07,episode_len_mean=360.07,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.033289501791646546, 'mean_inference_ms': 0.28276485168749144, 'mean_action_processing_ms': 0.017436543291101666, 'mean_env_wait_ms': 0.0169022941775384, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=48000,num_agent_steps_trained=48000,num_env_steps_sampled=48000,num_env_steps_trained=48000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=48000,timers={'training_iteration_time_ms': 3172.397, 'load_time_ms': 0.245, 'load_throughput': 16318661.609, 'learn_time_ms': 2796.744, 'learn_throughput': 1430.235, 'synch_weights_time_ms': 1.502},counters={'num_env_steps_sampled': 48000, 'num_env_steps_trained': 48000, 'num_agent_steps_sampled': 48000, 'num_agent_steps_trained': 48000},perf={'cpu_util_percent': 30.074999999999996, 'ram_util_percent': 77.2875} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}.\n",
      "Trial PPO_CartPole-v1_89997_00000 reported evaluation={'episode_reward_max': 500.0, 'episode_reward_min': 230.0, 'episode_reward_mean': 469.35, 'episode_len_mean': 469.35, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [500.0, 500.0, 500.0, 336.0, 500.0, 500.0, 500.0, 500.0, 230.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 321.0, 500.0, 500.0], 'episode_lengths': [500, 500, 500, 336, 500, 500, 500, 500, 230, 500, 500, 500, 500, 500, 500, 500, 500, 321, 500, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.02390916157300608, 'mean_inference_ms': 0.2467588247630073, 'mean_action_processing_ms': 0.01534220526655348, 'mean_env_wait_ms': 0.01423137601556702, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_agent_steps_sampled_this_iter': 9387, 'num_env_steps_sampled_this_iter': 9387, 'timesteps_this_iter': 9387},custom_metrics={},episode_media={},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.0046875, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.439946808353547, 'policy_loss': -0.00938525903289036, 'vf_loss': 9.449298934526341, 'vf_explained_var': 0.014728497177041987, 'kl': 0.00707311727644914, 'entropy': 0.5210499244671996, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 56000, 'num_env_steps_trained': 56000, 'num_agent_steps_sampled': 56000, 'num_agent_steps_trained': 56000},sampler_results={'episode_reward_max': 500.0, 'episode_reward_min': 21.0, 'episode_reward_mean': 409.14, 'episode_len_mean': 409.14, 'episode_media': {}, 'episodes_this_iter': 9, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [112.0, 274.0, 21.0, 500.0, 324.0, 308.0, 217.0, 100.0, 377.0, 500.0, 78.0, 335.0, 415.0, 207.0, 499.0, 384.0, 272.0, 476.0, 387.0, 298.0, 312.0, 453.0, 500.0, 397.0, 478.0, 500.0, 500.0, 500.0, 342.0, 263.0, 436.0, 326.0, 123.0, 398.0, 384.0, 500.0, 339.0, 308.0, 469.0, 500.0, 500.0, 463.0, 500.0, 500.0, 154.0, 251.0, 500.0, 500.0, 444.0, 500.0, 374.0, 401.0, 156.0, 410.0, 332.0, 440.0, 383.0, 498.0, 500.0, 500.0, 500.0, 242.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 247.0, 500.0, 307.0, 500.0, 480.0, 500.0, 500.0, 500.0, 325.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 251.0, 281.0, 500.0, 500.0, 500.0, 500.0, 500.0, 363.0], 'episode_lengths': [112, 274, 21, 500, 324, 308, 217, 100, 377, 500, 78, 335, 415, 207, 499, 384, 272, 476, 387, 298, 312, 453, 500, 397, 478, 500, 500, 500, 342, 263, 436, 326, 123, 398, 384, 500, 339, 308, 469, 500, 500, 463, 500, 500, 154, 251, 500, 500, 444, 500, 374, 401, 156, 410, 332, 440, 383, 498, 500, 500, 500, 242, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 247, 500, 307, 500, 480, 500, 500, 500, 325, 500, 500, 500, 500, 500, 500, 500, 500, 500, 251, 281, 500, 500, 500, 500, 500, 363]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.032857653200633614, 'mean_inference_ms': 0.2826660049507093, 'mean_action_processing_ms': 0.01743855663127893, 'mean_env_wait_ms': 0.01693302936739092, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}},episode_reward_max=500.0,episode_reward_min=21.0,episode_reward_mean=409.14,episode_len_mean=409.14,episodes_this_iter=9,policy_reward_min={},policy_reward_max={},policy_reward_mean={},sampler_perf={'mean_raw_obs_processing_ms': 0.032857653200633614, 'mean_inference_ms': 0.2826660049507093, 'mean_action_processing_ms': 0.01743855663127893, 'mean_env_wait_ms': 0.01693302936739092, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=4,num_agent_steps_sampled=56000,num_agent_steps_trained=56000,num_env_steps_sampled=56000,num_env_steps_trained=56000,num_env_steps_sampled_this_iter=4000,num_env_steps_trained_this_iter=4000,num_steps_trained_this_iter=4000,agent_timesteps_total=56000,timers={'training_iteration_time_ms': 3150.573, 'load_time_ms': 0.253, 'load_throughput': 15790320.941, 'learn_time_ms': 2774.909, 'learn_throughput': 1441.489, 'synch_weights_time_ms': 1.539},counters={'num_env_steps_sampled': 56000, 'num_env_steps_trained': 56000, 'num_agent_steps_sampled': 56000, 'num_agent_steps_trained': 56000},perf={'cpu_util_percent': 19.1, 'ram_util_percent': 77.22500000000001} with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'off_policy_estimation_methods': {}, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': 2, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 415, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 18:27:57,556\tINFO tune.py:737 -- Total run time: 67.54 seconds (66.91 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RAY TUNE API .run() IN A LOOP UNTIL STOP CONDITION\n",
    "# Note about Ray Tune verbosity.\n",
    "# Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "# 0 = silent\n",
    "# 1 = only status updates, no logging messages\n",
    "# 2 = status and brief trial results, includes logging messages\n",
    "# 3 = status and detailed trial results, includes logging messages\n",
    "# Defaults to 3.\n",
    "###############\n",
    "\n",
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "evaluation_interval = 100   #100, num training episodes to run between eval steps\n",
    "verbosity = 2 # Tune screen verbosity\n",
    "\n",
    "trainer = tune.run(\"PPO\", \n",
    "                    \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\n",
    "          # \"training_iteration\": 200,  # stop if achieved 200 episodes\n",
    "          # \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\n",
    "          },  \n",
    "              \n",
    "    # training config params\n",
    "    config = config_train.to_dict(),\n",
    "                    \n",
    "    #redirect logs instead of default ~/ray_results/\n",
    "    local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq = checkpoint_freq,\n",
    "    checkpoint_at_end=True,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    verbose = verbosity,\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc331b91-dcb8-4225-8e30-94415c6b0c11",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Scroll through the output in the cell above.  Look for a single table row that looks like this: <br>\n",
    "\n",
    "<img src=\"images/ppo_cartpole_tune_output.png\"></img>\n",
    "\n",
    "What this telling you is that Tune ran your experiment.  It was terminated when Reward reached 409.14, which satisified the stopping criteria of Reward >= 400, out of an environment max reward possible of 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd024c",
   "metadata": {},
   "source": [
    "## Evaluate a RLlib model <a class=\"anchor\" id=\"eval_rllib\"></a>\n",
    "\n",
    "RLlib trainers can be evaluated by*:\n",
    "<ul>\n",
    "    <li>Examining <b>Ray Tune</b> trainer object </li>\n",
    "    <li>Examining <b>Ray Tune</b> experiment trial results </li>\n",
    "    <li>Visualizing training progress in <b>TensorBoard</b></li>\n",
    "    </ul>\n",
    "\n",
    "*RLlib trainer objects can also be examined manually, but it gives the same info you already saw in the single episode .train() output: <i>rllib_trainer.evaluate()</i>\n",
    "\n",
    "**Ray Tune trainer object** <br>\n",
    "First, let's start by looking at the trainer object. How long did training take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb3c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  66.01 seconds,    1.10 minutes\n"
     ]
    }
   ],
   "source": [
    "# Get RLlib default stats\n",
    "stats = trainer.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')\n",
    "\n",
    "# Typically takes about 67 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cad71",
   "metadata": {},
   "source": [
    "**Ray Tune experiment trial results**\n",
    "\n",
    "Read all the experiment trials into a single pandas dataframe.  The dataframe will have 1 row per trial.\n",
    "\n",
    "Below, we see a dataframe with only 1 row because Tune only ran 1 trial.  (Because we did not specify a hyperparameter space to search for tuning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99483c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (1, 422)\n",
      "Index(['episode_reward_max', 'episode_reward_min', 'episode_reward_mean',\n",
      "       'episode_len_mean', 'episodes_this_iter', 'num_healthy_workers',\n",
      "       'num_agent_steps_sampled', 'num_agent_steps_trained',\n",
      "       'num_env_steps_sampled', 'num_env_steps_trained',\n",
      "       ...\n",
      "       'info/learner/default_policy/learner_stats/total_loss',\n",
      "       'info/learner/default_policy/learner_stats/policy_loss',\n",
      "       'info/learner/default_policy/learner_stats/vf_loss',\n",
      "       'info/learner/default_policy/learner_stats/vf_explained_var',\n",
      "       'info/learner/default_policy/learner_stats/kl',\n",
      "       'info/learner/default_policy/learner_stats/entropy',\n",
      "       'info/learner/default_policy/learner_stats/entropy_coeff',\n",
      "       'config/evaluation_config/tf_session_args/gpu_options/allow_growth',\n",
      "       'config/evaluation_config/tf_session_args/device_count/CPU',\n",
      "       'config/evaluation_config/multiagent/policies/default_policy'],\n",
      "      dtype='object', length=422)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>num_agent_steps_sampled</th>\n",
       "      <th>num_agent_steps_trained</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89997_00000</th>\n",
       "      <td>500.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>409.14</td>\n",
       "      <td>409.14</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>56000</td>\n",
       "      <td>56000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "trial_id                                                                   \n",
       "89997_00000               500.0                21.0               409.14   \n",
       "\n",
       "             episode_len_mean  episodes_this_iter  num_healthy_workers  \\\n",
       "trial_id                                                                 \n",
       "89997_00000            409.14                   9                    4   \n",
       "\n",
       "             num_agent_steps_sampled  num_agent_steps_trained  \n",
       "trial_id                                                       \n",
       "89997_00000                    56000                    56000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read trainer results in a pandas dataframe\n",
    "df = trainer.results_df\n",
    "\n",
    "print(f\"df.shape: {df.shape}\")  #Only 1 trial\n",
    "print(df.columns)\n",
    "df.iloc[:,0:8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd6b1e",
   "metadata": {},
   "source": [
    "For how many episodes did training run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15cdff25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of episodes for the 1st trial\n",
    "df.iloc[0,:].episodes_this_iter  \n",
    "\n",
    "# Answer is 9 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed7e6a",
   "metadata": {},
   "source": [
    "What were the best parameter values?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53a815bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out, the following cell output is long because there are many parameters!\n",
    "\n",
    "# trainer.get_best_config(metric=\"episode_reward_mean\", mode=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd62d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize the training progress in TensorBoard\n",
    "\n",
    "RLlib automatically creates logs for your trained RLlib models that can be visualized in TensorBoard.  To visualize the performance of your RL model:\n",
    "\n",
    "<ol>\n",
    "    <li>Open a terminal</li>\n",
    "    <li><i><b>cd</b></i> into the logdir path from the above cell's output.</li>\n",
    "    <li><i><b>ls</b></i></li>\n",
    "    <li>You should see files that look like: checkpoint_NNNNNN</li>\n",
    "    <li>To be able to compare all your experiments, cd one dir level up.\n",
    "    <li><i><b>cd ..</b></i>  \n",
    "    <li><i><b>tensorboard --logdir . </b></i></li>\n",
    "    <li>Look at the url in the message, and open it in a browser</li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a04eff",
   "metadata": {},
   "source": [
    "#### Screenshot of Tensorboard\n",
    "\n",
    "TensorBoard will give you many pages of charts.  Below displaying just Train/Eval mean and min rewards.\n",
    "\n",
    "The charts below are showing \"sample efficiency\", the number of training steps it took to achieve a certain level of performance.\n",
    "\n",
    "<b>Train Performance:</b> <br>\n",
    "\n",
    "---\n",
    "<img src=\"images/ppo_cartpole_training_rewards.png\" width=\"80%\" />\n",
    "\n",
    "<b>Eval Performance:</b> <br>\n",
    "<img src=\"images/ppo_cartpole_eval_rewards.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf198368",
   "metadata": {},
   "source": [
    "## Play and render the game as a video <a class=\"anchor\" id=\"reload_rllib\"></a>\n",
    "\n",
    "To do this, we need to reload the desired RLlib model from checkpoint and then run the model inference mode on the environment it was trained on.  \n",
    "\n",
    "Where is the best model checkpoint file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b94d2c-9c28-411a-ad9d-ab46357b5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_89997_00000_0_2022-07-10_18-26-50\n",
      "/Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_89997_00000_0_2022-07-10_18-26-50/checkpoint_000014/checkpoint-14\n"
     ]
    }
   ],
   "source": [
    "# Get best checkpoint path\n",
    "logdir = trainer.get_best_logdir(metric=\"evaluation_reward_mean\", mode=\"max\")\n",
    "print(logdir)\n",
    "\n",
    "# Get last checkpoint path\n",
    "checkpoint = trainer.get_last_checkpoint()\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683354a3-bd3f-42be-badf-70a05ad0071d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Restore the desired, already-trained RLlib model from checkpoint file.</b>  \n",
    "\n",
    "You will need:\n",
    "<ul>\n",
    "    <li>Your <b>algorithm's config class</b> and exact same <a href=\"#intro_rllib_api\">config settings you used to train your model.</a></li>\n",
    "    <li>Name of the <b>environment</b> you used to train the model.</li>\n",
    "    <li>Path to the desired <b>checkpoint</b> file you want to use to restore the model.</li>\n",
    "    </ul>\n",
    "\n",
    "See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b933a724-7840-4fb7-bf0c-0b64e627c183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 18:27:57,896\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-07-10 18:27:57,897\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-07-10 18:28:01,293\tINFO trainable.py:590 -- Restored on 127.0.0.1 from checkpoint: /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/my_PPO_logs/PPO/PPO_CartPole-v1_89997_00000_0_2022-07-10_18-26-50/checkpoint_000014/checkpoint-14\n",
      "2022-07-10 18:28:01,298\tINFO trainable.py:599 -- Current state after restoring: {'_iteration': 14, '_timesteps_total': None, '_time_total': 60.11512041091919, '_episodes_total': 409}\n"
     ]
    }
   ],
   "source": [
    "# Create new Agent and restore its state from the last checkpoint.\n",
    "\n",
    "# create an empty agent\n",
    "agent = config_train.build(env=env_name)\n",
    "\n",
    "# restore the agent from the checkpoint\n",
    "agent.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e4a55-fbe1-439e-9a5f-76818a2289e5",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Record a video of the trained model doing inference in the environment it was trained on.</b>\n",
    "<br><br>\n",
    "\n",
    "Gym includes video recording and saving capability in its wrapper <i>`gym.wrappers.RecordVideo`</i>, so we will import and use that method to wrap the <i>`gym.make()`</i> method.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "👍 During inference, call the RLlib API method <b>compute_single_action()</b>, which uses the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference). \n",
    "</div>\n",
    "\n",
    "Execute the cell below, and you will see a video of the rollouts, so you can verify visually that the agent is starting from a near perfect score.\n",
    "\n",
    "Note that the CartPole environment's perfect score is 500.  Since we trained our model to around 400, the restored agent should appear very stable. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03f9f3bb-9ea2-471d-98d2-19632a79ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/miniforge3/envs/ray_dev/lib/python3.8/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/christy/Documents/github_ray_summit_2022/ray-summit-2022-training/ray-rllib/ppo_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done: reward = 500.0\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "#############\n",
    "## Create the env to do inference on\n",
    "#############\n",
    "# Try this first to make sure it is working\n",
    "# env = gym.make(env_name)\n",
    "# Once you have confirmed it works, wrap the method\n",
    "# RecordVideo() takes as input a path name where to store the video\n",
    "# RecordVideo() includes its own render step, records, and saves the video.\n",
    "env = RecordVideo(gym.make(env_name), \"ppo_video\", )\n",
    "obs = env.reset()\n",
    "\n",
    "#############\n",
    "## Use the restored model and run it in inference mode\n",
    "## You will see a pop-up video rendering for about 10 seconds\n",
    "#############\n",
    "num_episodes_during_inference = 1\n",
    "num_episodes = 0\n",
    "episode_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while num_episodes < num_episodes_during_inference:\n",
    "    # Compute an action (`a`).\n",
    "    a = agent.compute_single_action(observation=obs)\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, _ = env.step(a)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    # Is the episode `done`? -> Reset.\n",
    "    if done:\n",
    "        print(f\"Episode done: reward = {episode_reward}\")\n",
    "        obs = env.reset()\n",
    "        num_episodes += 1\n",
    "        episode_reward = 0.0\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# The restored agent manages to achieve a perfect score during the 1 episode rollout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb7940-bb3c-4899-a7fa-66a676ba30bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Play and share your video .mp4 file with others if you want.</b>\n",
    "<br><br>\n",
    "\n",
    "Show a video player in the notebook, and play the video you just recorded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b3aa827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"ppo_video/rl-video-episode-0.mp4\" controls  width=\"500\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_video='ppo_video/rl-video-episode-0.mp4'\n",
    "Video(cart_pole_video, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40b3830e-d0ae-4794-85b2-4b245faef04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray if you are done\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fbd96",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb74fe",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. How would choose another algorithm to train Cart Pole?  Hint:  Look at the [RLlib algorithm doc page](https://docs.ray.io/en/master/rllib/rllib-algorithms.html).  How would you change the choice of RLlib algorithm from <b>PPO to TD3</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511893b",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
