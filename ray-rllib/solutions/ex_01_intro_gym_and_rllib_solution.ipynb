{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710f04f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook 01. Introduction to the OpenAI Gym Environment and RLlib Algorithm top-level APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved<br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "\n",
    "‚û°Ô∏è [Next notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn:\n",
    " * [What is an Environment in Reinforcement Learning (RL)?](#intro_env)\n",
    " * [Overview of RL terminology](#intro_rl)\n",
    " * [Introduction to OpenAI Gym environments](#intro_gym)\n",
    " * [High-level OpenAI Gym API calls](#intro_gym_api)\n",
    " * [Overview of RLlib](#intro_rllib)\n",
    " * [Train a policy using an algorithm from RLlib](#intro_rllib_api)\n",
    " * [Evaluate a RLlib policy](#eval_rllib)\n",
    " * [Reload RLlib policy from checkpoint and run inference](#reload_rllib)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aca4e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is an environment in RL? <a class=\"anchor\" id=\"intro_env\"></a>\n",
    "\n",
    "Solving a problem in RL begins with an **environment**. In the simplest definition of RL:\n",
    "\n",
    "> An **agent** interacts with an **environment** and receives a reward.\n",
    "\n",
    "An environment in RL is the agent's world, it is a simulation of the problem to be solved. \n",
    "\n",
    "<img src=\"../images/env_key_concept1.png\" width=\"50%\" />\n",
    "\n",
    "The **environment** simulator might be of a:\n",
    "<ul>\n",
    "    <li>real, physical machine such as a gas turbine or autonomous vehicle</li>\n",
    "    <li>real, abstract system such as user behavior on a website or the stock market</li>\n",
    "    <li>virtual sytem on a computer such as a board game or a video game</li>\n",
    "    </ul>\n",
    "    \n",
    "The **agent** represents what is triggering the actions.  For example it could be:\n",
    "<ul>\n",
    "    <li>a software system that is triggering actions for machines</li>\n",
    "    <li>a type of user or investor</li>\n",
    "    <li>a game player or game system that is competing against real players </li>\n",
    "    </ul> \n",
    "<br>    \n",
    "    \n",
    "<b>Comparison of RL to supervised learning</b> <br>\n",
    "<ul>\n",
    "    <li><u><i>Data</i></u>.  In supervised learning, you start with a labeled dataset.  In contrast, the <b>data in RL is not given up front; the environment acts as a data generator</b>.  One can also do RL on a pre-collected dataset (called offline RL), we will touch on offline RL later. </li> <br>\n",
    "    <li><u><i>Training</i></u>.  In supervised learning, a ML algorithm is trained on ALL the labeled training data AT ONCE.  <b>RL trains over a sequence of feedback loops.</b>  The RL algorithm optimizes the sum of individual rewards over repeated lifetimes (episodes) of sequential decisions: action -> fedback -> improved action -> repeat. </li><br>\n",
    "    <li><u><i>Evaluation</i></u>.  In supervised learning, a ML algorithm is evaluated on ALL the hold-out validation data AT ONCE. <b>RL REPEATEDLY evaluates a policy at different time steps</b>, typically whenever you save a checkpoint file. Evaluation at particular points in time in RL is similar in concept to \"backtesting\" in time series forecasting. RL evaluations are specific to a time step. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Why bother with an Agent, Environment, and RL?</b>  <br>\n",
    "Supervised learning can be too shortsighted or overlook important, changing user intents or business conditions.  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "<b> üí° Reinforcement learning (RL) is a powerful technique when there are sequential decision-making processes, and you want to optimize for long-term possibly delayed rewards.  <br>\n",
    "    üí° RL can also work when there is no existing model to rely on or you want to improve over an existing decision-making strategy. </b> \n",
    "</div> \n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf844e4",
   "metadata": {},
   "source": [
    "## Overview of RL terminology <a class=\"anchor\" id=\"intro_rl\"></a>\n",
    "\n",
    "An RL environment consists of: \n",
    "\n",
    "1. all possible actions (**action space**)\n",
    "2. a complete description of the environment, nothing hidden (**state space**)\n",
    "3. an observation by the agent of certain parts of the state (**observation space**)\n",
    "4. **reward**, which is the only feedback the agent receives after each action.\n",
    "\n",
    "The model that tries to maximize the expected sum over all future rewards is called a **policy**. The policy is a function mapping the environment's observations to an action to take, usually written **œÄ** (s(t)) -> a(t).  <i>In deep reinforcement learning, this function is a neural network</i>.\n",
    "\n",
    "<b>Policy vs Model? </b>\n",
    "In traditional supervised learning, model means a¬†trained algorithm, or a learned function.\n",
    "\n",
    "> <i>In RL, a model is roughly equivalent to a policy, but policy is more specific</i> because it is trained in a specific environment.  For deployment, we use the word \"model\" because more people understand the ML meaning of a trained model.\n",
    "\n",
    "Below is a high-level image of how the Agent and Environment work together to train a Policy in a RL simulation feedback loop in RLlib.\n",
    "\n",
    "<img src=\"../images/env_key_concept2.png\" width=\"98%\" />\n",
    "\n",
    "The **RL simulation feedback loop** repeatedly collects data, for one (single-agent case) or multiple (multi-agent case) policies, trains the policies on these collected data, and makes sure the policies' weights are kept in synch. \n",
    "\n",
    "During simulation loops, the environment collects observations, taken actions, receives rewards and so-called **done** flags, indicating the boundaries of different episodes the agents play through in the simulation.\n",
    "\n",
    "Each simulation iteration is called a <b>time step</b>.  The simulation iterations of action -> reward -> next state -> train -> repeat, until the end state, is called an **episode**, or in RLlib, a **rollout**.  At the end of the episode, when the <i>done</i> flag is True, we call RLlib method .reset(), which sets the <i>done</i> flag to False again.\n",
    "> üëâ Each episode consists of one or many time steps.\n",
    "\n",
    "<b>Per episode</b> (or between **done** flag == True), the RL simulation feedback loop repeats up to some specified end state (termination state or timesteps). Examples of termination are:\n",
    "<ul>\n",
    "    <li>the end of a maze (termination state)</li>  \n",
    "    <li>the player died in a game (termination state)</li>\n",
    "    <li>after 60 videos watched in a recommender system (timesteps).</li>\n",
    "    </ul>\n",
    "    \n",
    "<b>Why train for many episodes?</b>  When you are doing machine learning, you do not just do something once and report the result.  You do it many times, to make sure you did not just get \"lucky\" one time.  RL is similar.  By training for many episodes, you collect more data, which provides more variance, which is hopefully more realistic.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>üí° In RL, the policy is trained by repeating trials, or episodes (or rollouts), then reporting the calculated reward typically as an average of all achieved rewards per episode.  The cumulative sum of all mean episode rewards is called the Return.</b> \n",
    "</div>\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07193fc6",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym example: frozen lake <a class=\"anchor\" id=\"intro_gym\"></a>\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) is a well-known reference library of RL environments. \n",
    "\n",
    "#### 1. import gym\n",
    "\n",
    "Below is how you would import gym and view all available environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742e5847",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "Num Gym Environments: 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[EnvSpec(FrozenLake-v1), EnvSpec(FrozenLake8x8-v1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import gym\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# List all available gym environments\n",
    "all_env  =  list(gym.envs.registry.all())\n",
    "print(f'Num Gym Environments: {len(all_env)}')\n",
    "\n",
    "# You could loop through and list all environments if you wanted\n",
    "# [print(e) for e in all_env]\n",
    "envs_starting_with_f = [e for e in all_env if str(e).startswith(\"EnvSpec(Frozen\")]\n",
    "envs_starting_with_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddd364",
   "metadata": {},
   "source": [
    "#### 2. Instatiate your Gym object\n",
    "\n",
    "The way you instantiate a Gym environment is with the **make()** function.\n",
    "\n",
    "The .make() function takes arguments:\n",
    "- **name of the Gym environment**, type: str, Required.\n",
    "- **runtime parameter values**, Optional.\n",
    "\n",
    "For the required string argument, you need to know the Gym name.  You can find the Gym name in the Gym documentation for environments, either:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "    \n",
    "Below is an example of how to create a basic Gym environment, [frozen lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/).  We can see below that the termination condition of an episode will be <b>TimeLimit</b> (the environment automatically ends an episode and sets done=True after this many timesteps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00d01a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n",
      "env_spec: EnvSpec(FrozenLake-v1)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"FrozenLake-v1\"\n",
    "\n",
    "# Instantiate gym env object with a runtime parameter value (is_slippery).\n",
    "# is_slippery=True specifies the environment is stochastic\n",
    "# is_slippery=False is the same as \"deterministic=True\"\n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    is_slippery=False,  # whether the environment behaves deterministically or not\n",
    ")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c1e21",
   "metadata": {},
   "source": [
    "#### 3. Inspect the environment action and observations spaces\n",
    "\n",
    "Gym Environments can be deterministic or stochastic.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Deterministic</b> if the current state + selected action determines the next state of the environment.  <i>Chess is an example of a deterministic environment</i>, since all possible states/action combinations can be described as a discrete set of rules with states bounded by the pieces and size of the board.</li>\n",
    "    <li>\n",
    "        <b>Stochastic</b> if the policy output action is a probability distribution over a set of possible actions at time step t. In this case, the agent needs to compute its action from the policy in two steps. i) sample actions from the policy according to the probability distribution, ii) compute log likelihoods of the actions. <i>Random visitors to a website is an example of a stochastic environment</i>. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Gym actions.</b> The action_space describes the numerical structure of the legitimate actions that can be applied to the environment. \n",
    "\n",
    "For example, if we have 4 possible discrete actions, we could encode them as:\n",
    "<ul>\n",
    "    <li>0: LEFT</li>\n",
    "    <li>1: DOWN</li>\n",
    "    <li>2: RIGHT</li>\n",
    "    <li>3: UP</li>\n",
    "</ul>\n",
    "\n",
    "<b>Gym observations.</b>  The observation_space defines the structure as well as the legitimate values for the observation of a state of the environment.  \n",
    "\n",
    "For example, if we have a 4x4 grid, we could encode them as {0,1,2,3, 4, ‚Ä¶ ,15} for grid positions ((0,0), (0,1), (0,2), (0,3), ‚Ä¶. (3,3)).\n",
    "\n",
    "From the Gym [documentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) about the frozen lake environment, we see: <br>\n",
    "\n",
    "|Frozen Lake      | Gym space   |\n",
    "|---------------- | ----------- |\n",
    "|Action Space     | Discrete(4) |\n",
    "|Observation Space| Discrete(16)|\n",
    " \n",
    "<b><a href=\"https://github.com/openai/gym/tree/master/gym/spaces\">Gym spaces</a></b> are gym data types.  The main types are `Discrete` for discrete numbers and `Box` for continuous numbers.  \n",
    "\n",
    "Gym Space `Discrete` elements are Python type `int`, and Gym Space `Box` are Python type `float32`.\n",
    "\n",
    "Below is an example how to inspect the environment action and observations spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78620a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a gym environment.\n",
      "\n",
      "gym action space: Discrete(4)\n",
      "gym observation space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# check if it is a gym instance\n",
    "if isinstance(env, gym.Env):\n",
    "    print(\"This is a gym environment.\")\n",
    "    print()\n",
    "\n",
    "    # print gym Spaces\n",
    "    if isinstance(env.action_space, gym.spaces.Space):\n",
    "        print(f\"gym action space: {env.action_space}\")\n",
    "    if isinstance(env.observation_space, gym.spaces.Space):\n",
    "        print(f\"gym observation space: {env.observation_space}\") \n",
    "        \n",
    "# Note: the action space is discrete with 4 possible actions.\n",
    "# Note: the observation space is 4x4 and thus runs from 0 to 15.\n",
    "# Note: if we chose 8x8, the observation space would change to Discrete(64)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad91e6",
   "metadata": {},
   "source": [
    "#### 4. Inspect gym environment default & runtime parameters\n",
    "\n",
    "Gym environments contain 2 sets of parameters that are set after the environment object is instantiated.\n",
    "<ul>\n",
    "    <li><b>Default parameters</b> are fixed in the Gym environment code itself.</li>\n",
    "    <li><b>Runtime parameters</b> are passed into the make() function as **kwargs.</li>\n",
    "    </ul>\n",
    "\n",
    "Below is an example of how to inspect the environment parameters.  Notice we can tell from the parameters that our frozen lake environment is: <br>\n",
    "1) <i>Deterministic</i>, and <br>\n",
    "2) Episode terminates with time step condition <i>max_episode_steps</i> = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4dc0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default spec params...\n",
      "id: FrozenLake-v1\n",
      "reward_threshold: 0.7\n",
      "nondeterministic: False\n",
      "max_episode_steps: 100\n",
      "order_enforce: True\n",
      "\n",
      "Runtime spec params...\n",
      "map_name: 4x4\n",
      "is_slippery: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect env.spec parameters\n",
    " \n",
    "# View default env spec params that are hard-coded in Gym code itself\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "# rewards above this value considered \"success\"\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "# env is deterministic or stochastic\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "# number of time steps per episode\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "# must reset before step or render\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\") \n",
    "\n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print()\n",
    "print(\"Runtime spec params...\")\n",
    "# Note: gym > v21 use just .kwargs instead of ._kwargs\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    "\n",
    "# Note:  We can tell that our frozen lake environment is: \n",
    "# 1) Success criteria is rewards >= 0.7\n",
    "# 2) Deterministic\n",
    "# 3) Episode terminates when number time_steps = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd5020",
   "metadata": {
    "tags": []
   },
   "source": [
    "## High-level OpenAI Gym API calls <a class=\"anchor\" id=\"intro_gym_api\"></a>\n",
    "\n",
    "The most basic Gym API methods are:\n",
    "<ul>\n",
    "    <li><b>env.reset()</b> <br>Reset the environment to an initial state.  Returns the initial observation.  <b>You should call this method every time at the start of a new episode.</b></li>\n",
    "    <li><b>env.step(action)</b> <br>Take an action from the possible action space values.  It <b><i>takes an action as input</i></b>, computes the state of the environment after applying that action and <b><i>returns the 4-tuple (next-observation, reward, done, info)</i></b>.</li>\n",
    "    <li><b>env.render()</b>  <br>Visually inspect the environment. This is for human/debugging purposes; it is not seen by the agent/algorithm.  Note you cannot inspect an environment before it has been initialized with env.reset().</li>\n",
    "    <li><b>env.close()</b> <br>Close an environment.</li>\n",
    "    </ul>\n",
    "    \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° <b>To play an episode, call reset() first!  <br>\n",
    "üí° After that, continue to call step() until the environment automatically returns done=True.</b> \n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd2bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Print the starting observation.  \n",
    "# Recall possible observations are between 4x4 grid.\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee81cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 1, reward: 0.0, done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Take an action\n",
    "# Recall the possible actions are: 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "\n",
    "new_obs, reward, done, _ = env.step(2) #Right\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()\n",
    "new_obs, reward, done, _ = env.step(1) #Down\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec105",
   "metadata": {},
   "source": [
    "We can also try to run an action in the frozen lake environment which is outside the defined number range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0620a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this cell if you want whole notebook to run without errors\n",
    "\n",
    "# Try to take an invalid action\n",
    "\n",
    "#env.step(4) # invalid\n",
    "\n",
    "# should see KeyError below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9c090-0c94-47b7-8e7d-a54af12c446f",
   "metadata": {},
   "source": [
    "To test out your environment, typically you will loop through a few episodes to make sure it works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8a6854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6883b819a654b22b922bfe8cd41d6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# The following three lines are for rendering purposes only.\n",
    "# They allow us to render the env frame-by-frame in-place\n",
    "# (w/o creating a huge output which we would then have to scroll through).\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with out:\n",
    "\n",
    "    # Putting the Gym simple API methods together.\n",
    "    # Here is a pattern for running a bunch of episodes.\n",
    "    num_episodes = 5 # Number of episodes you want to run the agent\n",
    "    total_reward = 0.0  # Initialize reward to 0\n",
    "\n",
    "    # Loop through episodes\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        # Reset the environment at the start of each episode\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Loop through time steps per episode\n",
    "        while True:\n",
    "            # take random action, but you can also do something more intelligent \n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # If the epsiode is up, then start another one\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Render the env (in place).\n",
    "            time.sleep(0.3)\n",
    "            out.clear_output(wait=True)\n",
    "            print(f\"episode: {ep}\")\n",
    "            print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "            env.render()\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f1156",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview of RLlib <a class=\"anchor\" id=\"intro_rllib\"></a>\n",
    "\n",
    "<img width=\"7%\" src=\"../images/rllib-logo.png\"> is the most comprehensive open-source Reinforcement Learning framework. **[RLlib](https://github.com/ray-project/ray/tree/master/rllib)** is <b>distributed by default</b> since it is built on top of **[Ray](https://docs.ray.io/en/latest/)**, an easy-to-use, open-source, distributed computing framework for Python that can handle complex, heterogeneous applications. Ray and RLlib run on compute clusters on any cloud without vendor lock.  RLlib Resources:\n",
    "<ol>\n",
    "    <li>The doc page on <a href=\"https://docs.ray.io/en/master/rllib/index.html\">ray.io website</a></li>\n",
    "    <li><a href=\"https://github.com/ray-project/ray/tree/master/rllib\">RLlib source code</a></li>\n",
    "    </ol>\n",
    "\n",
    "RLlib includes <b>25+</b> available [algorithms](https://docs.ray.io/en/master/rllib/rllib-algorithms.html), converted to both <img width=\"3%\" src=\"../images/tensorflow-logo.png\">_TensorFlow_ and <img width=\"3%\" src=\"../images/pytorch-logo.png\">_PyTorch_, covering different sub-categories of RL: _model-free_, _offline RL_, _model-based_, and _gradient-free_.¬†Almost any RLlib algorithm can learn in a <b>multi-agent</b> setting.¬†Many algorithms support <b>RNNs</b> and <b>LSTMs</b>.\n",
    "\n",
    "On a very high level, RLlib is organized by **environments**, **algorithms**, **examples**, **tuned_examples**, and **models**.  \n",
    "\n",
    "    ray\n",
    "    |- rllib\n",
    "    |  |- env \n",
    "    |  |- algorithms\n",
    "    |  |  |- alpha_zero \n",
    "    |  |  |- appo \n",
    "    |  |  |- ppo \n",
    "    |  |  |- ... \n",
    "    |  |- examples \n",
    "    |  |- tuned_examples\n",
    "    |  |- models\n",
    "\n",
    "Within **_env_** you will find [classes](https://docs.ray.io/en/latest/rllib/package_ref/env.html) that allow RLlib to handle e.g. the multi-agent cases (which gym does NOT cover).  RLlib automatically supports any **OpenAI Gym environment** (which supports most user cases). RLlib also handle external environments that have strict performance or hosting requirements. <i>(In the next notebook, we will use the **RLlib MultiAgentEnv** base class to create a **multi agent** environment).</i>\n",
    "\n",
    "Within **_examples_** you will find some examples of common custom rllib use cases.  \n",
    "\n",
    "Within **_tuned\\_examples_**, you will find, sorted by algorithm, suggested hyperparameter value choices within .yaml files. Ray¬†RLlib team ran simulations/benchmarks to find suggested hyperparameter value choices.¬†¬†These¬†files are used¬†for daily testing, and weekly hard-task testing to make sure they all run at speed,¬†for both TF and Torch.¬†Helps give you a leg-up with initial parameter choices!\n",
    "\n",
    "Within **_models_**, you will find building blocks for NNs, default models that RLlib will use (for either <img width=\"3%\" src=\"../images/tensorflow-logo.png\">_TensorFlow_ or <img width=\"3%\" src=\"../images/pytorch-logo.png\">_PyTorch_). For example, here are building blocks for DNN, CNN, RNN, and LSTM. \n",
    "\n",
    "In this tutorial, we will mainly focus on the **_algorithms_** package, where we will find RLlib algos to train policies on environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a23493-80b1-4032-a98f-f14827026501",
   "metadata": {},
   "source": [
    "## Train a policy using an algorithm from RLlib <a class=\"anchor\" id=\"intro_rllib_api\"></a>\n",
    "\n",
    "Once you have an environment, next you need to decide which RL algorithm to use.  There are many factors to consider when selecting which algorithm to use on your environment.  Following are some high-level best practices.\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>The first distinction comes from your action space</b>, i.e., do you have discrete (e.g. LEFT, RIGHT, ‚Ä¶) or continuous actions (ex: go to a certain speed)? To check high-level if an algorithm will work, look at the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">RLlib algorithms doc page</a>.  <i>Algorithms are listed according to whether or not they support Discrete action spaces vs Continuous action spaces or both.</i> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Choose a stable algorithm.</b>  Look at the cumulative rewards per time step, they should rise steadily.  You do not want an algorithm where reward jumps up and down a lot.\n",
    "    </li>\n",
    "    <li><b>Choose the most sample-efficient algorithm that works for your environment</b>.  Look at the cumulative rewards per time step, they should rise quickly. <i>PPO is extremely sample-efficient.  SAC is much less sample-efficient.</i>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "#### Step 1.  Import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5655b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d2983-90fd-48bd-b61a-1ee79b62770f",
   "metadata": {},
   "source": [
    "#### Step 2. Check environment for errors   \n",
    "\n",
    "Before you start training, it is a good idea to check the environment for errors.  RLlib provides a convenient [Environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) for this.  It checks that the environment is compatible with OpenAI Gym and RLlib (and outputs a warning if necessary).\n",
    "\n",
    "Below, we check our Frozen Lake environment for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b4730c-ad46-42c9-ae70-ef1f551883b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "\n",
    "# How to check you do not have any environment errors\n",
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53918a3d-3c4f-4d89-8d22-4ff449cef6c7",
   "metadata": {},
   "source": [
    "#### Step 3. Calculate an environment baseline\n",
    "\n",
    "Let's run through the environment, acting randomly, without rendering, and record the mean reward.  The purpose of this is to obtain a baseline before training a RLlib algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° If you are doing benchmarks, this random policy is often called a <b>\"baseline\".</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64dea06e-facf-486b-b140-079af058bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************\n",
      "Baseline Mean Reward=0.02+/-0.13 (out of success=0.7)\n",
      "Baseline won 50.0 times over 3000 episodes (23091 timesteps)\n",
      "Approx 0.02 wins per episode\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "# Putting the Gym simple API methods together.\n",
    "# Here is a pattern for running a bunch of episodes.\n",
    "num_episodes = 3000 # Number of episodes you want to run the agent\n",
    "num_timesteps = 0\n",
    "# Collect all episode rewards here\n",
    "episode_rewards = []\n",
    "\n",
    "# Loop through episodes\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    # Loop through time steps per episode\n",
    "    while True:\n",
    "        # take random action, but you can also do something more intelligent \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # If the epsiode is up, then start another one\n",
    "        num_timesteps += 1\n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            break\n",
    "\n",
    "# calculate mean_reward\n",
    "env_mean_random_reward = np.mean(episode_rewards)\n",
    "env_sd_reward = np.std(episode_rewards)\n",
    "# calculate number of wins\n",
    "total_reward = np.sum(episode_rewards)\n",
    "    \n",
    "print()\n",
    "print(\"**************\")\n",
    "print(f\"Baseline Mean Reward={env_mean_random_reward:.2f}+/-{env_sd_reward:.2f}\", end=\"\")\n",
    "print(f\" (out of success={env_spec.reward_threshold})\")\n",
    "print(f\"Baseline won {total_reward} times over {num_episodes} episodes ({num_timesteps} timesteps)\")\n",
    "print(f\"Approx {total_reward/num_episodes:.2f} wins per episode\")\n",
    "print(\"**************\")\n",
    "        \n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a757e7",
   "metadata": {},
   "source": [
    "#### Step 4.  Select an algorithm and find that algorithm's config class  \n",
    "\n",
    "Here is how to find an <b>RLlib algorithm's config class</b>.\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo\">algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the start of the <b>config class definition</b>.</li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, and </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61b7804-1332-45fe-a5bc-4b20364c02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# Default PPO config values\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1cb28-1f27-4257-9521-6c7157d0e3ab",
   "metadata": {},
   "source": [
    "#### Step 5. Choose your config settings and instantiate a config object with those settings\n",
    "\n",
    "As of Ray 1.13, RLlib configs been converted from primitive Python dictionaries into Objects. This makes them harder to print, but easier to set/pass.\n",
    "\n",
    "**Note about RLlib config values precedence**\n",
    "<ol>\n",
    "    <li><i>Highest precedence</i>: <b>user's algorithm config settings at time of training</b>.  These override all other config settings.</li>\n",
    "    <li><i>Lower precedence</i>: <b>specific RLlib algorithm (e.g. PPO) config</b>:  \n",
    "        <ol>\n",
    "            <li>Open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.  </li>\n",
    "            <li>Search the github code file for the start of the <b>config class definition</b>.</li>\n",
    "            <li>Scroll down to the config class <b>__init()__</b> method.</li>\n",
    "            <ol>\n",
    "            <li><i>Algorithm default hyperparameter values are here</i>.</li>\n",
    "            </ol>\n",
    "        </ol>\n",
    "    <li><i>Lowest</i> precedence: Common RLlib <b><a href\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py#L58\">generic algorithm config</a></b> settings.</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422a7baf-cbd8-4141-a9d7-974239231c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common RLlib generic (for all algorithms) config values\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "config = AlgorithmConfig()\n",
    "\n",
    "# # uncomment below to see the long list of RLlib general config values\n",
    "# print(f\"RLlib's general default training config values:\")\n",
    "# print(pretty_print(config.to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bb6a2",
   "metadata": {},
   "source": [
    "**Note about num_workers**\n",
    "\n",
    "Number of Ray workers is the number of parallel workers or actors for rollouts.  Actual num_workers will be what you specifiy+1 for head node.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° <b>For num_workers, use ONE LESS than the number of cores you want to use</b> (or omit this argument and let Ray automatically use all cores)!\n",
    "</div>\n",
    "\n",
    "\n",
    "Below, num_workers = 4,  <br>\n",
    "means actual number processors used = 5 (including head node). <br>\n",
    "Since I know 8 is #cpu on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79de5e14-dd95-4f58-a4b3-712b0b328c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x288a16a00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOConfig object\n",
    "config = PPOConfig()\n",
    "\n",
    "# Setup our config object to use our environment\n",
    "config.environment(env=\"FrozenLake-v1\")\n",
    "\n",
    "# Decide if you want torch or tensorflow DL framework.  Default is \"tf\"\n",
    "config.framework(framework=\"torch\")\n",
    "\n",
    "# +1 for head node, num parallel workers or actors for rollouts\n",
    "config.rollouts(num_rollout_workers=1)\n",
    "\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "config.debugging(seed=415, log_level=\"ERROR\")\n",
    "\n",
    "# Setup evaluation\n",
    "# Explicitly set \"explore\"=False to override default\n",
    "config.evaluation(evaluation_interval=10, \n",
    "                evaluation_duration=20, \n",
    "                evaluation_config = {\"explore\" : False})\n",
    "\n",
    "# Setup sampling rollout workers\n",
    "config.rollouts(num_rollout_workers=4, \n",
    "                num_envs_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb86ff8",
   "metadata": {},
   "source": [
    "#### Step 6. Instantiate an algorithm from the environment and algorithm config objects\n",
    "\n",
    "**Two ways to train RLlib policies***\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/package_ref/index.html\">RLlib API.</a> The main methods are:</li>\n",
    "    <ul>\n",
    "        <li>train()</li>\n",
    "        <li>save()</li>\n",
    "        <li>evaluate()</li>\n",
    "        <li><b>restore()</b></li>\n",
    "        <li><b>compute_single_action()</b></li>\n",
    "    </ul>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/tune/api_docs/overview.html\">Ray Tune API.</a>  The main methods are:</li>\n",
    "        <ul>\n",
    "            <li><b>run()</b></li>\n",
    "    </ul>\n",
    "    </ol>\n",
    "    \n",
    "*3rd way is RLlib CLI from command line using .yml file, but the .yml file is undocumented: <i>rllib train -f [myfile_name].yml</i><br>\n",
    "\n",
    "<b>RLlib API train()</b> will train for 1 <i>iteration</i> only.  Good for debugging since every single output will be shown for the single iteration.  \n",
    "\n",
    "<b>Ray Tune API run()</b> is usually more convenient since with 1 function call you get experiment management: hyperparameter tuning, save checkpoints, evaluate, and training up to a stopping criteria.\n",
    "\n",
    "‚úîBoth methods will run the RLlib [environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) you saw earlier in this notebook (Step 2. Check environment).\n",
    "\n",
    "<b>RLlib API restore()</b> will reload a checkpointed RLlib model for Serving and Offline learning, even if the model was trained using Tune.  Tune API methods will not work for this.\n",
    "\n",
    "<b>RLlib API compute_single_action()</b> will use the trained <i>`policy`</i> (RL word for trained model) and use that for inference on an environment.   \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In summary: <br>\n",
    "    üí° <b>Train</b> a RLlib algorithm with Ray Tune method <b>`.run()`</b>  <br>\n",
    "    üëâ  <b>Develop</b> or debug a RLlib algorithm with RLlib method <b>`.train()`</b> <br>\n",
    "    üëâ  <b>Restore</b> a RLlib policy with RLlib  method <b>`.restore()`</b> <br>\n",
    "    üëâ  <b>Run inference</b> on an environment using a trained policy with RLlib method <b>`.compute_single_action()`</b>\n",
    "</div>\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8685e-43e9-4bab-8f37-a206328d1a7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SINGLE .TRAIN() OUTPUT\n",
    "\n",
    "# instantiate an algo instance\n",
    "ppo_algo = config.build()\n",
    "print(f\"Algorithm type: {type(ppo_algo)}\")\n",
    "\n",
    "# Perform single `.train() iteration` call\n",
    "# Result is a Python dict object\n",
    "result = ppo_algo.train()\n",
    "\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a6107f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm type: <class 'ray.rllib.algorithms.ppo.ppo.PPO'>\n",
      "Iteration=0, Mean Reward=0.01+/-0.00\n",
      "Checkpoints saved at results/PPO/checkpoint_000001\n",
      "Iteration=10, Mean Reward=0.15+/-0.04\n",
      "Checkpoints saved at results/PPO/checkpoint_000011\n",
      "Iteration=20, Mean Reward=0.21+/-0.09\n",
      "Checkpoints saved at results/PPO/checkpoint_000021\n",
      "Iteration=29, Mean Reward=0.62+/-0.20\n",
      "Checkpoints saved at results/PPO/checkpoint_000030\n",
      "PPO won 66.0 times over 3180 episodes (122640 timesteps)\n",
      "Approx 0.02 wins per episode\n",
      "Training took 83.55 seconds\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RLLIB API .train() IN A LOOP\n",
    "# To train for N number of episodes, you put .train() into a loop, \n",
    "# similar to the way we ran the Gym env.step() in a loop.\n",
    "###############\n",
    "# start fresh in case ray already running\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "ppo_algo = config.build()\n",
    "print(f\"Algorithm type: {type(ppo_algo)}\")\n",
    "\n",
    "# train the Algorithm instance for 30 iterations\n",
    "num_iterations = 30\n",
    "rewards = []\n",
    "checkpoint_dir = \"results/PPO/\"\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Call its `train()` method\n",
    "    result = ppo_algo.train()\n",
    "    \n",
    "    # Extract reward from results.\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "    # print something every 10 episodes\n",
    "    if ((i % 10 == 0) or (i == num_iterations-1)):\n",
    "        print(f\"Iteration={i}, Mean Reward={result['episode_reward_mean']:.2f}\",end=\"\")\n",
    "        try:\n",
    "            print(f\"+/-{np.std(rewards):.2f}\")\n",
    "        except:\n",
    "            print()\n",
    "        # save checkpoint file\n",
    "        checkpoint_file = ppo_algo.save(checkpoint_dir)\n",
    "        print(f\"Checkpoints saved at {checkpoint_file}\")\n",
    "        # evaluate the policy\n",
    "        eval_result = ppo_algo.evaluate()\n",
    "\n",
    "# convert num_iterations to num_episodes\n",
    "num_episodes = len(result[\"hist_stats\"][\"episode_lengths\"]) * num_iterations\n",
    "# convert num_iterations to num_timesteps\n",
    "num_timesteps = sum(result[\"hist_stats\"][\"episode_lengths\"] * num_iterations)\n",
    "# calculate number of wins\n",
    "num_wins = np.sum(result[\"hist_stats\"][\"episode_reward\"])\n",
    "\n",
    "# train time\n",
    "print(f\"PPO won {num_wins} times over {num_episodes} episodes ({num_timesteps} timesteps)\") \n",
    "print(f\"Approx {num_wins/num_episodes:.2f} wins per episode\")\n",
    "print(f\"Training took {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abdec6-6b0e-4fc4-8882-d4d2313f872f",
   "metadata": {},
   "source": [
    "<b>Understanding the output of RLlib .train()</b>\n",
    "\n",
    "‚¨ÜÔ∏è Notice above, the `train()` method returns a dictionary containing information about the iteration of training. Here \"iteration\" consists of many episodes, the exact number depending on config values.  \n",
    "\n",
    "<b>Compare the PPO Training results to Random Baseline <br></b>\n",
    "- PPO Mean Reward=~0.61+/-0.22.  This is much higher than baseline!\n",
    "> Baseline Mean Reward=~0.02+/-0.13 (out of success=0.7) <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    ‚úî <b>PPO mean reward is approx 30x higher than the random baseline! <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9f496-72ed-4a43-a62b-77b07c6e381d",
   "metadata": {},
   "source": [
    "<br>\n",
    "What were the last parameter values?  <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6112db93-9b70-4249-a9a1-0a48941b23af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "    'grad_gnorm': 1.3092542168433947,\n",
       "    'cur_kl_coeff': 9.765625000000002e-05,\n",
       "    'cur_lr': 5.0000000000000016e-05,\n",
       "    'total_loss': 0.15082930225825128,\n",
       "    'policy_loss': 0.0049029359894414105,\n",
       "    'vf_loss': 0.14592618676283026,\n",
       "    'vf_explained_var': -0.020948633711825135,\n",
       "    'kl': 0.0018398162273860898,\n",
       "    'entropy': 0.1457263630083812,\n",
       "    'entropy_coeff': 0.0},\n",
       "   'model': {},\n",
       "   'custom_metrics': {},\n",
       "   'num_agent_steps_trained': 128.0}},\n",
       " 'num_env_steps_sampled': 120000,\n",
       " 'num_env_steps_trained': 120000,\n",
       " 'num_agent_steps_sampled': 120000,\n",
       " 'num_agent_steps_trained': 120000}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment_results.get_best_config(metric=\"episode_reward_mean\", mode=\"mean\")\n",
    "result['info']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee7f6991-6d60-4cd4-a72e-b945fbc4050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "ppo_algo.stop()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad01ce-76ce-4141-b064-8785db90a06e",
   "metadata": {},
   "source": [
    "‚¨áÔ∏è Below for completeness, is an example how to do this same thing using `Ray Tune`.  We won't go into this right now, because it will be covered very soon in another notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e34aa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##############\n",
    "# # EXAMPLE USING RAY TUNE API .run() 1 UNTIL STOPPING CONDITION\n",
    "# # For completeness, here is how to use Ray Tune's .run() method\n",
    "# ##############\n",
    "\n",
    "# # To start fresh, restart Ray in case it is already running\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "\n",
    "# experiment_results = tune.run(\"PPO\", \n",
    "                    \n",
    "#     # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "#     stop={\n",
    "#           # \"episode_reward_mean\": 0.2, # stop if achieve 0.2 out of max 0.7\n",
    "#           \"training_iteration\": 22,  # stop if achieved 200 iterations\n",
    "#           # \"timesteps_total\": 3000,  # stop if achieved 3000 timesteps\n",
    "#           },  \n",
    "              \n",
    "#     # training config params\n",
    "#     config = config.to_dict(),\n",
    "                    \n",
    "#     #redirect logs to relative path instead of default ~/ray_results/\n",
    "#     local_dir = \"my_Tune_PPO_logs\",\n",
    "         \n",
    "#     # set frequency saving checkpoints >= evaulation_interval\n",
    "#     checkpoint_freq = 7,\n",
    "#     checkpoint_at_end=False,\n",
    "         \n",
    "#     # Reduce logging messages\n",
    "#     ###############\n",
    "#     # Note about Ray Tune verbosity.\n",
    "#     # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "#     # 0 = silent\n",
    "#     # 1 = only status updates, no logging messages\n",
    "#     # 2 = status and brief trial results, includes logging messages\n",
    "#     # 3 = status and detailed trial results, includes logging messages\n",
    "#     # Defaults to 3.\n",
    "#     ###############                          \n",
    "#     verbose = 2,\n",
    "#     )\n",
    "\n",
    "# print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd024c",
   "metadata": {},
   "source": [
    "## Evaluate a RLlib Policy <a class=\"anchor\" id=\"eval_rllib\"></a>\n",
    "\n",
    "Traditional Supervised ML splits data into train/valid/test, and runs evaluate on the valid dataset AFTER the model has been trained.  RL, on the other hand, runs evaluation typically every time a checkpoint is saved.  \n",
    "\n",
    "RLlib policies can be evaluated by:\n",
    "<ul>\n",
    "    <li>Calling RLlib Algorithm API <b>.evaluate()</b> typically every time <b>.save()</b> is called.</li>\n",
    "    <li>Visualizing training progress in <b>TensorBoard</b></li>\n",
    "    <li>Examining Ray Tune experiment results - will be covered very soon in another notebook!  </li>\n",
    "    </ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd62d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize the training progress in TensorBoard\n",
    "\n",
    "<b>Ray Tune</b> automatically creates logs for your trained RLlib models that can be visualized in TensorBoard.  Ray Tune logs are stored in the specified redirect `local_dir`; or if none specified then the logs are stored in `~/ray_results/`.\n",
    "\n",
    "<b>RLlib Algorithm .train() requires an explicit .save() step</b> in order to create logs.  The default format for .save() is Ray Tune .json logs compatible with TensorBoard.  Unlike Ray Tune, using .save(), it is only possible to store logs in `~/ray_results/`.  You cannot change the location of the TensorBoard logs.\n",
    "\n",
    "To visualize the performance of your RL policy:\n",
    "\n",
    "<ol>\n",
    "    <li>Open a terminal</li>\n",
    "    <li><i><b>cd</b></i> into the correct log directory.</li>\n",
    "    <li><i><b>ls</b></i></li>\n",
    "    <li>You should see files such as: <i>result.json, params.json, ... </i></li>\n",
    "    <li>To be able to compare all your experiments, cd one dir level up.\n",
    "    <li><i><b>cd ..</b></i>  \n",
    "    <li><i><b>tensorboard --logdir . </b></i></li>\n",
    "    <li>Look at the url in the message, and open it in a browser</li>\n",
    "        </ol>\n",
    "        \n",
    "Note Step 7 above: if running RLlib on a cluster, use <a href=\"https://blog.tensorflow.org/2019/12/introducing-tensorboarddev-new-way-to.html\">tensorboard.dev</a> instead.  Navigate to the directory on the head node where `ray_results/` directory is located.  From there, run \n",
    "`tensorboard dev upload --logdir .`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a04eff",
   "metadata": {},
   "source": [
    "#### Screenshot of Tensorboard\n",
    "\n",
    "TensorBoard will give you many pages of charts.  Below displaying just Train/Eval max and mean rewards.\n",
    "\n",
    "The charts below are showing \"sample efficiency\", the number of training steps it took to achieve a certain level of performance.\n",
    "\n",
    "<b>Train Performance:</b> <br>\n",
    "\n",
    "---\n",
    "<img src=\"../images/frozen_lake_training_rewards.png\" width=\"80%\" />\n",
    "\n",
    "<b>Eval Performance:</b> <br>\n",
    "<img src=\"../images/frozen_lake_eval_rewards.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf198368",
   "metadata": {},
   "source": [
    "## Reload RLlib policy from checkpoint and run inference <a class=\"anchor\" id=\"reload_rllib\"></a>\n",
    "\n",
    "We want to reload the desired RLlib model from checkpoint file and then run the policy in inference mode on the environment it was trained on.  \n",
    "\n",
    "You will need:\n",
    "<ul>\n",
    "    <li>Your <b>algorithm's config class</b></li>\n",
    "    <li>Name of the <b>environment</b> you used to train the policy.</li>\n",
    "    <li>Path to the desired <b>checkpoint</b> file you want to use to restore the policy.</li>\n",
    "    </ul>\n",
    "\n",
    "#### Step 1. Find the best model checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "723176ca-d6e0-4b2b-ace9-78d820ab3390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results/PPO/checkpoint_000030/checkpoint-30\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE GETTING CHECKPOINT FROM RLLIB TRAIN\n",
    "\n",
    "# Enter the last checkpoint manually\n",
    "checkpoint = \"results/PPO/checkpoint_000030/checkpoint-30\"\n",
    "print(f\"\\n{checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b94d2c-9c28-411a-ad9d-ab46357b5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE GETTING CHECKPOINT FROM RAY TUNE\n",
    "\n",
    "# # Get best checkpoint path\n",
    "# checkpoint_path = experiment_results.get_best_logdir(metric=\"evaluation_reward_mean\", mode=\"max\")\n",
    "# # checkpoint_path = \"my_Tune_PPO_logs/PPO/PPO_CartPole-v1_a3973_00000_0_2022-08-10_19-09-32/checkpoint_000020/checkpoint-18\"\n",
    "# print(checkpoint_path)\n",
    "\n",
    "# # Get last checkpoint\n",
    "# checkpoint = experiment_results.get_last_checkpoint()\n",
    "# print(f\"\\n{checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f109b-8618-4f2a-8484-2a0b62890d6e",
   "metadata": {},
   "source": [
    "#### Step 2. Re-initialize an already-trained algorithm object from the checkpoint file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b933a724-7840-4fb7-bf0c-0b64e627c183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 15:32:17,189\tINFO trainable.py:668 -- Restored on 127.0.0.1 from checkpoint: results/PPO/checkpoint_000030\n",
      "2022-08-15 15:32:17,191\tINFO trainable.py:677 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 79.15324401855469, '_episodes_total': 8264}\n"
     ]
    }
   ],
   "source": [
    "# Create new Algorithm and restore its state from the last checkpoint.\n",
    "\n",
    "# create an empty Algorithm\n",
    "algo = config.build()\n",
    "\n",
    "# restore the agent from the checkpoint\n",
    "algo.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e4a55-fbe1-439e-9a5f-76818a2289e5",
   "metadata": {},
   "source": [
    "#### Step 3. Play and render the game\n",
    "\n",
    "Now we want to play the trained policy doing inference in the environment it was trained on.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "‚úî During inference, call the RLlib API method <b>compute_single_action()</b>: <br>\n",
    "\n",
    "üëç Uses the trained <i>policy</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>rollout</i> (RLlib word for episode during inference). \n",
    "</div>\n",
    "\n",
    "‚¨áÔ∏è Below we play the game 100 times using the PPO already-trained policy.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfb9414e-0e8a-4da6-a147-e34bef9fef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************\n",
      "PPO mean_reward: 0.54 out of success: 0.7 after 100 episodes or 3284 time steps\n",
      "PPO won 54.0 times over 100 plays\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## Create the env to do inference on\n",
    "#############\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "\n",
    "# Use the restored algorithm from checkpoint and run it in inference mode\n",
    "episode_reward = 0.0\n",
    "done = False\n",
    "num_episodes = 0\n",
    "num_steps = 0\n",
    "\n",
    "while num_episodes < 100:\n",
    "    # Compute an action (`a`).\n",
    "    a = algo.compute_single_action(observation=obs)\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, _ = env.step(a)\n",
    "    episode_reward += reward\n",
    "    num_steps += 1\n",
    "    \n",
    "    # Is the episode `done`? -> Reset.\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        num_episodes += 1\n",
    "\n",
    "# calculate mean_reward\n",
    "print()\n",
    "print(\"**************\")\n",
    "mean_reward = episode_reward / num_episodes\n",
    "print(f\"PPO mean_reward: {mean_reward:.2f} out of success: {env_spec.reward_threshold} after {num_episodes} episodes or {num_steps} time steps\")\n",
    "print(f\"PPO won {episode_reward} times over {num_episodes} plays\")\n",
    "print(\"**************\")\n",
    "        \n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94252692-912c-47cf-9f78-d7a3094f5ba6",
   "metadata": {},
   "source": [
    "<b>How does our inferenced policy compare to the Random baseline? <br></b>\n",
    "- PPO wins ~54 times over 100 plays.  This is much higher than baseline!\n",
    "> Baseline won ~53.0 times over 3000 plays (episodes) <br>\n",
    "\n",
    "\n",
    "‚¨áÔ∏è Below we render the game using the PPO policy, so we can visually inspect the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74e910fd-4e05-472e-a2c5-b88f31b6858f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c26a9733c64dbbb1a084e26ad7bce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The following three lines are for rendering purposes only.\n",
    "# They allow us to render the env frame-by-frame in-place\n",
    "# (w/o creating a huge output which we would then have to scroll through).\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with out:\n",
    "\n",
    "    #############\n",
    "    ## Create the env to do inference on\n",
    "    #############\n",
    "    env = gym.make(env_name)\n",
    "    obs = env.reset()\n",
    "\n",
    "    #############\n",
    "    ## Use the restored policy and run it in inference mode\n",
    "    ## Run compute_single_action() in inference episodes loop\n",
    "    ## You will see an ASCII rendering in-place for about 10 seconds\n",
    "    #############\n",
    "    episode_reward = 0.0\n",
    "    done = False\n",
    "    num_episodes = 0\n",
    "\n",
    "    while num_episodes < 5:\n",
    "        # Compute an action (`a`).\n",
    "        a = algo.compute_single_action(observation=obs)\n",
    "        # Send the computed action `a` to the env.\n",
    "        obs, reward, done, _ = env.step(a)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Is the episode `done`? -> Reset.\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            num_episodes += 1\n",
    "\n",
    "        # Render the env (in place).\n",
    "        time.sleep(0.3)\n",
    "        out.clear_output(wait=True)\n",
    "        print(f\"episode: {num_episodes}\")\n",
    "        print(f\"obs: {obs}, reward: {episode_reward}, done: {done}\")\n",
    "        env.render()\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fbd96",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learned:\n",
    "* What a gym Environment is, and how the gym.Env API is used define sequential decision making problems using python code\n",
    "* How RLlib looks like on the surface (where to find its algorithms and top-level APIs)\n",
    "* How to train a RLlib algorithm using `.train()` and a built-in gym.Env (\"frozen lake\")\n",
    "* Where to find checkpoint files, logs, tensorboard files, etc..\n",
    "* How to play and render some episodes from a gym.Env using a trained RLlib algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb74fe",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "#### How would you choose another algorithm to train Frozen Lake?\n",
    "\n",
    "Hint:  Look at the [RLlib algorithm doc page](https://docs.ray.io/en/master/rllib/rllib-algorithms.html).\n",
    "How would you change the choice of RLlib algorithm from <b>PPO to DQN</b>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "840adfbd-fb10-4688-98c6-1b9d05eebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "# Default DQN config values\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(DQNConfig().to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68a356e6-c654-484e-bbdd-1f727943c8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.dqn.dqn.DQNConfig at 0x2911826a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DQNConfig object\n",
    "dqn_config = DQNConfig()\n",
    "\n",
    "# Setup our config object to use our environment\n",
    "dqn_config.environment(env=\"FrozenLake-v1\")\n",
    "\n",
    "# Decide if you want torch or tensorflow DL framework.  Default is \"tf\"\n",
    "dqn_config.framework(framework=\"torch\")\n",
    "\n",
    "# +1 for head node, num parallel workers or actors for rollouts\n",
    "dqn_config.rollouts(num_rollout_workers=1)\n",
    "\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "dqn_config.debugging(seed=415, log_level=\"ERROR\")\n",
    "\n",
    "# Setup evaluation\n",
    "# Explicitly set \"explore\"=False to override default\n",
    "dqn_config.evaluation(evaluation_interval=10, \n",
    "                evaluation_duration=20, \n",
    "                evaluation_config = {\"explore\" : False})\n",
    "\n",
    "# Setup sampling rollout workers\n",
    "dqn_config.rollouts(num_rollout_workers=1, \n",
    "                num_envs_per_worker=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3ba06a6-231c-40fa-8c17-907614dd0275",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm type: <class 'ray.rllib.algorithms.dqn.dqn.DQN'>\n",
      "Iteration=0, Mean Reward=0.00+/-0.00\n",
      "Checkpoints saved at results/DQN/checkpoint_000001\n",
      "Iteration=10, Mean Reward=0.16+/-0.05\n",
      "Checkpoints saved at results/DQN/checkpoint_000011\n",
      "Iteration=20, Mean Reward=0.42+/-0.16\n",
      "Checkpoints saved at results/DQN/checkpoint_000021\n",
      "Iteration=29, Mean Reward=0.48+/-0.19\n",
      "Checkpoints saved at results/DQN/checkpoint_000030\n",
      "DQN won 48.0 times over 3000 episodes (115800 timesteps)\n",
      "Approx 0.02 wins per episode\n",
      "Training took 99.39 seconds\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# EXAMPLE USING RLLIB API .train() IN A LOOP\n",
    "# To train for N number of episodes, you put .train() into a loop, \n",
    "# similar to the way we ran the Gym env.step() in a loop.\n",
    "###############\n",
    "\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "dqn_algo = dqn_config.build()\n",
    "print(f\"Algorithm type: {type(dqn_algo)}\")\n",
    "\n",
    "# train the Algorithm instance for 30 iterations\n",
    "num_iterations = 30\n",
    "rewards = []\n",
    "checkpoint_dir = \"results/DQN/\"\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Call its `train()` method\n",
    "    result = dqn_algo.train()\n",
    "    \n",
    "    # Extract reward from results.\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "    # print something every 10 episodes\n",
    "    if ((i % 10 == 0) or (i == num_iterations-1)):\n",
    "        print(f\"Iteration={i}, Mean Reward={result['episode_reward_mean']:.2f}\",end=\"\")\n",
    "        try:\n",
    "            print(f\"+/-{np.std(rewards):.2f}\")\n",
    "        except:\n",
    "            print()\n",
    "        # save checkpoint file\n",
    "        checkpoint_file = dqn_algo.save(checkpoint_dir)\n",
    "        print(f\"Checkpoints saved at {checkpoint_file}\")\n",
    "        # evaluate the policy\n",
    "        eval_result = dqn_algo.evaluate()\n",
    "\n",
    "# convert num_iterations to num_episodes\n",
    "num_episodes = len(result[\"hist_stats\"][\"episode_lengths\"]) * num_iterations\n",
    "# convert num_iterations to num_timesteps\n",
    "num_timesteps = sum(result[\"hist_stats\"][\"episode_lengths\"] * num_iterations)\n",
    "# calculate number of wins\n",
    "num_wins = np.sum(result[\"hist_stats\"][\"episode_reward\"])\n",
    "\n",
    "# train time\n",
    "print(f\"DQN won {num_wins} times over {num_episodes} episodes ({num_timesteps} timesteps)\")\n",
    "print(f\"Approx {num_wins/num_episodes:.2f} wins per episode\")\n",
    "print(f\"Training took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# # To stop the Algorithm and release its blocked resources, use:\n",
    "# dqn_algo.stop()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ea75f-ab28-4b29-94c1-5fc910151cae",
   "metadata": {},
   "source": [
    "Compare the DQN Training results to Random Baseline.   \n",
    "- DQN Mean Reward=~0.48+/-0.19.  This is much higher than baseline!\n",
    "> Baseline Mean Reward=~0.02+/-0.13 (out of success=0.7) <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    ‚úî <b>DQN mean reward is approx 24x higher than the random baseline! <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda31f6-880c-4fab-b313-ab17d3d0e17f",
   "metadata": {},
   "source": [
    "<br>\n",
    "‚¨áÔ∏è Below we play the game using the DQN trained policy 100 times, similar to what we did with the PPO trained policy..\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40a73e59-cae3-4a16-a5ba-2bd9b43fa431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results/DQN/checkpoint_000030/checkpoint-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 15:34:28,036\tINFO trainable.py:668 -- Restored on 127.0.0.1 from checkpoint: results/DQN/checkpoint_000030\n",
      "2022-08-15 15:34:28,039\tINFO trainable.py:677 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 95.39320707321167, '_episodes_total': 1889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************\n",
      "DQN mean_reward: 0.48 out of success: 0.7 after 100 episodes or 3849 time steps\n",
      "DQN won 48.0 times over 100 plays\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "# Enter the last checkpoint manually\n",
    "checkpoint = \"results/DQN/checkpoint_000030/checkpoint-30\"\n",
    "print(f\"\\n{checkpoint}\")\n",
    "\n",
    "# create an empty Algorithm\n",
    "algo = dqn_config.build()\n",
    "\n",
    "# restore the agent from the checkpoint\n",
    "algo.restore(checkpoint)\n",
    "\n",
    "#############\n",
    "## Create the env to do inference on\n",
    "#############\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "\n",
    "# Use the restored model and run it in inference mode\n",
    "episode_reward = 0.0\n",
    "done = False\n",
    "num_episodes = 0\n",
    "num_steps = 0\n",
    "\n",
    "while num_episodes < 100:\n",
    "    # Compute an action (`a`).\n",
    "    a = algo.compute_single_action(observation=obs)\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, _ = env.step(a)\n",
    "    episode_reward += reward\n",
    "    num_steps += 1\n",
    "    \n",
    "    # Is the episode `done`? -> Reset.\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        num_episodes += 1\n",
    "\n",
    "# calculate mean_reward\n",
    "print()\n",
    "print(\"**************\")\n",
    "mean_reward = episode_reward / num_episodes\n",
    "print(f\"DQN mean_reward: {mean_reward:.2f} out of success: {env_spec.reward_threshold} after {num_episodes} episodes or {num_steps} time steps\")\n",
    "print(f\"DQN won {episode_reward} times over {num_episodes} plays\")\n",
    "print(\"**************\")\n",
    "        \n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294ac52-0c87-42b2-832a-04f193204fd6",
   "metadata": {},
   "source": [
    "<b>How does our inferenced policy compare to the Random baseline? <br></b>\n",
    "- DQN wins ~48 times over 100 plays.  This is much higher than baseline!\n",
    "> Baseline won ~53.0 times over 3000 plays (episodes) <br>\n",
    "\n",
    "<br>\n",
    "‚¨áÔ∏è Below we render the game using the DQN policy, so we can visually inspect the environment, as humans.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2e467bb-a770-4f5e-a2e9-f4de4dfb1878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4bf15a2b95483090d2557a10490bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The following three lines are for rendering purposes only.\n",
    "# They allow us to render the env frame-by-frame in-place\n",
    "# (w/o creating a huge output which we would then have to scroll through).\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with out:\n",
    "\n",
    "    #############\n",
    "    ## Create the env to do inference on\n",
    "    #############\n",
    "    env = gym.make(env_name)\n",
    "    obs = env.reset()\n",
    "\n",
    "    #############\n",
    "    ## Use the restored model and run it in inference mode\n",
    "    ## Run compute_single_action() in inference episodes loop\n",
    "    ## You will see a an ASCII rendering in-place for about 10 seconds\n",
    "    #############\n",
    "    episode_reward = 0.0\n",
    "    done = False\n",
    "    num_episodes = 0\n",
    "\n",
    "    while num_episodes < 5:\n",
    "        # Compute an action (`a`).\n",
    "        a = algo.compute_single_action(observation=obs)\n",
    "        # Send the computed action `a` to the env.\n",
    "        obs, reward, done, _ = env.step(a)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Is the episode `done`? -> Reset.\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            num_episodes += 1\n",
    "\n",
    "        # Render the env (in place).\n",
    "        time.sleep(0.3)\n",
    "        out.clear_output(wait=True)\n",
    "        print(f\"episode: {num_episodes}\")\n",
    "        print(f\"obs: {obs}, reward: {episode_reward}, done: {done}\")\n",
    "        env.render()\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511893b",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [OpenAI Gym Environments](https://www.gymlibrary.ml/)\n",
    "2. [Ray doc page](https://docs.ray.io/en/latest/)\n",
    "3. [Rllib github](https://github.com/ray-project/ray/tree/master/rllib)\n",
    "4. [RLlib Algorithms doc page](https://docs.ray.io/en/master/rllib/rllib-algorithms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa00b0e-9136-4d52-9977-b1845064b60e",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "\n",
    "‚û° [Next notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9a924cb-af94-43dd-9e4e-4fab1b648302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray if you are done\n",
    "import ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
