{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Serving Challenges\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "ðŸ“– [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "âž¡ [Next notebook](./ex_02_ray_serve_fastapi.ipynb) <br>\n",
    "\n",
    "### Learning Objective:\n",
    "In this introductory tutorial, you will:\n",
    "\n",
    "* learn about model serving challenges\n",
    "* understand the why Ray Serve and its concepts, components and architecture\n",
    "* utilize Ray Serve APIs to create and serve deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Challenges of Model Serving\n",
    "\n",
    "Model development happens in a data science research environment. There are many challenges, such as feature engineering, model selection, missing or messy data, yet there are tools at the data scientists' disposal. By contrast, model deployment to production faces an entirely different set of challenges and requires different tools. We must bridge the divide as much as possible.\n",
    "\n",
    "So what are some of the challenges of model serving?\n",
    "\n",
    "<img src=\"images/serve_challenges.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### 1. It Should Be Framework Agnostic\n",
    "\n",
    "First, model serving frameworks must be able to serve models from popular frameworks and libraries like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks, in order to get the best model. \n",
    "\n",
    "Second, machine learning models are typically surrounded by (or work in conjunction with) \n",
    "lots of application or business logic. For example, some model serving is implemented as a RESTful service to which scoring requests are made. Often this is too restrictive, as some additional processing, such as fetching additional data from a online feature store, to augment the request data, may be desired as part of the scoring process, and the performance overhead of remote calls may be suboptimal.\n",
    "\n",
    "### 2. Pure Python or Pythonic\n",
    "\n",
    "In general, model serving should be intuitive for developers and simple to configure and run. Hence, it is desirable to use pure Python and to avoid verbose configurations using YAML files or other means. \n",
    "\n",
    "Data scientists and engineers use Python and Python-based ML frameworks to develop their machine learning models, so they should also be able to use Python to deploy their machine learning applications. This need is growing more critical as online learning applications combine training and serving in the same applications.\n",
    "\n",
    "### 3. Simple and Scalable\n",
    "\n",
    "Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "\n",
    "### 4. DevOps/MLOps Integrations\n",
    "\n",
    "Model serving deployments need to integrate with existing \"DevOps\" CI/CD practices for controlled, audited, and predicatble releases. Patterns like [Canary Deployment](https://martinfowler.com/bliki/CanaryRelease.html) are particularly useful for testing the efficacy of a new model before replacing existing models, just as this pattern is useful for other software deployments.\n",
    "\n",
    "### 5. Flexible Deployment Patterns\n",
    "\n",
    "There are unique deployment patterns, too. For example, it should be easy to deploy a forest of models, to split traffic to different instances, and to score data in batches for greater efficiency.\n",
    "\n",
    "See also this [Ray blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) on the challenges of model serving and the way Ray Serve addresses them. It also provides an example of starting with a simple model, then deploying a more sophisticated model into the running application. Along the same lines, this blog post, [Serving ML Models in Production Common Patterns](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns) discusses how deployment patterns for model serving and how you can use Ray Serve. Additionally, listen to this webinar: [Building a scalable ML model serving API with Ray Serve](https://www.anyscale.com/events/2021/09/09/building-a-scalable-ml-model-serving-api-with-ray-serve). This introductory webinar highlights how Ray Serve makes it easy to deploy, operate and scale a machine learning API.\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ray Serve?\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a scalable, framework-agnostic and Python-first model serving library built on [Ray](https://ray.io).\n",
    "\n",
    "<img src=\"images/ray_serve_overview.png\" width=\"70%\" height=\"40%\"> \n",
    "\n",
    "For users, Ray Serve offers these benefits:\n",
    "\n",
    "* **Framework Agnostic**: You can use the same toolkit to serve everything from deep learning models built with [PyTorch](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial), [Tensorflow](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), or [Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), to [scikit-Learn](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial) models, to arbitrary business logic.\n",
    "* **Python First:** Configure your model serving with pure Python code. No YAML or JSON configurations required.\n",
    "\n",
    "Since Serve is built on Ray, it also allows you to scale to many machines, in your datacenter or in cloud environments, and it allows you to leverage all of the other Ray frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve Architecture and components\n",
    "\n",
    "<img src=\"images/serve-architecture-2.0.png\" height=\"40%\" width=\"70%\">\n",
    "\n",
    "There are three kinds of actors that are created to make up a Serve instance:\n",
    "- **Controller**: A global actor unique to each Serve instance that manages\n",
    "  the control plane. The Controller is responsible for creating, updating, and\n",
    "  destroying other actors. Serve API calls like creating or getting a deployment\n",
    "  make remote calls to the Controller.\n",
    "- **HTTP Proxy**: By default there is one HTTP proxy actor on the head node. This actor runs a [Uvicorn](https://www.uvicorn.org/) HTTP\n",
    "  server that accepts incoming requests, forwards them to replicas, and\n",
    "  responds once they are completed.  For scalability and high availability,\n",
    "  you can also run a proxy on each node in the cluster via the `location` field of [`http_options`](core-apis).\n",
    "- **Replicas**: Actors that actually execute the code in response to a\n",
    "  request. For example, they may contain an instantiation of an ML model. Each\n",
    "  replica processes individual requests from the HTTP proxy (these may be batched\n",
    "  by the replica using `@serve.batch`, see the [batching](https://docs.ray.io/en/latest/serve/ml-models.html#serve-batching) docs).\n",
    "\n",
    "For more details, see this [key concepts](https://docs.ray.io/en/latest/serve/index.html) and [architecture](https://docs.ray.io/en/latest/serve/architecture.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetime of a Request\n",
    "\n",
    "When an HTTP request is sent to the router, the following things happen:\n",
    "\n",
    " * The HTTP request is received and parsed.\n",
    "\n",
    " * The correct deployment associated with the HTTP url path is looked up. The request is placed on a queue.\n",
    "\n",
    " * For each request in a deployment queue, an available replica is looked up and the request is sent to it. If there are no available replicas (there are more than max_concurrent_queries requests outstanding), the request is left in the queue until an outstanding request is finished.\n",
    "\n",
    "Each replica maintains a queue of requests and executes one at a time, possibly using asyncio to process them concurrently. If the handler (the function for the deployment or __call__) is async, the replica will not wait for the handler to run; otherwise, the replica will block until the handler returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Simple Ray Serve Examples\n",
    "\n",
    "We'll explore a more detailed example later in this notebook, where we actually serve ML models. Here we explore how deployments are simple with Ray Serve! We will first use a function that does \"scoring,\" sufficient for _stateless_ scenarios, then use a class, which enables _stateful_ scenarios.\n",
    "\n",
    "But first, initialize Ray as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "import requests  # for making web requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.11</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.11', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-08-08_13-33-24_860971_17639/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-08-08_13-33-24_860971_17639/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-08-08_13-33-24_860971_17639', 'metrics_export_port': 52000, 'gcs_address': '127.0.0.1:56681', 'address': '127.0.0.1:56681', 'dashboard_agent_listen_port': 52365, 'node_id': '16c743807f0d7c9853a49f5a543dbd87ddf424518fbbb297dce157b1'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple function that will be served by Ray. As with Ray Tasks, we can decorate this function with `@serve.deployment`, meaning this is going to be\n",
    "deployed on Ray Serve as a function to which we can send Starlette requests.\n",
    "\n",
    "It takes in a `request`, extracts the request parameter with key \"name,\"\n",
    "and returns an echoed string. \n",
    "\n",
    "Simple to illustrate that Ray Serve can also serve Python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Python function deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def hello(request):\n",
    "    name = request.query_params[\"name\"]\n",
    "    return f\"Hello {name}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `<func_name>.bind()` method to deploy it on Ray Serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a Python function for serving by binding it to a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = hello.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run Ray Serve and deploy the model, simply call `serve.run(bound_handle)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=18569)\u001b[0m INFO 2022-08-08 13:33:33,369 controller 18569 http_state.py:123 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-16c743807f0d7c9853a49f5a543dbd87ddf424518fbbb297dce157b1' on node '16c743807f0d7c9853a49f5a543dbd87ddf424518fbbb297dce157b1' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(ServeController pid=18569)\u001b[0m INFO 2022-08-08 13:33:34,816 controller 18569 deployment_state.py:1277 - Adding 1 replicas to deployment 'hello'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO:     Started server process [18578]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='hello')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.run(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send some requests to our Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: Hello request_0!\n",
      " 1: Hello request_1!\n",
      " 2: Hello request_2!\n",
      " 3: Hello request_3!\n",
      " 4: Hello request_4!\n",
      " 5: Hello request_5!\n",
      " 6: Hello request_6!\n",
      " 7: Hello request_7!\n",
      " 8: Hello request_8!\n",
      " 9: Hello request_9!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,888 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 3.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,894 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 2.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,898 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,902 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,907 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,912 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,917 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,887 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,893 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,898 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,902 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,906 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,911 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,916 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,921 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,920 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.get(f\"http://127.0.0.1:8000/hello?name=request_{i}\").text\n",
    "    print(f'{i:2d}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `hello request_N` in the output. \n",
    "\n",
    "Now let's serve another \"model\" in the same Ray Serve instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,929 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 2.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:36,934 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 2.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,928 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=18583)\u001b[0m INFO 2022-08-08 13:33:36,933 hello hello#aYfVqn replica.py:505 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "import starlette\n",
    "from starlette.requests import Request\n",
    "\n",
    "@serve.deployment\n",
    "class SimpleModel:\n",
    "    def __init__(self):\n",
    "        self.weight = 0.5\n",
    "        self.bias = 1\n",
    "        self.prediction = 0.0\n",
    "\n",
    "    def __call__(self, starlette_request):\n",
    "        data = starlette_request.query_params['data']\n",
    "        self.prediction = float(data) * self.weight * random() + self.bias\n",
    "        return {\"prediction\": self.prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=18569)\u001b[0m INFO 2022-08-08 13:33:37,401 controller 18569 deployment_state.py:1277 - Adding 1 replicas to deployment 'SimpleModel'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=18569)\u001b[0m INFO 2022-08-08 13:33:39,373 controller 18569 deployment_state.py:1302 - Removing 1 replicas from deployment 'hello'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='SimpleModel')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model = SimpleModel.bind()\n",
    "serve.run(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Send some requests to our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction  : {\"prediction\": 1.2083630123910705}\n",
      "prediction  : {\"prediction\": 1.0758797678071188}\n",
      "prediction  : {\"prediction\": 1.201087171997287}\n",
      "prediction  : {\"prediction\": 1.125423103835021}\n",
      "prediction  : {\"prediction\": 1.07045986879704}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:42,484 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 3.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:42,488 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=18604)\u001b[0m INFO 2022-08-08 13:33:42,483 SimpleModel SimpleModel#OvBGsm replica.py:505 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=18604)\u001b[0m INFO 2022-08-08 13:33:42,488 SimpleModel SimpleModel#OvBGsm replica.py:505 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=18604)\u001b[0m INFO 2022-08-08 13:33:42,493 SimpleModel SimpleModel#OvBGsm replica.py:505 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:42,494 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 2.9ms\n"
     ]
    }
   ],
   "source": [
    "url = f\"http://127.0.0.1:8000/SimpleModel\"\n",
    "for i in range(5):\n",
    "    print(f\"prediction  : {requests.get(url, params={'data': random()}).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut down Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=18569)\u001b[0m INFO 2022-08-08 13:33:49,694 controller 18569 deployment_state.py:1302 - Removing 2 replicas from deployment 'Predictor'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=18569)\u001b[0m INFO 2022-08-08 13:33:49,697 controller 18569 deployment_state.py:1302 - Removing 2 replicas from deployment 'Predictor_1'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=18569)\u001b[0m INFO 2022-08-08 13:33:49,699 controller 18569 deployment_state.py:1302 - Removing 1 replicas from deployment 'ServeHandleDemo'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18578)\u001b[0m INFO 2022-08-08 13:33:49,625 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 45.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ServeHandleDemo pid=18655)\u001b[0m INFO 2022-08-08 13:33:49,625 ServeHandleDemo ServeHandleDemo#YoxtQo replica.py:505 - HANDLE __call__ OK 41.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Predictor_1 pid=18653)\u001b[0m INFO 2022-08-08 13:33:49,624 Predictor_1 Predictor_1#LnMFmK replica.py:505 - HANDLE predict OK 0.2ms\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Here are some things you can try:\n",
    "\n",
    "1. Write function or class and deploy it. You can modify class `SimpleMode`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "* Try the framework specific tutorials below with Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework-Specific Tutorials\n",
    "\n",
    "Ray Serve seamlessly integrates with popular Python ML libraries. Below are tutorials with some of these frameworks to help get you started.\n",
    "\n",
    " * [PyTorch Tutorial](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial)\n",
    " * [Scikit-Learn Tutorial](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial)\n",
    " * [Keras and Tensorflow Tutorial](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "We will learn how you can use Ray Serve integration with [MLflow](https://mlflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“– [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "âž¡ [Next notebook](./ex_02_ray_serve_fastapi.ipynb) <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
