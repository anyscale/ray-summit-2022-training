{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Serving Challenges\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "ðŸ“– [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "âž¡ [Next notebook](./ex_02_ray_serve_fastapi.ipynb) <br>\n",
    "\n",
    "### Learning Objective:\n",
    "In this introductory tutorial, you will:\n",
    "\n",
    "* Learn about model serving challenges\n",
    "* Understand Ray Architecture\n",
    "* Explore the fundamentals of Deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Challenges of Model Serving\n",
    "\n",
    "Model development happens in a data science research environment. There are many challenges, such as feature engineering, model selection, missing or messy data, yet there are tools at the data scientists' disposal. By contrast, model deployment to production faces an entirely different set of challenges and requires different tools. We must bridge the divide as much as possible.\n",
    "\n",
    "So what are some of the challenges of model serving?\n",
    "\n",
    "<img src=\"images/serve_challenges.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### 1. It Should Be Framework Agnostic\n",
    "\n",
    "First, model serving frameworks must be able to serve models from popular frameworks and libraries like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks, in order to get the best model. \n",
    "\n",
    "Second, machine learning models are typically surrounded by (or work in conjunction with) \n",
    "lots of application or business logic. For example, some model serving is implemented as a RESTful service to which scoring requests are made. Often this is too restrictive, as some additional processing, such as fetching additional data from a online feature store, to augment the request data, may be desired as part of the scoring process, and the performance overhead of remote calls may be suboptimal.\n",
    "\n",
    "### 2. Pure Python or Pythonic\n",
    "\n",
    "In general, model serving should be intuitive for developers and simple to configure and run. Hence, it is desirable to use pure Python and to avoid verbose configurations using YAML files or other means. \n",
    "\n",
    "Data scientists and engineers use Python and Python-based ML frameworks to develop their machine learning models, so they should also be able to use Python to deploy their machine learning applications. This need is growing more critical as online learning applications combine training and serving in the same applications.\n",
    "\n",
    "### 3. Simple and Scalable\n",
    "\n",
    "Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "\n",
    "### 4. DevOps/MLOps Integrations\n",
    "\n",
    "Model serving deployments need to integrate with existing \"DevOps\" CI/CD practices for controlled, audited, and predicatble releases. Patterns like [Canary Deployment](https://martinfowler.com/bliki/CanaryRelease.html) are particularly useful for testing the efficacy of a new model before replacing existing models, just as this pattern is useful for other software deployments.\n",
    "\n",
    "### 5. Flexible Deployment Patterns\n",
    "\n",
    "There are unique deployment patterns, too. For example, it should be easy to deploy a forest of models, to split traffic to different instances, and to score data in batches for greater efficiency.\n",
    "\n",
    "See also this [Ray blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) on the challenges of model serving and the way Ray Serve addresses them. It also provides an example of starting with a simple model, then deploying a more sophisticated model into the running application. Along the same lines, this blog post, [Serving ML Models in Production Common Patterns](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns) discusses how deployment patterns for model serving and how you can use Ray Serve. Additionally, listen to this webinar: [Building a scalable ML model serving API with Ray Serve](https://www.anyscale.com/events/2021/09/09/building-a-scalable-ml-model-serving-api-with-ray-serve). This introductory webinar highlights how Ray Serve makes it easy to deploy, operate and scale a machine learning API.\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ray Serve?\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a scalable, framework-agnostic and Python-first model serving library built on [Ray](https://ray.io).\n",
    "\n",
    "<img src=\"images/ray_serve_overview.png\" width=\"70%\" height=\"40%\"> \n",
    "\n",
    "For users, Ray Serve offers these benefits:\n",
    "\n",
    "* **Framework Agnostic**: You can use the same toolkit to serve everything from deep learning models built with [PyTorch](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial), [Tensorflow](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), or [Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), to [scikit-Learn](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial) models, to arbitrary business logic.\n",
    "* **Python First:** Configure your model serving with pure Python code. No YAML or JSON configurations required.\n",
    "\n",
    "Since Serve is built on Ray, it also allows you to scale to many machines, in your datacenter or in cloud environments, and it allows you to leverage all of the other Ray frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve Architecture and components\n",
    "\n",
    "<img src=\"images/serve-architecture-2.0.png\" height=\"40%\" width=\"70%\">\n",
    "\n",
    "There are three kinds of actors that are created to make up a Serve instance:\n",
    "- **Controller**: A global actor unique to each Serve instance that manages\n",
    "  the control plane. The Controller is responsible for creating, updating, and\n",
    "  destroying other actors. Serve API calls like creating or getting a deployment\n",
    "  make remote calls to the Controller.\n",
    "- **HTTP Proxy**: By default there is one HTTP proxy actor on the head node. This actor runs a [Uvicorn](https://www.uvicorn.org/) HTTP\n",
    "  server that accepts incoming requests, forwards them to replicas, and\n",
    "  responds once they are completed.  For scalability and high availability,\n",
    "  you can also run a proxy on each node in the cluster via the `location` field of [`http_options`](core-apis).\n",
    "- **Replicas**: Actors that actually execute the code in response to a\n",
    "  request. For example, they may contain an instantiation of an ML model. Each\n",
    "  replica processes individual requests from the HTTP proxy (these may be batched\n",
    "  by the replica using `@serve.batch`, see the [batching](https://docs.ray.io/en/latest/serve/ml-models.html#serve-batching) docs).\n",
    "\n",
    "For more details, see this [key concepts](https://docs.ray.io/en/latest/serve/index.html) and [architecture](https://docs.ray.io/en/latest/serve/architecture.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetime of a Request\n",
    "\n",
    "When an HTTP request is sent to the router, the following things happen:\n",
    "\n",
    " * The HTTP request is received and parsed.\n",
    "\n",
    " * The correct deployment associated with the HTTP url path is looked up. The request is placed on a queue.\n",
    "\n",
    " * For each request in a deployment queue, an available replica is looked up and the request is sent to it. If there are no available replicas (there are more than max_concurrent_queries requests outstanding), the request is left in the queue until an outstanding request is finished.\n",
    "\n",
    "Each replica maintains a queue of requests and executes one at a time, possibly using asyncio to process them concurrently. If the handler (the function for the deployment or __call__) is async, the replica will not wait for the handler to run; otherwise, the replica will block until the handler returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Ray Serve Examples\n",
    "\n",
    "__Deployments__ are the basic unit of request handling in Serve. Each Deployment maintains multiple identical _replicas_, and forwards requests at random to each one. Deployments scale by adding more replicas to respond to traffic faster.\n",
    "\n",
    "In this lesson, we will explore the fundamentals of Deployments. All of the other concepts in Serve build on top of deployments, so it is important to understand them well! \n",
    "\n",
    "If this all feels a bit basic, don't worry: we'll build a functional ML Pipeline later in the course, and handle details such as HTTP parsing and model composition. For now, let's see how Ray Serve makes deployments simple! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray imports\n",
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.11</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.11', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-08-15_15-19-09_655661_13297/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-08-15_15-19-09_655661_13297/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-08-15_15-19-09_655661_13297', 'metrics_export_port': 65034, 'gcs_address': '127.0.0.1:64942', 'address': '127.0.0.1:64942', 'dashboard_agent_listen_port': 52365, 'node_id': '105f6f137ba4fd4f09076c3ca0b39146248072d42d59d84969eab624'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start ray\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World: A minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our first deployment. The `@serve.deployment` decorator transforms a python function into a deployment. In this case, the `hello()` function will be invoked when the `/hello` HTTP endpoint is hit.\n",
    "\n",
    "The `hello` deployment just says \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our first deployment. The `@serve.deployment` decorator transforms a python function into a deployment. This  In this case, the `hello()` function will be invoked when the HTTP end\n",
    "\n",
    "The `hello` deployment just says \"Hello World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def hello():\n",
    "    return \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy our model using Ray Serve.\n",
    "\n",
    "First call `hello.bind()` to get a lazy handle to the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_handle = hello.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call `serve.run` to start a server listening on `localhost:8000`, register the `hello` deployment to handle requests sent to the `/hello` endpoint, and to get a live handle to the running deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:19:16,968 controller 13671 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-105f6f137ba4fd4f09076c3ca0b39146248072d42d59d84969eab624' on node '105f6f137ba4fd4f09076c3ca0b39146248072d42d59d84969eab624' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO:     Started server process [13692]\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:19:18,504 controller 13671 deployment_state.py:1232 - Adding 1 replica to deployment 'hello'.\n"
     ]
    }
   ],
   "source": [
    "hello_handle = serve.run(lazy_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run `serve status` from the command line to see the status of the deployment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m 2022-08-15 15:19:22,774\tINFO worker.py:1312 -- Connecting to existing Ray cluster at address: 127.0.0.1:64942...\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m 2022-08-15 15:19:22,784\tINFO worker.py:1481 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_status:\n",
      "  status: RUNNING\n",
      "  message: ''\n",
      "  deployment_timestamp: 0.0\n",
      "deployment_statuses:\n",
      "- name: hello\n",
      "  status: HEALTHY\n",
      "  message: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Help Jupyter find the serve CLI\n",
    "export PATH=\"/home/ray/anaconda3/bin:$PATH\"\n",
    "\n",
    "serve status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to the text below if your deployment worked."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "app_status:\n",
    "  status: RUNNING\n",
    "  message: ''\n",
    "  deployment_timestamp: 0.0\n",
    "deployment_statuses:\n",
    "- name: hello\n",
    "  status: HEALTHY\n",
    "  message: ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 ways to query a deployment with Ray Serve. You can either use the handle or send an HTTP request to the appropriate endpoint. In this case, we need to send a GET request to `http://localhost:8000/hello` to hit our endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query with Ray API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query using Ray handle\n",
    "await hello_handle.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query with the HTTP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the requests library\n",
    "import requests\n",
    "\n",
    "\n",
    "# Ray Serve runs on port 8000 by default\n",
    "SERVE_URL = \"http://localhost:8000\"\n",
    "\n",
    "def http_get(path: str):\n",
    "    \"\"\"Send a GET request with the requests library\"\"\"\n",
    "    return requests.get(SERVE_URL + path).text\n",
    "\n",
    "# Query using HTTP\n",
    "http_get('/hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query with Command Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-08-15 15:19:24--  http://localhost:8000/hello\n",
      "Resolving localhost (localhost)... ::1, 127.0.0.1\n",
      "Connecting to localhost (localhost)|::1|:8000... failed: Connection refused.\n",
      "Connecting to localhost (localhost)|127.0.0.1|:8000... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/plain]\n",
      "Saving to: â€˜STDOUTâ€™\n",
      "\n",
      "     0K                                                         391K=0s\n",
      "\n",
      "2022-08-15 15:19:24 (391 KB/s) - written to stdout [12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget -O - 'http://localhost:8000/hello' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have successfully created, deployed, and queried a minimal model using Ray Serve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fibonacci: Scaling a simple compute intensive task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will demonstrate how Ray Serve makes it easy to scale compute intensive workloads across multiple CPUs. We will use a toy Fibonacci example to show how Serve supports initializing a model at deploy time, and then running compute intensive inferences with that model at request time.\n",
    "\n",
    "**Problem**: Create a deployment that\n",
    "1. At initialization: Accepts a name and records it. Initialize a ComputeIntensiveModel\n",
    "2. At query time: Run the `ComputeIntensiveModel.forward()` helper function, and return the result, along with it's name.\n",
    "3. Handles 3 QPS\n",
    "\n",
    "`ComputeIntensiveModel.forward()` calculates a large fibonacci number (very) inefficiently to simulate running inference on a large ML model. It is CPU bound, and consumes real CPU resources. \n",
    "\n",
    "Most other python serving frameworks optimize QPS by concurrently handling several requests on a single CPU, which works well for IO-bound tasks that have a lot of idle time. This approach doesn't help scale CPU-bound workloads. The only way to scale CPU-bound workloads to high QPS is to use more CPUs.\n",
    "\n",
    "\n",
    "In this example, we show how Ray Serve makes it easy to scale to leverage more compute and deliver higher QPS on compute bound tasks.\n",
    "\n",
    "Let's start by timing `ComputeIntensiveModel.forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 s Â± 61 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from helper import ComputeIntensiveModel\n",
    "\n",
    "model = ComputeIntensiveModel()\n",
    "%timeit model.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each invocation should take around 1.2 seconds to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our `serve.deployment` decorator on a class. The initializer of the class will be called at __deploy__ time, and the `__call__` magic method will be called when the deployment receives a request. \n",
    "\n",
    "Let's initialize our model in the initializer, and then call `model.forward()` function to the `__call__` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import compute_intensive_workload\n",
    "\n",
    "@serve.deployment\n",
    "class Fibonacci:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.model = ComputeIntensiveModel()\n",
    "\n",
    "    def __call__(self):\n",
    "        return f\"Hello from {self.name}! \" + self.model.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy `Fibonacci` with 1 replica. We specify the number of replicas in the `.options()` method on the class. We then pass the constructor arguments, in this case just `name`, to the `.bind()` function to obtain a lazy handle. As before, calling `serve.run` will deploy our model and return a live handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:19:35,096 controller 13671 deployment_state.py:1232 - Adding 1 replica to deployment 'Fibonacci'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:19:38,190 controller 13671 deployment_state.py:1258 - Removing 1 replica from deployment 'hello'.\n"
     ]
    }
   ],
   "source": [
    "lazy_handle = Fibonacci.options(num_replicas=1).bind(name=\"Fibonacci\")\n",
    "fib_handle = serve.run(lazy_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status again to make sure our model is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_status:\n",
      "  status: RUNNING\n",
      "  message: ''\n",
      "  deployment_timestamp: 0.0\n",
      "deployment_statuses:\n",
      "- name: Fibonacci\n",
      "  status: HEALTHY\n",
      "  message: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Help Jupyter find the serve CLI\n",
    "export PATH=\"/home/ray/anaconda3/bin:$PATH\"\n",
    "\n",
    "serve status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the `http_get` helper defined in the previous section to send an HTTP request to our deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello from Fibonacci! The 33nd fibonacci number is 3524578'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "http_get('/Fibonacci')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Measuring QPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a load test against our deployment to measure QPS. \n",
    "\n",
    "We will use the `load_test` function from the helper file, which hits a specified endpoint with a specified number of concurrent requests and returns the measured QPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running load test:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:44,918 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1387.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:44,917 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1381.6ms\n",
      "Running load test:  10%|â–ˆ         | 1/10 [00:01<00:12,  1.40s/it]\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:46,404 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1389.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:46,403 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1386.0ms\n",
      "Running load test:  20%|â–ˆâ–ˆ        | 2/10 [00:02<00:11,  1.45s/it]\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:47,896 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 2880.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:47,895 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1491.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:49,288 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1392.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:50,467 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1178.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:51,697 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1230.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:52,980 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1283.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:54,171 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1190.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:55,365 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1194.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:56,606 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1239.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13806)\u001b[0m INFO 2022-08-15 15:19:57,900 Fibonacci Fibonacci#vorHKc replica.py:482 - HANDLE __call__ OK 1278.3ms\n",
      "Running load test:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:37,  5.36s/it]\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,910 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12894.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,912 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12896.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,913 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12896.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,913 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12896.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,913 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12895.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,913 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12896.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,914 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12896.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:19:57,914 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 12896.2ms\n",
      "Running load test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 10 requests in 12.913265943527222. 0.774 QPS average. 1.291s per request.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from helper import load_test\n",
    "\n",
    "# Run 10 queries in parallel and calculate QPS for 1 replica\n",
    "qps_1 = await load_test('/Fibonacci', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 1 replica, so we are only able to serve a little less than 1 QPS.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part! Let's scale up the Fibonacci model to improve throughput. We want to hit 3 QPS, so we will need roughly 5 replicas. Let's also increase the number of requests sent to get a more accurate measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:19:58,085 controller 13671 deployment_state.py:1232 - Adding 4 replicas to deployment 'Fibonacci'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:20:00,385 controller 13671 deployment_state.py:1189 - Stopping 1 replicas of deployment 'Fibonacci' with outdated versions.\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:20:02,565 controller 13671 deployment_state.py:1232 - Adding 1 replica to deployment 'Fibonacci'.\n",
      "Running load test:   5%|â–Œ         | 1/20 [00:01<00:24,  1.31s/it]\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:06,361 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1296.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:06,368 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1302.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:06,378 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1312.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13901)\u001b[0m INFO 2022-08-15 15:20:06,360 Fibonacci Fibonacci#vIiJpP replica.py:482 - HANDLE __call__ OK 1291.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13898)\u001b[0m INFO 2022-08-15 15:20:06,367 Fibonacci Fibonacci#KOFlvi replica.py:482 - HANDLE __call__ OK 1297.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13899)\u001b[0m INFO 2022-08-15 15:20:06,377 Fibonacci Fibonacci#YcWrek replica.py:482 - HANDLE __call__ OK 1307.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:06,414 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1347.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:06,423 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 1356.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13900)\u001b[0m INFO 2022-08-15 15:20:06,417 Fibonacci Fibonacci#LTSZtn replica.py:482 - HANDLE __call__ OK 1305.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13927)\u001b[0m INFO 2022-08-15 15:20:06,413 Fibonacci Fibonacci#NHWtNw replica.py:482 - HANDLE __call__ OK 1301.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13901)\u001b[0m INFO 2022-08-15 15:20:07,849 Fibonacci Fibonacci#vIiJpP replica.py:482 - HANDLE __call__ OK 1487.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13898)\u001b[0m INFO 2022-08-15 15:20:07,858 Fibonacci Fibonacci#KOFlvi replica.py:482 - HANDLE __call__ OK 1490.2ms\n",
      "Running load test:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:02<00:06,  2.32it/s]\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:07,896 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 2828.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:07,904 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 2837.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13900)\u001b[0m INFO 2022-08-15 15:20:07,894 Fibonacci Fibonacci#LTSZtn replica.py:482 - HANDLE __call__ OK 1477.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13899)\u001b[0m INFO 2022-08-15 15:20:07,871 Fibonacci Fibonacci#YcWrek replica.py:482 - HANDLE __call__ OK 1493.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13927)\u001b[0m INFO 2022-08-15 15:20:07,903 Fibonacci Fibonacci#NHWtNw replica.py:482 - HANDLE __call__ OK 1489.6ms\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m 2022-08-15 15:20:08,468\tINFO (unknown file):0 -- Task failed with unretryable exception: TaskID(42ab2d636f1e3b0feb2abdd22bb58ebbdc02744401000000).\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 709, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 713, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 655, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2243, in ray._raylet.CoreWorker.run_async_func_in_event_loop\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     return self.__get_result()\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     raise self._exception\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/ray/python/ray/util/tracing/tracing_helper.py\", line 498, in _resume_span\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     return await method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/controller.py\", line 185, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     return await (self.long_poll_host.listen_for_change(keys_to_snapshot_ids))\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/_private/long_poll.py\", line 249, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     raise asyncio.TimeoutError(\"Polling request timed out.\")\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m asyncio.exceptions.TimeoutError: Polling request timed out.\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13901)\u001b[0m INFO 2022-08-15 15:20:09,277 Fibonacci Fibonacci#vIiJpP replica.py:482 - HANDLE __call__ OK 1428.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13900)\u001b[0m INFO 2022-08-15 15:20:09,326 Fibonacci Fibonacci#LTSZtn replica.py:482 - HANDLE __call__ OK 1430.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13898)\u001b[0m INFO 2022-08-15 15:20:09,288 Fibonacci Fibonacci#KOFlvi replica.py:482 - HANDLE __call__ OK 1430.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13899)\u001b[0m INFO 2022-08-15 15:20:09,316 Fibonacci Fibonacci#YcWrek replica.py:482 - HANDLE __call__ OK 1444.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13927)\u001b[0m INFO 2022-08-15 15:20:09,333 Fibonacci Fibonacci#NHWtNw replica.py:482 - HANDLE __call__ OK 1428.6ms\n",
      "Running load test:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.36it/s]\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,598 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5531.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,599 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5491.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,599 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5488.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13901)\u001b[0m INFO 2022-08-15 15:20:10,597 Fibonacci Fibonacci#vIiJpP replica.py:482 - HANDLE __call__ OK 1319.9ms\n",
      "Running load test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 20 requests in 5.611516237258911. 3.564 QPS average. 0.281s per request.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scale up\n",
    "num_replicas = 5\n",
    "lazy_handle = Fibonacci.options(num_replicas=num_replicas).bind(name=\"BiggerFasterFibonacci\")\n",
    "serve.run(lazy_handle)\n",
    "\n",
    "# Rerun the load test\n",
    "qps_5 = await load_test('/Fibonacci', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty big improvement! We are just about hitting 3 QPS now. Let's calculate the improvements and make a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,608 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5540.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,608 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5497.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,608 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5497.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,645 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5577.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,645 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5535.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,646 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5534.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,651 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5540.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,652 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5540.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,662 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5551.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=13692)\u001b[0m INFO 2022-08-15 15:20:10,662 http_proxy 127.0.0.1 http_proxy.py:315 - GET / 200 5551.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13900)\u001b[0m INFO 2022-08-15 15:20:10,650 Fibonacci Fibonacci#LTSZtn replica.py:482 - HANDLE __call__ OK 1323.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13898)\u001b[0m INFO 2022-08-15 15:20:10,607 Fibonacci Fibonacci#KOFlvi replica.py:482 - HANDLE __call__ OK 1318.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13899)\u001b[0m INFO 2022-08-15 15:20:10,644 Fibonacci Fibonacci#YcWrek replica.py:482 - HANDLE __call__ OK 1328.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Fibonacci pid=13927)\u001b[0m INFO 2022-08-15 15:20:10,661 Fibonacci Fibonacci#NHWtNw replica.py:482 - HANDLE __call__ OK 1327.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6x improvement from scaling to 5 replicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m 2022-08-15 15:20:11,340\tINFO (unknown file):0 -- Task failed with unretryable exception: TaskID(0372e098b4cc7df8eb2abdd22bb58ebbdc02744401000000).\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 709, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 713, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 655, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2243, in ray._raylet.CoreWorker.run_async_func_in_event_loop\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     return self.__get_result()\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     raise self._exception\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/ray/python/ray/util/tracing/tracing_helper.py\", line 498, in _resume_span\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     return await method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/controller.py\", line 185, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     return await (self.long_poll_host.listen_for_change(keys_to_snapshot_ids))\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/_private/long_poll.py\", line 249, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m     raise asyncio.TimeoutError(\"Polling request timed out.\")\n",
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m asyncio.exceptions.TimeoutError: Polling request timed out.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "improvement = round(qps_5 / qps_1, 2)\n",
    "print(f\"{improvement}x improvement from scaling to {num_replicas} replicas\")\n",
    "\n",
    "# Plot the improvement using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "plt.bar([\"1 replica\", f\"{num_replicas} replicas\"], [qps_1, qps_5])\n",
    "plt.xlabel(\"Number of replicas\")\n",
    "plt.ylabel(\"QPS served\")\n",
    "plt.title(\"QPS vs number of replicas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That's a significant QPS improvement. Thanks to Ray, Serve can scale to hundreds of replicas to handle high QPS applications for compute intensive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut down Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=13671)\u001b[0m INFO 2022-08-15 15:20:11,625 controller 13671 deployment_state.py:1258 - Removing 5 replicas from deployment 'Fibonacci'.\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Here are some exercises to become more familiar with Ray Serve:\n",
    "\n",
    "1. Try different numbers of replicas. Is there a limit to how high the QPS can go? Why?\n",
    "2. What is the overhead of sending a request via HTTP vs using the handle directly? \n",
    "3. Do requests get sent to different replicas? (check the Ray Dashboard)\n",
    "4. Write your own Serve Deployment and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "* Try the tutorials below with Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Framework-Specific Tutorials\n",
    "\n",
    "Ray Serve seamlessly integrates with popular Python ML libraries. Below are tutorials with some of these frameworks to help get you started.\n",
    "\n",
    " * [PyTorch Tutorial](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial)\n",
    " * [Scikit-Learn Tutorial](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial)\n",
    " * [Keras and Tensorflow Tutorial](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial)\n",
    " * [Ray Serve MLflow Deployment Plugin](https://github.com/ray-project/mlflow-ray-serve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "We will learn how you can use Ray Serve integration with [MLflow](https://mlflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“– [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "âž¡ [Next notebook](./ex_02_ray_serve_fastapi.ipynb) <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
