{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Serving Challenges\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "ðŸ“– [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "âž¡ [Next notebook](./ex_02_ray_serve_mlflow.ipynb) <br>\n",
    "\n",
    "### Learning Objective:\n",
    "In this introductory tutorial, you will:\n",
    "\n",
    "* learn about model serving challenges\n",
    "* understand the why Ray Serve and its concepts, components and architecture\n",
    "* utilize Ray Serve APIs to create and serve deployments\n",
    "* access deployments using two different methods\n",
    "* learn how to scale deployments using replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Challenges of Model Serving\n",
    "\n",
    "Model development happens in a data science research environment. There are many challenges, such as feature engineering, model selection, missing or messy data, yet there are tools at the data scientists' disposal. By contrast, model deployment to production faces an entirely different set of challenges and requires different tools. We must bridge the divide as much as possible.\n",
    "\n",
    "So what are some of the challenges of model serving?\n",
    "\n",
    "<img src=\"images/serve_challenges.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### 1. It Should Be Framework Agnostic\n",
    "\n",
    "First, model serving frameworks must be able to serve models from popular frameworks and libraries like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks, in order to get the best model. \n",
    "\n",
    "Second, machine learning models are typically surrounded by (or work in conjunction with) \n",
    "lots of application or business logic. For example, some model serving is implemented as a RESTful service to which scoring requests are made. Often this is too restrictive, as some additional processing, such as fetching additional data from a online feature store, to augment the request data, may be desired as part of the scoring process, and the performance overhead of remote calls may be suboptimal.\n",
    "\n",
    "### 2. Pure Python or Pythonic\n",
    "\n",
    "In general, model serving should be intuitive for developers and simple to configure and run. Hence, it is desirable to use pure Python and to avoid verbose configurations using YAML files or other means. \n",
    "\n",
    "Data scientists and engineers use Python and Python-based ML frameworks to develop their machine learning models, so they should also be able to use Python to deploy their machine learning applications. This need is growing more critical as online learning applications combine training and serving in the same applications.\n",
    "\n",
    "### 3. Simple and Scalable\n",
    "\n",
    "Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "\n",
    "### 4. DevOps/MLOps Integrations\n",
    "\n",
    "Model serving deployments need to integrate with existing \"DevOps\" CI/CD practices for controlled, audited, and predicatble releases. Patterns like [Canary Deployment](https://martinfowler.com/bliki/CanaryRelease.html) are particularly useful for testing the efficacy of a new model before replacing existing models, just as this pattern is useful for other software deployments.\n",
    "\n",
    "### 5. Flexible Deployment Patterns\n",
    "\n",
    "There are unique deployment patterns, too. For example, it should be easy to deploy a forest of models, to split traffic to different instances, and to score data in batches for greater efficiency.\n",
    "\n",
    "See also this [Ray blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) on the challenges of model serving and the way Ray Serve addresses them. It also provides an example of starting with a simple model, then deploying a more sophisticated model into the running application. Along the same lines, this blog post, [Serving ML Models in Production Common Patterns](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns) discusses how deployment patterns for model serving and how you can use Ray Serve. Additionally, listen to this webinar: [Building a scalable ML model serving API with Ray Serve](https://www.anyscale.com/events/2021/09/09/building-a-scalable-ml-model-serving-api-with-ray-serve). This introductory webinar highlights how Ray Serve makes it easy to deploy, operate and scale a machine learning API.\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ray Serve?\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a scalable, framework-agnostic and Python-first model serving library built on [Ray](https://ray.io).\n",
    "\n",
    "<img src=\"images/ray_serve_overview.png\" width=\"70%\" height=\"40%\"> \n",
    "\n",
    "For users, Ray Serve offers these benefits:\n",
    "\n",
    "* **Framework Agnostic**: You can use the same toolkit to serve everything from deep learning models built with [PyTorch](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial), [Tensorflow](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), or [Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), to [scikit-Learn](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial) models, to arbitrary business logic.\n",
    "* **Python First:** Configure your model serving with pure Python code. No YAML or JSON configurations required.\n",
    "\n",
    "Since Serve is built on Ray, it also allows you to scale to many machines, in your datacenter or in cloud environments, and it allows you to leverage all of the other Ray frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve Architecture and components\n",
    "\n",
    "<img src=\"images/architecture.png\" height=\"40%\" width=\"70%\">\n",
    "\n",
    "There are three kinds of actors that are created to make up a Serve instance:\n",
    "\n",
    "**Controller**: A global actor unique to each Serve instance that manages the control plane. The Controller is responsible for creating, updating, and destroying other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "**Router**: There is one router per node. Each router is a Uvicorn HTTP server that accepts incoming requests, forwards them to replicas, and responds once they are completed.\n",
    "\n",
    "**Worker Replica**: Worker replicas actually execute the code in response to a request. For example, they may contain an instantiation of an ML model. Each replica processes individual requests from the routers (they may be batched by the replica using `@serve.batch`, see the [batching docs](https://docs.ray.io/en/latest/serve/ml-models.html#serve-batching)).\n",
    "\n",
    "<img src=\"images/request_flow.png\" height=\"50%\" width=\"75%\">\n",
    "\n",
    "For more details, see this [key concepts](https://docs.ray.io/en/latest/serve/index.html) and [architecture](https://docs.ray.io/en/latest/serve/architecture.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetime of a Request\n",
    "\n",
    "When an HTTP request is sent to the router, the following things happen:\n",
    "\n",
    " * The HTTP request is received and parsed.\n",
    "\n",
    " * The correct deployment associated with the HTTP url path is looked up. The request is placed on a queue.\n",
    "\n",
    " * For each request in a deployment queue, an available replica is looked up and the request is sent to it. If there are no available replicas (there are more than max_concurrent_queries requests outstanding), the request is left in the queue until an outstanding request is finished.\n",
    "\n",
    "Each replica maintains a queue of requests and executes one at a time, possibly using asyncio to process them concurrently. If the handler (the function for the deployment or __call__) is async, the replica will not wait for the handler to run; otherwise, the replica will block until the handler returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Simple Ray Serve Examples\n",
    "\n",
    "We'll explore a more detailed example later in this notebook, where we actually serve ML models. Here we explore how deployments are simple with Ray Serve! We will first use a function that does \"scoring,\" sufficient for _stateless_ scenarios, then use a class, which enables _stateful_ scenarios.\n",
    "\n",
    "<img src=\"images/func_class_deployment.png\" width=\"80%\" height=\"50%\">\n",
    "\n",
    "But first, initialize Ray as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "import requests  # for making web requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0rc0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.8.13', ray_version='2.0.0rc0', ray_commit='a0588094ec52b45a878f59e98258cd5e90f4ec36', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-08-05_20-26-51_570341_29784/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-08-05_20-26-51_570341_29784/sockets/raylet', 'webui_url': '127.0.0.1:8266', 'session_dir': '/tmp/ray/session_2022-08-05_20-26-51_570341_29784', 'metrics_export_port': 64642, 'gcs_address': '127.0.0.1:64308', 'address': '127.0.0.1:64308', 'dashboard_agent_listen_port': 52365, 'node_id': 'd5d37d9a470e2fb2b980c6dec0747d81483471a4ba6f52fb1de45718'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize Ray Serve itself. Note that we did not have to start a Ray cluster explicity. If one is not running `serve.start()` will automatically launch a Ray cluster, otherwise it'll connect to an exisisting instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:00,838 controller 29932 http_state.py:123 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:NBHGiv:SERVE_PROXY_ACTOR-d5d37d9a470e2fb2b980c6dec0747d81483471a4ba6f52fb1de45718' on node 'd5d37d9a470e2fb2b980c6dec0747d81483471a4ba6f52fb1de45718' listening on '127.0.0.1:8000'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve._private.client.ServeControllerClient at 0x11e616fd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO:     Started server process [29934]\n"
     ]
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define our stateless function for processing requests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple function that will be served by Ray. As with Ray Tasks, we can decorate this function with `@serve.deployment`, meaning this is going to be\n",
    "deployed on Ray Serve as function to which we can send Starlette requests.\n",
    "\n",
    "It takes in a `request`, extracts the request parameter with key \"name,\"\n",
    "and returns an echoed string. \n",
    "\n",
    "Simple to illustrate that Ray Serve can also serve Python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Python function deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def hello(request):\n",
    "    name = request.query_params[\"name\"]\n",
    "    return f\"Hello {name}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `<func_name>.deploy()` method to deploy it on Ray Serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a Python function for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:04,501 controller 29932 deployment_state.py:1232 - Adding 1 replicas to deployment 'hello'.\n"
     ]
    }
   ],
   "source": [
    "hello.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send some requests to our Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: Hello request_0!\n",
      " 1: Hello request_1!\n",
      " 2: Hello request_2!\n",
      " 3: Hello request_3!\n",
      " 4: Hello request_4!\n",
      " 5: Hello request_5!\n",
      " 6: Hello request_6!\n",
      " 7: Hello request_7!\n",
      " 8: Hello request_8!\n",
      " 9: Hello request_9!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,137 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 4.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,146 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 6.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,157 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 6.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,160 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 1.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,164 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 1.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,167 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 1.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,171 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 1.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,136 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,141 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,155 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,160 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,163 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,167 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,170 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,173 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,173 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 1.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,178 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 1.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:06,181 http_proxy 127.0.0.1 http_proxy.py:315 - GET /hello 200 1.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,178 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:hello pid=29936)\u001b[0m INFO 2022-08-05 20:27:06,181 hello hello#xGVZOh replica.py:482 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.get(f\"http://127.0.0.1:8000/hello?name=request_{i}\").text\n",
    "    print(f'{i:2d}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `hello request_N` in the output. \n",
    "\n",
    "Now let's serve another \"model\" in the same Ray Serve instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import starlette\n",
    "from starlette.requests import Request\n",
    "\n",
    "@serve.deployment\n",
    "class SimpleModel:\n",
    "    def __init__(self):\n",
    "        self.weight = 0.5\n",
    "        self.bias = 1\n",
    "        self.prediction = 0.0\n",
    "\n",
    "    def __call__(self, starlette_request):\n",
    "        data = starlette_request.query_params['data']\n",
    "        self.prediction = float(data) * self.weight * random() + self.bias\n",
    "        return {\"prediction\": self.prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:08,332 controller 29932 deployment_state.py:1232 - Adding 1 replicas to deployment 'SimpleModel'.\n"
     ]
    }
   ],
   "source": [
    "SimpleModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Send some requests to our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction  : {\"prediction\": 1.2492219367040882}\n",
      "prediction  : {\"prediction\": 1.0216153948312}\n",
      "prediction  : {\"prediction\": 1.0299163807604585}\n",
      "prediction  : {\"prediction\": 1.0428100425301339}\n",
      "prediction  : {\"prediction\": 1.2859704013650775}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:09,642 http_proxy 127.0.0.1 http_proxy.py:315 - GET /SimpleModel 200 3.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:09,646 http_proxy 127.0.0.1 http_proxy.py:315 - GET /SimpleModel 200 1.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:09,649 http_proxy 127.0.0.1 http_proxy.py:315 - GET /SimpleModel 200 1.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:09,653 http_proxy 127.0.0.1 http_proxy.py:315 - GET /SimpleModel 200 1.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:09,656 http_proxy 127.0.0.1 http_proxy.py:315 - GET /SimpleModel 200 1.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=29939)\u001b[0m INFO 2022-08-05 20:27:09,641 SimpleModel SimpleModel#TGQttZ replica.py:482 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=29939)\u001b[0m INFO 2022-08-05 20:27:09,645 SimpleModel SimpleModel#TGQttZ replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=29939)\u001b[0m INFO 2022-08-05 20:27:09,649 SimpleModel SimpleModel#TGQttZ replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=29939)\u001b[0m INFO 2022-08-05 20:27:09,652 SimpleModel SimpleModel#TGQttZ replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SimpleModel pid=29939)\u001b[0m INFO 2022-08-05 20:27:09,655 SimpleModel SimpleModel#TGQttZ replica.py:482 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "url = f\"http://127.0.0.1:8000/SimpleModel\"\n",
    "for i in range(5):\n",
    "    print(f\"prediction  : {requests.get(url, params={'data': random()}).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': Deployment(name=hello,version=None,route_prefix=/hello),\n",
       " 'SimpleModel': Deployment(name=SimpleModel,version=None,route_prefix=/SimpleModel)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.list_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to deploy ML models and query them via two methods:\n",
    " 1. **ServeHandle API** gives you control and a pythonic interface to your deployments\n",
    " 2. **HTTP** offers an HTTP client and web interface to access your deployments. This could be suitable for web application sending an HTTP request to your model deployment \n",
    " <img src=\"images/func_class_deployment_2.png\" width=\"80%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example model stored in a pickled format at an accessible path in the cloud storage or model registry\n",
    "that can be reloaded and deserialized into a model instance. Once deployed\n",
    "in Ray Serve, we can use it for prediction. The prediction is a fake condition,\n",
    "based on threshold of weight greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def predict(self, data):\n",
    "        return random() + data if data > 0.5 else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@serve.deployment\n",
    "class Deployment:\n",
    "    # Take in a path to load your desired model\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.path = path\n",
    "        self.model = Model(path)\n",
    "        # Get the pid on which this deployment is running on\n",
    "        self.pid = os.getpid()\n",
    "\n",
    "    # Deployments are callable. Here we simply return a prediction from\n",
    "    # our request\n",
    "    def __call__(self, starlette_request) -> str:\n",
    "        # Request came via an HTTP\n",
    "        if isinstance(starlette_request, starlette.requests.Request):\n",
    "            data = starlette_request.query_params['data']\n",
    "        else:\n",
    "            # Request came via a ServerHandle API method call.\n",
    "            data = starlette_request\n",
    "        pred = self.model.predict(float(data))\n",
    "        return f\"(pid: {self.pid}); path: {self.path}; data: {float(data):.3f}; prediction: {pred:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two distinct deployments of the same class as two replicas. \n",
    "Associate each deployment with a unique 'name'. This name can be used to fetch its respective ServeHandle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:24,135 controller 29932 deployment_state.py:1232 - Adding 1 replicas to deployment 'rep-1'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:25,172 controller 29932 deployment_state.py:1232 - Adding 1 replicas to deployment 'rep-2'.\n"
     ]
    }
   ],
   "source": [
    "Deployment.options(name=\"rep-1\", num_replicas=1).deploy(\"/model/rep-1.pkl\")\n",
    "Deployment.options(name=\"rep-2\", num_replicas=1).deploy(\"/model/rep-2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List deployments again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': Deployment(name=hello,version=None,route_prefix=/hello), 'SimpleModel': Deployment(name=SimpleModel,version=None,route_prefix=/SimpleModel), 'rep-1': Deployment(name=rep-1,version=None,route_prefix=/rep-1), 'rep-2': Deployment(name=rep-2,version=None,route_prefix=/rep-2)}\n"
     ]
    }
   ],
   "source": [
    "print(serve.list_deployments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Access each deployment using the ServeHandle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are retrieving a sync handle inside an asyncio loop. Try getting Deployment.get_handle(.., sync=False) to get better performance. Learn more at https://docs.ray.io/en/latest/serve/handle-guide.html#sync-and-async-handles\n",
      "You are retrieving a sync handle inside an asyncio loop. Try getting Deployment.get_handle(.., sync=False) to get better performance. Learn more at https://docs.ray.io/en/latest/serve/handle-guide.html#sync-and-async-handles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle name : rep-1\n",
      "prediction  : (pid: 29945); path: /model/rep-1.pkl; data: 0.170; prediction: 0.170\n",
      "--\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 29947); path: /model/rep-2.pkl; data: 0.664; prediction: 0.956\n",
      "--\n",
      "handle name : rep-1\n",
      "prediction  : (pid: 29945); path: /model/rep-1.pkl; data: 0.769; prediction: 1.061\n",
      "--\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 29947); path: /model/rep-2.pkl; data: 0.566; prediction: 1.535\n",
      "--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:rep-1 pid=29945)\u001b[0m INFO 2022-08-05 20:27:26,562 rep-1 rep-1#ZEFtCs replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:rep-1 pid=29945)\u001b[0m INFO 2022-08-05 20:27:26,573 rep-1 rep-1#ZEFtCs replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:rep-2 pid=29947)\u001b[0m INFO 2022-08-05 20:27:26,570 rep-2 rep-2#tklbvF replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:rep-2 pid=29947)\u001b[0m INFO 2022-08-05 20:27:26,576 rep-2 rep-2#tklbvF replica.py:482 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for d_name in [\"rep-1\", \"rep-2\"]:\n",
    "        # Get handle to the each deployment and invoke its method.\n",
    "        # Which replica the request is dispatched to is determined\n",
    "        # by the Router actor.\n",
    "        handle = serve.get_deployment(d_name).get_handle()\n",
    "        print(f\"handle name : {d_name}\")\n",
    "        print(f\"prediction  : {ray.get(handle.remote(random()))}\")\n",
    "        print(\"-\" * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Access deployment via HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle name : rep-1\n",
      "prediction  : (pid: 29945); path: /model/rep-1.pkl; data: 0.791; prediction: 1.760\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 29947); path: /model/rep-2.pkl; data: 0.195; prediction: 0.195\n",
      "handle name : rep-1\n",
      "prediction  : (pid: 29945); path: /model/rep-1.pkl; data: 0.865; prediction: 1.116\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 29947); path: /model/rep-2.pkl; data: 0.112; prediction: 0.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:28,562 http_proxy 127.0.0.1 http_proxy.py:315 - GET /rep-1 200 5.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:28,567 http_proxy 127.0.0.1 http_proxy.py:315 - GET /rep-2 200 2.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:28,570 http_proxy 127.0.0.1 http_proxy.py:315 - GET /rep-1 200 1.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29934)\u001b[0m INFO 2022-08-05 20:27:28,573 http_proxy 127.0.0.1 http_proxy.py:315 - GET /rep-2 200 1.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:rep-1 pid=29945)\u001b[0m INFO 2022-08-05 20:27:28,562 rep-1 rep-1#ZEFtCs replica.py:482 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:rep-1 pid=29945)\u001b[0m INFO 2022-08-05 20:27:28,570 rep-1 rep-1#ZEFtCs replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:rep-2 pid=29947)\u001b[0m INFO 2022-08-05 20:27:28,567 rep-2 rep-2#tklbvF replica.py:482 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:rep-2 pid=29947)\u001b[0m INFO 2022-08-05 20:27:28,573 rep-2 rep-2#tklbvF replica.py:482 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for d_name in [\"rep-1\", \"rep-2\"]:\n",
    "        # Send HTTP request along with data payload\n",
    "        url = f\"http://127.0.0.1:8000/{d_name}\"\n",
    "        print(f\"handle name : {d_name}\")\n",
    "        print(f\"prediction  : {requests.get(url, params={'data': random()}).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut down Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:30,376 controller 29932 deployment_state.py:1257 - Removing 1 replicas from deployment 'hello'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:30,379 controller 29932 deployment_state.py:1257 - Removing 1 replicas from deployment 'SimpleModel'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:30,384 controller 29932 deployment_state.py:1257 - Removing 1 replicas from deployment 'rep-1'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=29932)\u001b[0m INFO 2022-08-05 20:27:30,385 controller 29932 deployment_state.py:1257 - Removing 1 replicas from deployment 'rep-2'.\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Here are some things you can try:\n",
    "\n",
    "1. Increase the number of replicas. For each of Method 1 and Method 2, send ten requests\n",
    "2. Do requests get sent to different replicas? (check the pids or the Ray Dashboard)\n",
    "3. Write function or class and deploy it. You can modify class `Deployment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "* Try the tutorials below with Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework-Specific Tutorials\n",
    "\n",
    "Ray Serve seamlessly integrates with popular Python ML libraries. Below are tutorials with some of these frameworks to help get you started.\n",
    "\n",
    " * [PyTorch Tutorial](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial)\n",
    " * [Scikit-Learn Tutorial](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial)\n",
    " * [Keras and Tensorflow Tutorial](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial)\n",
    " * [Ray Serve MLflow Deployment Plugin](https://github.com/ray-project/mlflow-ray-serve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "We will learn how you can use Ray Serve integration with [MLflow](https://mlflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“– [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "âž¡ [Next notebook](./ex_02_ray_serve_mlflow.ipynb) <br>\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7870826f35578f78988c006074c778c7fb98b01979d0d06d7a011367199c39c2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
