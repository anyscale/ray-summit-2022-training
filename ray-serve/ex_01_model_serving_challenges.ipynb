{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Serving Challenges\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "\n",
    "### Learning Objective:\n",
    "In this introductory tutorial, you will:\n",
    "\n",
    "* learn about model serving challenges\n",
    "* understand the why Ray Serve and its concepts, components and architecture\n",
    "* utilize Ray Serve APIs to create and serve deployments\n",
    "* access deployments using two different methods\n",
    "* learn how to scale deployments using replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Challenges of Model Serving\n",
    "\n",
    "Model development happens in a data science research environment. There are many challenges, such as feature engineering, model selection, missing or messy data, yet there are tools at the data scientists' disposal. By contrast, model deployment to production faces an entirely different set of challenges and requires different tools. We must bridge the divide as much as possible.\n",
    "\n",
    "So what are some of the challenges of model serving?\n",
    "\n",
    "<img src=\"images/serve_challenges.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### 1. It Should Be Framework Agnostic\n",
    "\n",
    "First, model serving frameworks must be able to serve models from popular frameworks and libraries like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks, in order to get the best model. \n",
    "\n",
    "Second, machine learning models are typically surrounded by (or work in conjunction with) \n",
    "lots of application or business logic. For example, some model serving is implemented as a RESTful service to which scoring requests are made. Often this is too restrictive, as some additional processing, such as fetching additional data from a online feature store, to augment the request data, may be desired as part of the scoring process, and the performance overhead of remote calls may be suboptimal.\n",
    "\n",
    "### 2. Pure Python or Pythonic\n",
    "\n",
    "In general, model serving should be intuitive for developers and simple to configure and run. Hence, it is desirable to use pure Python and to avoid verbose configurations using YAML files or other means. \n",
    "\n",
    "Data scientists and engineers use Python and Python-based ML frameworks to develop their machine learning models, so they should also be able to use Python to deploy their machine learning applications. This need is growing more critical as online learning applications combine training and serving in the same applications.\n",
    "\n",
    "### 3. Simple and Scalable\n",
    "\n",
    "Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "\n",
    "### 4. DevOps/MLOps Integrations\n",
    "\n",
    "Model serving deployments need to integrate with existing \"DevOps\" CI/CD practices for controlled, audited, and predicatble releases. Patterns like [Canary Deployment](https://martinfowler.com/bliki/CanaryRelease.html) are particularly useful for testing the efficacy of a new model before replacing existing models, just as this pattern is useful for other software deployments.\n",
    "\n",
    "### 5. Flexible Deployment Patterns\n",
    "\n",
    "There are unique deployment patterns, too. For example, it should be easy to deploy a forest of models, to split traffic to different instances, and to score data in batches for greater efficiency.\n",
    "\n",
    "See also this [Ray blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) on the challenges of model serving and the way Ray Serve addresses them. It also provides an example of starting with a simple model, then deploying a more sophisticated model into the running application. Along the same lines, this blog post, [Serving ML Models in Production Common Patterns](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns) discusses how deployment patterns for model serving and how you can use Ray Serve. Additionally, listen to this webinar: [Building a scalable ML model serving API with Ray Serve](https://www.anyscale.com/events/2021/09/09/building-a-scalable-ml-model-serving-api-with-ray-serve). This introductory webinar highlights how Ray Serve makes it easy to deploy, operate and scale a machine learning API.\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ray Serve?\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a scalable, framework-agnostic and Python-first model serving library built on [Ray](https://ray.io).\n",
    "\n",
    "<img src=\"images/ray_serve_overview.png\" width=\"70%\" height=\"40%\"> \n",
    "\n",
    "For users, Ray Serve offers these benefits:\n",
    "\n",
    "* **Framework Agnostic**: You can use the same toolkit to serve everything from deep learning models built with [PyTorch](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial), [Tensorflow](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), or [Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), to [scikit-Learn](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial) models, to arbitrary business logic.\n",
    "* **Python First:** Configure your model serving with pure Python code. No YAML or JSON configurations required.\n",
    "\n",
    "Since Serve is built on Ray, it also allows you to scale to many machines, in your datacenter or in cloud environments, and it allows you to leverage all of the other Ray frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve Architecture and components\n",
    "\n",
    "<img src=\"images/architecture.png\" height=\"40%\" width=\"70%\">\n",
    "\n",
    "There are three kinds of actors that are created to make up a Serve instance:\n",
    "\n",
    "**Controller**: A global actor unique to each Serve instance that manages the control plane. The Controller is responsible for creating, updating, and destroying other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "**Router**: There is one router per node. Each router is a Uvicorn HTTP server that accepts incoming requests, forwards them to replicas, and responds once they are completed.\n",
    "\n",
    "**Worker Replica**: Worker replicas actually execute the code in response to a request. For example, they may contain an instantiation of an ML model. Each replica processes individual requests from the routers (they may be batched by the replica using `@serve.batch`, see the [batching docs](https://docs.ray.io/en/latest/serve/ml-models.html#serve-batching)).\n",
    "\n",
    "<img src=\"images/request_flow.png\" height=\"50%\" width=\"75%\">\n",
    "\n",
    "For more details, see this [key concepts](https://docs.ray.io/en/latest/serve/index.html) and [architecture](https://docs.ray.io/en/latest/serve/architecture.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetime of a Request\n",
    "\n",
    "When an HTTP request is sent to the router, the following things happen:\n",
    "\n",
    " * The HTTP request is received and parsed.\n",
    "\n",
    " * The correct deployment associated with the HTTP url path is looked up. The request is placed on a queue.\n",
    "\n",
    " * For each request in a deployment queue, an available replica is looked up and the request is sent to it. If there are no available replicas (there are more than max_concurrent_queries requests outstanding), the request is left in the queue until an outstanding request is finished.\n",
    "\n",
    "Each replica maintains a queue of requests and executes one at a time, possibly using asyncio to process them concurrently. If the handler (the function for the deployment or __call__) is async, the replica will not wait for the handler to run; otherwise, the replica will block until the handler returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Simple Ray Serve Examples\n",
    "\n",
    "We'll explore a more detailed example later in this notebook, where we actually serve ML models. Here we explore how deployments are simple with Ray Serve! We will first use a function that does \"scoring,\" sufficient for _stateless_ scenarios, then use a class, which enables _stateful_ scenarios.\n",
    "\n",
    "<img src=\"images/func_class_deployment.png\" width=\"80%\" height=\"50%\">\n",
    "\n",
    "But first, initialize Ray as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "import requests  # for making web requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.13', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-20_14-52-14_809086_28305/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-20_14-52-14_809086_28305/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-06-20_14-52-14_809086_28305', 'metrics_export_port': 62957, 'gcs_address': '127.0.0.1:62439', 'address': '127.0.0.1:62439', 'node_id': '9b0f3c213bac500fa54297efc63f8cab356609b196cf944b2601daa6'})\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ctx = ray.init(logging_level=logging.ERROR)\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard url: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dashboard url: http://{ctx.address_info['webui_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize Ray Serve itself. Note that we did not have to start a Ray cluster explicity. If one is not running `serve.start()` will automatically launch a Ray cluster, otherwise it'll connect to an exisisting instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:52:24,173 controller 28347 checkpoint_path.py:17 - Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:52:24,279 controller 28347 http_state.py:112 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:PDZCHs:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.client.ServeControllerClient at 0x7fee109baf40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO:     Started server process [28349]\n"
     ]
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define our stateless function for processing requests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple function that will be served by Ray. As with Ray Tasks, we can decorate this function with `@serve.deployment`, meaning this is going to be\n",
    "deployed on Ray Serve as function to which we can send Starlette requests.\n",
    "\n",
    "It takes in a `request`, extracts the request parameter with key \"name,\"\n",
    "and returns an echoed string. \n",
    "\n",
    "Simple to illustrate that Ray Serve can also serve Python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Python function deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def hello(request):\n",
    "    name = request.query_params[\"name\"]\n",
    "    return f\"Hello {name}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `<func_name>.deploy()` method to deploy it on Ray Serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a Python function for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:52:33,093 controller 28347 deployment_state.py:1216 - Adding 1 replicas to deployment 'hello'.\n"
     ]
    }
   ],
   "source": [
    "hello.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send some requests to our Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: Hello request_0!\n",
      " 1: Hello request_1!\n",
      " 2: Hello request_2!\n",
      " 3: Hello request_3!\n",
      " 4: Hello request_4!\n",
      " 5: Hello request_5!\n",
      " 6: Hello request_6!\n",
      " 7: Hello request_7!\n",
      " 8: Hello request_8!\n",
      " 9: Hello request_9!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,801 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 4.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,806 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 2.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,812 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 2.4ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,799 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,805 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,811 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,851 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 22.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,859 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 4.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,867 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 3.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,874 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 4.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,881 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 3.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,887 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 2.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:36,892 http_proxy 127.0.0.1 http_proxy.py:310 - GET /hello 200 2.3ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,849 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,857 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,865 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,873 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,879 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,886 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(hello pid=28352)\u001b[0m INFO 2022-06-20 14:52:36,891 hello hello#DpfdVK replica.py:478 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.get(f\"http://127.0.0.1:8000/hello?name=request_{i}\").text\n",
    "    print(f'{i:2d}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `hello request_N` in the output. \n",
    "\n",
    "Now let's serve another \"model\" in the same Ray Serve instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import starlette\n",
    "from starlette.requests import Request\n",
    "\n",
    "@serve.deployment\n",
    "class SimpleModel:\n",
    "    def __init__(self):\n",
    "        self.weight = 0.5\n",
    "        self.bias = 1\n",
    "        self.prediction = 0.0\n",
    "\n",
    "    def __call__(self, starlette_request):\n",
    "        data = starlette_request.query_params['data']\n",
    "        self.prediction = float(data) * self.weight * random() + self.bias\n",
    "        return {\"prediction\": self.prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:52:42,794 controller 28347 deployment_state.py:1216 - Adding 1 replicas to deployment 'SimpleModel'.\n"
     ]
    }
   ],
   "source": [
    "SimpleModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Send some requests to our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction  : {\n",
      "  \"prediction\": 1.2726174462051227\n",
      "}\n",
      "prediction  : {\n",
      "  \"prediction\": 1.2260916252403744\n",
      "}\n",
      "prediction  : {\n",
      "  \"prediction\": 1.0306495817785275\n",
      "}\n",
      "prediction  : {\n",
      "  \"prediction\": 1.2936652816738028\n",
      "}\n",
      "prediction  : {\n",
      "  \"prediction\": 1.1603943621278465\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:47,469 http_proxy 127.0.0.1 http_proxy.py:310 - GET /SimpleModel 200 3.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:47,475 http_proxy 127.0.0.1 http_proxy.py:310 - GET /SimpleModel 200 2.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:47,480 http_proxy 127.0.0.1 http_proxy.py:310 - GET /SimpleModel 200 2.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:47,485 http_proxy 127.0.0.1 http_proxy.py:310 - GET /SimpleModel 200 2.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:52:47,491 http_proxy 127.0.0.1 http_proxy.py:310 - GET /SimpleModel 200 2.5ms\n",
      "\u001b[2m\u001b[36m(SimpleModel pid=28417)\u001b[0m INFO 2022-06-20 14:52:47,468 SimpleModel SimpleModel#WtYzTj replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(SimpleModel pid=28417)\u001b[0m INFO 2022-06-20 14:52:47,474 SimpleModel SimpleModel#WtYzTj replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(SimpleModel pid=28417)\u001b[0m INFO 2022-06-20 14:52:47,479 SimpleModel SimpleModel#WtYzTj replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(SimpleModel pid=28417)\u001b[0m INFO 2022-06-20 14:52:47,484 SimpleModel SimpleModel#WtYzTj replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(SimpleModel pid=28417)\u001b[0m INFO 2022-06-20 14:52:47,490 SimpleModel SimpleModel#WtYzTj replica.py:478 - HANDLE __call__ OK 0.2ms\n"
     ]
    }
   ],
   "source": [
    "url = f\"http://127.0.0.1:8000/SimpleModel\"\n",
    "for i in range(5):\n",
    "    print(f\"prediction  : {requests.get(url, params={'data': random()}).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': Deployment(name=hello,version=None,route_prefix=/hello),\n",
       " 'SimpleModel': Deployment(name=SimpleModel,version=None,route_prefix=/SimpleModel)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.list_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to deploy ML models and query them via two methods:\n",
    " 1. **ServeHandle API** gives you control and a pythonic interface to your deployments\n",
    " 2. **HTTP** offers an HTTP client and web interface to access your deployments. This could be suitable for web application sending an HTTP request to your model deployment \n",
    " <img src=\"images/func_class_deployment_2.png\" width=\"80%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example model stored in a pickled format at an accessible path in the cloud storage or model registry\n",
    "that can be reloaded and deserialized into a model instance. Once deployed\n",
    "in Ray Serve, we can use it for prediction. The prediction is a fake condition,\n",
    "based on threshold of weight greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def predict(self, data):\n",
    "        return random() + data if data > 0.5 else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@serve.deployment\n",
    "class Deployment:\n",
    "    # Take in a path to load your desired model\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.path = path\n",
    "        self.model = Model(path)\n",
    "        # Get the pid on which this deployment is running on\n",
    "        self.pid = os.getpid()\n",
    "\n",
    "    # Deployments are callable. Here we simply return a prediction from\n",
    "    # our request\n",
    "    def __call__(self, starlette_request) -> str:\n",
    "        # Request came via an HTTP\n",
    "        if isinstance(starlette_request, starlette.requests.Request):\n",
    "            data = starlette_request.query_params['data']\n",
    "        else:\n",
    "            # Request came via a ServerHandle API method call.\n",
    "            data = starlette_request\n",
    "        pred = self.model.predict(float(data))\n",
    "        return f\"(pid: {self.pid}); path: {self.path}; data: {float(data):.3f}; prediction: {pred:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two distinct deployments of the same class as two replicas. \n",
    "Associate each deployment with a unique 'name'. This name can be used to fetch its respective ServeHandle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:53:31,198 controller 28347 deployment_state.py:1216 - Adding 1 replicas to deployment 'rep-1'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:53:33,230 controller 28347 deployment_state.py:1216 - Adding 1 replicas to deployment 'rep-2'.\n"
     ]
    }
   ],
   "source": [
    "Deployment.options(name=\"rep-1\", num_replicas=1).deploy(\"/model/rep-1.pkl\")\n",
    "Deployment.options(name=\"rep-2\", num_replicas=1).deploy(\"/model/rep-2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List deployments again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': Deployment(name=hello,version=None,route_prefix=/hello), 'SimpleModel': Deployment(name=SimpleModel,version=None,route_prefix=/SimpleModel), 'rep-1': Deployment(name=rep-1,version=None,route_prefix=/rep-1), 'rep-2': Deployment(name=rep-2,version=None,route_prefix=/rep-2)}\n"
     ]
    }
   ],
   "source": [
    "print(serve.list_deployments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Access each deployment using the ServeHandle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle name : rep-1\n",
      "prediction  : (pid: 28447); path: /model/rep-1.pkl; data: 0.344; prediction: 0.344\n",
      "--\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 28449); path: /model/rep-2.pkl; data: 0.243; prediction: 0.243\n",
      "--\n",
      "handle name : rep-1\n",
      "prediction  : (pid: 28447); path: /model/rep-1.pkl; data: 0.451; prediction: 0.451\n",
      "--\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 28449); path: /model/rep-2.pkl; data: 0.797; prediction: 1.245\n",
      "--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(rep-1 pid=28447)\u001b[0m INFO 2022-06-20 14:54:17,983 rep-1 rep-1#mJjaYv replica.py:478 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(rep-1 pid=28447)\u001b[0m INFO 2022-06-20 14:54:17,994 rep-1 rep-1#mJjaYv replica.py:478 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(rep-2 pid=28449)\u001b[0m INFO 2022-06-20 14:54:17,989 rep-2 rep-2#IOPcwX replica.py:478 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(rep-2 pid=28449)\u001b[0m INFO 2022-06-20 14:54:18,001 rep-2 rep-2#IOPcwX replica.py:478 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for d_name in [\"rep-1\", \"rep-2\"]:\n",
    "        # Get handle to the each deployment and invoke its method.\n",
    "        # Which replica the request is dispatched to is determined\n",
    "        # by the Router actor.\n",
    "        handle = serve.get_deployment(d_name).get_handle()\n",
    "        print(f\"handle name : {d_name}\")\n",
    "        print(f\"prediction  : {ray.get(handle.remote(random()))}\")\n",
    "        print(\"-\" * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Access deployment via HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle name : rep-1\n",
      "prediction  : (pid: 28447); path: /model/rep-1.pkl; data: 0.704; prediction: 1.152\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 28449); path: /model/rep-2.pkl; data: 0.097; prediction: 0.097\n",
      "handle name : rep-1\n",
      "prediction  : (pid: 28447); path: /model/rep-1.pkl; data: 0.592; prediction: 0.676\n",
      "handle name : rep-2\n",
      "prediction  : (pid: 28449); path: /model/rep-2.pkl; data: 0.049; prediction: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:54:24,120 http_proxy 127.0.0.1 http_proxy.py:310 - GET /rep-1 200 3.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:54:24,127 http_proxy 127.0.0.1 http_proxy.py:310 - GET /rep-2 200 3.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:54:24,133 http_proxy 127.0.0.1 http_proxy.py:310 - GET /rep-1 200 2.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28349)\u001b[0m INFO 2022-06-20 14:54:24,139 http_proxy 127.0.0.1 http_proxy.py:310 - GET /rep-2 200 3.2ms\n",
      "\u001b[2m\u001b[36m(rep-1 pid=28447)\u001b[0m INFO 2022-06-20 14:54:24,119 rep-1 rep-1#mJjaYv replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(rep-1 pid=28447)\u001b[0m INFO 2022-06-20 14:54:24,132 rep-1 rep-1#mJjaYv replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(rep-2 pid=28449)\u001b[0m INFO 2022-06-20 14:54:24,126 rep-2 rep-2#IOPcwX replica.py:478 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(rep-2 pid=28449)\u001b[0m INFO 2022-06-20 14:54:24,138 rep-2 rep-2#IOPcwX replica.py:478 - HANDLE __call__ OK 0.2ms\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for d_name in [\"rep-1\", \"rep-2\"]:\n",
    "        # Send HTTP request along with data payload\n",
    "        url = f\"http://127.0.0.1:8000/{d_name}\"\n",
    "        print(f\"handle name : {d_name}\")\n",
    "        print(f\"prediction  : {requests.get(url, params={'data': random()}).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut down Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:54:31,696 controller 28347 deployment_state.py:1240 - Removing 1 replicas from deployment 'hello'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:54:31,700 controller 28347 deployment_state.py:1240 - Removing 1 replicas from deployment 'SimpleModel'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:54:31,703 controller 28347 deployment_state.py:1240 - Removing 1 replicas from deployment 'rep-1'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=28347)\u001b[0m INFO 2022-06-20 14:54:31,706 controller 28347 deployment_state.py:1240 - Removing 1 replicas from deployment 'rep-2'.\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Here are some things you can try:\n",
    "\n",
    "1. Increase the number of replicas. For each of Method 1 and Method 2, send ten requests\n",
    "2. Do requests get sent to different replicas? (check the pids or the Ray Dashboard)\n",
    "3. Write function or class and deploy it. You can modify class `Deployment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "* Try the tutorials below with Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework-Specific Tutorials\n",
    "\n",
    "Ray Serve seamlessly integrates with popular Python ML libraries. Below are tutorials with some of these frameworks to help get you started.\n",
    "\n",
    " * [PyTorch Tutorial](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial)\n",
    " * [Scikit-Learn Tutorial](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial)\n",
    " * [Keras and Tensorflow Tutorial](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial)\n",
    " * [Ray Serve MLflow Deployment Plugin](https://github.com/ray-project/mlflow-ray-serve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "We will learn how you can use Ray Serve integration with [MLflow](https://mlflow.org/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7870826f35578f78988c006074c778c7fb98b01979d0d06d7a011367199c39c2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
