{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6962854",
   "metadata": {},
   "source": [
    "# Ray AIR Demonstration: End to End ML from Training to Serving using PyTorch\n",
    "\n",
    "Adapted from https://docs.ray.io/en/master/ray-air/examples/torch_image_example.html\n",
    "\n",
    "Ray Serve is part of Ray AI Runtime (AIR). Now you have learned Ray Serve, we can't wait to show you the bigger picture of where Serve fits in the end to end ML lifecycle.\n",
    "\n",
    "Ray AI Runtime (AIR) is a scalable and unified toolkit for ML applications. AIR enables simple scaling of individual workloads, end-to-end workflows, and popular ecosystem frameworks, all in just Python.\n",
    "\n",
    "![air-layering](https://docs.ray.io/en/master/_images/ray-air.svg)\n",
    "\n",
    "AIR builds on Ray’s best-in-class libraries for Preprocessing, Training, Tuning, Scoring, Serving, and Reinforcement Learning to bring together an ecosystem of integrations.\n",
    "\n",
    "\n",
    "Ray AIR aims to simplify the ecosystem of machine learning frameworks, platforms, and tools. It does this by leveraging Ray to provide a seamless, unified, and open experience for scalable ML:\n",
    "\n",
    "![air-integration](https://docs.ray.io/en/master/_images/why-air-2.svg)\n",
    "\n",
    "\n",
    "1. **Seamless Dev to Prod**: AIR reduces friction going from development to production. With Ray and AIR, the same Python code scales seamlessly from a laptop to a large cluster.\n",
    "\n",
    "2. **Unified ML API**: AIR’s unified ML API enables swapping between popular frameworks, such as XGBoost, PyTorch, and HuggingFace, with just a single class change in your code.\n",
    "\n",
    "3. **Open and Extensible**: AIR and Ray are fully open-source and can run on any cluster, cloud, or Kubernetes. Build custom components and integrations on top of scalable developer APIs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial demonstrates how to train an image classifier using the [Ray AI Runtime](air) (AIR), then perform batch scoring as well as online serving. \n",
    "\n",
    "You should be familiar with [PyTorch](https://pytorch.org/) before starting the tutorial. If you need a refresher, read PyTorch's [training a classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) tutorial.\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "* Install the [Ray AI Runtime](air). You'll need Ray 1.13 later to run this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d806ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'ray[air]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d588ce2",
   "metadata": {},
   "source": [
    "* Install `requests`, `torch`, `torchvision`, `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a70a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ec14f",
   "metadata": {},
   "source": [
    "## Load and normalize CIFAR-10\n",
    "\n",
    "We'll train our classifier on a popular image dataset called [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "First, let's load CIFAR-10 into a Ray Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f2e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xmo/miniforge3/envs/rc1/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-08-16 01:00:13,565\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m.\n",
      "2022-08-16 01:00:14,508\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "\u001b[2m\u001b[36m(_get_read_tasks pid=28284)\u001b[0m 2022-08-16 01:00:14,506\tWARNING torch_datasource.py:55 -- `SimpleTorchDatasource` doesn't support parallel reads. The `parallelism` argument will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_execute_read_task pid=28284)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 01:00:19,978\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "\u001b[2m\u001b[36m(_get_read_tasks pid=28284)\u001b[0m 2022-08-16 01:00:19,976\tWARNING torch_datasource.py:55 -- `SimpleTorchDatasource` doesn't support parallel reads. The `parallelism` argument will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_execute_read_task pid=28284)\u001b[0m Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.data.datasource import SimpleTorchDatasource\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "def train_dataset_factory():\n",
    "    return torchvision.datasets.CIFAR10(root=\"./data\", download=True, train=True, transform=transform)\n",
    "\n",
    "def test_dataset_factory():\n",
    "    return torchvision.datasets.CIFAR10(root=\"./data\", download=True, train=False, transform=transform)\n",
    "\n",
    "train_dataset: ray.data.Dataset = ray.data.read_datasource(SimpleTorchDatasource(), dataset_factory=train_dataset_factory)\n",
    "test_dataset: ray.data.Dataset = ray.data.read_datasource(SimpleTorchDatasource(), dataset_factory=test_dataset_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e7db56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=1, num_rows=50000, schema=<class 'tuple'>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b59e8",
   "metadata": {},
   "source": [
    "Note that {py:class}`SimpleTorchDatasource <ray.data.datasource.SimpleTorchDatasource>` loads all data into memory, so you shouldn't use it with larger datasets.\n",
    "\n",
    "Next, let's represent our data using pandas dataframes instead of tuples. This lets us call methods like {py:meth}`Dataset.iter_torch_batches <ray.data.Dataset.iter_torch_batches>` later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c485ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches:   0%|                                                | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=28284)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.22s/it]\n",
      "Read->Map_Batches:   0%|                                                | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=28284)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|████████████████████████████████████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from ray.data.extensions import TensorArray\n",
    "import torch\n",
    "\n",
    "\n",
    "def convert_batch_to_pandas(batch: Tuple[torch.Tensor, int]) -> pd.DataFrame:\n",
    "    images = TensorArray([image.numpy() for image, _ in batch])\n",
    "    labels = [label for _, label in batch]\n",
    "\n",
    "    df = pd.DataFrame({\"image\": images, \"label\": labels})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map_batches(convert_batch_to_pandas)\n",
    "test_dataset = test_dataset.map_batches(convert_batch_to_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b416bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=1, num_rows=50000, schema={image: TensorDtype(shape=(3, 32, 32), dtype=float32), label: int64})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc4086",
   "metadata": {},
   "source": [
    "## Train a convolutional neural network\n",
    "\n",
    "Now that we've created our datasets, let's define the training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8b129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d85c9d",
   "metadata": {},
   "source": [
    "We define our training logic in a function called `train_loop_per_worker`.\n",
    "\n",
    "`train_loop_per_worker` contains regular PyTorch code with a few notable exceptions:\n",
    "* We wrap our model with {py:func}`train.torch.prepare_model <ray.train.torch.prepare_model>`.\n",
    "* We call {py:func}`session.get_dataset_shard <ray.air.session.get_dataset_shard>` and {py:meth}`Dataset.iter_torch_batches <ray.data.Dataset.iter_torch_batches>` to convert a subset of our training data to a Torch dataset.\n",
    "* We save model state using {py:func}`session.report <ray.air.session.report>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d32d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train\n",
    "from ray.air import session, Checkpoint\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    model = train.torch.prepare_model(Net())\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    train_dataset_shard = session.get_dataset_shard(\"train\").iter_torch_batches(\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    for epoch in range(2):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataset_shard):\n",
    "            # get the inputs and labels\n",
    "            inputs, labels = data[\"image\"], data[\"label\"]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        session.report(\n",
    "            dict(running_loss=running_loss),\n",
    "            checkpoint=Checkpoint.from_dict(dict(model=model.module.state_dict())),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58100f87",
   "metadata": {},
   "source": [
    "Finally, we can train our model. This should take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a51244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-16 01:01:06 (running for 00:00:23.14)<br>Memory usage on this node: 25.1/64.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/10 CPUs, 0/0 GPUs, 0.0/36.25 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/xmo/ray_results/TorchTrainer_2022-08-16_01-00-42<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  running_loss</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _time_this_iter_s</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_86ba4_00000</td><td>TERMINATED</td><td>127.0.0.1:28383</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         21.3122</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">  1660636865</td><td style=\"text-align: right;\">          0.0527029</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m [W ProcessGroupGloo.cpp:715] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m 2022-08-16 01:00:45,483\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m [W ProcessGroupGloo.cpp:715] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m 2022-08-16 01:00:46,550\tINFO train_loop_utils.py:300 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m 2022-08-16 01:00:46,550\tINFO train_loop_utils.py:347 -- Wrapping provided model in DDP.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m /Users/xmo/miniforge3/envs/rc1/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py:122: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m   return torch.as_tensor(ndarray, dtype=dtype, device=device)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m /Users/xmo/miniforge3/envs/rc1/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py:122: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m   return torch.as_tensor(ndarray, dtype=dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m [1,  2000] loss: 2.206\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m [1,  2000] loss: 2.211\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m [1,  4000] loss: 1.857\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m [1,  4000] loss: 1.889\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m [1,  6000] loss: 1.663\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m [1,  6000] loss: 1.652\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m [1,  8000] loss: 1.576\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m [1,  8000] loss: 1.557\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m [1, 10000] loss: 1.486\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m [1, 10000] loss: 1.531\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28387)\u001b[0m [1, 12000] loss: 1.435\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=28388)\u001b[0m [1, 12000] loss: 1.442\n",
      "Result for TorchTrainer_86ba4_00000:\n",
      "  _time_this_iter_s: 18.94872808456421\n",
      "  _timestamp: 1660636865\n",
      "  _training_iteration: 1\n",
      "  date: 2022-08-16_01-01-05\n",
      "  done: false\n",
      "  experiment_id: de85b3b22d89446ea767befc5abf6b21\n",
      "  hostname: Simons-MacBook-Pro-16\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 28383\n",
      "  running_loss: 669.5595621913671\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 21.260777950286865\n",
      "  time_this_iter_s: 21.260777950286865\n",
      "  time_total_s: 21.260777950286865\n",
      "  timestamp: 1660636865\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 86ba4_00000\n",
      "  warmup_time: 0.0023081302642822266\n",
      "  \n",
      "Result for TorchTrainer_86ba4_00000:\n",
      "  _time_this_iter_s: 0.052702903747558594\n",
      "  _timestamp: 1660636865\n",
      "  _training_iteration: 2\n",
      "  date: 2022-08-16_01-01-05\n",
      "  done: true\n",
      "  experiment_id: de85b3b22d89446ea767befc5abf6b21\n",
      "  experiment_tag: '0'\n",
      "  hostname: Simons-MacBook-Pro-16\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 28383\n",
      "  running_loss: 0.0\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 21.3122239112854\n",
      "  time_this_iter_s: 0.051445960998535156\n",
      "  time_total_s: 21.3122239112854\n",
      "  timestamp: 1660636865\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 86ba4_00000\n",
      "  warmup_time: 0.0023081302642822266\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 01:01:06,164\tINFO tune.py:758 -- Total run time: 23.27 seconds (23.13 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"batch_size\": 2},\n",
    "    datasets={\"train\": train_dataset},\n",
    "    scaling_config=ScalingConfig(num_workers=2)\n",
    ")\n",
    "result = trainer.fit()\n",
    "latest_checkpoint = result.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4faa9",
   "metadata": {},
   "source": [
    "To scale your training script, create a [Ray Cluster](cluster-index) and increase the number of workers. If your cluster contains GPUs, add `\"use_gpu\": True` to your scaling config.\n",
    "\n",
    "```{code-block} python\n",
    "scaling_config=ScalingConfig(num_workers=8, \"use_gpu=True)\n",
    "```\n",
    "\n",
    "## Test the network on the test data\n",
    "\n",
    "Let's see how our model performs.\n",
    "\n",
    "To classify images in the test dataset, we'll need to create a {py:class}`Predictor <ray.train.predictor.Predictor>`.\n",
    "\n",
    "{py:class}`Predictors <ray.train.predictor.Predictor>` load data from checkpoints and efficiently perform inference. In contrast to {py:class}`TorchPredictor <ray.train.torch.TorchPredictor>`, which performs inference on a single batch, {py:class}`BatchPredictor <ray.train.batch_predictor.BatchPredictor>` performs inference on an entire dataset. Because we want to classify all of the images in the test dataset, we'll use a {py:class}`BatchPredictor <ray.train.batch_predictor.BatchPredictor>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990ec534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Map Progress (1 actors 1 pending):   0%|                                | 0/1 [00:00<?, ?it/s]\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m /Users/xmo/miniforge3/envs/rc1/lib/python3.8/site-packages/ray/air/util/data_batch_conversion.py:158: SettingWithCopyWarning: \n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m   df.loc[:, col_name] = TensorArray(col)\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m /Users/xmo/miniforge3/envs/rc1/lib/python3.8/site-packages/ray/air/util/data_batch_conversion.py:158: SettingWithCopyWarning: \n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m   df.loc[:, col_name] = TensorArray(col)\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m /Users/xmo/miniforge3/envs/rc1/lib/python3.8/site-packages/ray/air/util/data_batch_conversion.py:158: SettingWithCopyWarning: \n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\u001b[2m\u001b[36m(BlockWorker pid=28406)\u001b[0m   df.loc[:, col_name] = TensorArray(col)\n",
      "Map Progress (1 actors 1 pending): 100%|████████████████████████| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "predict_dataset = test_dataset.drop_columns(cols=[\"label\"])\n",
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=latest_checkpoint,\n",
    "    predictor_cls=TorchPredictor,\n",
    "    model=Net(),\n",
    ")\n",
    "\n",
    "outputs: ray.data.Dataset = batch_predictor.predict(\n",
    "    data=test_dataset, dtype=torch.float, feature_columns=[\"image\"], keep_columns=[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20fd044",
   "metadata": {},
   "source": [
    "Our model outputs a list of energies for each class. To classify an image, we\n",
    "choose the class that has the highest energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c8a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 35.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': array([-1.5615891 , -1.8779886 ,  0.80665046,  2.2954726 ,  0.05051345,\n",
      "        1.0009389 ,  1.3282954 , -1.025135  , -1.2868532 , -1.8736922 ],\n",
      "      dtype=float32), 'label': 3, 'prediction': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_logits_to_classes(df):\n",
    "    best_class = df[\"predictions\"].map(lambda x: x.argmax())\n",
    "    df[\"prediction\"] = best_class\n",
    "    return df\n",
    "\n",
    "predictions = outputs.map_batches(\n",
    "    convert_logits_to_classes, batch_format=\"pandas\"\n",
    ")\n",
    "\n",
    "predictions.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973efc6",
   "metadata": {},
   "source": [
    "Now that we've classified all of the images, let's figure out which images were\n",
    "classified correctly. The ``predictions`` dataset contains predicted labels and \n",
    "the ``test_dataset`` contains the true labels. To determine whether an image \n",
    "was classified correctly, we join the two datasets and check if the predicted \n",
    "labels are the same as the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e6233ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 52.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 3, 'label': 3, 'correct': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_prediction_scores(df):\n",
    "    df[\"correct\"] = df[\"prediction\"] == df[\"label\"]\n",
    "    return df[[\"prediction\", \"label\", \"correct\"]]\n",
    "\n",
    "scores = predictions.map_batches(calculate_prediction_scores)\n",
    "\n",
    "scores.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d401d91",
   "metadata": {},
   "source": [
    "To compute our test accuracy, we'll count how many images the model classified \n",
    "correctly and divide that number by the total number of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29b2e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 108.38it/s]\n",
      "Shuffle Reduce: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 174.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4944"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sum(on=\"correct\") / scores.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c84e2c",
   "metadata": {},
   "source": [
    "## Deploy the network and make a prediction\n",
    "\n",
    "Our model seems to perform decently, so let's deploy the model to an \n",
    "endpoint. This'll allow us to make predictions over the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2faaf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=28454)\u001b[0m INFO 2022-08-16 01:01:20,420 controller 28454 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-149bc79c679e815f8878f39ebb27086bc31a30b1b3796b7cef49280c' on node '149bc79c679e815f8878f39ebb27086bc31a30b1b3796b7cef49280c' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(ServeController pid=28454)\u001b[0m INFO 2022-08-16 01:01:21,039 controller 28454 deployment_state.py:1232 - Adding 1 replicas to deployment 'PredictorDeployment'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28469)\u001b[0m INFO:     Started server process [28469]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='PredictorDeployment')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import NdArray\n",
    "\n",
    "\n",
    "def json_to_numpy(payload: NdArray) -> pd.DataFrame:\n",
    "    \"\"\"Accepts an NdArray JSON from an HTTP body and converts it to a Numpy Array.\"\"\"\n",
    "    # Have to explicitly convert to float since np.array reads as a double.\n",
    "    arr = np.array(payload.array, dtype=np.float32)\n",
    "    return arr\n",
    "\n",
    "\n",
    "serve.run(\n",
    "    PredictorDeployment.bind(\n",
    "        TorchPredictor,\n",
    "        latest_checkpoint,\n",
    "        batching_params=False,\n",
    "        model=Net(),\n",
    "        http_adapter=json_to_numpy,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90327e8a",
   "metadata": {},
   "source": [
    "Let's classify a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40da3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = test_dataset.take(1)\n",
    "array = np.expand_dims(np.array(batch[0][\"image\"]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25e5d563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 32, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d94b7",
   "metadata": {},
   "source": [
    "You can perform inference against a deployed model by posting a dictionary with an `\"array\"` key. To learn more about the default input schema, read the {py:class}`NdArray <ray.serve.http_adapters.NdArray>` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45bd85d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.5615893602371216,\n",
       "  -1.8779891729354858,\n",
       "  0.8066505193710327,\n",
       "  2.295473337173462,\n",
       "  0.05051347613334656,\n",
       "  1.0009385347366333,\n",
       "  1.3282963037490845,\n",
       "  -1.0251355171203613,\n",
       "  -1.286853551864624,\n",
       "  -1.8736921548843384]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=28469)\u001b[0m INFO 2022-08-16 01:01:26,301 http_proxy 127.0.0.1 http_proxy.py:315 - POST / 200 13.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:PredictorDeployment pid=28474)\u001b[0m INFO 2022-08-16 01:01:26,300 PredictorDeployment PredictorDeployment#GtFTdH replica.py:482 - HANDLE __call__ OK 10.1ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\"array\": array.tolist()}\n",
    "response = requests.post(\"http://localhost:8000/\", json=payload)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0caf4-9ffd-4afa-9de7-4457efec112f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "270f2d85f4357bdfaf8bd07e0bf53472a3d1c8c66509b0720fe6bb0939d92e56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
