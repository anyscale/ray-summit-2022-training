{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0e73d7-e9ab-4bae-88d7-58ece7ae3563",
   "metadata": {},
   "source": [
    "# Ray Tune - An end-to-end example of using XGBoost with Ray Tune and Ray Serve\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "This example illustrates how you can use Ray Libraries for an end-to-end example.\n",
    "\n",
    "![](images/xgboost_tune_serve.png)\n",
    "\n",
    "1. Use XGBoost to train a baseline model, using default hyperparameters\n",
    "2. Use XGBoost to train another model, using \"guessed\" hyperparemeters\n",
    "3. Use Tune to HPO and train the best XGBoost model\n",
    "4. Use ASHAscheduler to use early-stopping\n",
    "5. Save the best trial model\n",
    "6. Fetch the best saved model\n",
    "7. Run some predictions\n",
    "8. Create deployment and deploy the best trained model to Ray Serve\n",
    "9. Send request for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf75060-cb6e-4baf-b389-4595800ac0c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png\" width=\"40%\" height=\"20%\" aligh=\"center\">\n",
    "\n",
    "XGBoost is currently one of the most popular machine learning algorithms for regression and classification. It performs very well on a large selection of tasks, and is the key to success in many Kaggle competitions.\n",
    "\n",
    "Derived mainly from [documentaton](https://docs.ray.io/en/latest/tune/tutorials/tune-xgboost.html), this tutorial will give you a quick introduction to XGBoost, show you how to train an XGBoost model, and then guide you on how to optimize XGBoost parameters using Ray Tune to get the best performance. In particular, we will cover the following:\n",
    "\n",
    " * What is XGBoost\n",
    " * Training a simple XGBoost classifier\n",
    " * XGBoost Hyperparameters\n",
    " * Tuning the configuration parameters\n",
    " * Early stopping\n",
    " * Conclusion\n",
    " * Further References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e9230-25bd-4ae2-aa80-412e03264a59",
   "metadata": {},
   "source": [
    "### What is XGBoost\n",
    "XGBoost is an acronym for eXtreme Gradient Boosting. Internally, XGBoost uses decision trees. Instead of training just one large decision tree, XGBoost and other related algorithms train many small decision trees. The intuition behind this is that even though single decision trees can be inaccurate and suffer from high variance, combining the output of a large number of these weak learners can actually lead to a strong learner, resulting in better predictions and less variance.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune-xgboost-ensemble.svg\" width=\"70%\" height=\"50%\"> \n",
    "\n",
    "A single decision tree (left) might be able to get to an accuracy of 70% for a binary classification task. By combining the output of several small decision trees, an ensemble learner (right) might end up with a higher accuracy of 90%.¶\n",
    "\n",
    "Boosting algorithms start with a single small decision tree and evaluate how well it predicts the given examples. When building the next tree, those samples that have been misclassified before have a higher chance of being used to generate the tree. This is useful because it avoids overfitting to samples that can be easily classified and instead tries to come up with models that are able to classify hard examples, too. Please [see here](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205) for a more thorough introduction to bagging and boosting algorithms.\n",
    "\n",
    "There are many boosting algorithms. In their core, they are all very similar. XGBoost uses second-level derivatives to find splits that maximize the **gain** (the inverse of the **loss**) - hence the name. In practice, there really is no drawback in using XGBoost over other boosting algorithms - in fact, it usually shows the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee9882-088c-4b8a-bad7-2935a54cb4ee",
   "metadata": {},
   "source": [
    "### Training a simple XGBoost classifier\n",
    "\n",
    "Let’s first see how a simple XGBoost classifier can be trained. We’ll use the `breast_cancer` dataset included in the sklearn dataset collection. This is a `binary classification` dataset. Given 30 different input features, our task is to learn to identify subjects with breast cancer and those without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d79db-147d-4ef1-a5d9-fd24f50a2fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2a938b-92b9-458d-99be-d0cd9841fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "PIMA_INDIAN_DATA_FILE=os.path.join(os.getcwd(), \"data/pima-indians-diabetes.data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8548e8e0-8f3f-421d-b196-0231f4cda2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/anyscale-academy/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef767f-579e-4921-98f1-222cc050c25a",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4603825-af7d-4aa5-a15d-6eadf25931cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to load the data\n",
    "def get_data():\n",
    "    dataset = loadtxt(PIMA_INDIAN_DATA_FILE, delimiter=\",\")\n",
    "    # split data into X and y\n",
    "    X = dataset[:, 0:8]\n",
    "    y = dataset[:, 8]\n",
    "    # Split into train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae2490-ca85-4fb4-b69f-8273e799bad1",
   "metadata": {},
   "source": [
    "### Step 1: Train the baseline model\n",
    "\n",
    "Let's define our standard or regular XGBoost trainer (function). It takes in XGBoost configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7235317f-8db8-492f-8ed3-65c8b96faf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Load dataset\n",
    "\n",
    "    train_x, test_x, train_y, test_y = get_data()\n",
    "    \n",
    "    # Build input DMatrices for XGBoost\n",
    "    train_set = xgb.DMatrix(train_x, label=train_y)\n",
    "    test_set  = xgb.DMatrix(test_x, label=test_y)\n",
    "    \n",
    "    # Train the classifier\n",
    "    results = {}\n",
    "    bst = xgb.train(\n",
    "        config,\n",
    "        train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        evals_result=results,\n",
    "        verbose_eval=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6be96-a6fc-46a3-b728-31deedbfc3ce",
   "metadata": {},
   "source": [
    "Define our basic minimal and default configurations for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76b6b17-39d5-43dc-8b22-f712ebe81936",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8b8b0-891a-477f-a82b-dcb5aa1aa9a3",
   "metadata": {},
   "source": [
    "### Train the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57313cb5-ce21-4ebd-b58d-712b3a2967f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.60491\teval-error:0.28347\n",
      "[1]\teval-logloss:0.55934\teval-error:0.25984\n",
      "[2]\teval-logloss:0.53068\teval-error:0.25591\n",
      "[3]\teval-logloss:0.51795\teval-error:0.24803\n",
      "[4]\teval-logloss:0.51153\teval-error:0.24409\n",
      "[5]\teval-logloss:0.50935\teval-error:0.24803\n",
      "[6]\teval-logloss:0.50818\teval-error:0.25591\n",
      "[7]\teval-logloss:0.51097\teval-error:0.24803\n",
      "[8]\teval-logloss:0.51760\teval-error:0.25591\n",
      "[9]\teval-logloss:0.51912\teval-error:0.24409\n",
      "Accuracy: 0.7559\n"
     ]
    }
   ],
   "source": [
    "results = train_model(config=configs)\n",
    "accuracy = 1. - results[\"eval\"][\"error\"][-1]\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4289e-bb0b-48e7-8333-7f58c72033b9",
   "metadata": {},
   "source": [
    "As you can see, the code is quite simple. First, the dataset is loaded and split into a test and train set. The XGBoost model is trained with `xgb.train()`. XGBoost automatically evaluates metrics we specified on the test set. In our case it calculates the `logloss` and the prediction error, which is the percentage of misclassified examples. To calculate the accuracy, we just have to subtract the error from 1.0. In this simple example, most runs result in an accuracy of about 0.75.\n",
    "\n",
    "What if you want further accuracy, or want to use XGBoost's additional parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4c90c-d5b1-4c6b-b7c4-bc62de1b887c",
   "metadata": {},
   "source": [
    "### XGBoost Hyperparameters\n",
    "\n",
    "Even with the default settings, XGBoost was able to get to a good accuracy on the breast cancer dataset. However, as in many machine learning algorithms, there are many knobs to tune which might lead to even better performance. Let’s explore some of them below.\n",
    "\n",
    "#### Maximum tree depth\n",
    "Remember that XGBoost internally uses many decision tree models to come up with predictions. When training a decision tree, we need to tell the algorithm how large the tree may get. The parameter for this is called the tree depth.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune-xgboost-depth.svg\" width=\"30%\" height=\"10%\">\n",
    "\n",
    "In this image, the left tree has a depth of 2, and the right tree a depth of 3. Note that with each level, 2^(𝑑−1) splits are added, where d is the depth of the tree.¶\n",
    "\n",
    "Tree depth is a property that concerns the model complexity. If you only allow short trees, the models are likely not very precise - they underfit the data. If you allow very large trees, the single models are likely to overfit to the data. In practice, a number between 2 and 6 is often a good starting point for this parameter.\n",
    "\n",
    "XGBoost’s default value is 3.\n",
    "\n",
    "#### Minimum child weight\n",
    "When a decision tree creates new leaves, it splits up the remaining data at one node into two groups. If there are only few samples in one of these groups, it often doesn’t make sense to split it further. One of the reasons for this is that the model is harder to train when we have fewer samples.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune-xgboost-weight.svg\" width=\"20%\" height=\"10%\">\n",
    "\n",
    "In this example, we start with 100 examples. At the first node, they are split into 4 and 96 samples, respectively. In the next step, our model might find that it doesn’t make sense to split the 4 examples more. It thus only continues to add leaves on the right side.\n",
    "\n",
    "The parameter used by the model to decide if it makes sense to split a node is called the minimum child weight. In the case of linear regression, this is just the absolute number of nodes requried in each child. In other objectives, this value is determined using the weights of the examples, hence the name.\n",
    "\n",
    "The larger the value, the more constrained the trees are and the less deep they will be. This parameter thus also affects the model complexity. Values can range between 0 and infinity and are dependent on the sample size. For our ca. 500 examples in the breast cancer dataset, values between 0 and 10 should be sensible.\n",
    "\n",
    "XGBoost’s default value is 1.\n",
    "\n",
    "#### Subsample size\n",
    "Each decision tree we add is trained on a subsample of the total training dataset. The probabilities for the samples are weighted according to the XGBoost algorithm, but we can decide on which fraction of the samples we want to train each decision tree on.\n",
    "\n",
    "Setting this value to 0.7 would mean that we randomly sample 70% of the training dataset before each training iteration.\n",
    "\n",
    "XGBoost’s default value is 1.\n",
    "\n",
    "#### Learning rate / Eta\n",
    "Remember that XGBoost sequentially trains many decision trees, and that later trees are more likely trained on data that has been misclassified by prior trees. In effect this means that earlier trees make decisions for easy samples (i.e. those samples that can easily be classified) and later trees make decisions for harder samples. It is then sensible to assume that the later trees are less accurate than earlier trees.\n",
    "\n",
    "To address this fact, XGBoost uses a parameter called Eta, which is sometimes called the learning rate. Don’t confuse this with learning rates from gradient descent!\n",
    "\n",
    "Typical values for this parameter are between `0.01 and 0.3`.\n",
    "\n",
    "XGBoost’s default value is 0.3.\n",
    "\n",
    "#### Number of boost rounds\n",
    "Lastly, we can decide on how many boosting rounds we perform, which means how many decision trees we ultimately train. When we do heavy subsampling or use small learning rate, it might make sense to increase the number of boosting rounds.\n",
    "\n",
    "XGBoost’s default value is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3c84c-61fb-44e9-92ab-e91b1dfdf83f",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "Let’s see how this looks like in code! We just need to adjust our config dict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2cb8f-287b-4833-bb00-29597502c55d",
   "metadata": {},
   "source": [
    "### Step 2: Use some guessed hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694282d0-073e-4cd3-aa74-fed64af7ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    \"max_depth\": 2,\n",
    "    \"min_child_weight\": 0,\n",
    "    \"subsample\": 0.8,\n",
    "    \"eta\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96268d86-57e4-4982-8c76-bb7c5775dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.64008\teval-error:0.24803\n",
      "[1]\teval-logloss:0.60108\teval-error:0.23622\n",
      "[2]\teval-logloss:0.57741\teval-error:0.23622\n",
      "[3]\teval-logloss:0.55778\teval-error:0.24803\n",
      "[4]\teval-logloss:0.54114\teval-error:0.25984\n",
      "[5]\teval-logloss:0.52809\teval-error:0.26772\n",
      "[6]\teval-logloss:0.52078\teval-error:0.26378\n",
      "[7]\teval-logloss:0.51649\teval-error:0.25984\n",
      "[8]\teval-logloss:0.51054\teval-error:0.25197\n",
      "[9]\teval-logloss:0.50366\teval-error:0.24409\n",
      "Accuracy: 0.7559\n"
     ]
    }
   ],
   "source": [
    "results = train_model(config)\n",
    "accuracy = 1. - results[\"eval\"][\"error\"][-1]\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc36b1-0310-488d-9bd6-b24a376b33e5",
   "metadata": {},
   "source": [
    "**Note**: The accuracy is slightly lower than the default parameters used above because we randomly chose the parameters.\n",
    "\n",
    "What if we want to get the best combination of all the parameters? This is where tuning hyperparameters helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8628b2-ef18-4cde-b22f-5ada73119e63",
   "metadata": {},
   "source": [
    "### Step 3: Tuning the configuration parameters for HPO\n",
    "XGBoosts default parameters already lead to a good accuracy, and even our guesses in the last section should result in accuracies well above 90%. However, our guesses were just that: guesses. Often we do not know what combination of parameters would actually lead to the best results on a machine learning task.\n",
    "\n",
    "Unfortunately, there are infinitely many combinations of hyperparameters we could try out. Should we combine `max_depth=3` with `subsample=0.8` or with `subsample=0.9?` What about the other parameters?\n",
    "\n",
    "This is where hyperparameter tuning comes into play. By using tuning libraries such as Ray Tune, we can try out combinations of hyperparameters. Using sophisticated search strategies, these parameters can be selected so that they are likely to lead to good results (avoiding an expensive exhaustive search). Also, trials that do not perform well can be preemptively stopped to reduce waste of computing resources. \n",
    "\n",
    "Lastly, Ray Tune also takes care of training these runs in parallel, greatly increasing search speed.\n",
    "\n",
    "Let’s start with a basic example on how to use Tune for this. We just need to make a few changes to our code-block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b81e6ab-0a42-4758-9903-68397c133f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c823223-d24d-452a-aab2-fe05160eb16c",
   "metadata": {},
   "source": [
    "Add tune report to our XGBoost training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b3ea33-e18d-49af-be08-ba9422246d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tuned_model(config, checkpoint_dir=None):\n",
    "    \n",
    "    # Load dataset \n",
    "    train_x, test_x, train_y, test_y = get_data()\n",
    "    \n",
    "    # Build input DMatrices for XGBoost\n",
    "    train_set = xgb.DMatrix(train_x, label=train_y)\n",
    "    test_set  = xgb.DMatrix(test_x, label=test_y)\n",
    "        \n",
    "    # Train the classifier\n",
    "    results = {}\n",
    "    xgb.train(\n",
    "         config,\n",
    "         train_set,\n",
    "         evals=[(test_set, \"eval\")],\n",
    "         evals_result=results,\n",
    "         verbose_eval=False)\n",
    "    \n",
    "    # Return prediction accuracy\n",
    "    accuracy = 1. - results[\"eval\"][\"error\"][-1]\n",
    "    tune.report(mean_accuracy=accuracy, done=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d0596-c1d7-46fd-9ee3-e2c7e65d3507",
   "metadata": {},
   "source": [
    "### Define our Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593d513d-4f74-44f0-b4ce-e78c8293854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    \"max_depth\": tune.randint(1, 9),\n",
    "    \"min_child_weight\": tune.choice([1, 2, 3]),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"eta\": tune.loguniform(1e-4, 1e-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c00cb8-13f1-403d-8660-8360d7c1fdb2",
   "metadata": {},
   "source": [
    "### Use Ray Tune parallelize our Hyperparameters tuning\n",
    "\n",
    "This will automatically launch a Ray cluster on your laptop and schedule tasks. The `num_samples=10` option we pass to `tune.run()` means that we sample 10 different hyperparameter configurations from this search space, run across 10 CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11811f30-a360-48a2-8757-06a7c22c314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-16 16:46:43 (running for 00:00:28.74)<br>Memory usage on this node: 14.7/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/14.7 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/jules/ray_results/train_tuned_model_2022-03-16_16-46-13<br>Number of trials: 10/10 (10 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:46:43,374\tINFO tune.py:639 -- Total run time: 29.64 seconds (28.73 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(train_tuned_model,\n",
    "         resources_per_trial={\"cpu\": 10},\n",
    "         config=config,\n",
    "         mode=\"min\",\n",
    "         verbose=1,\n",
    "         num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c97fee-179e-4d27-baca-a3e25940a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparamter config:  {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.9197123063487549, 'eta': 0.010124005890316683}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameter config: \", analysis.get_best_config(metric=\"mean_accuracy\", mode=\"min\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22e9cb-efe9-42d1-955e-b4c0ca874055",
   "metadata": {},
   "source": [
    "### Step 4: Early ASHAScheduler for early stopping\n",
    "\n",
    "Currently, in our example above, Tune samples 10 different hyperparameter configurations and trains a full XGBoost on all of them. In our small example, training is very fast. However, if training were done on a large dataset, it would\n",
    "take much longer and a significant amount of computer resources would be spent on trials that would eventually show a bad performance, e.g., a low accuracy. It would be good if we could identify these trials early and stop them, so we don’t waste any resources.\n",
    "\n",
    "This is where Tune’s Schedulers shine. A Tune `TrialScheduler` is responsible for starting and stopping trials. Tune implements a number of different schedulers, each described in the Tune documentation. For our example, we will use the `AsyncHyperBandScheduler` or `ASHAScheduler`.\n",
    "\n",
    "The basic idea of this scheduler is simple. We sample a number of hyperparameter configurations. Each of these configurations is trained for a specific number of iterations. After these iterations, only the best performing hyperparameters are retained. These are selected according to some loss metric, usually an evaluation loss. This cycle is repeated until we end up with the best configuration.\n",
    "\n",
    "The `ASHAScheduler` needs to know three things:\n",
    "\n",
    " * Which metric should be used to identify badly performing trials?\n",
    "\n",
    " * Should this metric be maximized or minimized?\n",
    "\n",
    " * How many iterations does each trial train for?\n",
    "\n",
    "There are more parameters, which are explained in the [documentation](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-schedulers).\n",
    "\n",
    "Lastly, we have to report the loss metric to Tune. We do this with a Callback that XGBoost accepts and calls after each evaluation round. Ray Tune comes with [two XGBoost callbacks](https://docs.ray.io/en/latest/tune/api_docs/integration.html#tune-integration-xgboost) we can use for this. The `TuneReportCallback` just reports the evaluation metrics back to Tune. The `TuneReportCheckpointCallback` also saves checkpoints after each evaluation round. We will just use the latter in this example so that we can retrieve the saved model later.\n",
    "\n",
    "These parameters from the `eval_metrics` configuration setting are then automatically reported to Tune via the callback. Here, the raw error will be reported, not the accuracy. To display the best reached accuracy, we will inverse it later.\n",
    "\n",
    "We will also load the best checkpointed model so that we can use it for predictions. The best model is selected with respect to the `metric` and `mode` parameters we pass to `tune.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d237dc3c-ca49-4218-b736-c750f7347eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.xgboost import TuneReportCheckpointCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965a645-4772-4d51-93cd-da74e598cdbe",
   "metadata": {},
   "source": [
    "Let's modify our training function and add our callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608cbf39-3540-4299-801c-c102e989f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tuned_asha_model(config: dict):\n",
    "    # This is a simple training function to be passed into Tune\n",
    "    # Load dataset\n",
    "    \n",
    "    # Load dataset \n",
    "    train_x, test_x, train_y, test_y = get_data()\n",
    "    \n",
    "    # Build input matrices for XGBoost\n",
    "    train_set = xgb.DMatrix(train_x, label=train_y)\n",
    "    test_set = xgb.DMatrix(test_x, label=test_y)\n",
    "    # Train the classifier, using the Tune callback\n",
    "    xgb.train(\n",
    "        config,\n",
    "        train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        verbose_eval=False,\n",
    "        callbacks=[TuneReportCheckpointCallback(filename=\"model.xgb\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab47b5-84da-4f6c-bb08-c6a1d502b27f",
   "metadata": {},
   "source": [
    "Write a helper function for loading callbacks and returning the best model with best configuration\n",
    "after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c81ec894-b7b7-4527-879e-75ffbe735afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_checkpoint(analysis):\n",
    "    best_bst = xgb.Booster()\n",
    "    best_model_path = os.path.join(analysis.best_checkpoint, \"model.xgb\")\n",
    "    best_bst.load_model(best_model_path)\n",
    "    accuracy = 1. - analysis.best_result[\"eval-error\"]\n",
    "    print(f\"Best model parameters: {analysis.best_config}\")\n",
    "    print(f\"Best model total accuracy: {accuracy:.4f}\")\n",
    "    print(f\"checkpoint best model path: {best_model_path}\")\n",
    "    return best_bst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e6edf-34d6-4b40-b244-2702d0ae5785",
   "metadata": {},
   "source": [
    "Wrapper around our trainer to do actual tuning:\n",
    " * define search space\n",
    " * define our ASHAScheduler\n",
    " * run `tune.run(...)`\n",
    " * return the ExperimentAnalysis object from `tune.run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99a6ee60-a84e-4d22-b7de-2b2997449c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_xgboost():\n",
    "    search_space = {\n",
    "        # You can mix constants with search space objects.\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"max_depth\": tune.randint(1, 9),\n",
    "        \"min_child_weight\": tune.choice([1, 2, 3]),\n",
    "        \"subsample\": tune.uniform(0.5, 1.0),\n",
    "        \"eta\": tune.loguniform(1e-4, 1e-1)\n",
    "    }\n",
    "    # This will enable aggressive early stopping of bad trials.\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=10,  # 10 training iterations\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        train_tuned_asha_model,   # our training function\n",
    "        metric=\"eval-logloss\", # eval metric\n",
    "        mode=\"min\",            # mode \n",
    "        # You can add \"gpu\": 0.1 to allocate GPUs\n",
    "        resources_per_trial={\"cpu\": 1},\n",
    "        config=search_space,\n",
    "        num_samples=10,\n",
    "        verbose=1,\n",
    "        scheduler=scheduler)\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121c4c3-508a-4e08-8092-6cc82ba5beb6",
   "metadata": {},
   "source": [
    "Let's tune with our `ASHAScheduler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63ddc4d0-e22a-4cb6-9fb9-3c9bdafa7d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-16 16:47:39 (running for 00:00:03.48)<br>Memory usage on this node: 15.2/32.0 GiB<br>Using AsyncHyperBand: num_stopped=10\n",
       "Bracket: Iter 8.000: -0.6566915 | Iter 4.000: -0.6798280000000001 | Iter 2.000: -0.687086 | Iter 1.000: -0.692715<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/14.7 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 74ec5_00008 with eval-logloss=0.590693 and parameters={'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.545816348116087, 'eta': 0.04609844502524226}<br>Result logdir: /Users/jules/ray_results/train_tuned_asha_model_2022-03-16_16-47-36<br>Number of trials: 10/10 (10 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:47:39,853\tINFO tune.py:639 -- Total run time: 3.60 seconds (3.46 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune_xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4dd1ae8-762f-4864-b9de-81d89dcf0076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparamter config:  {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.545816348116087, 'eta': 0.04609844502524226}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparamter config: \", analysis.get_best_config(metric=\"eval-logloss\", mode=\"min\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09933b8-fa8c-4eb3-921f-d7ddf395e19b",
   "metadata": {},
   "source": [
    "As you can see, most trials have been stopped only after a few iterations. Only the two most promising trials were run for the full 10 iterations.\n",
    "\n",
    "You can also ensure that all available resources are being used as the scheduler terminates trials, freeing them up. This can be done through the `ResourceChangingScheduler`. An example of this can be found here: [xgboost_dynamic_resources_example](https://docs.ray.io/en/latest/tune/examples/xgboost_dynamic_resources_example.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d543fd87-7d0d-4082-9468-ae04d4b32219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.545816348116087, 'eta': 0.04609844502524226}\n",
      "Best model total accuracy: 0.7677\n",
      "checkpoint best model path: /Users/jules/ray_results/train_tuned_asha_model_2022-03-16_16-47-36/train_tuned_asha_model_74ec5_00008_8_eta=0.046098,max_depth=4,min_child_weight=3,subsample=0.54582_2022-03-16_16-47-37/checkpoint_000005/model.xgb\n"
     ]
    }
   ],
   "source": [
    "best_bst = get_best_model_checkpoint(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93415f23-5af4-408e-80f8-30c7472119b7",
   "metadata": {},
   "source": [
    "### Step 5: Persist the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80fbb6cd-8f9c-49f5-9081-1d83e6f5b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bst.save_model(\"best_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25a098-b8dd-44b1-9642-4baab94f42e5",
   "metadata": {},
   "source": [
    "### Get some test data for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc2a280e-3dcb-4dbc-a959-b0d56e2927b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "train_x, test_x, train_y, test_y = get_data()\n",
    "test_set = xgb.DMatrix(test_x, label=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed80c6e-944b-4cc5-91f7-b9c45dabcfb6",
   "metadata": {},
   "source": [
    "### Step 6: Load the best persisted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21dfaebf-48ab-4026-9131-0b3e95296987",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_model = xgb.Booster()\n",
    "bst_model.load_model(\"best_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d54ace-3aed-4296-9c94-e0925b1ed372",
   "metadata": {},
   "source": [
    "### Step 7: Test some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6135ba55-61fb-4128-a3cc-bfe0fd58c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = bst_model.predict(test_set)[:-1]\n",
    "predictions = [round(value) for value in pred]\n",
    "predictions[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c88dd-7da2-45ff-a2be-20af8da24bf5",
   "metadata": {},
   "source": [
    "### Step 8: Create Deployment for Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47f595eb-5b34-46ad-bc9e-cb846ffb6386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11b7df61-dbc9-486f-9e09-3f14a3817b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=2, route_prefix=\"/regressor\")\n",
    "class XGBPimaIndianModel:\n",
    "    def __init__(self):\n",
    "        # Load the best saved model \n",
    "        self.bst_model = xgb.Booster()\n",
    "        self.bst_model.load_model(\"best_model.json\")\n",
    "        print(type(self.bst_model))\n",
    "        print(\"Best saved model loaded\")\n",
    "        \n",
    "    async def __call__(self, starlette_request:Request):\n",
    "        payload = await starlette_request.json()\n",
    "        pred = xgb.DMatrix([np.array(list(payload.values()), dtype=np.float64)])\n",
    "        prediction = round(np.float64(self.bst_model.predict(pred)[0]))\n",
    "        \n",
    "        return {\"result\": prediction}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d7408-fb6a-42ea-8b6d-76daa4875a63",
   "metadata": {},
   "source": [
    "### Step 9: Let's Deploy the model to Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48351e50-bd74-4c4b-bf40-c8072644e86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=65556)\u001b[0m 2022-03-16 16:49:01,251\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=65556)\u001b[0m 2022-03-16 16:49:01,355\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:hiAFWu:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-16 16:49:02,321\tINFO api.py:521 -- Started Serve instance in namespace '8f9e83df-2c81-4ce2-96b9-125f25062128'.\n",
      "2022-03-16 16:49:02,327\tINFO api.py:262 -- Updating deployment 'XGBPimaIndianModel'. component=serve deployment=XGBPimaIndianModel\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=65732)\u001b[0m INFO:     Started server process [65732]\n",
      "\u001b[2m\u001b[36m(ServeController pid=65556)\u001b[0m 2022-03-16 16:49:02,422\tINFO deployment_state.py:920 -- Adding 2 replicas to deployment 'XGBPimaIndianModel'. component=serve deployment=XGBPimaIndianModel\n",
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65734)\u001b[0m /usr/local/anaconda3/envs/anyscale-academy/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65734)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65735)\u001b[0m /usr/local/anaconda3/envs/anyscale-academy/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65735)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "2022-03-16 16:49:04,234\tINFO api.py:274 -- Deployment 'XGBPimaIndianModel' is ready at `http://127.0.0.1:8000/regressor`. component=serve deployment=XGBPimaIndianModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65734)\u001b[0m <class 'xgboost.core.Booster'>\n",
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65734)\u001b[0m Best saved model loaded\n",
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65735)\u001b[0m <class 'xgboost.core.Booster'>\n",
      "\u001b[2m\u001b[36m(XGBPimaIndianModel pid=65735)\u001b[0m Best saved model loaded\n"
     ]
    }
   ],
   "source": [
    "serve.start()\n",
    "XGBPimaIndianModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001c26e-0430-46e3-88e4-1ada78857c74",
   "metadata": {},
   "source": [
    "### Step 10: Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1e9120b-efa4-4912-b229-5c2873c19369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_request_inputs = [\n",
    "        {\"Pregnancies\": 6,\n",
    "         \"Glucose\": 148,\n",
    "         \"BloodPressure\": 72,\n",
    "         \"SkinThickness\": 35,\n",
    "         \"Insulin\": 0,\n",
    "         \"BMI\": np.float64(33.6),\n",
    "         \"DiabetesPedigree\": np.float64(0.625),\n",
    "         \"Age\": 50,\n",
    "         },\n",
    "        {\"Pregnancies\": 10,\n",
    "         \"Glucose\": 168,\n",
    "         \"BloodPressure\": 74,\n",
    "         \"SkinThickness\": 0,\n",
    "         \"Insulin\": 0,\n",
    "         \"BMI\": 38.0,\n",
    "         \"DiabetesPedigree\": 0.537,\n",
    "         \"Age\": 34,\n",
    "         },\n",
    "        {\"Pregnancies\": 10,\n",
    "         \"Glucose\": 39,\n",
    "         \"BloodPressure\": 80,\n",
    "         \"SkinThickness\": 0,\n",
    "         \"Insulin\": 0,\n",
    "         \"BMI\": 27.1,\n",
    "         \"DiabetesPedigree\": 1.441,\n",
    "         \"Age\": 57,\n",
    "         },\n",
    "        {\"Pregnancies\": 1,\n",
    "         \"Glucose\": 103,\n",
    "         \"BloodPressure\": 30,\n",
    "         \"SkinThickness\": 38,\n",
    "         \"Insulin\": 83,\n",
    "         \"BMI\": 43.3,\n",
    "         \"DiabetesPedigree\": 0.183,\n",
    "         \"Age\": 33,\n",
    "         }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c161a79-1b5d-4d47-8059-5ffcb6e96515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 1}\n",
      "{'result': 1}\n",
      "{'result': 0}\n",
      "{'result': 0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "for sri in sample_request_inputs: \n",
    "    response = requests.get(\"http://localhost:8000/regressor\", json=sri).json()\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f8352b8-7b5e-455c-a61e-eee38f1086da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da77de-8f0a-4f7b-8059-d7efbe7d19e2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You should now have a basic understanding on how to train XGBoost models and on how to tune the hyperparameters to yield the best results. In our simple example, Tuning the parameters didn’t make a huge difference for the accuracy. But in larger applications, intelligent hyperparameter tuning can make the difference between a model that doesn’t seem to learn at all, and a model that outperforms all the other ones.\n",
    "\n",
    "Also, once you tuned and persisted the models, you can easily deploy the best model to Ray Serve and score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b6a55-53ec-4e3f-9478-87eb1234b1c2",
   "metadata": {},
   "source": [
    "### Further References\n",
    "\n",
    "1. [XGBoost Hyperparameter Tuning - A Visual Guide](https://kevinvecmanis.io/machine%20learning/hyperparameter%20tuning/dataviz/python/2019/05/11/XGBoost-Tuning-Visual-Guide.html)\n",
    "\n",
    "2. [Notes on XGBoost Parameter Tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)\n",
    "\n",
    "3. [Doing XGBoost Hyperparameter Tuning the smart way](https://towardsdatascience.com/doing-xgboost-hyper-parameter-tuning-the-smart-way-part-1-of-2-f6d255a45dde)\n",
    "4. [Three ways to speed up XGBoost model training](https://www.anyscale.com/blog/three-ways-to-speed-up-xgboost-model-training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
