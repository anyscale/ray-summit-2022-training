{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214ce2a5-6b11-41d9-86f4-a2b4f0d1a6d9",
   "metadata": {},
   "source": [
    "### Model Composition ServerHandle APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_04_inference_graphs.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_ray_serve_fastapi.ipynb) <br>\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### Learning Objective:\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    " * compose complex models using ServeHandle APIs\n",
    " * deploy each discreate model as a seperate model deployment\n",
    " * use a single class deployment to include individual as a single model composition\n",
    " * deploy and serve this singluar model composition\n",
    "\n",
    "\n",
    "In this short tutorial we going to use HuggingFace Transformer ü§ó to accomplish three tasks:\n",
    " 1. Analyse the sentiment of a tweet: Positive or Negative\n",
    " 2. Translate it into French\n",
    " 3. Demonstrate the model composition deployment pattern using ServeHandle APIs\n",
    " \n",
    " <img src=\"images/sentiment_analysis.jpeg\" width=\"70%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23d1ccb-6951-4e22-aafd-48c74c4635de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package pickle5 becomes unnecessary in Python 3.8 and above. Its presence may confuse libraries including Ray. Please uninstall the package.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TranslationPipeline, TextClassificationPipeline\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709b04e-cc7d-4869-8a9f-919d20a2b4cd",
   "metadata": {},
   "source": [
    "These are example üê¶ tweets, some made up, some extracted from a dog lover's twitter handle. In a real use case,\n",
    "these could come live from a Tweeter handle using [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873b8bfd-1ee3-4d9c-bea4-e66924747ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [\"Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\",\n",
    "          \"Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\",\n",
    "          \"You little dog shit, you peed and pooed on my new carpet. Bad dog!\",\n",
    "          \"I would completely believe you. Dogs and little children - very innocent and open to seeing such things\",\n",
    "          \"You've got too much time on your paws. Go check on the skittle. under the, fridge\",\n",
    "          \"You sneaky little devil, I can't live without you!!!\",\n",
    "          \"It's true what they say about dogs: they are you BEST BUDDY, no matter what!\",\n",
    "          \"This dog is way dope, just can't enough of her\",\n",
    "          \"This dog is way cool, just can't enough of her\",\n",
    "          \"Is a dog really the best pet?\",\n",
    "          \"Cats are better than dogs\",\n",
    "          \"Totally dissastified with the dog. Worst dog ever\",\n",
    "          \"Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90287e-b3b9-498c-a2fc-fca7169b90fc",
   "metadata": {},
   "source": [
    "Utiliy function to fetch a tweet; these could very well be live tweets coming from Twitter API for a user or a #hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e6e502-0e23-470d-84f3-fbfdf80f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(i):\n",
    "    text = TWEETS[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13f240-0412-4f14-943d-98a71d5d2b43",
   "metadata": {},
   "source": [
    "### Sentiment model deployment\n",
    "\n",
    "Our function deployment model to analyse the tweet using a pretrained transformer from HuggingFace ü§ó.\n",
    "Note we have number of `replicas=1` but to scale it, we can increase the number of replicas, as\n",
    "we have done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50cf772-0fa6-4eaf-8288-b1cb5836f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=1)\n",
    "class SentimentTweet:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        self.pipeline = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, task=\"sentiment-analysis\")\n",
    "\n",
    "    def sentiment(self, text: str):\n",
    "        return (f\"label:{self.pipeline(text)[0]['label']}; score={self.pipeline(text)[0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5f64-5c28-4af0-b266-4566e1f40c1a",
   "metadata": {},
   "source": [
    "### Translation model deployment\n",
    "\n",
    "Our function to translate a tweet from English --> French using a pretrained Transformer from HuggingFace ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a813a1c8-c577-4028-ad09-8b4dea439e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=2)\n",
    "class TranslateTweet:\n",
    "    def __init__(self):\n",
    "         self.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "         self.model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "         self.use_gpu = 0 if torch.cuda.is_available() else -1\n",
    "         self.pipeline = TranslationPipeline(self.model, self.tokenizer, task=\"translation_en_to_fr\", device=self.use_gpu)\n",
    "\n",
    "    def translate(self, text: str):\n",
    "        return self.pipeline(text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575b2a-11c3-40ea-8522-64ed27fdcbdf",
   "metadata": {},
   "source": [
    "### Use the Model Composition pattern\n",
    "\n",
    "<img src=\"images/tweet_composition.png\" width=\"60%\" height=\"25%\">\n",
    "\n",
    "A composed class is deployed with both sentiment analysis and translations models' ServeHandles initialized in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08cec76c-3ef5-4741-914b-edb2c4fd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/composed\", num_replicas=2)\n",
    "class ComposedModel:\n",
    "    def __init__(self, translate, sentiment):\n",
    "        # fetch and initialize deployment handles\n",
    "        self.translate_model = translate\n",
    "        self.sentiment_model = sentiment\n",
    "\n",
    "    async def __call__(self, http_request):\n",
    "        data = await http_request.json()\n",
    "        sentiment_ref = await self.sentiment_model.sentiment.remote(data)\n",
    "        trans_text_ref = await self.translate_model.translate.remote(data)\n",
    "        sentiment_val = await sentiment_ref\n",
    "        trans_text = await trans_text_ref\n",
    "\n",
    "        return {'Sentiment': sentiment_val, 'Translated Text': trans_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26264457-b699-48ff-b5a8-f9a8d66c95f6",
   "metadata": {},
   "source": [
    "### Deploy our models \n",
    "\n",
    "Deploy our models. Our composed class is deployed with both sentiment analysis and translations models' `ClassNode` initialized in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39401d3-8fb0-410c-bb61-d25164fbeab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 09:07:21,167\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m.\n",
      "\u001b[2m\u001b[36m(ServeController pid=9248)\u001b[0m INFO 2022-08-09 09:07:24,728 controller 9248 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-a6f42a350ca73993ae1a89a7fbd2ce7538292181d99e548557ed1643' on node 'a6f42a350ca73993ae1a89a7fbd2ce7538292181d99e548557ed1643' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO:     Started server process [9261]\n",
      "\u001b[2m\u001b[36m(ServeController pid=9248)\u001b[0m INFO 2022-08-09 09:07:26,169 controller 9248 deployment_state.py:1232 - Adding 2 replicas to deployment 'TranslateTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=9248)\u001b[0m INFO 2022-08-09 09:07:26,186 controller 9248 deployment_state.py:1232 - Adding 1 replicas to deployment 'SentimentTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=9248)\u001b[0m INFO 2022-08-09 09:07:26,197 controller 9248 deployment_state.py:1232 - Adding 2 replicas to deployment 'ComposedModel'.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m /Users/archit/anaconda3/envs/ray-py38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m /Users/archit/anaconda3/envs/ray-py38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m /Users/archit/anaconda3/envs/ray-py38/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:998: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m /Users/archit/anaconda3/envs/ray-py38/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:998: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='ComposedModel')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_cls_node = TranslateTweet.bind()\n",
    "sentiment_cls_node = SentimentTweet.bind()\n",
    "compose_cls_node = ComposedModel.bind(translate_cls_node,sentiment_cls_node )\n",
    "\n",
    "serve.run(compose_cls_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea6006-d578-4eaa-9e95-6b2b6d4ff04a",
   "metadata": {},
   "source": [
    "### Send HTTP requests to our deployment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e72dd6-df3b-4236-bd78-d4b0b9bec98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:44,801 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 123.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9651218056678772\", \"Translated Text\": \"Ce soir, j'ai \\u00e9t\\u00e9 fou parce que ma m\\u00e8re ne me laisse pas jouer avec ce chien.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:46,319 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 5975.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m INFO 2022-08-09 09:07:46,316 TranslateTweet TranslateTweet#iIieSg replica.py:482 - HANDLE translate OK 1631.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9307)\u001b[0m INFO 2022-08-09 09:07:46,318 ComposedModel ComposedModel#ofgqzy replica.py:482 - HANDLE __call__ OK 5971.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:50,574 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 85.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.99788898229599\", \"Translated Text\": \"Parfois. quand j'ennuie. je ne regarderai rien. et essayerai de convaincre l'homme.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m INFO 2022-08-09 09:07:51,616 TranslateTweet TranslateTweet#dnowFq replica.py:482 - HANDLE translate OK 1119.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9306)\u001b[0m INFO 2022-08-09 09:07:51,618 ComposedModel ComposedModel#EmADna replica.py:482 - HANDLE __call__ OK 5291.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:51,619 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 5296.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:51,712 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 82.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : You little dog shit, you peed and pooed on my new carpet. Bad dog!\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9984055161476135\", \"Translated Text\": \"Je n'ai pas eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:55,456 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 3831.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m INFO 2022-08-09 09:07:55,454 TranslateTweet TranslateTweet#dnowFq replica.py:482 - HANDLE translate OK 3819.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9307)\u001b[0m INFO 2022-08-09 09:07:55,455 ComposedModel ComposedModel#ofgqzy replica.py:482 - HANDLE __call__ OK 3829.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:55,549 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 83.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : I would completely believe you. Dogs and little children - very innocent and open to seeing such things\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9997748732566833\", \"Translated Text\": \"Je vous croyais tout \\u00e0 fait: chiens et petits enfants - tr\\u00e8s innocents et ouverts \\u00e0 ce genre de choses\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:56,562 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1102.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9306)\u001b[0m INFO 2022-08-09 09:07:56,561 ComposedModel ComposedModel#EmADna replica.py:482 - HANDLE __call__ OK 1100.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m INFO 2022-08-09 09:07:56,560 TranslateTweet TranslateTweet#iIieSg replica.py:482 - HANDLE translate OK 1090.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:56,661 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 89.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : You've got too much time on your paws. Go check on the skittle. under the, fridge\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9995866417884827\", \"Translated Text\": \"Vous avez trop de temps sur vos pattes.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:57,579 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1012.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m INFO 2022-08-09 09:07:57,577 TranslateTweet TranslateTweet#iIieSg replica.py:482 - HANDLE translate OK 1001.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9307)\u001b[0m INFO 2022-08-09 09:07:57,578 ComposedModel ComposedModel#ofgqzy replica.py:482 - HANDLE __call__ OK 1010.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:57,664 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 76.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : You sneaky little devil, I can't live without you!!!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9949393272399902\", \"Translated Text\": \"Du petit diable, je ne peux pas vivre sans vous !!!\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:58,116 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 533.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m INFO 2022-08-09 09:07:58,114 TranslateTweet TranslateTweet#dnowFq replica.py:482 - HANDLE translate OK 522.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9306)\u001b[0m INFO 2022-08-09 09:07:58,115 ComposedModel ComposedModel#EmADna replica.py:482 - HANDLE __call__ OK 531.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:58,268 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 142.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : It's true what they say about dogs: they are you BEST BUDDY, no matter what!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9996572732925415\", \"Translated Text\": \"C'est vrai ce qu'ils disent sur les chiens : ils sont tu MEILLEUR BUDDY, peu importe quoi!\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:59,291 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1171.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m INFO 2022-08-09 09:07:59,289 TranslateTweet TranslateTweet#dnowFq replica.py:482 - HANDLE translate OK 1160.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9307)\u001b[0m INFO 2022-08-09 09:07:59,290 ComposedModel ComposedModel#ofgqzy replica.py:482 - HANDLE __call__ OK 1169.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:59,373 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 72.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : This dog is way dope, just can't enough of her\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9972212314605713\", \"Translated Text\": \"Ce chien est assez dope, il ne peut pas assez de lui\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:07:59,892 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 597.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9306)\u001b[0m INFO 2022-08-09 09:07:59,891 ComposedModel ComposedModel#EmADna replica.py:482 - HANDLE __call__ OK 594.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m INFO 2022-08-09 09:07:59,889 TranslateTweet TranslateTweet#iIieSg replica.py:482 - HANDLE translate OK 585.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:07:59,987 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 70.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : This dog is way cool, just can't enough of her\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9847628474235535\", \"Translated Text\": \"Ce chien est bien cool, il ne peut pas assez de lui\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:08:00,478 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 582.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m INFO 2022-08-09 09:08:00,466 TranslateTweet TranslateTweet#iIieSg replica.py:482 - HANDLE translate OK 546.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9307)\u001b[0m INFO 2022-08-09 09:08:00,475 ComposedModel ComposedModel#ofgqzy replica.py:482 - HANDLE __call__ OK 577.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:08:00,607 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 67.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Is a dog really the best pet?\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.998790442943573\", \"Translated Text\": \"Est-ce qu'un chien est vraiment le meilleur animal de compagnie?\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:08:00,992 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 494.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:08:01,060 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 58.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m INFO 2022-08-09 09:08:00,990 TranslateTweet TranslateTweet#dnowFq replica.py:482 - HANDLE translate OK 446.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9306)\u001b[0m INFO 2022-08-09 09:08:00,991 ComposedModel ComposedModel#EmADna replica.py:482 - HANDLE __call__ OK 472.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Cats are better than dogs\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9986716508865356\", \"Translated Text\": \"Les chats sont meilleurs que les chiens\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:08:01,350 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 354.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9303)\u001b[0m INFO 2022-08-09 09:08:01,349 TranslateTweet TranslateTweet#dnowFq replica.py:482 - HANDLE translate OK 343.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9307)\u001b[0m INFO 2022-08-09 09:08:01,350 ComposedModel ComposedModel#ofgqzy replica.py:482 - HANDLE __call__ OK 352.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:08:01,426 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 65.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Totally dissastified with the dog. Worst dog ever\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9998103976249695\", \"Translated Text\": \"Tr\\u00e8s d\\u00e9sassass\\u00e9 avec le chien, pire chien jamais\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:08:01,948 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 594.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9306)\u001b[0m INFO 2022-08-09 09:08:01,947 ComposedModel ComposedModel#EmADna replica.py:482 - HANDLE __call__ OK 591.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m INFO 2022-08-09 09:08:01,946 TranslateTweet TranslateTweet#iIieSg replica.py:482 - HANDLE translate OK 583.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=9305)\u001b[0m INFO 2022-08-09 09:08:02,092 SentimentTweet SentimentTweet#pUmLpl replica.py:482 - HANDLE sentiment OK 109.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9929038882255554\", \"Translated Text\": \"Le chien briliant lise mes humeurs comme un livre, ressent mes humeurs et r\\u00e9agit.\"}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(TWEETS)):\n",
    "    tweet = fetch_tweet_text(i)\n",
    "    response = requests.post(\"http://127.0.0.1:8000/composed\", json=tweet)\n",
    "    print(f\"tweet request... : {tweet}\")\n",
    "    print(f\"tweet response:{response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d9b3f-e349-4af6-b9c1-e4ff0a2dab9e",
   "metadata": {},
   "source": [
    "Gracefully shutdown the Ray serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786fcf2e-1afa-4900-bc3f-0fc8844d5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=9261)\u001b[0m INFO 2022-08-09 09:08:03,000 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1047.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=9304)\u001b[0m INFO 2022-08-09 09:08:02,998 TranslateTweet TranslateTweet#iIieSg replica.py:482 - HANDLE translate OK 987.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=9307)\u001b[0m INFO 2022-08-09 09:08:02,999 ComposedModel ComposedModel#ofgqzy replica.py:482 - HANDLE __call__ OK 1045.3ms\n",
      "\u001b[2m\u001b[36m(ServeController pid=9248)\u001b[0m INFO 2022-08-09 09:08:03,129 controller 9248 deployment_state.py:1257 - Removing 2 replicas from deployment 'TranslateTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=9248)\u001b[0m INFO 2022-08-09 09:08:03,132 controller 9248 deployment_state.py:1257 - Removing 1 replicas from deployment 'SentimentTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=9248)\u001b[0m INFO 2022-08-09 09:08:03,133 controller 9248 deployment_state.py:1257 - Removing 2 replicas from deployment 'ComposedModel'.\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d581-6a31-43f2-9083-94578042efdf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Add more tweets to `TWEETS` with different sentiments.\n",
    "2. Check the score (and if you speak and read French, what you think of the translation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e209-4437-426e-a425-39b443df68a1",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Instead of French, use a language transformer of your choice\n",
    "2. What about Neutral tweets? Try using [vaderSentiment](https://github.com/cjhutto/vaderSentiment)\n",
    "3. Solution for 2) is [here](https://github.com/anyscale/academy/blob/main/ray-serve/05-Ray-Serve-SentimentAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476075b-d538-4ba4-84fa-02688b013a4d",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "We'll further explore model composition using [Deploymant Graph APIs](https://docs.ray.io/en/latest/serve/deployment-graph.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d2797-a9ca-4f2f-bcd2-477667109fac",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_04_inference_graphs.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_ray_serve_fastapi.ipynb) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
