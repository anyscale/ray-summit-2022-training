{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214ce2a5-6b11-41d9-86f4-a2b4f0d1a6d9",
   "metadata": {},
   "source": [
    "### Model Composition ServerHandle APIs\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "In this short tutorial we going to use HuggingFace Transformer 🤗 to accomplish three tasks:\n",
    " 1. Analyse the sentiment of a tweet: Positive or Negative\n",
    " 2. Translate it into French\n",
    " 3. Demonstrate the model composition deployment pattern using ServeHandle APIs\n",
    " \n",
    " <img src=\"images/sentiment_analysis.jpeg\" width=\"70%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23d1ccb-6951-4e22-aafd-48c74c4635de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TranslationPipeline, TextClassificationPipeline\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709b04e-cc7d-4869-8a9f-919d20a2b4cd",
   "metadata": {},
   "source": [
    "These are example 🐦 tweets, some made up, some extracted from a dog lover's twitter handle. In a real use case,\n",
    "these could come live from a Tweeter handle using [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873b8bfd-1ee3-4d9c-bea4-e66924747ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [\"Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\",\n",
    "          \"Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\",\n",
    "          \"You little dog shit, you peed and pooed on my new carpet. Bad dog!\",\n",
    "          \"I would completely believe you. Dogs and little children - very innocent and open to seeing such things\",\n",
    "          \"You've got too much time on your paws. Go check on the skittle. under the, fridge\",\n",
    "          \"You sneaky little devil, I can't live without you!!!\",\n",
    "          \"It's true what they say about dogs: they are you BEST BUDDY, no matter what!\",\n",
    "          \"This dog is way dope, just can't enough of her\",\n",
    "          \"This dog is way cool, just can't enough of her\",\n",
    "          \"Is a dog really the best pet?\",\n",
    "          \"Cats are better than dogs\",\n",
    "          \"Totally dissastified with the dog. Worst dog ever\",\n",
    "          \"Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90287e-b3b9-498c-a2fc-fca7169b90fc",
   "metadata": {},
   "source": [
    "Utiliy function to fetch a tweet; these could very well be live tweets coming from Twitter API for a user or a #hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e6e502-0e23-470d-84f3-fbfdf80f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(i):\n",
    "    text = TWEETS[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13f240-0412-4f14-943d-98a71d5d2b43",
   "metadata": {},
   "source": [
    "### Sentiment model deployment\n",
    "\n",
    "Our function deployment model to analyse the tweet using a pretrained transformer from HuggingFace 🤗.\n",
    "Note we have number of `replicas=1` but to scale it, we can increase the number of replicas, as\n",
    "we have done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50cf772-0fa6-4eaf-8288-b1cb5836f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=1)\n",
    "def sentiment_model(text: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, task=\"sentiment-analysis\")\n",
    "\n",
    "    return pipeline(text)[0]['label'], pipeline(text)[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5f64-5c28-4af0-b266-4566e1f40c1a",
   "metadata": {},
   "source": [
    "### Translation model deployment\n",
    "\n",
    "Our function to translate a tweet from English --> French using a pretrained Transformer from HuggingFace 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a813a1c8-c577-4028-ad09-8b4dea439e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate a tweet from English --> French \n",
    "# using a pretrained Transformer from HuggingFace\n",
    "@serve.deployment(num_replicas=2)\n",
    "def translate_model(text: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "    model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "    use_gpu = 0 if torch.cuda.is_available() else -1\n",
    "    pipeline = TranslationPipeline(model, tokenizer, task=\"translation_en_to_fr\", device=use_gpu)\n",
    "\n",
    "    return pipeline(text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575b2a-11c3-40ea-8522-64ed27fdcbdf",
   "metadata": {},
   "source": [
    "### Use the Model Composition pattern\n",
    "\n",
    "<img src=\"images/tweet_composition.png\" width=\"60%\" height=\"25%\">\n",
    "\n",
    "A composed class is deployed with both sentiment analysis and translations models' ServeHandles initialized in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cec76c-3ef5-4741-914b-edb2c4fd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/composed\", num_replicas=2)\n",
    "class ComposedModel:\n",
    "    def __init__(self):\n",
    "        # fetch and initialize deployment handles\n",
    "        self.translate_model = translate_model.get_handle(sync=False)\n",
    "        self.sentiment_model = sentiment_model.get_handle(sync=False)\n",
    "\n",
    "    async def __call__(self, starlette_request):\n",
    "        data = starlette_request.query_params['data']\n",
    "\n",
    "        sentiment, score = await(await self.sentiment_model.remote(data))\n",
    "        trans_text = await(await self.translate_model.remote(data))\n",
    "\n",
    "        return {'Sentiment': sentiment, 'score': score, 'Translated Text': trans_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dc58b-d3a4-4ee2-876d-832983ff4642",
   "metadata": {},
   "source": [
    "Start a Ray Serve instance. Note that if Ray cluster does not exist, it will create one and attach the Ray Serve\n",
    "instance to it. If one exists it'll run on that Ray cluster instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc61315-27e0-4b1b-a4aa-ae6abd2faa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:30:44,284\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:30:46,924\tINFO checkpoint_path.py:15 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:30:47,028\tINFO http_state.py:106 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:jHFlgD:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-06-08 19:30:47,948\tINFO api.py:794 -- Started Serve instance in namespace 'serve'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=94975)\u001b[0m INFO:     Started server process [94975]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fb49a073eb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26264457-b699-48ff-b5a8-f9a8d66c95f6",
   "metadata": {},
   "source": [
    "### Deploy our models \n",
    "\n",
    "Deploy our models. As seen before in other tutorials, this is as simple and intuitive as invoking `<func_or_class_name>.deploy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39401d3-8fb0-410c-bb61-d25164fbeab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:30:47,967\tINFO api.py:615 -- Updating deployment 'sentiment_model'. component=serve deployment=sentiment_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:30:48,013\tINFO deployment_state.py:1216 -- Adding 1 replicas to deployment 'sentiment_model'. component=serve deployment=sentiment_model\n",
      "2022-06-08 19:30:50,977\tINFO api.py:630 -- Deployment 'sentiment_model' is ready at `http://127.0.0.1:8000/sentiment_model`. component=serve deployment=sentiment_model\n",
      "2022-06-08 19:30:50,984\tINFO api.py:615 -- Updating deployment 'translate_model'. component=serve deployment=translate_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:30:51,002\tINFO deployment_state.py:1216 -- Adding 2 replicas to deployment 'translate_model'. component=serve deployment=translate_model\n",
      "2022-06-08 19:30:54,004\tINFO api.py:630 -- Deployment 'translate_model' is ready at `http://127.0.0.1:8000/translate_model`. component=serve deployment=translate_model\n",
      "2022-06-08 19:30:54,011\tINFO api.py:615 -- Updating deployment 'ComposedModel'. component=serve deployment=ComposedModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:30:54,109\tINFO deployment_state.py:1216 -- Adding 2 replicas to deployment 'ComposedModel'. component=serve deployment=ComposedModel\n",
      "2022-06-08 19:30:57,021\tINFO api.py:630 -- Deployment 'ComposedModel' is ready at `http://127.0.0.1:8000/composed`. component=serve deployment=ComposedModel\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.deploy()\n",
    "translate_model.deploy()\n",
    "ComposedModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea6006-d578-4eaa-9e95-6b2b6d4ff04a",
   "metadata": {},
   "source": [
    "### Send HTTP requests to our deployment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e72dd6-df3b-4236-bd78-d4b0b9bec98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending tweet request... : Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m /usr/local/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m /usr/local/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:921: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94980)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.965121328830719, 'Translated Text': \"Ce soir, j'ai été fou parce que ma mère ne me laisse pas jouer avec ce chien.\"}\n",
      "Sending tweet request... : Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m /usr/local/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m /usr/local/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:921: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(translate_model pid=94979)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'NEGATIVE', 'score': 0.99788898229599, 'Translated Text': \"Parfois. quand j'ennuie. je ne regarderai rien. et essayerai de convaincre l'homme.\"}\n",
      "Sending tweet request... : You little dog shit, you peed and pooed on my new carpet. Bad dog!\n",
      "{'Sentiment': 'NEGATIVE', 'score': 0.9984055161476135, 'Translated Text': \"Je n'ai pas eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression\"}\n",
      "Sending tweet request... : I would completely believe you. Dogs and little children - very innocent and open to seeing such things\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.9997748732566833, 'Translated Text': 'Je vous croyais tout à fait: chiens et petits enfants - très innocents et ouverts à ce genre de choses'}\n",
      "Sending tweet request... : You've got too much time on your paws. Go check on the skittle. under the, fridge\n",
      "{'Sentiment': 'NEGATIVE', 'score': 0.9995866417884827, 'Translated Text': 'Vous avez trop de temps sur vos pattes.'}\n",
      "Sending tweet request... : You sneaky little devil, I can't live without you!!!\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.9949393272399902, 'Translated Text': 'Du petit diable, je ne peux pas vivre sans vous !!!'}\n",
      "Sending tweet request... : It's true what they say about dogs: they are you BEST BUDDY, no matter what!\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.9996572732925415, 'Translated Text': \"C'est vrai ce qu'ils disent sur les chiens : ils sont tu MEILLEUR BUDDY, peu importe quoi!\"}\n",
      "Sending tweet request... : This dog is way dope, just can't enough of her\n",
      "{'Sentiment': 'NEGATIVE', 'score': 0.9972212314605713, 'Translated Text': 'Ce chien est assez dope, il ne peut pas assez de lui'}\n",
      "Sending tweet request... : This dog is way cool, just can't enough of her\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.9847627282142639, 'Translated Text': 'Ce chien est bien cool, il ne peut pas assez de lui'}\n",
      "Sending tweet request... : Is a dog really the best pet?\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.998790442943573, 'Translated Text': \"Est-ce qu'un chien est vraiment le meilleur animal de compagnie?\"}\n",
      "Sending tweet request... : Cats are better than dogs\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.9986716508865356, 'Translated Text': 'Les chats sont meilleurs que les chiens'}\n",
      "Sending tweet request... : Totally dissastified with the dog. Worst dog ever\n",
      "{'Sentiment': 'NEGATIVE', 'score': 0.9998103976249695, 'Translated Text': 'Très désassassé avec le chien, pire chien jamais'}\n",
      "Sending tweet request... : Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.9929038882255554, 'Translated Text': 'Le chien briliant lise mes humeurs comme un livre, ressent mes humeurs et réagit.'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(TWEETS)):\n",
    "    tweet = fetch_tweet_text(i)\n",
    "    print(F\"Sending tweet request... : {tweet}\")\n",
    "    resp = requests.get(\"http://127.0.0.1:8000/composed\", params={'data': tweet})\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d9b3f-e349-4af6-b9c1-e4ff0a2dab9e",
   "metadata": {},
   "source": [
    "Gracefully shutdown the Ray serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786fcf2e-1afa-4900-bc3f-0fc8844d5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:32:26,031\tINFO deployment_state.py:1242 -- Removing 1 replicas from deployment 'sentiment_model'. component=serve deployment=sentiment_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:32:26,034\tINFO deployment_state.py:1242 -- Removing 2 replicas from deployment 'translate_model'. component=serve deployment=translate_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:32:26,038\tINFO deployment_state.py:1242 -- Removing 2 replicas from deployment 'ComposedModel'. component=serve deployment=ComposedModel\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d581-6a31-43f2-9083-94578042efdf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Add more tweets with different sentiments.\n",
    "2. Check the score (and if you speak and read French, what you think of the translation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e209-4437-426e-a425-39b443df68a1",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Instead of French, use a language transformer of your choice\n",
    "2. What about Neutral tweets? Try using [vaderSentiment](https://github.com/cjhutto/vaderSentiment)\n",
    "3. Solution for 2) is [here](https://github.com/anyscale/academy/blob/main/ray-serve/05-Ray-Serve-SentimentAnalysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
