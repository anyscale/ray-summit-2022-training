{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214ce2a5-6b11-41d9-86f4-a2b4f0d1a6d9",
   "metadata": {},
   "source": [
    "### Model Composition ServerHandle APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_04_inference_graphs.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_ray_serve_fastapi.ipynb) <br>\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### Learning Objective:\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    " * compose complex models using ServeHandle APIs\n",
    " * deploy each discreate model as a seperate model deployment\n",
    " * use a single class deployment to include individual as a single model composition\n",
    " * deploy and serve this singluar model composition\n",
    "\n",
    "\n",
    "In this short tutorial we going to use HuggingFace Transformer ü§ó to accomplish three tasks:\n",
    " 1. Analyse the sentiment of a tweet: Positive or Negative\n",
    " 2. Translate it into French\n",
    " 3. Demonstrate the model composition deployment pattern using ServeHandle APIs\n",
    " \n",
    " <img src=\"images/sentiment_analysis.jpeg\" width=\"70%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23d1ccb-6951-4e22-aafd-48c74c4635de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package pickle5 becomes unnecessary in Python 3.8 and above. Its presence may confuse libraries including Ray. Please uninstall the package.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TranslationPipeline, TextClassificationPipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709b04e-cc7d-4869-8a9f-919d20a2b4cd",
   "metadata": {},
   "source": [
    "These are example üê¶ tweets, some made up, some extracted from a dog lover's twitter handle. In a real use case,\n",
    "these could come live from a Tweeter handle using [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873b8bfd-1ee3-4d9c-bea4-e66924747ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [\"Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\",\n",
    "          \"Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\",\n",
    "          \"You little dog shit, you peed and pooed on my new carpet. Bad dog!\",\n",
    "          \"I would completely believe you. Dogs and little children - very innocent and open to seeing such things\",\n",
    "          \"You've got too much time on your paws. Go check on the skittle. under the, fridge\",\n",
    "          \"You sneaky little devil, I can't live without you!!!\",\n",
    "          \"It's true what they say about dogs: they are you BEST BUDDY, no matter what!\",\n",
    "          \"This dog is way dope, just can't enough of her\",\n",
    "          \"This dog is way cool, just can't enough of her\",\n",
    "          \"Is a dog really the best pet?\",\n",
    "          \"Cats are better than dogs\",\n",
    "          \"Totally dissastified with the dog. Worst dog ever\",\n",
    "          \"Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90287e-b3b9-498c-a2fc-fca7169b90fc",
   "metadata": {},
   "source": [
    "Utiliy function to fetch a tweet; these could very well be live tweets coming from Twitter API for a user or a #hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e6e502-0e23-470d-84f3-fbfdf80f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(i):\n",
    "    text = TWEETS[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13f240-0412-4f14-943d-98a71d5d2b43",
   "metadata": {},
   "source": [
    "### Sentiment model deployment\n",
    "\n",
    "Our function deployment model to analyse the tweet using a pretrained transformer from HuggingFace ü§ó.\n",
    "Note we have number of `replicas=1` but to scale it, we can increase the number of replicas, as\n",
    "we have done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50cf772-0fa6-4eaf-8288-b1cb5836f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=1)\n",
    "class SentimentTweet:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", model_max_length=128)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        self.pipeline = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, task=\"sentiment-analysis\")\n",
    "\n",
    "    def sentiment(self, text: str):\n",
    "        return (f\"label:{self.pipeline(text)[0]['label']}; score={self.pipeline(text)[0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5f64-5c28-4af0-b266-4566e1f40c1a",
   "metadata": {},
   "source": [
    "### Translation model deployment\n",
    "\n",
    "Our function to translate a tweet from English --> French using a pretrained Transformer from HuggingFace ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a813a1c8-c577-4028-ad09-8b4dea439e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=2)\n",
    "class TranslateTweet:\n",
    "    def __init__(self):\n",
    "         self.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=128)\n",
    "         self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "         self.use_gpu = 0 if torch.cuda.is_available() else -1\n",
    "         self.pipeline = TranslationPipeline(self.model, self.tokenizer, task=\"translation_en_to_fr\", device=self.use_gpu)\n",
    "\n",
    "    def translate(self, text: str):\n",
    "        return self.pipeline(text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575b2a-11c3-40ea-8522-64ed27fdcbdf",
   "metadata": {},
   "source": [
    "### Use the Model Composition pattern\n",
    "\n",
    "<img src=\"images/tweet_composition.png\" width=\"60%\" height=\"25%\">\n",
    "\n",
    "A composed class is deployed with both sentiment analysis and translations models' ServeHandles initialized in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08cec76c-3ef5-4741-914b-edb2c4fd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/composed\", num_replicas=2)\n",
    "class ComposedModel:\n",
    "    def __init__(self, translate, sentiment):\n",
    "        # fetch and initialize deployment handles\n",
    "        self.translate_model = translate\n",
    "        self.sentiment_model = sentiment\n",
    "\n",
    "    async def __call__(self, http_request):\n",
    "        data = await http_request.json()\n",
    "        sentiment_ref = await self.sentiment_model.sentiment.remote(data)\n",
    "        trans_text_ref = await self.translate_model.translate.remote(data)\n",
    "        sentiment_val = await sentiment_ref\n",
    "        trans_text = await trans_text_ref\n",
    "\n",
    "        return {'Sentiment': sentiment_val, 'Translated Text': trans_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26264457-b699-48ff-b5a8-f9a8d66c95f6",
   "metadata": {},
   "source": [
    "### Deploy our models \n",
    "\n",
    "Deploy our models. Our composed class is deployed with both sentiment analysis and translations models' `ClassNode` initialized in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39401d3-8fb0-410c-bb61-d25164fbeab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 09:38:11,565\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m.\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m INFO 2022-08-09 09:38:14,922 controller 16150 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-023b316d1dbebbf2bc7feec83e47d2bd90781c7c4e5c3d5481729d9a' on node '023b316d1dbebbf2bc7feec83e47d2bd90781c7c4e5c3d5481729d9a' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO:     Started server process [16160]\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m INFO 2022-08-09 09:38:16,360 controller 16150 deployment_state.py:1232 - Adding 2 replicas to deployment 'TranslateTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m INFO 2022-08-09 09:38:16,380 controller 16150 deployment_state.py:1232 - Adding 1 replicas to deployment 'SentimentTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m INFO 2022-08-09 09:38:16,392 controller 16150 deployment_state.py:1232 - Adding 2 replicas to deployment 'ComposedModel'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='ComposedModel')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_cls_node = TranslateTweet.bind()\n",
    "sentiment_cls_node = SentimentTweet.bind()\n",
    "compose_cls_node = ComposedModel.bind(translate_cls_node,sentiment_cls_node )\n",
    "\n",
    "serve.run(compose_cls_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea6006-d578-4eaa-9e95-6b2b6d4ff04a",
   "metadata": {},
   "source": [
    "### Send HTTP requests to our deployment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e72dd6-df3b-4236-bd78-d4b0b9bec98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:37,682 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 110.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9651218056678772\", \"Translated Text\": \"Ce soir, j'ai \\u00e9t\\u00e9 fou parce que ma m\\u00e8re ne me laisse pas jouer avec ce chien.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:39,146 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 6660.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16164)\u001b[0m INFO 2022-08-09 09:38:39,144 TranslateTweet TranslateTweet#gIwrMB replica.py:482 - HANDLE translate OK 1565.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16167)\u001b[0m INFO 2022-08-09 09:38:39,145 ComposedModel ComposedModel#KRrrri replica.py:482 - HANDLE __call__ OK 6655.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:43,408 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 97.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.99788898229599\", \"Translated Text\": \"Parfois. quand j'ennuie. je ne regarderai rien. et essayerai de convaincre l'homme.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:44,452 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 5301.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16164)\u001b[0m INFO 2022-08-09 09:38:44,450 TranslateTweet TranslateTweet#gIwrMB replica.py:482 - HANDLE translate OK 1132.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16168)\u001b[0m INFO 2022-08-09 09:38:44,451 ComposedModel ComposedModel#WaapSf replica.py:482 - HANDLE __call__ OK 5296.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:44,545 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 83.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : You little dog shit, you peed and pooed on my new carpet. Bad dog!\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9984055161476135\", \"Translated Text\": \"Je n'ai pas eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:48,505 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 4049.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16167)\u001b[0m INFO 2022-08-09 09:38:48,505 ComposedModel ComposedModel#KRrrri replica.py:482 - HANDLE __call__ OK 4047.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16165)\u001b[0m INFO 2022-08-09 09:38:48,504 TranslateTweet TranslateTweet#jUzajq replica.py:482 - HANDLE translate OK 4036.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:48,602 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 86.1ms\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m 2022-08-09 09:38:48,785\tINFO (unknown file):0 -- Task failed with unretryable exception: TaskID(0e125f9e5cb53cb9fc5f3af36eef1725e1393f6001000000).\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 709, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 713, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 655, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2243, in ray._raylet.CoreWorker.run_async_func_in_event_loop\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     return self.__get_result()\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     raise self._exception\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/ray/python/ray/util/tracing/tracing_helper.py\", line 498, in _resume_span\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     return await method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/controller.py\", line 184, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     return await (self.long_poll_host.listen_for_change(keys_to_snapshot_ids))\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/_private/long_poll.py\", line 249, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     raise asyncio.TimeoutError(\"Polling request timed out.\")\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m asyncio.exceptions.TimeoutError: Polling request timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : I would completely believe you. Dogs and little children - very innocent and open to seeing such things\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9997748732566833\", \"Translated Text\": \"Je vous croyais tout \\u00e0 fait: chiens et petits enfants - tr\\u00e8s innocents et ouverts \\u00e0 ce genre de choses\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:49,853 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1344.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16168)\u001b[0m INFO 2022-08-09 09:38:49,852 ComposedModel ComposedModel#WaapSf replica.py:482 - HANDLE __call__ OK 1341.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16165)\u001b[0m INFO 2022-08-09 09:38:49,851 TranslateTweet TranslateTweet#jUzajq replica.py:482 - HANDLE translate OK 1329.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:49,952 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 89.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : You've got too much time on your paws. Go check on the skittle. under the, fridge\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9995866417884827\", \"Translated Text\": \"Vous avez trop de temps sur vos pattes.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:50,877 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1020.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16164)\u001b[0m INFO 2022-08-09 09:38:50,875 TranslateTweet TranslateTweet#gIwrMB replica.py:482 - HANDLE translate OK 1008.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16168)\u001b[0m INFO 2022-08-09 09:38:50,876 ComposedModel ComposedModel#WaapSf replica.py:482 - HANDLE __call__ OK 1018.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:50,963 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 76.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : You sneaky little devil, I can't live without you!!!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9949393272399902\", \"Translated Text\": \"Du petit diable, je ne peux pas vivre sans vous !!!\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:51,412 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 531.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16164)\u001b[0m INFO 2022-08-09 09:38:51,410 TranslateTweet TranslateTweet#gIwrMB replica.py:482 - HANDLE translate OK 520.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16167)\u001b[0m INFO 2022-08-09 09:38:51,411 ComposedModel ComposedModel#KRrrri replica.py:482 - HANDLE __call__ OK 528.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:51,529 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 106.5ms\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m 2022-08-09 09:38:51,759\tINFO (unknown file):0 -- Task failed with unretryable exception: TaskID(a13e1e5d6a54c086fc5f3af36eef1725e1393f6001000000).\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 709, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 713, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 655, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2243, in ray._raylet.CoreWorker.run_async_func_in_event_loop\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     return self.__get_result()\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/anaconda3/envs/ray-py38/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     raise self._exception\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/ray/python/ray/util/tracing/tracing_helper.py\", line 498, in _resume_span\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     return await method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/controller.py\", line 184, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     return await (self.long_poll_host.listen_for_change(keys_to_snapshot_ids))\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m   File \"/Users/archit/ray/python/ray/serve/_private/long_poll.py\", line 249, in listen_for_change\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m     raise asyncio.TimeoutError(\"Polling request timed out.\")\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m asyncio.exceptions.TimeoutError: Polling request timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : It's true what they say about dogs: they are you BEST BUDDY, no matter what!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9996572732925415\", \"Translated Text\": \"C'est vrai ce qu'ils disent sur les chiens : ils sont tu MEILLEUR BUDDY, peu importe quoi!\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:52,653 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1236.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16168)\u001b[0m INFO 2022-08-09 09:38:52,652 ComposedModel ComposedModel#WaapSf replica.py:482 - HANDLE __call__ OK 1234.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16165)\u001b[0m INFO 2022-08-09 09:38:52,651 TranslateTweet TranslateTweet#jUzajq replica.py:482 - HANDLE translate OK 1225.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:52,738 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 75.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : This dog is way dope, just can't enough of her\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9972212314605713\", \"Translated Text\": \"Ce chien est assez dope, il ne peut pas assez de lui\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:53,309 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 652.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16167)\u001b[0m INFO 2022-08-09 09:38:53,303 ComposedModel ComposedModel#KRrrri replica.py:482 - HANDLE __call__ OK 645.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16165)\u001b[0m INFO 2022-08-09 09:38:53,286 TranslateTweet TranslateTweet#jUzajq replica.py:482 - HANDLE translate OK 621.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:53,414 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 91.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : This dog is way cool, just can't enough of her\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9847628474235535\", \"Translated Text\": \"Ce chien est bien cool, il ne peut pas assez de lui\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:53,899 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 583.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16164)\u001b[0m INFO 2022-08-09 09:38:53,897 TranslateTweet TranslateTweet#gIwrMB replica.py:482 - HANDLE translate OK 569.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16168)\u001b[0m INFO 2022-08-09 09:38:53,898 ComposedModel ComposedModel#WaapSf replica.py:482 - HANDLE __call__ OK 580.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:53,970 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 61.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Is a dog really the best pet?\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.998790442943573\", \"Translated Text\": \"Est-ce qu'un chien est vraiment le meilleur animal de compagnie?\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:54,387 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 484.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16164)\u001b[0m INFO 2022-08-09 09:38:54,385 TranslateTweet TranslateTweet#gIwrMB replica.py:482 - HANDLE translate OK 473.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16167)\u001b[0m INFO 2022-08-09 09:38:54,386 ComposedModel ComposedModel#KRrrri replica.py:482 - HANDLE __call__ OK 482.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:54,473 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 75.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Cats are better than dogs\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9986716508865356\", \"Translated Text\": \"Les chats sont meilleurs que les chiens\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:54,804 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 412.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16168)\u001b[0m INFO 2022-08-09 09:38:54,803 ComposedModel ComposedModel#WaapSf replica.py:482 - HANDLE __call__ OK 410.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16165)\u001b[0m INFO 2022-08-09 09:38:54,802 TranslateTweet TranslateTweet#jUzajq replica.py:482 - HANDLE translate OK 401.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:54,886 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 72.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Totally dissastified with the dog. Worst dog ever\n",
      "tweet response:{\"Sentiment\": \"label:NEGATIVE; score=0.9998103976249695\", \"Translated Text\": \"Tr\\u00e8s d\\u00e9sassass\\u00e9 avec le chien, pire chien jamais\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:55,429 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 621.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16167)\u001b[0m INFO 2022-08-09 09:38:55,427 ComposedModel ComposedModel#KRrrri replica.py:482 - HANDLE __call__ OK 617.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16165)\u001b[0m INFO 2022-08-09 09:38:55,425 TranslateTweet TranslateTweet#jUzajq replica.py:482 - HANDLE translate OK 608.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:SentimentTweet pid=16166)\u001b[0m INFO 2022-08-09 09:38:55,534 SentimentTweet SentimentTweet#FBEAev replica.py:482 - HANDLE sentiment OK 94.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet request... : Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\n",
      "tweet response:{\"Sentiment\": \"label:POSITIVE; score=0.9929038882255554\", \"Translated Text\": \"Le chien briliant lise mes humeurs comme un livre, ressent mes humeurs et r\\u00e9agit.\"}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(TWEETS)):\n",
    "    tweet = fetch_tweet_text(i)\n",
    "    response = requests.post(\"http://127.0.0.1:8000/composed\", json=tweet)\n",
    "    print(f\"tweet request... : {tweet}\")\n",
    "    print(f\"tweet response:{response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d9b3f-e349-4af6-b9c1-e4ff0a2dab9e",
   "metadata": {},
   "source": [
    "Gracefully shutdown the Ray serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786fcf2e-1afa-4900-bc3f-0fc8844d5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m INFO 2022-08-09 09:38:56,506 controller 16150 deployment_state.py:1257 - Removing 2 replicas from deployment 'TranslateTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m INFO 2022-08-09 09:38:56,509 controller 16150 deployment_state.py:1257 - Removing 1 replicas from deployment 'SentimentTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=16150)\u001b[0m INFO 2022-08-09 09:38:56,511 controller 16150 deployment_state.py:1257 - Removing 2 replicas from deployment 'ComposedModel'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16160)\u001b[0m INFO 2022-08-09 09:38:56,451 http_proxy 127.0.0.1 http_proxy.py:315 - POST /composed 200 1017.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:TranslateTweet pid=16164)\u001b[0m INFO 2022-08-09 09:38:56,449 TranslateTweet TranslateTweet#gIwrMB replica.py:482 - HANDLE translate OK 1005.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=16168)\u001b[0m INFO 2022-08-09 09:38:56,450 ComposedModel ComposedModel#WaapSf replica.py:482 - HANDLE __call__ OK 1014.6ms\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d581-6a31-43f2-9083-94578042efdf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Add more tweets to `TWEETS` with different sentiments.\n",
    "2. Check the score (and if you speak and read French, what you think of the translation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e209-4437-426e-a425-39b443df68a1",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Instead of French, use a language transformer of your choice\n",
    "2. What about Neutral tweets? Try using [vaderSentiment](https://github.com/cjhutto/vaderSentiment)\n",
    "3. Solution for 2) is [here](https://github.com/anyscale/academy/blob/main/ray-serve/05-Ray-Serve-SentimentAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476075b-d538-4ba4-84fa-02688b013a4d",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "We'll further explore model composition using [Deploymant Graph APIs](https://docs.ray.io/en/latest/serve/deployment-graph.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d2797-a9ca-4f2f-bcd2-477667109fac",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_04_inference_graphs.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_ray_serve_fastapi.ipynb) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
