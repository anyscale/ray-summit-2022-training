{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31674f5f-d743-467e-beb3-d75d4d4419f0",
   "metadata": {},
   "source": [
    "# Ray Serve - Inference Graphs APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook]() <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_03_model_composition.ipynb) <br>\n",
    "\n",
    "\n",
    "### Learning Objective:\n",
    "In this introductory tutorial, you will:\n",
    "\n",
    "* learn about motivations for inference graph APIs\n",
    "* understand main concepts \n",
    "* utilize inference graph APIs to create a single deployment\n",
    "* and score an inference graph end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387aeb3e-f14f-48df-9160-7f7300204a3d",
   "metadata": {},
   "source": [
    "### Motivation, Concepts and Features\n",
    "\n",
    "Machine learning serving systems are getting more complex, consisting of ensemble of models as deployment patterns with only a single deployment making a final prediction or outcome. For example,\n",
    "an image/video content may consist of a model doing classification and then followed by tagging, or fraud detection pipeline may have multiple policies, multi-stage ranking and recommendation, etc.\n",
    "\n",
    "<img src=\"images/deployment_patterns.png\" width=\"50%\" height=\"25%\">\n",
    "\n",
    "It makes sense, then, to create a multi-stage model into an inference graph, where each node within the graph could be a single model deployment performing a particular a prediction given an input from\n",
    "upstream node and generating result for consumption downstream node.\n",
    "\n",
    "One of the attractive feature of inference graph is the ability to easily build, test, and deploy an inference graph, in its totality, on your local machine and then deploy it onto a staging or production server‚Äîall using Python APIs. Second, while in production, a deployment graph can be modified or scaled dynamically, without touching the underlying Python code.\n",
    "\n",
    "Each node within an inference graph serves a purpose, as a functional unit of inference, and you can programmatically stitch together myriad types of functional nodes and building blocks of your DAG. Let's examine at what types comprise those building blocks as DAG nodes and some terms:\n",
    "\n",
    "**Deployment** : This is an end-to-end single, scalable, and upgradeable group of actors managed by Ray Serve.\n",
    "\n",
    "**DeploymentNode**: Each node within this inference graph a unit of execution in the graph, created by calling `.bind()` on a Ray Serve decorated class or function. In short, any function or class decorated with a `@serve.deployment` is a candidate a node within an inference graph.\n",
    "\n",
    "**InputNode**: As the name suggests, it's a special node that represents the input passed to a graph at runtime or inference. There can only be a single `InputNode` per a deployment graph.\n",
    "\n",
    "**Deployment Graph**: This is a composite of deployment nodes bound together to define an entire inference graph, which can be deployed behind an HTTP endpoint.\n",
    "\n",
    "An example of an entire inference graph, with all its nodes:\n",
    "\n",
    "<img src=\"images/deployment_graph_example.png\" width=\"50%\" height=\"25%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d0f53-3ea0-409f-8a9c-e70fd99ac287",
   "metadata": {},
   "source": [
    "### Five Simple steps to build an inference graph.\n",
    "\n",
    "Let's build the above inference graph using [Serve Graph Inference APIs](https://docs.ray.io/en/latest/serve/package-ref.html). We will build the following nodes in our directed acyclic graph (DAG):\n",
    " * InputNode\n",
    " * Prepocessor\n",
    " * AvgProcessor\n",
    " * Combiner\n",
    " * Model\n",
    " * Aggregator\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a06b0c-e4ad-4318-8b62-9eb3fbd8bd4b",
   "metadata": {},
   "source": [
    "### Step 1: Build processor nodes. \n",
    "\n",
    "Note that any function or class decorated with `@serve.deployment` can be converted into a node in the DAG. So let's define those functions. We are\n",
    "using `aysnc` to allow many requests or invocations of each respective deployment.\n",
    "\n",
    "As we noted above, you can convert a function or class deployment into a node by simply using the `.bind(..)` suffix; this will result an instance of a `DeploymentNode`, which is\n",
    "of type `DAGNode`, our basic building block.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40667ce-e2da-4dff-b608-976eb4ce6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import requests\n",
    "import starlette\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "from ray.serve.dag import InputNode\n",
    "from ray.serve.drivers import DAGDriver\n",
    "from ray.serve.http_adapters import json_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2060b486-94a2-427e-b672-0e9b446fe2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "async def preprocessor(input_data: str):\n",
    "    \"\"\"Simple feature processing that converts str to int\"\"\"\n",
    "    await asyncio.sleep(0.1) # Manual delay for blocking computation\n",
    "    return int(input_data)\n",
    "\n",
    "@serve.deployment\n",
    "async def avg_preprocessor(input_data):\n",
    "    \"\"\"Simple feature processing that returns average of input list as float.\"\"\"\n",
    "    await asyncio.sleep(0.15) # Manual delay for blocking computation\n",
    "    return sum(input_data) / len(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5c22f-d333-4613-a72e-82652f7bde74",
   "metadata": {},
   "source": [
    "### Step 2: Model nodes\n",
    "\n",
    "After we got the preprocessed inputs, we‚Äôre ready to combine them to construct the request object we want to sent to a model instantiated with different initial weights. This means we need:\n",
    "\n",
    "1. An model instance in the graph instantiated with initial weights\n",
    "2. A Combiner that references Model nodes for its runtime implementation by passing them as init args in `.bind()`\n",
    "3. The ability of Combiner to receive and merge preprocessed inputs for the same user input, even they might be produced async and received out of order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da241fb2-24de-440d-9f4f-41a1e5b5458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Model:\n",
    "    def __init__(self, weight: int):\n",
    "        self.weight = weight\n",
    "\n",
    "    async def forward(self, input: int):\n",
    "        await asyncio.sleep(0.3) # Manual delay for blocking computation\n",
    "        return f\"({self.weight} * {input})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df8489-6f95-442b-82cb-1dab04098521",
   "metadata": {},
   "source": [
    "### Step 3: Build over Combiner aggregation based on user input and operation\n",
    "\n",
    "Now we have the backbone of our DAG setup: splitting and preprocessing user inputs, aggregate into new request data and send to multiple models downstream. Let‚Äôs add a bit more dynamic flavor in it to demostrate deployment graph is fully python programmable by introducing control flow based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf809e81-b3fd-486d-a56d-c52a7f34bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Combiner:\n",
    "    def __init__(self, m: Model):\n",
    "        self.m = m\n",
    "\n",
    "    async def run(self, req_part_1, req_part_2, operation):\n",
    "        # Merge model input from two preprocessors\n",
    "        req = f\"({req_part_1} + {req_part_2})\"\n",
    "\n",
    "        # Submit to model for inference\n",
    "        r1_ref = self.m.forward.remote(req)\n",
    "\n",
    "        # Async gathering of model forward results for same request data\n",
    "        rst = await asyncio.gather(r1_ref)\n",
    "\n",
    "        # Control flow that determines runtime behavior based on user input\n",
    "        if operation == \"sum\":\n",
    "            return f\"sum({rst})\"\n",
    "        else:\n",
    "            return f\"max({rst})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf682ea-2810-4adb-9f6b-bd42d0fecb50",
   "metadata": {},
   "source": [
    "### Step 4: Build our InputNode and driver deployment to handle HTTP ingress¬∂\n",
    "\n",
    "Now we‚Äôve built the entire serve DAG with the topology, args binding and user input. It‚Äôs time to add the last piece for Serve ‚Äì a Driver deployment to expose and configure HTTP. We can configure it to start with two replicas in case the ingress of deployment becomes bottleneck of the DAG.\n",
    "\n",
    "Serve provides a default `DAGDriver` implementation that accepts HTTP request and orchestrate the deployment graph execution. You can import it from from `ray.serve.drivers import DAGDriver.`\n",
    "\n",
    "You can configure how does the `DAGDriver` convert `HTTP` request types. By default, we directly send in a `starlette.requests.Request` object to represent the whole request. You can also specifies [built-in adapters](https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-http-adapters). In this example, we will use a `json_request` adapter that parses `HTTP` body with `JSON` parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9feb302-e777-4635-ac7b-ea62418f5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG building done within the context manager for the InputNode\n",
    "with InputNode() as dag_input:\n",
    "    # Partial access of user input by index\n",
    "    preprocessed_1 = preprocessor.bind(dag_input[0])\n",
    "    preprocessed_2 = avg_preprocessor.bind(dag_input[1])\n",
    "    \n",
    "    # Create a model Node \n",
    "    m1 = Model.bind(1)\n",
    "    \n",
    "    # Use other DeploymentNode in bind()\n",
    "    combiner = Combiner.bind(m1)\n",
    "    \n",
    "    # Use output of function DeploymentNode in bind()\n",
    "    dag = combiner.run.bind(\n",
    "        preprocessed_1, preprocessed_2, dag_input[2]\n",
    "    )\n",
    "    \n",
    "    # Each serve dag has a driver deployment as ingress that can be user provided.\n",
    "    serve_dag = DAGDriver.options(route_prefix=\"/my-dag\", num_replicas=2).bind(\n",
    "        dag, http_adapter=json_request\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01629d4a-0277-48cc-ac04-ca1803675ce9",
   "metadata": {},
   "source": [
    "### Step 5: Test the full DAG in both python and http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf5852d-a3c6-49a0-8903-a1e44e24fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 08:40:30,835\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:34,658 controller 99732 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-7611d638b249e3daec94c13412c09f68c30b0b0ac5ce9dc2c8bea79e' on node '7611d638b249e3daec94c13412c09f68c30b0b0ac5ce9dc2c8bea79e' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=99749)\u001b[0m INFO:     Started server process [99749]\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:36,099 controller 99732 deployment_state.py:1232 - Adding 1 replicas to deployment 'preprocessor'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:36,116 controller 99732 deployment_state.py:1232 - Adding 1 replicas to deployment 'avg_preprocessor'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:36,127 controller 99732 deployment_state.py:1232 - Adding 1 replicas to deployment 'Model'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:36,138 controller 99732 deployment_state.py:1232 - Adding 1 replicas to deployment 'Combiner'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:36,149 controller 99732 deployment_state.py:1232 - Adding 2 replicas to deployment 'DAGDriver'.\n"
     ]
    }
   ],
   "source": [
    "dag_handle = serve.run(serve_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7971b-8ba8-4b0e-8885-fc46ca4d0a13",
   "metadata": {},
   "source": [
    "### Use Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76028c4c-2b95-47b7-9bbf-89ed98523ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum([ObjectRef(8a908affb261979c08cc7f48cf6fc65e41cbd9df0100000002000000)])\n",
      "Time spent: 0.23 secs.\n"
     ]
    }
   ],
   "source": [
    "# Python handle\n",
    "cur = time.time()\n",
    "print(ray.get(dag_handle.predict.remote([\"5\", [1, 2], \"sum\"])))\n",
    "print(f\"Time spent: {round(time.time() - cur, 2)} secs.\")\n",
    "# Http endpoint\n",
    "cur = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3912944-4d7d-467e-bc43-809d7753fc07",
   "metadata": {},
   "source": [
    "### Use HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e0fc5b2-e139-477d-a9c6-77725bda9a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"sum([ObjectRef(92344ed58307ae8a08cc7f48cf6fc65e41cbd9df0100000002000000)])\"\n",
      "Time spent: 0.25 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=99749)\u001b[0m INFO 2022-08-09 08:40:38,677 http_proxy 127.0.0.1 http_proxy.py:315 - POST /my-dag 307 3.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Model pid=99759)\u001b[0m INFO 2022-08-09 08:40:38,671 Model Model#VGiIin replica.py:482 - HANDLE forward OK 300.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:DAGDriver pid=99762)\u001b[0m INFO 2022-08-09 08:40:38,676 DAGDriver DAGDriver#xkuLPD replica.py:482 - HANDLE __call__ OK 0.2ms\n"
     ]
    }
   ],
   "source": [
    "# Http endpoint\n",
    "cur = time.time()\n",
    "print(requests.post(\"http://127.0.0.1:8000/my-dag\", json=[\"5\", [1, 2], \"sum\"]).text)\n",
    "print(f\"Time spent: {round(time.time() - cur, 2)} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d57966c-5e21-4aca-87ac-cc2cd337dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:Model pid=99759)\u001b[0m INFO 2022-08-09 08:40:39,217 Model Model#VGiIin replica.py:482 - HANDLE forward OK 300.5ms\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:39,287 controller 99732 deployment_state.py:1257 - Removing 1 replicas from deployment 'preprocessor'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:39,290 controller 99732 deployment_state.py:1257 - Removing 1 replicas from deployment 'avg_preprocessor'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:39,293 controller 99732 deployment_state.py:1257 - Removing 1 replicas from deployment 'Model'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:39,295 controller 99732 deployment_state.py:1257 - Removing 1 replicas from deployment 'Combiner'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=99732)\u001b[0m INFO 2022-08-09 08:40:39,301 controller 99732 deployment_state.py:1257 - Removing 2 replicas from deployment 'DAGDriver'.\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ee558-3b8d-4e0b-ba15-2e861f4c1f51",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try at least any three exercises here:\n",
    "\n",
    "1. Try some input with `max` operation instead of `sum`\n",
    "2. Try with larger list in input numbers\n",
    "3. Create a notebook and walk through an [extended example](https://docs.ray.io/en/latest/serve/deployment-graph.html#full-end-to-end-example-code) of above example\n",
    "4. [Optional] Add a `mean` operator as preprocessor\n",
    "5. [Optional] Run [Simple Inference Graph](extras/simple_inference_graph.ipynb) in the extras directory\n",
    "6. [Optional] Run [Simple Model Composition Graph](extras/simple_model_composition_graph.ipynb) in the extras directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf16a9-e98d-47fc-baa6-ef148438da3e",
   "metadata": {},
   "source": [
    "### Homework Challenge\n",
    " 1. Read the Ray Serve Deployment [blog](https://www.anyscale.com/blog/multi-model-composition-with-ray-serve-deployment-graphs)\n",
    " 2. Convert our [Model Compostion example](ex_03_model_composition.ipynb) into Inference graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f0522-9df7-46bc-9b34-8ea248f06491",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [Multi-model composition with Ray Serve deployment graphs](https://www.anyscale.com/blog/multi-model-composition-with-ray-serve-deployment-graphs)\n",
    "2. [Ray Serve 101](https://www.anyscale.com/events/2022/05/05/ray-serve-101-deploying-your-first-ml-model-locally-and-as-a-managed-service): Deploying your first ML model locally and as a managed service\n",
    "3. [Productionizing ML at scale with Ray Serve](https://www.anyscale.com/events/2022/04/14/productionizing-ml-at-scale-with-ray-serve) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda4425-b53b-4c95-b8a3-8d9f51a0c096",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook]() <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_03_model_composition.ipynb) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
